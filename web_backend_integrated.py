from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import asyncio
import os
import hashlib
import tempfile
import re
from datetime import datetime
from pathlib import Path
import logging
import base64
import json

# å¯¼å…¥ç°æœ‰çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨
from mindmap_generator import MindMapGenerator, MinimalDatabaseStub, get_logger, generate_mermaid_html, DocumentOptimizer

# å¯¼å…¥æ–‡æ¡£è§£æå™¨
from document_parser import DocumentParser

# å¯¼å…¥MinerUç›¸å…³æ¨¡å—
from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.config.enums import SupportedPdfParseMethod

# ======== Phase 1: å®Œæ•´çš„å†…å­˜æ ‘æ•°æ®ç»“æ„ ========

class NodeTreeNode:
    """å®Œæ•´çš„æ ‘èŠ‚ç‚¹æ•°æ®ç»“æ„"""
    def __init__(self, node_id: str):
        self.id = node_id
        self.children = []  # å­èŠ‚ç‚¹åˆ—è¡¨ï¼ŒæŒ‰æ•°å­—é¡ºåºæ’åº
        self.parent = None  # çˆ¶èŠ‚ç‚¹å¼•ç”¨
    
    def add_child(self, child_node):
        """æ·»åŠ å­èŠ‚ç‚¹å¹¶ç»´æŠ¤æ’åº"""
        child_node.parent = self
        self.children.append(child_node)
        # æŒ‰æ•°å­—é¡ºåºæ’åºå­èŠ‚ç‚¹
        self.children.sort(key=lambda x: self._get_sort_key(x.id))
    
    def _get_sort_key(self, node_id: str):
        """è·å–èŠ‚ç‚¹æ’åºé”®"""
        parts = node_id.split('.')
        try:
            return int(parts[-1])
        except ValueError:
            return 999
    
    def get_all_descendants(self):
        """è·å–æ‰€æœ‰å­å­™èŠ‚ç‚¹"""
        descendants = []
        for child in self.children:
            descendants.append(child)
            descendants.extend(child.get_all_descendants())
        return descendants
    
    def get_sibling_index(self):
        """è·å–åœ¨çˆ¶èŠ‚ç‚¹ä¸­çš„ç´¢å¼•"""
        if self.parent is None:
            return 0
        return self.parent.children.index(self)
    
    def get_siblings(self):
        """è·å–æ‰€æœ‰å…„å¼ŸèŠ‚ç‚¹ï¼ˆä¸åŒ…æ‹¬è‡ªå·±ï¼‰"""
        if self.parent is None:
            return []
        return [child for child in self.parent.children if child != self]

def build_tree_structure(node_mappings: Dict, mermaid_string: str) -> Dict[str, NodeTreeNode]:
    """æ„å»ºå®Œæ•´çš„å†…å­˜æ ‘ç»“æ„"""
    print(f"ğŸŒ³ [Phase 1] æ„å»ºæ ‘ç»“æ„ï¼ŒèŠ‚ç‚¹æ•°é‡: {len(node_mappings)}")
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹
    tree_nodes = {}
    for node_id in node_mappings.keys():
        tree_nodes[node_id] = NodeTreeNode(node_id)
        print(f"ğŸŒ³ [Phase 1] åˆ›å»ºèŠ‚ç‚¹: {node_id}")
    
    # ç¬¬äºŒæ­¥ï¼šå»ºç«‹çˆ¶å­å…³ç³»ï¼ˆåŸºäºç¼©è¿›å¼æ•°å­—IDï¼‰
    for node_id, node in tree_nodes.items():
        parent_prefix, sequence = split_id_helper(node_id)
        
        if parent_prefix is not None and parent_prefix in tree_nodes:
            parent_node = tree_nodes[parent_prefix]
            parent_node.add_child(node)
            print(f"ğŸŒ³ [Phase 1] å»ºç«‹å…³ç³»: {parent_prefix} -> {node_id}")
    
    # ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å’Œè°ƒè¯•ä¿¡æ¯
    root_nodes = [node for node in tree_nodes.values() if node.parent is None]
    print(f"ğŸŒ³ [Phase 1] æ ¹èŠ‚ç‚¹æ•°é‡: {len(root_nodes)}")
    
    for root in root_nodes:
        print(f"ğŸŒ³ [Phase 1] æ ¹èŠ‚ç‚¹: {root.id}, å­èŠ‚ç‚¹: {[child.id for child in root.children]}")
    
    return tree_nodes

def split_id_helper(node_id: str) -> tuple:
    """å°†èŠ‚ç‚¹IDåˆ†è§£ä¸ºçˆ¶IDå‰ç¼€å’Œè‡ªå·±çš„åºå·"""
    if '.' not in node_id:
        try:
            return (None, int(node_id))
        except ValueError:
            return (None, None)
    
    parts = node_id.split('.')
    try:
        sequence = int(parts[-1])
        parent_prefix = '.'.join(parts[:-1])
        return (parent_prefix, sequence)
    except ValueError:
        return (None, None)

def rename_subtree(node, new_id_prefix: str) -> Dict[str, str]:
    """è¿é”é‡å‘½åæ ¸å¿ƒå‡½æ•°"""
    rename_map = {}
    
    def recursive_rename(current_node, new_id: str):
        old_id = current_node.id
        current_node.id = new_id
        rename_map[old_id] = new_id
        
        for i, child in enumerate(current_node.children):
            child_new_id = f"{new_id}.{i + 1}"
            recursive_rename(child, child_new_id)
    
    recursive_rename(node, new_id_prefix)
    return rename_map

async def insert_divider_phase3(content: str, source_node_id: str, direction: str, new_node_id: str) -> Optional[str]:
    """Phase 3ä¸“ç”¨çš„åˆ†å‰²æ æ’å…¥å‡½æ•°"""
    try:
        print(f"ğŸ” [Phase 3æ’å…¥] æ’å…¥åˆ†å‰²æ : {new_node_id}, æ–¹å‘: {direction}")
        
        new_divider = f"--- {new_node_id} ---"
        
        if direction == 'child':
            # åœ¨æºèŠ‚ç‚¹å†…å®¹èŒƒå›´æœ«å°¾æ’å…¥
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                print(f"âŒ [Phase 3æ’å…¥] æœªæ‰¾åˆ°æºèŠ‚ç‚¹åˆ†å‰²æ : {source_node_id}")
                return None
            
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåˆ†å‰²æ æˆ–æ–‡æ¡£æœ«å°¾
            next_divider_pattern = r"\n--- [^-]+ ---"
            search_start = source_match.end()
            next_match = re.search(next_divider_pattern, content[search_start:])
            
            if next_match:
                insert_pos = search_start + next_match.start()
            else:
                insert_pos = len(content)
            
            return content[:insert_pos] + f"\n\n{new_divider}\n\n" + content[insert_pos:]
            
        elif direction == 'right-sibling':
            # åœ¨æºèŠ‚ç‚¹å­æ ‘æœ«å°¾æ’å…¥
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                return None
            
            # ç®€åŒ–å¤„ç†ï¼šåœ¨ä¸‹ä¸€ä¸ªåˆ†å‰²æ å‰æ’å…¥ï¼Œæˆ–æ–‡æ¡£æœ«å°¾
            next_divider_pattern = r"\n--- [^-]+ ---"
            search_start = source_match.end()
            next_match = re.search(next_divider_pattern, content[search_start:])
            
            if next_match:
                insert_pos = search_start + next_match.start()
            else:
                insert_pos = len(content)
            
            return content[:insert_pos] + f"\n\n{new_divider}\n\n" + content[insert_pos:]
            
        elif direction == 'left-sibling':
            # åœ¨æºèŠ‚ç‚¹åˆ†å‰²æ å‰æ’å…¥
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                return None
            
            insert_pos = source_match.start()
            return content[:insert_pos] + f"{new_divider}\n\n" + content[insert_pos:]
        
        return None
        
    except Exception as e:
        print(f"âŒ [Phase 3æ’å…¥é”™è¯¯] {str(e)}")
        return None

def update_mermaid_phase3(mermaid_string: str, new_node_id: str, new_node_label: str, direction: str, source_node_id: str) -> str:
    """Phase 3ä¸“ç”¨çš„mermaidæ›´æ–°å‡½æ•°"""
    try:
        updated_mermaid = mermaid_string or "graph TD"
        
        if not updated_mermaid.endswith('\n'):
            updated_mermaid += '\n'
        
        # æ·»åŠ æ–°èŠ‚ç‚¹å®šä¹‰
        new_node_def = f"    {new_node_id}[{new_node_label}]"
        updated_mermaid += new_node_def + '\n'
        
        # æ ¹æ®æ–¹å‘æ·»åŠ è¿æ¥
        if direction == 'child':
            connection = f"    {source_node_id} --> {new_node_id}"
        else:
            # åŒçº§èŠ‚ç‚¹ï¼šæ‰¾åˆ°æºèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹
            parent_pattern = rf'([A-Za-z0-9_.]+)\s*-->\s*{re.escape(source_node_id)}'
            parent_match = re.search(parent_pattern, updated_mermaid)
            
            if parent_match:
                parent_id = parent_match.group(1)
                connection = f"    {parent_id} --> {new_node_id}"
            else:
                # å¦‚æœæ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ï¼Œå‡è®¾æ˜¯æ ¹èŠ‚ç‚¹
                connection = f"    ROOT --> {new_node_id}"
        
        updated_mermaid += connection + '\n'
        
        print(f"ğŸ”„ [Phase 3 Mermaid] æ·»åŠ : {new_node_def}")
        print(f"ğŸ”„ [Phase 3 Mermaid] è¿æ¥: {connection}")
        
        return updated_mermaid
        
    except Exception as e:
        print(f"âŒ [Phase 3 Mermaidé”™è¯¯] {str(e)}")
        return mermaid_string or "graph TD"

# ======== End of Phase 1 & Phase 3 æ”¯æŒå‡½æ•° ========

app = FastAPI(title="Argument Structure Analyzer API (Integrated)", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Reactå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®æ—¥å¿—
logger = get_logger()

# åˆ›å»ºä¸Šä¼ ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# åˆ›å»ºPDFå¤„ç†ç›®å½•
PDF_OUTPUT_DIR = Path("pdf_outputs")
PDF_OUTPUT_DIR.mkdir(exist_ok=True)

# å­˜å‚¨æ–‡æ¡£çŠ¶æ€çš„å†…å­˜æ•°æ®åº“
document_status = {}

# å­˜å‚¨æ–‡æ¡£ç»“æ„çš„å†…å­˜æ•°æ®åº“
document_structures = {}

# Pydantic æ¨¡å‹å®šä¹‰
class AddNodeRequest(BaseModel):
    """æ·»åŠ èŠ‚ç‚¹çš„è¯·æ±‚æ¨¡å‹"""
    sourceNodeId: str
    direction: str  # 'child', 'left-sibling', 'right-sibling'
    parentId: Optional[str] = None
    label: Optional[str] = "æ–°èŠ‚ç‚¹"

class ArgumentStructureAnalyzer:
    """è®ºè¯ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        self.generator = MindMapGenerator()
        self.document_parser = DocumentParser()
        # æ·»åŠ DocumentOptimizerå®ä¾‹ç”¨äºAIè°ƒç”¨
        self.optimizer = DocumentOptimizer()
    
    def add_paragraph_ids(self, text: str) -> str:
        """ä¸ºæ–‡æœ¬çš„æ¯ä¸ªæ®µè½æ·»åŠ IDå·"""
        try:
            # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
            paragraphs = text.split('\n\n')
            processed_paragraphs = []
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():  # åªå¤„ç†éç©ºæ®µè½
                    # ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ IDæ ‡è®°
                    para_id = f"para-{i+1}"
                    processed_paragraph = f"[{para_id}] {paragraph.strip()}"
                    processed_paragraphs.append(processed_paragraph)
                else:
                    processed_paragraphs.append(paragraph)
            
            return '\n\n'.join(processed_paragraphs)
            
        except Exception as e:
            print(f"âŒ [æ®µè½IDæ·»åŠ é”™è¯¯] {str(e)}")
            return text
    
    async def generate_argument_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„"""
        try:
            # æ„å»ºåŸºäºæ®µè½çš„è®ºè¯ç»“æ„åˆ†æprompt
            prompt = f"""æˆ‘å¸Œæœ›ä½ æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åˆ†æå¸ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»æˆ‘æä¾›çš„ã€å·²ç»æŒ‰æ®µè½æ ‡è®°å¥½IDçš„æ–‡æœ¬ï¼Œå¹¶åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†æ¥åˆ†æå…¶è®ºè¯ç»“æ„ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š

ç¬¬ä¸€æ­¥ï¼šæ®µè½è§’è‰²è¯†åˆ«
- åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†ï¼ˆ[para-X]æ ‡è®°ï¼‰ï¼Œåˆ†ææ¯ä¸ªæ®µè½åœ¨è®ºè¯ä¸­çš„è§’è‰²
- ä¸è¦é‡æ–°åˆ’åˆ†æ®µè½ï¼Œè€Œæ˜¯åŸºäºç°æœ‰æ®µè½æ¥ç†è§£è®ºè¯é€»è¾‘
- è¯†åˆ«æ¯ä¸ªæ®µè½æ˜¯å¼•è¨€ã€è®ºç‚¹ã€è¯æ®ã€åé©³ã€ç»“è®ºç­‰å“ªç§ç±»å‹

ç¬¬äºŒæ­¥ï¼šæ„å»ºè®ºè¯ç»“æ„æµç¨‹å›¾
- åŸºäºæ®µè½çš„è®ºè¯è§’è‰²ï¼Œæ„å»ºé€»è¾‘æµç¨‹å›¾
- å°†å…·æœ‰ç›¸åŒæˆ–ç›¸å…³è®ºè¯åŠŸèƒ½çš„æ®µè½ç»„åˆæˆé€»è¾‘èŠ‚ç‚¹
- ç”¨ç®­å¤´è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»

ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€çš„ã€å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œä¸è¦åœ¨ JSON ä»£ç å—å‰åæ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡å­—ã€‚

è¿™ä¸ª JSON å¯¹è±¡å¿…é¡»åŒ…å«ä¸‰ä¸ªé¡¶çº§é”®ï¼š"mermaid_string"ã€"node_mappings" å’Œ "edges"ã€‚

mermaid_string:
- å€¼ä¸ºç¬¦åˆ Mermaid.js è¯­æ³•çš„æµç¨‹å›¾ï¼ˆgraph TDï¼‰
- å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ç»„ç›¸å…³çš„æ®µè½ï¼ˆåŸºäºè®ºè¯åŠŸèƒ½ï¼‰
- ğŸ†• èŠ‚ç‚¹ ID å¿…é¡»ä½¿ç”¨ç¼©è¿›å¼æ•°å­—å‘½åæ–¹æ¡ˆï¼šæ ¹èŠ‚ç‚¹ä½¿ç”¨ 1, 2, 3ï¼Œå­èŠ‚ç‚¹ä½¿ç”¨ 1.1, 1.2, 1.3ï¼Œå­™èŠ‚ç‚¹ä½¿ç”¨ 1.1.1, 1.1.2 ç­‰
- ğŸ†• ä¸è¦ä½¿ç”¨å­—æ¯IDï¼ˆå¦‚A, B, Cï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ•°å­—IDï¼ˆå¦‚1, 2, 1.1, 1.2ï¼‰
- èŠ‚ç‚¹æ ‡ç­¾åº”è¯¥ç®€æ´æ¦‚æ‹¬è¯¥ç»„æ®µè½çš„æ ¸å¿ƒè®ºè¯åŠŸèƒ½ï¼ˆä¸è¶…è¿‡20å­—ï¼‰
- ä½¿ç”¨ç®­å¤´ --> è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»

node_mappings:
- å€¼ä¸º JSON å¯¹è±¡ï¼Œé”®ä¸º Mermaid å›¾ä¸­çš„èŠ‚ç‚¹ IDï¼ˆå¿…é¡»æ˜¯æ•°å­—æ ¼å¼ï¼Œå¦‚ "1", "2", "1.1", "1.2"ï¼‰
- æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å€¼åŒ…å«ï¼š
  - "text_snippet": è¯¥èŠ‚ç‚¹åŒ…å«æ®µè½çš„æ ¸å¿ƒå†…å®¹æ€»ç»“ï¼ˆ30-80å­—ï¼‰
  - "paragraph_ids": æ„æˆè¯¥èŠ‚ç‚¹çš„æ®µè½IDæ•°ç»„ï¼ˆå¦‚ ["para-2", "para-3"]ï¼‰
  - "semantic_role": è¯¥èŠ‚ç‚¹åœ¨è®ºè¯ä¸­çš„è§’è‰²ï¼ˆå¦‚ "å¼•è¨€"ã€"æ ¸å¿ƒè®ºç‚¹"ã€"æ”¯æ’‘è¯æ®"ã€"åé©³"ã€"ç»“è®º" ç­‰ï¼‰

edges:
- å€¼ä¸ºå¯¹è±¡æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡ä»£è¡¨ä¸€æ¡è¾¹
- æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä¸¤ä¸ªé”®ï¼š
  - "source": è¾¹çš„èµ·å§‹èŠ‚ç‚¹IDï¼ˆæ•°å­—æ ¼å¼ï¼‰
  - "target": è¾¹çš„ç›®æ ‡èŠ‚ç‚¹IDï¼ˆæ•°å­—æ ¼å¼ï¼‰
- è¿™äº›è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»ä¸€è‡´

å…³é”®è¦æ±‚ï¼š
1. ğŸ†• æ‰€æœ‰èŠ‚ç‚¹ ID å¿…é¡»ä½¿ç”¨ç¼©è¿›å¼æ•°å­—å‘½åï¼š1, 2, 3, 1.1, 1.2, 1.1.1 ç­‰ï¼Œç»å¯¹ä¸èƒ½ä½¿ç”¨å­—æ¯
2. æ‰€æœ‰èŠ‚ç‚¹ ID å¿…é¡»åœ¨ mermaid_string ä¸­å­˜åœ¨
3. paragraph_ids å¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŸæ–‡çš„æ®µè½æ ‡è®° [para-X]ï¼Œä¸å¯ä¿®æ”¹
4. åŸæ–‡çš„æ¯ä¸ªæ®µè½éƒ½åº”è¯¥è¢«åˆ†é…ç»™è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹
5. èŠ‚ç‚¹çš„åˆ’åˆ†åº”è¯¥åŸºäºæ®µè½çš„è®ºè¯åŠŸèƒ½ï¼Œç›¸å…³åŠŸèƒ½çš„æ®µè½å¯ä»¥ç»„åˆåœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸­
6. æµç¨‹å›¾åº”è¯¥æ¸…æ™°å±•ç°è®ºè¯çš„é€»è¾‘æ¨ç†è·¯å¾„
7. ä¿æŒæ®µè½çš„å®Œæ•´æ€§ï¼Œä¸è¦æ‹†åˆ†æˆ–é‡ç»„æ®µè½å†…å®¹
8. edges æ•°ç»„ä¸­çš„æ¯æ¡è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»å®Œå…¨ä¸€è‡´

ç°åœ¨ï¼Œè¯·åˆ†æä»¥ä¸‹å¸¦æœ‰æ®µè½IDçš„æ–‡æœ¬ï¼š

{text_with_ids}"""
            
            # ä½¿ç”¨DocumentOptimizerçš„generate_completionæ–¹æ³•
            response = await self.optimizer.generate_completion(
                prompt, 
                max_tokens=2000,
                task="åˆ†æè®ºè¯ç»“æ„"
            )
            
            if not response:
                print(f"âŒ [APIè°ƒç”¨å¤±è´¥] æœªæ”¶åˆ°AIå“åº”")
                return {"success": False, "error": "APIè°ƒç”¨å¤±è´¥ï¼Œæœªæ”¶åˆ°AIå“åº”"}
            
            # è§£æJSONå“åº”
            try:
                # è¯¦ç»†è®°å½•åŸå§‹å“åº”
                print(f"ğŸ” [åŸå§‹AIå“åº”] é•¿åº¦: {len(response)} å­—ç¬¦")
                print(f"ğŸ” [åŸå§‹å“åº”å‰200å­—ç¬¦]: {response[:200]}")
                
                # æ›´å½»åº•çš„å“åº”æ¸…ç†
                clean_response = response.strip()
                
                # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                elif clean_response.startswith('```'):
                    clean_response = clean_response[3:]
                    
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                
                clean_response = clean_response.strip()
                
                # ç§»é™¤å¯èƒ½çš„è¯´æ˜æ–‡å­—ï¼Œåªä¿ç•™JSONéƒ¨åˆ†
                json_start = clean_response.find('{')
                json_end = clean_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    clean_response = clean_response[json_start:json_end+1]
                    print(f"ğŸ”§ [æå–JSON] æå–åˆ°JSONéƒ¨åˆ†ï¼Œé•¿åº¦: {len(clean_response)}")
                
                structure_data = json.loads(clean_response)
                
                # éªŒè¯å¿…è¦çš„é”®
                if 'mermaid_string' not in structure_data or 'node_mappings' not in structure_data:
                    print(f"âŒ [æ•°æ®ç»“æ„é”™è¯¯] å“åº”é”®: {list(structure_data.keys())}")
                    return {"success": False, "error": "AIå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼šç¼ºå°‘å¿…è¦çš„é”®"}
                
                # éªŒè¯èŠ‚ç‚¹æ˜ å°„çš„ç»“æ„
                node_mappings = structure_data['node_mappings']
                valid_mappings = {}
                
                for node_id, mapping in node_mappings.items():
                    if isinstance(mapping, dict):
                        valid_mapping = {
                            "text_snippet": mapping.get("text_snippet", "è¯­ä¹‰å—å†…å®¹"),
                            "paragraph_ids": mapping.get("paragraph_ids", []),
                            "semantic_role": mapping.get("semantic_role", "è®ºè¯è¦ç´ ")
                        }
                        valid_mappings[node_id] = valid_mapping
                
                structure_data['node_mappings'] = valid_mappings
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«edgeså­—æ®µ
                if 'edges' not in structure_data:
                    edges = []
                    mermaid_string = structure_data['mermaid_string']
                    edge_pattern = r'([A-Za-z0-9_]+)\s*-->\s*([A-Za-z0-9_]+)'
                    for match in re.finditer(edge_pattern, mermaid_string):
                        source, target = match.groups()
                        edges.append({"source": source, "target": target})
                    structure_data['edges'] = edges
                
                print(f"âœ… [è®ºè¯ç»“æ„åˆ†æ] æˆåŠŸç”ŸæˆåŒ…å« {len(structure_data['node_mappings'])} ä¸ªèŠ‚ç‚¹çš„æµç¨‹å›¾")
                
                # è¿”å›æˆåŠŸç»“æœ
                return {
                    "success": True,
                    "mermaid_code": structure_data['mermaid_string'],
                    "node_mappings": structure_data['node_mappings'],
                    "edges": structure_data['edges']
                }
                
            except json.JSONDecodeError as parse_error:
                print(f"âŒ [JSONè§£æé”™è¯¯] {str(parse_error)}")
                return {"success": False, "error": f"JSONè§£æå¤±è´¥: {str(parse_error)}"}
                
        except Exception as e:
            print(f"âŒ [è®ºè¯ç»“æ„åˆ†æé”™è¯¯] {str(e)}")
            return {"success": False, "error": f"AIåˆ†æå¤±è´¥: {str(e)}"}

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
argument_analyzer = ArgumentStructureAnalyzer()

def rebuild_content_with_physical_dividers(text_with_ids: str, node_mappings: Dict) -> str:
    """æ ¹æ®AIè¿”å›çš„node_mappingsé‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹"""
    try:
        print(f"ğŸ”§ [å†…å®¹é‡å»º] å¼€å§‹é‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹")
        
        # è§£æåŸå§‹å†…å®¹ï¼Œæå–æ®µè½IDå’Œå¯¹åº”çš„å†…å®¹
        paragraph_content_map = {}
        
        parts = re.split(r'(\[para-\d+\])', text_with_ids)
        current_paragraph_id = None
        current_content = ''
        
        for part in parts:
            para_match = re.match(r'\[para-(\d+)\]', part.strip())
            if para_match:
                if current_paragraph_id and current_content.strip():
                    paragraph_content_map[current_paragraph_id] = current_content.strip()
                
                current_paragraph_id = f"para-{para_match.group(1)}"
                current_content = ''
            else:
                if part.strip():
                    current_content += part
        
        if current_paragraph_id and current_content.strip():
            paragraph_content_map[current_paragraph_id] = current_content.strip()
        
        # æŒ‰ç…§node_mappingsé‡æ–°ç»„ç»‡å†…å®¹
        rebuilt_content_parts = []
        
        for node_id, node_data in node_mappings.items():
            rebuilt_content_parts.append(f"--- {node_id} ---\n")
            
            paragraph_ids = node_data.get('paragraph_ids', [])
            node_content_parts = []
            for para_id in paragraph_ids:
                if para_id in paragraph_content_map:
                    para_content = paragraph_content_map[para_id]
                    node_content_parts.append(f"[{para_id}] {para_content}")
            
            if node_content_parts:
                rebuilt_content_parts.append('\n\n'.join(node_content_parts))
                rebuilt_content_parts.append('\n\n')
        
        rebuilt_content = ''.join(rebuilt_content_parts).strip()
        
        print(f"âœ… [å†…å®¹é‡å»º] é‡å»ºå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(rebuilt_content)} å­—ç¬¦")
        
        return rebuilt_content
        
    except Exception as e:
        print(f"âŒ [å†…å®¹é‡å»ºé”™è¯¯] {str(e)}")
        return text_with_ids

@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£ï¼Œæ”¯æŒPDFã€MDå’ŒTXTæ–‡ä»¶"""
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = ['.md', '.txt']
    file_extension = Path(file.filename).suffix.lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .md å’Œ .txt æ–‡ä»¶")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£ID
        content_hash = hashlib.md5(content).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = Path(file.filename).stem
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        
        print(f"\nğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] {file.filename}")
        print(f"ğŸ†” [æ–‡æ¡£ID] {document_id}")
        print(f"ğŸ“Š [æ–‡ä»¶å¤§å°] {len(content)} å­—èŠ‚")
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶
        original_file_path = UPLOAD_DIR / f"{document_id}{file_extension}"
        with open(original_file_path, 'wb') as f:
            f.write(content)
        
        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        text_content = content.decode('utf-8')
        
        # å­˜å‚¨åˆ°å†…å­˜æ•°æ®åº“
        MinimalDatabaseStub.store_text(text_content)
        
        # ç«‹å³ä¸ºæ–‡æ¡£å†…å®¹æ·»åŠ æ®µè½ID
        print("ğŸ“ [å¤„ç†æ®µè½] ä¸ºä¸Šä¼ çš„æ–‡æ¡£æ·»åŠ æ®µè½IDæ ‡è®°...")
        content_with_ids = argument_analyzer.add_paragraph_ids(text_content)
        print(f"ğŸ“ [æ®µè½å¤„ç†å®Œæˆ] å·²ä¸ºæ–‡æ¡£æ·»åŠ æ®µè½IDï¼Œå†…å®¹é•¿åº¦: {len(content_with_ids)} å­—ç¬¦")
        
        # åˆå§‹åŒ–æ–‡æ¡£çŠ¶æ€
        document_status[document_id] = {
            "status": "uploaded",
            "content": text_content,
            "filename": file.filename,
            "file_type": file_extension,
            "original_file_path": str(original_file_path),
            "status_demo": "not_started",
            "mermaid_code_demo": None,
            "node_mappings_demo": {},
            "error_demo": None,
            "content_with_ids": content_with_ids
        }
        
        print(f"âœ… [ä¸Šä¼ æˆåŠŸ] æ–‡æ¡£å·²ä¿å­˜å¹¶å‡†å¤‡ç”Ÿæˆæ€ç»´å¯¼å›¾")
        
        return JSONResponse({
            "success": True,
            "document_id": document_id,
            "filename": file.filename,
            "content": text_content,
            "file_type": file_extension,
            "status": "uploaded",
            "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ"
        })
        
    except UnicodeDecodeError:
        print(f"âŒ [ç¼–ç é”™è¯¯] æ–‡ä»¶: {file.filename}")
        raise HTTPException(status_code=400, detail="æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯UTF-8ç¼–ç ")
    except Exception as e:
        print(f"âŒ [ä¸Šä¼ å¤±è´¥] æ–‡ä»¶: {file.filename}, é”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

@app.post("/api/generate-argument-structure/{document_id}")
async def generate_argument_structure(document_id: str):
    """ä¸ºæŒ‡å®šæ–‡æ¡£ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    # æ£€æŸ¥çŠ¶æ€
    if doc_info.get("status_demo") == "generating":
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­..."
        })
    
    if doc_info.get("status_demo") == "completed" and doc_info.get("mermaid_code_demo"):
        return JSONResponse({
            "success": True,
            "status": "completed",
            "mermaid_code": doc_info["mermaid_code_demo"],
            "node_mappings": doc_info.get("node_mappings_demo", {}),
            "message": "è®ºè¯ç»“æ„å·²ç”Ÿæˆ"
        })
    
    try:
        print(f"ğŸ”„ [å¼€å§‹åˆ†æ] ä¸ºæ–‡æ¡£ {document_id} å¯åŠ¨è®ºè¯ç»“æ„åˆ†æä»»åŠ¡")
        
        # æ›´æ–°çŠ¶æ€ä¸ºåˆ†æä¸­
        doc_info["status_demo"] = "generating"
        
        # å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„
        asyncio.create_task(generate_argument_structure_async(document_id, doc_info["content"]))
        
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "å¼€å§‹åˆ†æè®ºè¯ç»“æ„..."
        })
        
    except Exception as e:
        print(f"âŒ [å¯åŠ¨å¤±è´¥] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}")
        doc_info["status_demo"] = "error"
        doc_info["error_demo"] = str(e)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")

async def generate_argument_structure_async(document_id: str, content: str):
    """å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„"""
    try:
        print(f"ğŸ”„ [å¼‚æ­¥ä»»åŠ¡] å¼€å§‹ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆè®ºè¯ç»“æ„")
        
        # ä¸ºæ–‡æœ¬æ·»åŠ æ®µè½ID
        text_with_ids = argument_analyzer.add_paragraph_ids(content)
        
        # ç”Ÿæˆè®ºè¯ç»“æ„
        result = await argument_analyzer.generate_argument_structure(text_with_ids)
        
        if result["success"]:
            # ä½¿ç”¨é‡å»ºçš„content_with_ids
            rebuilt_content = rebuild_content_with_physical_dividers(text_with_ids, result["node_mappings"])
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document_status[document_id]["status_demo"] = "completed"
            document_status[document_id]["mermaid_code_demo"] = result["mermaid_code"]
            document_status[document_id]["node_mappings_demo"] = result["node_mappings"]
            document_status[document_id]["edges_demo"] = result["edges"]
            document_status[document_id]["content_with_ids"] = rebuilt_content
            
            print(f"âœ… [åˆ†æå®Œæˆ] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†ææˆåŠŸ")
        else:
            document_status[document_id]["status_demo"] = "error"
            document_status[document_id]["error_demo"] = result["error"]
            print(f"âŒ [åˆ†æå¤±è´¥] æ–‡æ¡£ {document_id}: {result['error']}")
            
    except Exception as e:
        print(f"âŒ [å¼‚æ­¥åˆ†æé”™è¯¯] æ–‡æ¡£ {document_id}: {str(e)}")
        document_status[document_id]["status_demo"] = "error"
        document_status[document_id]["error_demo"] = str(e)

@app.get("/api/document-status/{document_id}")
async def get_document_status(document_id: str):
    """è·å–æ–‡æ¡£çŠ¶æ€å’Œè®ºè¯ç»“æ„åˆ†æè¿›åº¦"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    response_data = {
        "success": True,
        "document_id": document_id,
        "filename": doc_info.get("filename"),
        "content": doc_info.get("content"),
        "file_type": doc_info.get("file_type", ".md"),
        
        # è®ºè¯ç»“æ„åˆ†æçŠ¶æ€
        "status_demo": doc_info.get("status_demo", "not_started"),
        "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
        "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
        "edges_demo": doc_info.get("edges_demo", []),
        "error_demo": doc_info.get("error_demo"),
        "content_with_ids": doc_info.get("content_with_ids"),
    }
    
    return JSONResponse(response_data)

@app.get("/api/document/{document_id}")
async def get_document(document_id: str):
    """è·å–æ–‡æ¡£å†…å®¹å’Œè®ºè¯ç»“æ„"""
    
    try:
        if document_id in document_status:
            doc_info = document_status[document_id]
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": doc_info["content"],
                "filename": doc_info.get("filename", ""),
                "file_type": doc_info.get("file_type", ".md"),
                "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
                "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
                "edges_demo": doc_info.get("edges_demo", []),
                "status_demo": doc_info.get("status_demo", "not_started"),
                "error_demo": doc_info.get("error_demo"),
                "content_with_ids": doc_info.get("content_with_ids")
            })
        else:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.post("/api/document/{document_id}/node/add")
async def add_node(document_id: str, request_data: AddNodeRequest):
    """æ·»åŠ æ–°èŠ‚ç‚¹åˆ°æ–‡æ¡£ç»“æ„ - ä½¿ç”¨å®Œæ•´çš„Phase 2+3é€»è¾‘"""
    try:
        print(f"ğŸš€ [Phase 2] æ”¶åˆ°æ·»åŠ èŠ‚ç‚¹è¯·æ±‚ - æ–‡æ¡£ID: {document_id}")
        print(f"ğŸš€ [Phase 2] è¯·æ±‚å‚æ•°: sourceNodeId={request_data.sourceNodeId}, direction={request_data.direction}, parentId={request_data.parentId}")
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        if document_id not in document_status:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"}
            )
        
        document_data = document_status[document_id]
        
        # è·å–å¿…è¦çš„æ–‡æ¡£æ•°æ®
        content_with_ids = document_data.get('content_with_ids', '')
        node_mappings = document_data.get('node_mappings_demo', {})
        mermaid_string = document_data.get('mermaid_code_demo', '')
        
        if not content_with_ids:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æœªåŒ…å«æ®µè½ID"}
            )
        
        # ======== Phase 1: æ„å»ºå®Œæ•´çš„å†…å­˜æ ‘ç»“æ„ ========
        print(f"ğŸŒ³ [Phase 1] å¼€å§‹æ„å»ºå†…å­˜æ ‘ç»“æ„...")
        tree_nodes = build_tree_structure(node_mappings, mermaid_string)
        
        # éªŒè¯æºèŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
        if request_data.sourceNodeId not in tree_nodes:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": f"æºèŠ‚ç‚¹ {request_data.sourceNodeId} ä¸å­˜åœ¨"}
            )
        
        source_node = tree_nodes[request_data.sourceNodeId]
        print(f"ğŸŒ³ [Phase 1] æºèŠ‚ç‚¹: {source_node.id}, å­èŠ‚ç‚¹æ•°: {len(source_node.children)}")
        
        # ======== Phase 2: å®Œæ•´çš„èŠ‚ç‚¹æ·»åŠ é€»è¾‘ ========
        print(f"ğŸš€ [Phase 2] å¼€å§‹å¤„ç† direction={request_data.direction}")
        
        rename_map = {}  # å­˜å‚¨æ‰€æœ‰éœ€è¦é‡å‘½åçš„æ˜ å°„
        new_node_id = ""
        
        if request_data.direction == 'child':
            print(f"ğŸš€ [Phase 2-child] å¤„ç†å­èŠ‚ç‚¹æ·»åŠ ")
            # a. parentNode å°±æ˜¯ sourceNode
            parent_node = source_node
            
            # b. è·å– parentNode.children åˆ—è¡¨ï¼Œæ–°èŠ‚ç‚¹çš„åºå·æ˜¯ len(children) + 1
            new_sequence = len(parent_node.children) + 1
            
            # c. ç”Ÿæˆ newNodeId (ä¾‹å¦‚ 1.2.3)
            new_node_id = f"{parent_node.id}.{new_sequence}"
            
            # d. æ­¤æ“ä½œä¸è§¦å‘é‡å‘½å
            print(f"ğŸš€ [Phase 2-child] æ–°èŠ‚ç‚¹ID: {new_node_id}, æ— éœ€é‡å‘½å")
            
        elif request_data.direction == 'right-sibling':
            print(f"ğŸš€ [Phase 2-right-sibling] å¤„ç†å³ä¾§åŒçº§æ·»åŠ ")
            
            # a. è·å– sourceNode çš„çˆ¶èŠ‚ç‚¹ parentNode
            parent_node = source_node.parent
            
            if parent_node is None:
                # æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-right-sibling] æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹")
                siblings = [node for node in tree_nodes.values() if node.parent is None]
                siblings.sort(key=lambda x: split_id_helper(x.id)[1] or 0)
                source_index = siblings.index(source_node)
                
                # c. åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å‘½å
                if source_index < len(siblings) - 1:
                    print(f"ğŸš€ [Phase 2-right-sibling] éœ€è¦é‡å‘½åï¼šæºèŠ‚ç‚¹ä¸æ˜¯æœ€åä¸€ä¸ªé¡¶çº§èŠ‚ç‚¹")
                    # d. é‡å‘½åæµç¨‹ï¼šä» source_index+1 å¼€å§‹çš„æ‰€æœ‰åç»­å…„å¼ŸèŠ‚ç‚¹
                    for i in range(source_index + 1, len(siblings)):
                        sibling = siblings[i]
                        old_id = sibling.id
                        old_sequence = split_id_helper(old_id)[1]
                        new_sequence = old_sequence + 1
                        new_sibling_id = str(new_sequence)
                        
                        # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                        subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                        rename_map.update(subtree_rename_map)
                        print(f"ğŸš€ [Phase 2-right-sibling] é‡å‘½åå…„å¼ŸèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
                
                # e. ç”Ÿæˆæ–°èŠ‚ç‚¹ID
                source_sequence = split_id_helper(source_node.id)[1]
                new_node_id = str(source_sequence + 1)
                
            else:
                # æºèŠ‚ç‚¹æœ‰çˆ¶èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-right-sibling] æºèŠ‚ç‚¹çˆ¶èŠ‚ç‚¹: {parent_node.id}")
                
                # b. åœ¨ parentNode.children æ•°ç»„ä¸­æ‰¾åˆ° sourceNode çš„ç´¢å¼• i
                source_index = parent_node.children.index(source_node)
                
                # c. åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å‘½å
                if source_index < len(parent_node.children) - 1:
                    print(f"ğŸš€ [Phase 2-right-sibling] éœ€è¦é‡å‘½åï¼šæºèŠ‚ç‚¹ä¸æ˜¯æœ€åä¸€ä¸ªå­èŠ‚ç‚¹")
                    
                    # d. é‡å‘½åæµç¨‹ï¼šä» parentNode.children[i+1] å¼€å§‹çš„æ‰€æœ‰åç»­å…„å¼ŸèŠ‚ç‚¹
                    for j in range(source_index + 1, len(parent_node.children)):
                        sibling = parent_node.children[j]
                        old_id = sibling.id
                        _, old_sequence = split_id_helper(old_id)
                        new_sequence = old_sequence + 1
                        new_sibling_id = f"{parent_node.id}.{new_sequence}"
                        
                        # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                        subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                        rename_map.update(subtree_rename_map)
                        print(f"ğŸš€ [Phase 2-right-sibling] é‡å‘½åå…„å¼ŸèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
                
                # e. ç”Ÿæˆ newNodeId: æ–°èŠ‚ç‚¹çš„åºå·æ˜¯åŸåºå· + 1
                _, source_sequence = split_id_helper(source_node.id)
                new_node_id = f"{parent_node.id}.{source_sequence + 1}"
                
        elif request_data.direction == 'left-sibling':
            print(f"ğŸš€ [Phase 2-left-sibling] å¤„ç†å·¦ä¾§åŒçº§æ·»åŠ ")
            
            # a. è·å– sourceNode çš„çˆ¶èŠ‚ç‚¹ parentNode
            parent_node = source_node.parent
            
            # c. æ–°èŠ‚ç‚¹çš„ID å°†æ˜¯ sourceNode å½“å‰çš„ID
            new_node_id = source_node.id
            
            if parent_node is None:
                # æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-left-sibling] æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹")
                siblings = [node for node in tree_nodes.values() if node.parent is None]
                siblings.sort(key=lambda x: split_id_helper(x.id)[1] or 0)
                source_index = siblings.index(source_node)
                
                # d. å¿…é¡»é‡å‘½åï¼šä» sourceNode å¼€å§‹çš„æ‰€æœ‰èŠ‚ç‚¹
                for i in range(source_index, len(siblings)):
                    sibling = siblings[i]
                    old_id = sibling.id
                    old_sequence = split_id_helper(old_id)[1]
                    new_sequence = old_sequence + 1
                    new_sibling_id = str(new_sequence)
                    
                    # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                    subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                    rename_map.update(subtree_rename_map)
                    print(f"ğŸš€ [Phase 2-left-sibling] é‡å‘½åèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
                    
            else:
                # æºèŠ‚ç‚¹æœ‰çˆ¶èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-left-sibling] æºèŠ‚ç‚¹çˆ¶èŠ‚ç‚¹: {parent_node.id}")
                
                # b. åœ¨ parentNode.children æ•°ç»„ä¸­æ‰¾åˆ° sourceNode çš„ç´¢å¼• i
                source_index = parent_node.children.index(source_node)
                
                # d. å¿…é¡»é‡å‘½åï¼šä» sourceNode (parentNode.children[i]) å¼€å§‹çš„æ‰€æœ‰åç»­å…„å¼ŸèŠ‚ç‚¹
                for j in range(source_index, len(parent_node.children)):
                    sibling = parent_node.children[j]
                    old_id = sibling.id
                    _, old_sequence = split_id_helper(old_id)
                    new_sequence = old_sequence + 1
                    new_sibling_id = f"{parent_node.id}.{new_sequence}"
                    
                    # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                    subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                    rename_map.update(subtree_rename_map)
                    print(f"ğŸš€ [Phase 2-left-sibling] é‡å‘½åèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
        
        else:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": f"ä¸æ”¯æŒçš„æ–¹å‘: {request_data.direction}"}
            )
        
        print(f"ğŸš€ [Phase 2] å®Œæˆé€»è¾‘å¤„ç†")
        print(f"ğŸš€ [Phase 2] æ–°èŠ‚ç‚¹ID: {new_node_id}")
        print(f"ğŸš€ [Phase 2] éœ€è¦é‡å‘½åçš„èŠ‚ç‚¹æ•°: {len(rename_map)}")
        print(f"ğŸš€ [Phase 2] é‡å‘½åæ˜ å°„: {rename_map}")
        
        # ======== Phase 3: åº”ç”¨å˜æ›´å¹¶è¿”å› ========
        print(f"âœ¨ [Phase 3] å¼€å§‹åº”ç”¨å˜æ›´...")
        
        new_node_label = request_data.label or "æ–°èŠ‚ç‚¹"
        
        # 1. åº”ç”¨é‡å‘½åï¼šéå†æ‰€æœ‰æ•°æ®ç»“æ„ï¼Œä½¿ç”¨ rename_map æ›¿æ¢æ—§IDä¸ºæ–°ID
        updated_content_with_ids = content_with_ids
        updated_node_mappings = node_mappings.copy()
        updated_mermaid_string = mermaid_string
        
        # åº”ç”¨é‡å‘½ååˆ° content_with_ids
        for old_id, new_id in rename_map.items():
            old_divider = f"--- {old_id} ---"
            new_divider = f"--- {new_id} ---"
            updated_content_with_ids = updated_content_with_ids.replace(old_divider, new_divider)
            print(f"âœ¨ [Phase 3] æ›´æ–°content: {old_divider} -> {new_divider}")
        
        # åº”ç”¨é‡å‘½ååˆ° node_mappings
        for old_id, new_id in rename_map.items():
            if old_id in updated_node_mappings:
                updated_node_mappings[new_id] = updated_node_mappings.pop(old_id)
                print(f"âœ¨ [Phase 3] æ›´æ–°node_mappings: {old_id} -> {new_id}")
        
        # åº”ç”¨é‡å‘½ååˆ° mermaid_string
        for old_id, new_id in rename_map.items():
            pattern = rf'\b{re.escape(old_id)}\b'
            updated_mermaid_string = re.sub(pattern, new_id, updated_mermaid_string)
            print(f"âœ¨ [Phase 3] æ›´æ–°mermaid: {old_id} -> {new_id}")
        
        # 2. æ’å…¥æ–°èŠ‚ç‚¹ï¼šå°†æ–°èŠ‚ç‚¹çš„åˆ†å‰²æ æ’å…¥åˆ°é‡å‘½ååçš„ content_with_ids ä¸­
        updated_content_with_ids = await insert_divider_phase3(
            updated_content_with_ids, 
            request_data.sourceNodeId,
            request_data.direction,
            new_node_id
        )
        
        if updated_content_with_ids is None:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ’å…¥ä½ç½®"}
            )
        
        # æ·»åŠ æ–°èŠ‚ç‚¹åˆ° node_mappings
        updated_node_mappings[new_node_id] = {
            "text_snippet": new_node_label,
            "paragraph_ids": [],
            "semantic_role": "æ–°æ·»åŠ çš„èŠ‚ç‚¹"
        }
        
        # æ›´æ–° mermaid_stringï¼Œæ·»åŠ æ–°èŠ‚ç‚¹è¿æ¥
        updated_mermaid_string = update_mermaid_phase3(
            updated_mermaid_string,
            new_node_id,
            new_node_label,
            request_data.direction,
            request_data.sourceNodeId
        )
        
        # 3. æ›´æ–°æ–‡æ¡£çŠ¶æ€
        document_status[document_id].update({
            'content_with_ids': updated_content_with_ids,
            'node_mappings_demo': updated_node_mappings,
            'mermaid_code_demo': updated_mermaid_string
        })
        
        print(f"âœ¨ [Phase 3] âœ… æˆåŠŸå®Œæˆæ‰€æœ‰å˜æ›´")
        print(f"âœ¨ [Phase 3] ğŸ“Š æœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
        print(f"   content_with_ids é•¿åº¦: {len(updated_content_with_ids)} å­—ç¬¦")
        print(f"   node_mappings æ•°é‡: {len(updated_node_mappings)}")
        print(f"   mermaid_code é•¿åº¦: {len(updated_mermaid_string)} å­—ç¬¦")
        print(f"   é‡å‘½åæ“ä½œæ•°: {len(rename_map)}")
        
        # è¿”å›æ›´æ–°åçš„å®Œæ•´æ–‡æ¡£
        return JSONResponse(content={
            "success": True,
            "message": "èŠ‚ç‚¹æ·»åŠ æˆåŠŸ",
            "document": document_status[document_id],
            "new_node_id": new_node_id,
            "rename_operations": len(rename_map)
        })
        
    except Exception as e:
        print(f"âŒ [Phase 2é”™è¯¯] æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {str(e)}"}
        )

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "message": "é›†æˆç‰ˆæœ¬ Argument Structure Analyzer API is running", "version": "Phase2+3 Integrated"}

@app.get("/")
async def root():
    return {"message": "é›†æˆç‰ˆæœ¬ Argument Structure Analyzer API is running", "phase": "Phase 2+3 Integrated"}

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - é›†æˆç‰ˆåç«¯APIæœåŠ¡ (Phase 2+3)")
    print("=" * 80)
    print("ğŸ“ æœåŠ¡åœ°å€: http://localhost:8001")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8001/docs")
    print("ğŸ”§ æœåŠ¡æ¨¡å¼: å¼€å‘æ¨¡å¼ (æ”¯æŒçƒ­é‡è½½)")
    print("ğŸš€ æ–°åŠŸèƒ½: å®Œæ•´çš„Phase 2+3èŠ‚ç‚¹æ·»åŠ é€»è¾‘")
    print("   âœ… è¿é”é‡å‘½åæ”¯æŒ")
    print("   âœ… ä¸‰ç§æ·»åŠ æ–¹å‘: child, left-sibling, right-sibling")
    print("   âœ… ç¼©è¿›å¼æ•°å­—IDå‘½å")
    print("=" * 80)
    print("ğŸš€ å¯åŠ¨æœåŠ¡ä¸­...")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8001, log_level="info") 