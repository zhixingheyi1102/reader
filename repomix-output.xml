This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/rules/senior-engineer-task-execution-rule.mdc
.env.example
.gitignore
.python-version
思维导图生成器技术实现详解.md
AI辅助阅读器-完整学习文档.md
api_responses/.gitignore
api_responses/README.md
Conda环境使用指南.md
document_parser.py
download_models.py
frontend/package.json
frontend/postcss.config.js
frontend/public/index.html
frontend/public/manifest.json
frontend/src/App.css
frontend/src/App.js
frontend/src/components/DocumentRenderer.css
frontend/src/components/DocumentRenderer.js
frontend/src/components/EditableNode.css
frontend/src/components/EditableNode.js
frontend/src/components/FlowDiagram.js
frontend/src/components/LogicalDivider.js
frontend/src/components/MermaidDiagram.js
frontend/src/components/PDFViewer.js
frontend/src/components/SortableDivider.js
frontend/src/components/SortableParagraph.js
frontend/src/components/TableOfContents.js
frontend/src/components/ThemeToggle.js
frontend/src/components/UploadPage.js
frontend/src/components/ViewerPageRefactored.js
frontend/src/contexts/ThemeContext.js
frontend/src/hooks/useDocumentViewer.js
frontend/src/hooks/useMindmapGeneration.js
frontend/src/hooks/usePanelResize.js
frontend/src/hooks/useScrollDetection.js
frontend/src/index.css
frontend/src/index.js
frontend/src/utils/api.js
frontend/src/utils/dataConverter.js
frontend/src/utils/flowDiagramExample.js
frontend/src/utils/layoutHelper.js
frontend/tailwind.config.js
markdown.md
mindmap_generator.py
package.json
README.md
requirements-web.txt
sample_input_document_as_markdown__durnovo_memo.md
sample_input_document_as_markdown__small.md
screenshots/mindmap-architecture.svg
start_conda_web_app.py
web_backend.py
WEB应用使用说明.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="思维导图生成器技术实现详解.md">
# 思维导图生成器技术实现详解

## 🏗️ 项目概述

本项目是一个基于大语言模型的智能思维导图生成器，能够自动分析文档内容并生成结构化的思维导图。系统采用现代异步架构，支持多种API提供商，具有强大的内容分析、去重和可视化能力。

## 📚 目录

1. [核心架构设计](#核心架构设计)
2. [配置系统](#配置系统)
3. [多API提供商支持](#多api提供商支持)
4. [文档类型检测](#文档类型检测)
5. [内容提取引擎](#内容提取引擎)
6. [智能去重算法](#智能去重算法)
7. [成本控制系统](#成本控制系统)
8. [输出格式生成](#输出格式生成)
9. [错误处理机制](#错误处理机制)
10. [性能优化策略](#性能优化策略)

---

## 🎯 核心架构设计

### 主要组件结构

```python
# 核心类继承关系和职责分工
class Config:
    """统一配置管理中心"""
    
class TokenUsageTracker:
    """成本和使用情况追踪"""
    
class DocumentOptimizer:
    """文档优化和API调用管理"""
    
class MindMapGenerator:
    """核心思维导图生成逻辑"""
    
class MinimalDatabaseStub:
    """轻量级数据存储接口"""
```

### 数据流架构

```
文档输入 → 类型检测 → 主题提取 → 子主题提取 → 详细信息提取 → 去重验证 → 格式生成 → 输出
    ↓         ↓         ↓          ↓            ↓          ↓        ↓
  缓存层   提示工程   并发处理    智能分块     相似度检测   验证层   多格式
```

---

## ⚙️ 配置系统

### 环境变量管理

```python
from decouple import Config as DecoupleConfig, RepositoryEnv

# 配置文件加载机制
config = DecoupleConfig(RepositoryEnv('.env'))

class Config:
    # API配置 - 支持多提供商
    OPENAI_API_KEY = config.get("OPENAI_API_KEY")
    OPENAI_BASE_URL = config.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
    ANTHROPIC_API_KEY = config.get('ANTHROPIC_API_KEY')
    DEEPSEEK_API_KEY = config.get('DEEPSEEK_API_KEY')
    GEMINI_API_KEY = config.get('GEMINI_API_KEY')
    API_PROVIDER = config.get('API_PROVIDER')  # 动态选择提供商
    
    # 模型配置 - 针对不同提供商优化
    CLAUDE_MODEL_STRING = "claude-3-5-haiku-latest"
    OPENAI_COMPLETION_MODEL = "gpt-4o-mini-2024-07-18"
    DEEPSEEK_COMPLETION_MODEL = "deepseek-chat"
    GEMINI_MODEL_STRING = "gemini-2.0-flash-lite"
    
    # 成本控制 - 精确到微美元
    OPENAI_INPUT_TOKEN_PRICE = 0.15/1000000
    OPENAI_OUTPUT_TOKEN_PRICE = 0.60/1000000
    # ... 其他提供商价格配置
```

### 动态配置系统

```python
def __init__(self):
    self.config = {
        'max_summary_length': 2500,
        'max_tokens': 3000,
        'max_topics': 6,        # 最大主题数
        'max_subtopics': 4,     # 每个主题最大子主题数
        'max_details': 8,       # 每个子主题最大详细信息数
        'similarity_threshold': {
            'topic': 75,        # 主题相似度阈值
            'subtopic': 70,     # 子主题相似度阈值
            'detail': 65        # 详细信息相似度阈值
        },
        'reality_check': {
            'batch_size': 8,    # 并行验证批次大小
            'min_verified_topics': 4,
            'min_verified_ratio': 0.6
        }
    }
```

---

## 🌐 多API提供商支持

### 统一客户端管理

```python
class DocumentOptimizer:
    def __init__(self):
        # 初始化多个API客户端
        self.openai_client = AsyncOpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL  # 支持硅基流动等兼容服务
        )
        self.anthropic_client = AsyncAnthropic(api_key=Config.ANTHROPIC_API_KEY)
        self.deepseek_client = AsyncOpenAI(
            api_key=Config.DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        self.gemini_client = genai.configure(api_key=Config.GEMINI_API_KEY)
```

### 智能API路由

```python
async def generate_completion(self, prompt: str, max_tokens: int = 5000, 
                            request_id: str = None, task: Optional[str] = None):
    """统一的API调用接口，根据配置自动路由到相应提供商"""
    
    if Config.API_PROVIDER == "CLAUDE":
        kwargs = {
            "model": Config.CLAUDE_MODEL_STRING,
            "max_tokens": min(max_tokens, Config.CLAUDE_MAX_TOKENS),
            "messages": [{"role": "user", "content": prompt}]
        }
        response = await self.anthropic_client.messages.create(**kwargs)
        return response.content[0].text
        
    elif Config.API_PROVIDER == "DEEPSEEK":
        kwargs = {
            "model": Config.DEEPSEEK_COMPLETION_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": min(max_tokens, Config.DEEPSEEK_MAX_TOKENS)
        }
        
        # 支持不同DeepSeek模型
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            response = await self.deepseek_client.chat.completions.create(**kwargs)
        else:  # reasoner模型
            response = await self.deepseek_client.chat.completions.create(**kwargs)
            
        return response.choices[0].message.content
        
    # ... 其他提供商实现
```

---

## 🔍 文档类型检测

### 支持的文档类型枚举

```python
class DocumentType(Enum):
    """支持的文档类型，每种类型对应不同的分析策略"""
    TECHNICAL = auto()      # 技术文档
    SCIENTIFIC = auto()     # 科学研究
    NARRATIVE = auto()      # 叙述文档
    BUSINESS = auto()       # 商业文档
    ACADEMIC = auto()       # 学术论文
    LEGAL = auto()          # 法律文档
    MEDICAL = auto()        # 医学文档
    INSTRUCTIONAL = auto()  # 指导手册
    ANALYTICAL = auto()     # 分析报告
    PROCEDURAL = auto()     # 流程文档
    GENERAL = auto()        # 通用文档
```

### 类型特定的提示工程

```python
def _initialize_prompts(self) -> None:
    """为每种文档类型初始化专门的提示模板"""
    self.type_specific_prompts = {
        DocumentType.TECHNICAL: {
            'topics': """分析这个技术文档，关注核心系统组件和关系。
            
首先，识别构成完整独立功能单元的主要架构或技术组件。
每个组件应该：
- 是独特的技术系统、模块或流程
- 足够独立，可以单独理解
- 对整体系统功能至关重要
- 与至少一个其他组件相连

避免以下主题：
- 过于细粒度（实现细节）
- 过于宽泛（整个系统类别）
- 没有系统影响的孤立功能
- 纯文档元素

思考：
1. 核心构建块是什么？
2. 这些部分如何组合？
3. 组件之间存在什么依赖关系？
4. 关键的技术边界是什么？

格式：返回表示最高级技术构建块的组件名称的JSON数组。""",

            'subtopics': """对于技术组件'{topic}'，识别其关键子组件和接口。

每个子主题应该：
- 代表此组件的关键方面
- 具有明确的技术职责
- 与系统其他部分接口
- 对组件的核心目的有贡献

考虑：
1. 此组件暴露什么接口？
2. 它的内部子系统是什么？
3. 它如何处理数据或处理请求？
4. 它为其他组件提供什么服务？
5. 它实现什么技术标准或协议？

格式：返回构成此组件架构的技术子主题名称的JSON数组。""",

            'details': """对于技术子主题'{subtopic}'，识别具体的实现方面和要求。

关注：
1. 关键算法或方法
2. 数据结构和格式
3. 协议规范
4. 性能特征
5. 错误处理方法
6. 安全考虑
7. 依赖关系和要求

包括具体的技术细节：
- 实现特定的
- 可测量或可测试的
- 对理解至关重要的
- 与集成相关的

格式：返回技术规范和实现细节的JSON数组。"""
        },
        
        DocumentType.SCIENTIFIC: {
            'topics': """分析这个科学文档，关注主要研究组件和方法框架。

识别主要科学主题：
- 代表完整的实验或理论单元
- 遵循科学方法原则
- 支持研究目标
- 基于既定的科学概念

考虑：
1. 主要研究问题是什么？
2. 使用了什么方法论方法？
3. 应用了什么理论框架？
4. 实施了什么实验设计？
5. 不同研究组件如何交互？

格式：返回主要科学主题或研究组件的JSON数组。""",
            # ... 科学文档的其他提示
        },
        # ... 其他文档类型的提示
    }
```

### 智能类型检测算法

```python
async def detect_document_type(self, content: str, request_id: str) -> DocumentType:
    """使用LLM进行智能文档类型检测"""
    
    detection_prompt = """你正在分析一个文档以确定其主要类型和结构。
    
各文档类型的关键特征：
- TECHNICAL: 包含系统规范、API文档或实现细节
- SCIENTIFIC: 包含研究方法、实验数据或科学分析
- BUSINESS: 包含商业策略、市场分析或组织规划
- ACADEMIC: 包含学术理论、文献综述或研究论文
- LEGAL: 包含法律条文、合同或法规
- MEDICAL: 包含医学诊断、治疗方案或健康信息
- NARRATIVE: 包含故事情节、人物发展或叙述结构
- INSTRUCTIONAL: 包含操作步骤、学习材料或指导手册
- ANALYTICAL: 包含数据分析、统计研究或评估报告
- PROCEDURAL: 包含工作流程、操作规程或标准程序
- GENERAL: 通用内容，不符合以上特定类别

仅返回最匹配的文档类型名称（如：TECHNICAL）。"""
    
    try:
        response = await self.optimizer.generate_completion(
            detection_prompt + f"\n\n文档内容预览：\n{content[:2000]}...", 
            max_tokens=50, 
            request_id=request_id,
            task="detecting_document_type"
        )
        
        # 解析响应并验证类型
        if response:
            detected_type = response.strip().upper()
            try:
                return DocumentType.from_str(detected_type)
            except ValueError:
                logger.warning(f"Unknown document type detected: {detected_type}")
                return DocumentType.GENERAL
        else:
            return DocumentType.GENERAL
            
    except Exception as e:
        logger.error(f"Error detecting document type: {str(e)}")
        return DocumentType.GENERAL
```

---

## 🧠 内容提取引擎

### 分层提取策略

```python
async def generate_mindmap(self, document_content: str, request_id: str) -> str:
    """三层分级提取：主题 → 子主题 → 详细信息"""
    
    # 第一层：主题提取
    main_topics = await self._extract_main_topics(
        document_content, type_prompts['topics'], request_id
    )
    
    # 第二层：子主题提取
    for topic in main_topics:
        subtopics = await self._extract_subtopics(
            topic, document_content, type_prompts['subtopics'], request_id
        )
        topic['subtopics'] = subtopics
        
        # 第三层：详细信息提取
        for subtopic in subtopics:
            details = await self._extract_details(
                subtopic, document_content, details_prompt_template, request_id
            )
            subtopic['details'] = details
```

### 智能分块处理

```python
async def _extract_main_topics(self, content: str, topics_prompt: str, request_id: str):
    """主题提取的分块并行处理"""
    
    # 计算最优分块大小
    chunk_size = min(len(content) // 3, 15000)  # 动态分块
    chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
    
    async def process_chunk(chunk: str, chunk_idx: int) -> List[Dict[str, Any]]:
        """并行处理每个文档块"""
        chunk_prompt = f"{topics_prompt}\n\n分析以下文档内容：\n{chunk}"
        
        try:
            response = await self._retry_generate_completion(
                chunk_prompt, 3000, request_id, f"extracting_main_topics_chunk_{chunk_idx}"
            )
            
            if response:
                # 解析和验证响应
                parsed_topics = self._parse_llm_response(response, "array")
                validated_topics = []
                
                for topic_data in parsed_topics:
                    if self._validate_topic(topic_data):
                        validated_topics.append({
                            'name': str(topic_data.get('name', '')).strip(),
                            'importance': str(topic_data.get('importance', 'medium')).lower(),
                            'chunk_source': chunk_idx
                        })
                
                return validated_topics
            
        except Exception as e:
            logger.warning(f"Error processing chunk {chunk_idx}: {str(e)}")
            
        return []
    
    # 并行处理所有块
    chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
    chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)
    
    # 合并结果并去重
    all_topics = []
    for result in chunk_results:
        if isinstance(result, list):
            all_topics.extend(result)
    
    return await self._consolidate_topics(all_topics, request_id)
```

### 高级JSON解析与恢复

```python
def _clean_json_response(self, response: str) -> str:
    """高级JSON响应清理，支持多种异常情况恢复"""
    
    if not response or not isinstance(response, str):
        return "[]"
    
    def find_json_structure(text: str) -> Optional[str]:
        """智能查找JSON结构"""
        # 查找数组模式
        array_match = re.search(r'\[[\s\S]*?\](?=\s*$|\s*[,}\]])', text)
        if array_match:
            return array_match.group(0)
        
        # 查找对象模式
        object_match = re.search(r'\{[\s\S]*?\}(?=\s*$|\s*[,\]])', text)
        if object_match:
            return object_match.group(0)
        
        return None
    
    def clean_characters(text: str) -> str:
        """清理控制字符，保留有效空格"""
        text = self.control_chars_regex.sub('', text)
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)  # 零宽字符
        return text
    
    def fix_json_syntax(text: str) -> str:
        """修复JSON语法错误"""
        # 修复尾随逗号
        text = re.sub(r',(\s*})', r'\1', text)
        text = re.sub(r',(\s*\])', r'\1', text)
        
        # 修复多重逗号
        text = re.sub(r',+', ',', text)
        
        # 修复未转义的引号
        text = self.unescaped_quotes_regex.sub(r'\\"', text)
        
        # 修复百分比格式
        text = self.percentage_regex1.sub(r'\1%', text)
        text = self.percentage_regex2.sub('%', text)
        
        return text
    
    # 逐步清理和恢复
    response = clean_characters(response)
    json_content = find_json_structure(response)
    
    if json_content:
        json_content = fix_json_syntax(json_content)
        
        try:
            # 验证JSON有效性
            json.loads(json_content)
            return json_content
        except json.JSONDecodeError:
            pass
    
    # 最后的恢复尝试
    return self._attempt_json_recovery(response)
```

---

## 🔄 智能去重算法

### 多层次相似度检测

```python
async def _batch_redundancy_check(self, items, content_type='topic', 
                                context_prefix='', batch_size=10):
    """批量冗余检查，支持并发相似度检测"""
    
    if len(items) <= 1:
        return items
    
    # 组合所有项目对进行比较
    pairs_to_check = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            pairs_to_check.append((i, j))
    
    # 分批处理以避免API限制
    similar_pairs = set()
    
    for batch_start in range(0, len(pairs_to_check), batch_size):
        batch = pairs_to_check[batch_start:batch_start + batch_size]
        
        async def check_pair(i, j):
            """检查两个项目的相似性"""
            item1_name = items[i]['name']
            item2_name = items[j]['name']
            
            # 首先使用快速字符串相似度
            quick_similarity = fuzz.ratio(item1_name, item2_name)
            threshold = self.config['similarity_threshold'][content_type]
            
            if quick_similarity >= threshold:
                # 使用LLM进行深度相似度检查
                context1 = f"{context_prefix}: {item1_name}" if context_prefix else item1_name
                context2 = f"{context_prefix}: {item2_name}" if context_prefix else item2_name
                
                is_similar = await self.check_similarity_llm(
                    item1_name, item2_name, context1, context2
                )
                
                if is_similar:
                    return (i, j)
            
            return None
        
        # 并行检查这批对
        batch_tasks = [check_pair(i, j) for i, j in batch]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        for result in batch_results:
            if result and not isinstance(result, Exception):
                similar_pairs.add(result)
    
    # 构建去重后的项目列表
    items_to_remove = set()
    for i, j in similar_pairs:
        # 保留重要性更高的项目
        importance_i = self._get_importance_value(items[i]['importance'])
        importance_j = self._get_importance_value(items[j]['importance'])
        
        if importance_i >= importance_j:
            items_to_remove.add(j)
        else:
            items_to_remove.add(i)
    
    # 返回去重后的项目
    deduplicated_items = [
        items[i] for i in range(len(items)) 
        if i not in items_to_remove
    ]
    
    logger.info(f"Removed {len(items_to_remove)} redundant {content_type}s "
               f"from {len(items)} total items")
    
    return deduplicated_items
```

### LLM驱动的语义相似度检测

```python
async def check_similarity_llm(self, text1: str, text2: str, 
                             context1: str, context2: str) -> bool:
    """使用LLM进行深度语义相似度检测"""
    
    similarity_prompt = f"""比较以下两个概念是否表达相同或高度相似的想法：

概念1: "{text1}"
上下文1: {context1}

概念2: "{text2}"  
上下文2: {context2}

评估标准：
1. 核心含义是否相同？
2. 所指向的对象/概念是否一致？
3. 在给定上下文中是否重复？
4. 是否可以合并而不损失重要信息？

如果两个概念高度相似或重复，回答"是"。
如果它们是不同的概念，即使相关，也回答"否"。

回答："""

    try:
        response = await self.optimizer.generate_completion(
            similarity_prompt, 
            max_tokens=50,
            task="checking_content_similarity"
        )
        
        if response:
            return response.strip().lower() in ['是', 'yes', 'true', '相似', 'similar']
        
    except Exception as e:
        logger.warning(f"Error in LLM similarity check: {str(e)}")
    
    return False
```

---

## 💰 成本控制系统

### 详细的使用追踪

```python
class TokenUsageTracker:
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost = 0
        self.call_counts = {}
        
        # 任务分类用于更好的报告
        self.task_categories = {
            'topics': ['extracting_main_topics', 'consolidating_topics', 'detecting_document_type'],
            'subtopics': ['extracting_subtopics', 'consolidate_subtopics'],
            'details': ['extracting_details', 'consolidate_details'],
            'similarity': ['checking_content_similarity'],
            'verification': ['verifying_against_source'],
            'emoji': ['selecting_emoji']
        }
        
        # 按类别初始化计数器
        self.call_counts_by_category = {category: 0 for category in self.task_categories}
        self.token_counts_by_category = {
            category: {'input': 0, 'output': 0} 
            for category in self.task_categories
        }
        self.cost_by_category = {category: 0 for category in self.task_categories}
```

### 动态成本计算

```python
def update(self, input_tokens: int, output_tokens: int, task: str):
    """根据不同API提供商计算精确成本"""
    
    task_cost = 0
    if Config.API_PROVIDER == "CLAUDE":
        task_cost = (
            input_tokens * Config.ANTHROPIC_INPUT_TOKEN_PRICE +
            output_tokens * Config.ANTHROPIC_OUTPUT_TOKEN_PRICE
        )
    elif Config.API_PROVIDER == "DEEPSEEK":
        # DeepSeek不同模型有不同定价
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            task_cost = (
                input_tokens * Config.DEEPSEEK_CHAT_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_CHAT_OUTPUT_PRICE
            )
        else:  # reasoner模型
            task_cost = (
                input_tokens * Config.DEEPSEEK_REASONER_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_REASONER_OUTPUT_PRICE
            )
    elif Config.API_PROVIDER == "GEMINI":
        task_cost = (
            input_tokens * Config.GEMINI_INPUT_TOKEN_PRICE +
            output_tokens * Config.GEMINI_OUTPUT_TOKEN_PRICE
        )
    else:  # OPENAI
        task_cost = (
            input_tokens * Config.OPENAI_INPUT_TOKEN_PRICE +
            output_tokens * Config.OPENAI_OUTPUT_TOKEN_PRICE
        )
    
    self.total_cost += task_cost
    # ... 更新其他统计信息
```

### 智能调用限制

```python
# 生成过程中的LLM调用限制
max_llm_calls = {
    'topics': 20,      # 主题提取调用限制
    'subtopics': 30,   # 子主题提取调用限制
    'details': 40      # 详细信息提取调用限制
}

# 动态字数限制
doc_words = len(document_content.split())
word_limit = min(doc_words * 0.9, 8000)  # 最多处理8000字

# 实时监控和早停机制
if self._llm_calls['topics'] >= max_llm_calls['topics']:
    logger.info("达到主题提取LLM调用限制")
    break

if current_word_count > word_limit * 0.95:
    logger.info(f"接近字数限制 {current_word_count}/{word_limit:.0f} 字")
    break
```

---

## 🎨 输出格式生成

### Mermaid语法生成

```python
def _generate_mermaid_mindmap(self, concepts: Dict[str, Any]) -> str:
    """生成完整的Mermaid思维导图语法"""
    
    mindmap_lines = ["mindmap"]
    
    # 根节点（文档emoji）
    self._add_node_to_mindmap({'name': ''}, mindmap_lines, indent_level=1)
    
    # 添加所有主题到根节点下
    for topic in concepts.get('central_theme', {}).get('subtopics', []):
        self._add_node_to_mindmap(topic, mindmap_lines, indent_level=2)
    
    return "\n".join(mindmap_lines)

def _add_node_to_mindmap(self, node: Dict[str, Any], mindmap_lines: List[str], 
                        indent_level: int) -> None:
    """递归添加节点到思维导图"""
    
    # 根据层级确定节点形状
    if indent_level == 1:  # 根节点
        shape = NodeShape.ROOT
        node_line = f"    {shape.apply('📄')}"
    elif indent_level == 2:  # 主题
        shape = NodeShape.TOPIC
        emoji = node.get('emoji', '')
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{emoji} {node['name']}"
        node_line = f"        {shape.apply(text)}"
    elif indent_level == 3:  # 子主题
        shape = NodeShape.SUBTOPIC
        emoji = node.get('emoji', '')
        text = f"{emoji} {node['name']}"
        node_line = f"            {shape.apply(text)}"
    else:  # 详细信息
        shape = NodeShape.DETAIL
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{importance} {node['name']}"
        node_line = f"                {shape.apply(text)}"
    
    mindmap_lines.append(node_line)
    
    # 递归处理子节点
    for subtopic in node.get('subtopics', []):
        self._add_node_to_mindmap(subtopic, mindmap_lines, indent_level + 1)
    
    for detail in node.get('details', []):
        self._add_node_to_mindmap(detail, mindmap_lines, indent_level + 1)
```

### HTML可视化生成

```python
def generate_mermaid_html(mermaid_code):
    """生成交互式HTML思维导图"""
    
    # 创建Mermaid Live Editor的编辑链接
    data = {
        "code": mermaid_code,
        "mermaid": {"theme": "default"}
    }
    json_string = json.dumps(data)
    compressed_data = zlib.compress(json_string.encode('utf-8'), level=9)
    base64_string = base64.urlsafe_b64encode(compressed_data).decode('utf-8').rstrip('=')
    edit_url = f'https://mermaid.live/edit#pako:{base64_string}'
    
    # 生成完整的HTML模板
    html_template = f'''<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Mermaid Mindmap</title>
  <!-- Tailwind CSS for styling -->
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <!-- Mermaid JS for rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.0/dist/mermaid.min.js"></script>
  <style>
    body {{ margin: 0; padding: 0; }}
    #mermaid {{
      width: 100%;
      height: calc(100vh - 64px);
      overflow: auto;
    }}
  </style>
</head>
<body class="bg-gray-100">
  <div class="flex items-center justify-between p-4 bg-white shadow">
    <h1 class="text-xl font-bold">Mermaid Mindmap</h1>
    <a href="{edit_url}" target="_blank" 
       class="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600">
       Edit in Mermaid Live Editor
    </a>
  </div>
  <div id="mermaid" class="p-4">
    <pre class="mermaid">
{mermaid_code}
    </pre>
  </div>
  <script>
    mermaid.initialize({{
      startOnLoad: true,
      securityLevel: 'loose',
      theme: 'default',
      mindmap: {{ useMaxWidth: true }},
      themeConfig: {{ controlBar: true }}
    }});
  </script>
</body>
</html>'''
    return html_template
```

### Markdown大纲转换

```python
def _convert_mindmap_to_markdown(self, mermaid_syntax: str) -> str:
    """将Mermaid语法转换为Markdown大纲"""
    
    markdown_lines = []
    lines = mermaid_syntax.split('\n')[1:]  # 跳过'mindmap'头部
    
    for line in lines:
        if not line.strip():
            continue
        
        # 计算缩进级别
        indent_level = len(re.match(r'^\s*', line).group()) // 4
        content = line.strip()
        
        if indent_level == 1 and '((📄))' in content:
            continue  # 跳过文档emoji节点
        elif indent_level == 2:  # 主题 -> H1
            node_text = re.search(r'\(\((.*?)\)\)', content)
            if node_text:
                markdown_lines.append(f"# {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 3:  # 子主题 -> H2
            node_text = re.search(r'\((.*?)\)', content)
            if node_text:
                markdown_lines.append(f"## {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 4:  # 详细信息 -> 列表项
            node_text = re.search(r'\[(.*?)\]', content)
            if node_text:
                markdown_lines.append(node_text.group(1).strip())
                markdown_lines.append("")
    
    # 清理多余的空行
    markdown_text = "\n".join(markdown_lines)
    markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
    
    return markdown_text.strip()
```

---

## ⚡ 错误处理机制

### 指数退避重试

```python
async def _retry_with_exponential_backoff(self, func, *args, **kwargs):
    """带抖动的指数退避重试机制"""
    
    retries = 0
    max_retries = self.retry_config['max_retries']
    base_delay = self.retry_config['base_delay']
    max_delay = self.retry_config['max_delay']
    
    while retries < max_retries:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            retries += 1
            if retries >= max_retries:
                raise
            
            # 指数退避 + 随机抖动
            delay = min(base_delay * (2 ** (retries - 1)), max_delay)
            actual_delay = random.uniform(0, delay)
            
            logger.warning(f"尝试 {retries}/{max_retries} 失败: {str(e)}. "
                         f"{actual_delay:.2f}秒后重试")
            
            await asyncio.sleep(actual_delay)
```

### 优雅降级策略

```python
try:
    # 尝试完整处理
    main_topics = await self._extract_main_topics(...)
except Exception as e:
    logger.warning(f"完整主题提取失败，尝试简化处理: {str(e)}")
    
    # 降级到更简单的方法
    main_topics = await self._extract_simple_topics(...)
    
    if not main_topics:
        # 最后的兜底方案
        main_topics = self._generate_default_topics(document_content)
```

---

## 🚀 性能优化策略

### 智能缓存系统

```python
def __init__(self):
    # 多层缓存
    self._content_cache = {}        # 内容缓存
    self._emoji_cache = {}          # emoji缓存
    self._similarity_cache = {}     # 相似度计算缓存
    
def _load_emoji_cache(self):
    """从磁盘加载emoji缓存"""
    try:
        if os.path.exists(self._emoji_file):
            with open(self._emoji_file, 'r', encoding='utf-8') as f:
                loaded_cache = json.load(f)
                # 将字符串键转换回元组
                self._emoji_cache = {tuple(eval(k)): v for k, v in loaded_cache.items()}
                logger.info(f"从缓存加载了 {len(self._emoji_cache)} 个emoji映射")
    except Exception as e:
        logger.warning(f"加载emoji缓存失败: {str(e)}")
        self._emoji_cache = {}
```

### 并发处理优化

```python
# 并行处理多个文档块
chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)

# 并行相似度检查
batch_tasks = [check_pair(i, j) for i, j in batch]
batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

# 并行验证
verification_tasks = [verify_node_in_chunk(node, chunk) for chunk in doc_chunks]
verification_results = await asyncio.gather(*verification_tasks, return_exceptions=True)
```

### 内存优化

```python
# 流式处理大文档
async def process_large_document(self, filepath: str):
    """流式处理大文档，避免内存溢出"""
    
    async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
        chunk_size = 8192
        buffer = ""
        
        async for line in f:
            buffer += line
            if len(buffer) >= chunk_size:
                # 处理当前块
                await self.process_chunk(buffer)
                buffer = ""
        
        # 处理剩余内容
        if buffer:
            await self.process_chunk(buffer)
```

---

## 📊 使用示例

### 基础使用

```python
# 创建生成器实例
generator = MindMapGenerator()

# 生成思维导图
mindmap = await generator.generate_mindmap(document_content, request_id)

# 生成HTML文件
html_output = generate_mermaid_html(mindmap)

# 保存文件
with open('mindmap.html', 'w', encoding='utf-8') as f:
    f.write(html_output)
```

### 高级配置

```python
# 自定义配置
generator = MindMapGenerator()
generator.config.update({
    'max_topics': 8,
    'similarity_threshold': {
        'topic': 80,
        'subtopic': 75,
        'detail': 70
    }
})

# 处理特定文档类型
doc_type = DocumentType.TECHNICAL
mindmap = await generator.generate_mindmap(
    document_content, 
    request_id,
    document_type=doc_type
)
```

---

## 🎯 总结

这个思维导图生成器展现了现代AI应用开发的最佳实践：

1. **模块化架构**: 清晰的组件分离和职责划分
2. **智能处理**: 基于LLM的文档理解和内容提取
3. **性能优化**: 并发处理、智能缓存和内存管理
4. **错误恢复**: 多层次的错误处理和优雅降级
5. **成本控制**: 精确的使用追踪和资源管理
6. **多格式输出**: 支持HTML、Markdown和Mermaid格式
7. **可扩展性**: 支持多种API提供商和文档类型

该项目不仅是一个功能完整的思维导图生成工具，更是学习现代Python异步编程、AI集成和系统设计的优秀案例。
</file>

<file path="AI辅助阅读器-完整学习文档.md">
# 📚 AI辅助阅读器代码库完整学习文档

## 📋 文档概述

这是一个**AI辅助阅读器**项目的完整学习文档，采用**前后端分离架构**，基于**Python FastAPI + React**技术栈构建。项目主要功能是将文档内容转换为交互式思维导图，实现智能阅读和可视化分析。

---

## 🏗️ 第一部分：项目结构详解

### 📋 整体架构概览

**核心特性**：
- 🤖 **多AI模型支持**：DeepSeek、OpenAI GPT、Claude、Gemini等
- 📄 **多格式支持**：Markdown (.md)、文本 (.txt) 文件
- 🎨 **交互式可视化**：基于ReactFlow的高质量思维导图
- 🔄 **实时同步**：文档阅读与思维导图联动高亮
- 💻 **现代化界面**：React + Tailwind CSS响应式设计

### 📁 详细目录结构

```
mindmap-generator-main/
├── 📄 README.md                     # 项目说明文档
├── 📄 requirements-web.txt          # Python依赖包列表
├── 📄 .env.example                  # 环境变量模板
├── 📄 start_conda_web_app.py        # 🚀 一键启动脚本
├── 📄 web_backend.py                # 🌐 FastAPI后端服务器
├── 📄 mindmap_generator.py          # 🧠 AI思维导图生成核心
├── 📄 document_parser.py            # 📄 文档解析器
├── 📄 mindmap_test.py               # 🧪 测试文件
└── 📁 frontend/                     # ⚛️ React前端应用
    ├── 📄 package.json              # Node.js项目配置
    ├── 📄 tailwind.config.js        # Tailwind CSS配置
    ├── 📁 public/                   # 静态资源
    │   ├── 📄 index.html           # HTML入口
    │   └── 📄 favicon.ico          # 图标
    └── 📁 src/                      # 源代码目录
        ├── 📄 index.js             # React应用入口
        ├── 📄 App.js               # 主应用组件
        ├── 📄 index.css            # 全局样式
        ├── 📁 components/          # 组件目录
        │   ├── 📄 UploadPage.js    # 文档上传页面
        │   ├── 📄 ViewerPageRefactored.js  # 主查看器页面
        │   ├── 📄 FlowDiagram.js   # ReactFlow思维导图
        │   └── 📄 EditableNode.js  # 可编辑节点组件
        ├── 📁 hooks/               # 自定义Hook
        │   ├── 📄 useScrollDetection.js    # 滚动检测和联动
        │   ├── 📄 useMindmapGeneration.js  # 思维导图生成
        │   └── 📄 useDocumentViewer.js     # 文档查看器
        ├── 📁 utils/               # 工具函数
        │   ├── 📄 api.js           # API客户端
        │   └── 📄 dataConverter.js # 数据转换器
        └── 📁 contexts/            # React上下文
            └── 📄 AuthContext.js   # 认证上下文
```

---

## 🚀 第二部分：编译方式详解

### 🔧 环境要求

- **Python**: 3.8+ (推荐3.9+)
- **Node.js**: 16+ (推荐18+)
- **操作系统**: Windows 10/11, macOS 10.15+, Linux (Ubuntu 18.04+)

### 🎯 一键编译启动（推荐）

```bash
# 1. 克隆项目
git clone <your-repo-url>
cd mindmap-generator-main

# 2. 🔥 一键编译和启动
python start_conda_web_app.py
```

**自动化流程**：
1. ✅ 检查Python环境和Conda环境
2. ✅ 安装后端Python依赖 (`requirements-web.txt`)
3. ✅ 检查Node.js环境，自动安装npm依赖
4. ✅ 启动后端FastAPI服务（8000端口）
5. ✅ 编译并启动前端React服务（3000端口）
6. ✅ 自动打开浏览器 (`http://localhost:3000`)

### 🔗 手动分步编译

#### 后端编译
```bash
# 1. 安装Python依赖
pip install -r requirements-web.txt

# 2. 启动后端服务
python web_backend.py
```

#### 前端编译
```bash
# 1. 进入前端目录
cd frontend

# 2. 安装依赖
npm install

# 3. 开发模式启动
npm start

# 4. 生产构建
npm run build
```

---

## 🎯 第三部分：入口详解

### 🚀 启动入口：`start_conda_web_app.py`

```python
def main():
    """应用启动的真正入口"""
    # 🔍 1. 环境检查
    check_conda_env()
    install_requirements()
    
    # 🔄 2. 启动服务
    backend_process = start_backend()    # 8000端口
    frontend_process = start_frontend()  # 3000端口
    
    # 🌐 3. 自动打开浏览器
    open_browser("http://localhost:3000")
```

### 🌐 后端入口：`web_backend.py`

```python
# FastAPI应用实例
app = FastAPI(title="AI辅助阅读器 API", version="1.0.0")

# 🔑 核心API路由
@app.post("/api/upload-document")         # 文档上传
@app.post("/api/generate-mindmap")        # 思维导图生成
@app.get("/api/document/{document_id}")   # 文档获取
@app.get("/api/mindmap/{document_id}")    # 思维导图获取

# 启动服务器
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### ⚛️ 前端入口：`index.js` → `App.js`

```javascript
// index.js - React应用入口
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(<App />);

// App.js - 主应用组件
function App() {
  return (
    <Router>
      <Routes>
        <Route path="/" element={<UploadPage />} />
        <Route path="/viewer" element={<ViewerPageRefactored />} />
      </Routes>
    </Router>
  );
}
```

---

## 🧠 第四部分：核心逻辑详解

### 🗺️ AI思维导图生成引擎

#### 核心类：`MindMapGenerator`

```python
class MindMapGenerator:
    """思维导图生成器 - 系统的AI核心"""
    
    async def generate_mindmap(self, document_content: str, request_id: str) -> str:
        """🔑 核心生成流程"""
        # 1️⃣ 文档类型检测
        doc_type = await self.detect_document_type(document_content, request_id)
        
        # 2️⃣ 提取主题 (支持大文档分块处理)
        topics = await self._extract_main_topics(document_content, topics_prompt, request_id)
        
        # 3️⃣ 批量相似性检查 (AI去重)
        filtered_topics = await self._batch_redundancy_check(topics, 'topic')
        
        # 4️⃣ 递归生成子主题和细节
        for topic in filtered_topics:
            subtopics = await self._extract_subtopics(topic, content, subtopics_prompt, request_id)
            for subtopic in subtopics:
                details = await self._extract_details(subtopic, content, details_prompt, request_id)
        
        # 5️⃣ 最终过滤和验证
        final_mindmap = await self.final_pass_filter_for_duplicative_content(mindmap_data)
        
        # 6️⃣ 转换为Mermaid语法
        return self._generate_mermaid_mindmap(final_mindmap)
```

### 🔄 前端状态管理核心

#### 1. **滚动联动系统：`useScrollDetection.js`**

```javascript
const useScrollDetection = (containerRef, documentId, currentMindmapMode, mermaidDiagramRef) => {
  // 🔑 双向联动状态
  const [activeContentBlockId, setActiveContentBlockId] = useState(null);
  const [highlightedNodeId, setHighlightedNodeId] = useState(null);
  
  // 🔑 正向联动：文档滚动 → 思维导图高亮
  const detectActiveSection = useCallback(() => {
    const sections = document.querySelectorAll('[data-block-id]');
    // 复杂的可视区域检测算法
    // 更新activeContentBlockId
    // 触发思维导图节点高亮
  }, []);
  
  // 🔑 反向联动：思维导图点击 → 文档滚动
  const scrollToSection = (item) => {
    const targetElement = document.querySelector(`[data-block-id="${item.id}"]`);
    if (targetElement) {
      targetElement.scrollIntoView({ behavior: 'smooth' });
    }
  };
};
```

#### 2. **ReactFlow集成：`FlowDiagram.js`**

```javascript
const FlowDiagram = ({ mindmapData, highlightedNodeId, onNodeClick }) => {
  // 🔑 节点变化处理 (解决高亮消失问题)
  const handleNodesChange = useCallback((changes) => {
    onNodesChange(changes);
    
    // 检测拖拽等变化，重新应用高亮
    const needsHighlightReapply = changes.some(change => 
      change.type === 'position' || change.type === 'dimensions'
    );
    
    if (needsHighlightReapply && highlightedNodeId) {
      setTimeout(() => {
        applyNodeHighlighting(highlightedNodeId);
      }, 150);
    }
  }, []);
  
  // 🔑 非破坏性高亮实现
  const applyNodeHighlighting = useCallback((nodeIdToHighlight) => {
    // 多策略节点查找
    // 直接DOM操作添加CSS类
    // 避免React状态冲突
  }, []);
};
```

---

## ⏱️ 第五部分：时序图详解

### 🔄 1. 文档上传和初始化流程

```mermaid
sequenceDiagram
    participant U as 👤 用户
    participant UP as 📤 UploadPage
    participant BE as 🌐 Backend API
    participant DP as 📄 DocumentParser
    participant VP as 📖 ViewerPage

    U->>UP: 1. 选择文件上传
    UP->>UP: 2. 验证文件类型(.md/.txt)
    UP->>BE: 3. POST /api/upload-document
    BE->>BE: 4. 生成document_id
    BE->>DP: 5. 解析文档内容
    DP-->>BE: 6. 返回解析结果
    BE-->>UP: 7. 返回document_id
    UP->>VP: 8. 跳转到查看器页面
    VP->>BE: 9. GET /api/document/{id}
    BE-->>VP: 10. 返回文档内容
    VP->>VP: 11. 渲染文档内容
```

### 🧠 2. AI思维导图生成流程

```mermaid
sequenceDiagram
    participant VP as 📖 ViewerPage
    participant BE as 🌐 Backend API
    participant MG as 🧠 MindMapGenerator
    participant AI as 🤖 AI模型

    VP->>BE: 1. POST /api/generate-mindmap
    BE->>MG: 2. 调用生成器
    MG->>AI: 3. 检测文档类型
    AI-->>MG: 4. 返回文档类型
    MG->>AI: 5. 提取主题
    AI-->>MG: 6. 返回主题列表
    MG->>AI: 7. 提取子主题
    AI-->>MG: 8. 返回子主题
    MG->>AI: 9. 提取细节
    AI-->>MG: 10. 返回细节
    MG->>MG: 11. 去重和验证
    MG->>MG: 12. 转换为Mermaid语法
    MG-->>BE: 13. 返回思维导图
    BE-->>VP: 14. 返回生成结果
    VP->>VP: 15. 渲染思维导图
```

### 🔄 3. 滚动联动高亮流程

```mermaid
sequenceDiagram
    participant U as 👤 用户
    participant VP as 📖 ViewerPage
    participant SD as 🔄 ScrollDetection
    participant FD as 🎨 FlowDiagram

    U->>VP: 1. 滚动文档
    VP->>SD: 2. 触发滚动事件
    SD->>SD: 3. 检测当前可见区域
    SD->>SD: 4. 计算activeContentBlockId
    SD->>FD: 5. 更新highlightedNodeId
    FD->>FD: 6. 查找对应节点
    FD->>FD: 7. 应用高亮样式
    
    Note over U,FD: 反向联动
    U->>FD: 8. 点击思维导图节点
    FD->>SD: 9. 触发scrollToSection
    SD->>VP: 10. 滚动到对应内容
    VP->>VP: 11. 高亮对应段落
```

---

## 🔧 第六部分：各个步骤关键实现函数

### 🧠 AI思维导图生成核心函数

#### 1. **主生成函数：`generate_mindmap()`**
```python
async def generate_mindmap(self, document_content: str, request_id: str) -> str:
    """🔑 核心生成流程 - 完整版"""
    # 文档类型检测
    doc_type = await self.detect_document_type(document_content, request_id)
    
    # 提取主题
    topics = await self._extract_main_topics(document_content, topics_prompt, request_id)
    
    # 批量相似性检查
    filtered_topics = await self._batch_redundancy_check(topics, 'topic')
    
    # 递归生成子主题和细节
    for topic in filtered_topics:
        subtopics = await self._extract_subtopics(topic, content, subtopics_prompt, request_id)
        
    # 最终过滤和验证
    final_mindmap = await self.final_pass_filter_for_duplicative_content(mindmap_data)
    
    # 转换为Mermaid语法
    return self._generate_mermaid_mindmap(final_mindmap)
```

#### 2. **AI模型统一接口：`DocumentOptimizer.generate_completion()`**
```python
async def generate_completion(self, prompt: str, max_tokens: int = 5000, request_id: str = None, task: str = None) -> str:
    """统一的AI模型调用接口"""
    if Config.API_PROVIDER == "CLAUDE":
        # Claude API调用
        async with self.anthropic_client.messages.stream(...) as stream:
            message = await stream.get_final_message()
            return message.content[0].text
    elif Config.API_PROVIDER == "OPENAI":
        # OpenAI API调用
        response = await self.openai_client.chat.completions.create(...)
        return response.choices[0].message.content
    elif Config.API_PROVIDER == "DEEPSEEK":
        # DeepSeek API调用
        response = await self.deepseek_client.chat.completions.create(...)
        return response.choices[0].message.content
    # ... 其他模型
```

### 🔄 前端核心函数

#### 1. **滚动联动核心：`useScrollDetection()`**
```javascript
const useScrollDetection = (containerRef, documentId, currentMindmapMode, mermaidDiagramRef) => {
  // 🔑 滚动检测和节点高亮
  const highlightMermaidNode = useCallback((nodeId) => {
    // 复杂的节点查找逻辑
    const selectors = [
      `[data-id="${nodeId}"]`,
      `#${nodeId}`,
      `[id*="${nodeId}"]`,
      `g[data-id="${nodeId}"]`
    ];
    
    // 多策略查找和高亮
    selectors.forEach(selector => {
      const nodes = document.querySelectorAll(selector);
      nodes.forEach(node => {
        if (!node.classList.contains('mermaid-highlighted-node')) {
          node.classList.add('mermaid-highlighted-node');
        }
      });
    });
  }, []);
  
  // 🔑 反向联动：思维导图到文档
  const scrollToSection = (item) => {
    const targetElement = document.querySelector(`[data-block-id="${item.id}"]`);
    if (targetElement) {
      targetElement.scrollIntoView({ behavior: 'smooth', block: 'center' });
    }
  };
};
```

#### 2. **ReactFlow集成核心：`FlowDiagram.js`**
```javascript
// 🔑 节点变化处理 (解决高亮消失问题)
const handleNodesChange = useCallback((changes) => {
  onNodesChange(changes);
  
  // 检测需要重新应用高亮的变化
  const needsHighlightReapply = changes.some(change => 
    change.type === 'position' || 
    change.type === 'dimensions' ||
    change.type === 'select'
  );
  
  if (needsHighlightReapply && highlightedNodeId) {
    setTimeout(() => {
      applyNodeHighlighting(highlightedNodeId);
    }, 150);
  }
}, []);

// 🔑 非破坏性高亮实现
const applyNodeHighlighting = useCallback((nodeIdToHighlight) => {
  // 多策略节点查找
  const strategies = [
    () => document.querySelector(`[data-id="${nodeIdToHighlight}"]`),
    () => document.querySelector(`#${nodeIdToHighlight}`),
    () => document.querySelector(`.react-flow__node[data-id="${nodeIdToHighlight}"]`)
  ];
  
  let foundElement = null;
  for (const strategy of strategies) {
    foundElement = strategy();
    if (foundElement) break;
  }
  
  if (foundElement) {
    foundElement.classList.add('highlighted-node');
  }
}, []);
```

### 🌐 API通信核心函数

#### 1. **文档处理API：`web_backend.py`**
```python
@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """文档上传和处理入口"""
    # 文件验证
    if not file.filename.endswith(('.md', '.txt')):
        raise HTTPException(status_code=400, detail="不支持的文件格式")
    
    # 生成文档ID
    document_id = hashlib.md5(f"{file.filename}{time.time()}".encode()).hexdigest()
    
    # 解析文档内容
    content = await file.read()
    parsed_content = DocumentParser.parse(content.decode('utf-8'))
    
    # 存储和返回
    store_document(document_id, parsed_content)
    return {"document_id": document_id, "status": "success"}

@app.post("/api/generate-mindmap")
async def generate_mindmap_endpoint(request: GenerateMindmapRequest):
    """思维导图生成API"""
    generator = MindMapGenerator()
    
    # 获取文档内容
    document = await get_document_by_id(request.document_id)
    
    # 生成思维导图
    mindmap_result = await generator.generate_mindmap(
        document['content'], 
        request.request_id
    )
    
    return {"mindmap": mindmap_result, "status": "completed"}
```

#### 2. **前端API客户端：`api.js`**
```javascript
// 文档上传
export const uploadDocument = async (file, onProgress) => {
  const formData = new FormData();
  formData.append('file', file);
  
  return await fetch('/api/upload-document', {
    method: 'POST',
    body: formData
  });
};

// 思维导图生成
export const generateMindmap = async (documentId, options = {}) => {
  const response = await fetch('/api/generate-mindmap', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      document_id: documentId,
      request_id: generateRequestId(),
      ...options
    })
  });
  
  return await response.json();
};

// 轮询状态
export const pollMindmapStatus = async (requestId) => {
  const response = await fetch(`/api/mindmap-status/${requestId}`);
  return await response.json();
};
```

---

## 🎯 总结

这个AI辅助阅读器是一个**复杂而精密的系统**，涉及：

### 🔑 核心技术栈
- **后端**: Python + FastAPI + 多AI模型集成
- **前端**: React + ReactFlow + Tailwind CSS
- **通信**: RESTful API + WebSocket (可选)
- **部署**: 一键启动脚本 + Docker支持

### 🧠 核心创新点
1. **AI驱动的智能分析**: 多模型支持，智能文档类型检测
2. **双向联动机制**: 文档滚动与思维导图高亮完美同步
3. **非破坏性高亮**: 解决React组件状态管理复杂性
4. **分块处理算法**: 支持大文档的高效处理
5. **成本追踪系统**: 完整的AI调用成本管理

### 🔄 系统特点
- **高度模块化**: 每个功能模块职责明确
- **错误处理完善**: 多层次异常处理和重试机制
- **性能优化**: 异步处理、批量操作、智能缓存
- **用户体验**: 实时反馈、进度提示、响应式设计

这个系统展示了**现代AI应用开发的最佳实践**，是学习AI集成、前端状态管理、后端API设计的优秀案例。

---

📝 **学习建议**：
1. 先理解整体架构和数据流
2. 重点关注AI生成逻辑和前端联动机制
3. 实践时可以从简单功能开始，逐步扩展
4. 注意异步处理和错误处理的实现细节

---

*文档生成时间：2024年12月*
*版本：v1.0*
</file>

<file path="Conda环境使用指南.md">
# Conda环境使用指南

## 前言

本指南专门针对在conda环境（如MinerU）中运行思维导图生成器的用户。解决了常见的依赖问题和Google Generative AI集成问题。

## 🎯 在Conda环境中运行Web应用

### 1. 确保您已经激活了py312环境

```bash
# 激活您的conda环境
conda activate py312

# 确认Python版本
python --version
```

### 2. 使用专用的Conda启动脚本

```bash
# 使用专门为conda环境优化的启动脚本
python start_conda_web_app.py
```

## 🔧 环境配置

### 方式1: 使用Conda + Pip混合安装（推荐）

```bash
# 首先用conda安装基础包
conda install -y numpy scikit-learn requests

# 然后用pip安装Web相关包
pip install fastapi uvicorn python-multipart aiofiles

# 最后运行启动脚本
python start_conda_web_app.py
```

### 方式2: 纯Pip安装

```bash
# 直接用pip安装所有依赖
pip install -r requirements-web.txt

# 运行启动脚本
python start_conda_web_app.py
```

## 🚀 快速启动

如果您的conda环境已经配置好，只需要一条命令：

```bash
python start_conda_web_app.py
```

启动脚本会自动：
- ✅ 检测conda环境
- ✅ 检查Python版本兼容性
- ✅ 安装缺失的依赖
- ✅ 启动前后端服务
- ✅ 自动打开浏览器

## 📋 环境要求

- **Python**: 3.8+ (您的py312完全符合)
- **Node.js**: 16+ (用于React前端)
- **Conda**: 任意版本
- **操作系统**: Windows/Linux/macOS

## 🛠️ 依赖解决

如果遇到依赖冲突：

```bash
# 更新conda
conda update conda

# 更新pip
python -m pip install --upgrade pip

# 清理并重新安装
pip install --force-reinstall -r requirements-web.txt
```

## 🌐 访问地址

启动成功后访问：
- **前端**: http://localhost:3000
- **后端API**: http://localhost:8000
- **API文档**: http://localhost:8000/docs

## ⚡ 性能优化

在conda环境中的优化建议：

1. **使用conda-forge频道**:
   ```bash
   conda config --add channels conda-forge
   ```

2. **创建专用环境**（可选）:
   ```bash
   conda create -n mindmap python=3.12
   conda activate mindmap
   ```

3. **预安装科学计算包**:
   ```bash
   conda install numpy scikit-learn
   ```

## 🚨 常见问题

### Q: 提示权限错误
A: 在conda环境中使用 `--user` 标志：
```bash
pip install --user -r requirements-web.txt
```

### Q: Node.js未找到
A: 安装Node.js:
```bash
# 使用conda安装
conda install nodejs npm

# 或从官网下载: https://nodejs.org/
```

### Q: 端口被占用
A: 修改端口或结束占用进程：
```bash
# 使用不同端口
uvicorn web_backend:app --port 8001
```

现在您可以在py312 conda环境中运行：
```bash
python start_conda_web_app.py
```

## 环境设置

### 1. 激活Conda环境
```bash
conda activate MinerU
```

### 2. 安装依赖包

**重要**：确保安装正确的Google Generative AI包：

```bash
# 安装核心依赖
pip install -r requirements.txt

# 或者单独安装已修复的依赖
pip install google-generativeai>=0.3.0
pip install openai anthropic aiofiles termcolor fuzzywuzzy
```

### 3. 环境变量配置

创建 `.env` 文件：
```env
# 选择API提供商（必需）
API_PROVIDER=DEEPSEEK  # 或 OPENAI, CLAUDE, GEMINI

# DeepSeek配置（推荐）
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# 其他API配置（可选）
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
```

## 问题解决

### Google Generative AI 错误

**错误信息**：`module 'google.generativeai' has no attribute 'Client'`

**解决方案**：
1. 确保安装了正确的包：
   ```bash
   pip uninstall google-genai  # 卸载旧包
   pip install google-generativeai>=0.3.0  # 安装正确的包
   ```

2. 重启Python环境：
   ```bash
   conda deactivate
   conda activate MinerU
   ```

### 运行测试

```bash
# 测试基本功能
python mindmap_generator.py

# 测试Web应用
python start_conda_web_app.py
```

## 推荐配置

对于在MinerU环境中使用DeepSeek的用户：

```env
API_PROVIDER=DEEPSEEK
DEEPSEEK_API_KEY=your_key_here
```

DeepSeek提供：
- 成本效益高
- 响应速度快  
- 支持中文处理
- 与MinerU环境兼容性好

## 故障排除

### 依赖冲突
```bash
# 清理pip缓存
pip cache purge

# 重新安装依赖
pip install --force-reinstall -r requirements.txt
```

### 权限问题
```bash
# 使用用户级安装
pip install --user google-generativeai>=0.3.0
```

### 网络问题
```bash
# 使用国内镜像源
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple google-generativeai>=0.3.0
```

## 验证安装

运行以下Python代码验证安装：

```python
# 验证导入
try:
    import google.generativeai as genai
    print("✅ Google Generative AI 导入成功")
except ImportError as e:
    print(f"❌ 导入失败: {e}")

# 验证其他依赖
import openai, anthropic, aiofiles
print("✅ 其他依赖导入成功")
```

## 支持的API提供商

| 提供商 | 环境变量 | 推荐程度 | 备注 |
|-------|---------|---------|------|
| DeepSeek | DEEPSEEK_API_KEY | ⭐⭐⭐⭐⭐ | 推荐，成本低，中文友好 |
| OpenAI | OPENAI_API_KEY | ⭐⭐⭐⭐ | 质量高，成本较高 |
| Claude | ANTHROPIC_API_KEY | ⭐⭐⭐⭐ | 质量优秀，成本适中 |
| Gemini | GEMINI_API_KEY | ⭐⭐⭐ | 免费额度，需要正确配置 |
</file>

<file path="WEB应用使用说明.md">
# 智能思维导图生成器 - Web应用版

## 🎯 项目概述

基于您现有的思维导图生成器项目，我已经为您创建了一个完整的Web应用。该应用包含：

- **React前端**: 现代化的用户界面，支持文件上传和可视化
- **FastAPI后端**: 高性能的Python API服务
- **响应式布局**: 左侧MD阅读器(2/3) + 右侧思维导图(1/3)
- **智能AI分析**: 基于您现有的LLM驱动的内容分析引擎

## 🏗️ 项目结构

```
📁 您的项目根目录/
├── 📄 mindmap_generator.py      # 原有的核心生成器
├── 📄 web_backend.py            # FastAPI后端服务
├── 📄 start_web_app.py          # 一键启动脚本
├── 📄 requirements-web.txt      # Web版Python依赖
├── 📄 .env                      # API配置（需要您配置）
├── 📁 frontend/                 # React前端应用
│   ├── 📄 package.json         # 前端依赖配置
│   ├── 📄 tailwind.config.js   # 样式配置
│   ├── 📁 src/
│   │   ├── 📄 App.js           # 主应用组件
│   │   ├── 📄 App.css          # 主样式文件
│   │   └── 📁 components/
│   │       ├── 📄 UploadPage.js    # 文件上传页面
│   │       ├── 📄 ViewerPage.js    # 文档查看页面
│   │       └── 📄 MermaidDiagram.js # 思维导图组件
│   └── 📁 public/
│       └── 📄 index.html       # HTML模板
└── 📁 uploads/                  # 上传文件存储目录
```

## 🚀 快速开始

### 方式一：一键启动（推荐）

```bash
# 直接运行启动脚本
python start_web_app.py
```

启动脚本会自动：
- ✅ 检查并安装Python依赖
- ✅ 检查Node.js环境
- ✅ 安装前端依赖
- ✅ 启动后端和前端服务
- ✅ 自动打开浏览器

### 方式二：手动启动

```bash
# 1. 安装Python依赖
pip install -r requirements-web.txt

# 2. 进入前端目录并安装依赖
cd frontend
npm install

# 3. 启动后端（新终端）
python -m uvicorn web_backend:app --host 0.0.0.0 --port 8000 --reload

# 4. 启动前端（新终端）
cd frontend
npm start
```

## 🔧 环境配置

确保您的 `.env` 文件包含正确的API配置：

```env
# 推荐配置：硅基流动
OPENAI_API_KEY="您的硅基流动API密钥"
OPENAI_BASE_URL="https://api.siliconflow.cn/v1"
API_PROVIDER="OPENAI"

# 其他API（可选）
DEEPSEEK_API_KEY=""
ANTHROPIC_API_KEY=""
GEMINI_API_KEY=""
```

## 🌐 访问地址

启动成功后，您可以访问：

- **前端应用**: http://localhost:3000
- **后端API**: http://localhost:8000
- **API文档**: http://localhost:8000/docs

## 📱 功能特性

### 🎨 用户界面

1. **上传页面**
   - 支持拖拽上传 `.md` 和 `.txt` 文件
   - 文件大小限制：10MB
   - 实时上传进度显示
   - 优雅的错误处理

2. **查看页面**
   - **左侧 (2/3宽度)**: Markdown阅读器
     - 语法高亮
     - 响应式排版
     - 自定义样式优化
   - **右侧 (1/3宽度)**: Mermaid思维导图
     - 实时渲染
     - 交互式图表
     - 代码复制功能

### 🔧 工具功能

- **下载功能**: 
  - 下载原始Markdown文件
  - 下载Mermaid图表代码
- **在线编辑**: 直接跳转到Mermaid Live Editor
- **响应式设计**: 适配各种屏幕尺寸

## 🎯 使用流程

### 1. 访问应用
打开浏览器访问 `http://localhost:3000`

### 2. 上传文件
- 拖拽文件到上传区域，或点击选择文件
- 支持的格式：`.md`、`.txt`
- 系统会自动验证文件类型和大小

### 3. 查看结果
- 上传成功后自动跳转到查看页面
- 左侧阅读Markdown内容
- 右侧查看AI生成的思维导图

### 4. 导出和分享
- 使用工具栏下载文件或图表代码
- 点击"编辑图表"在线编辑Mermaid代码

## 🔍 技术架构

### 前端技术栈
- **React 18**: 现代化React框架
- **React Router**: 单页应用路由
- **Tailwind CSS**: 实用优先的CSS框架
- **Mermaid**: 图表渲染引擎
- **React Markdown**: Markdown渲染组件
- **React Dropzone**: 文件拖拽上传
- **Axios**: HTTP客户端
- **React Hot Toast**: 消息提示

### 后端技术栈
- **FastAPI**: 高性能异步Web框架
- **Uvicorn**: ASGI服务器
- **您的现有引擎**: 
  - MindMapGenerator
  - 多API提供商支持
  - 智能内容分析

## 🛠️ 开发和自定义

### 修改样式
编辑 `frontend/src/App.css` 或使用Tailwind工具类

### 添加新功能
- 前端：在 `frontend/src/components/` 添加新组件
- 后端：在 `web_backend.py` 添加新API端点

### 配置调整
- 修改 `frontend/package.json` 调整前端配置
- 修改 `web_backend.py` 调整API配置

## 🔒 安全考虑

- **文件验证**: 严格的文件类型和大小检查
- **API安全**: CORS配置和请求验证
- **错误处理**: 优雅的错误边界和用户反馈
- **数据清理**: 自动清理临时文件

## 🚨 故障排除

### 常见问题

1. **端口占用**
   ```bash
   # 更改端口
   uvicorn web_backend:app --port 8001
   ```

2. **API密钥问题**
   - 检查 `.env` 文件配置
   - 确保API密钥有效且有足够余额

3. **依赖安装失败**
   ```bash
   # 清理并重新安装
   pip install --force-reinstall -r requirements-web.txt
   ```

4. **前端构建失败**
   ```bash
   cd frontend
   rm -rf node_modules package-lock.json
   npm install
   ```

### 日志查看

- **后端日志**: 在运行uvicorn的终端查看
- **前端日志**: 在浏览器开发者工具的Console查看

## 📞 支持和反馈

如果您遇到任何问题或有改进建议，请：

1. 检查上述故障排除指南
2. 查看控制台日志
3. 确认API配置是否正确
4. 验证网络连接是否正常

## 🎉 总结

这个Web应用完美结合了您现有的AI思维导图生成能力和现代化的Web界面。用户可以：

- 📤 **轻松上传**文档
- 👀 **并排查看**原文和思维导图  
- 💾 **便捷导出**结果
- 🎨 **享受现代化**的用户体验

现在您可以运行 `python start_web_app.py` 来启动这个完整的Web应用了！
</file>

<file path=".cursor/rules/senior-engineer-task-execution-rule.mdc">
---
description: 
globs: 
alwaysApply: true
---
Rule:
You are a senior engineer with deep experience building production-grade AI agents, automations, and workflow systems. Every task you execute must follow this procedure without exception:
	
1.Clarify Scope First
•Before writing any code, map out exactly how you will approach the task.
•Confirm your interpretation of the objective.
•Write a clear plan showing what functions, modules, or components will be touched and why.
•Do not begin implementation until this is done and reasoned through.
	
2.Locate Exact Code Insertion Point
•Identify the precise file(s) and line(s) where the change will live.
•Never make sweeping edits across unrelated files.
•If multiple files are needed, justify each inclusion explicitly.
•Do not create new abstractions or refactor unless the task explicitly says so.
	
3.Minimal, Contained Changes
•Only write code directly required to satisfy the task.
•Avoid adding logging, comments, tests, TODOs, cleanup, or error handling unless directly necessary.
•No speculative changes or “while we’re here” edits.
•All logic should be isolated to not break existing flows.
	
4.Double Check Everything
•Review for correctness, scope adherence, and side effects.
•Ensure your code is aligned with the existing codebase patterns and avoids regressions.
•Explicitly verify whether anything downstream will be impacted.
	
5.Deliver Clearly
•Summarize what was changed and why.
•List every file modified and what was done in each.
•If there are any assumptions or risks, flag them for review.
	

Reminder: You are not a co-pilot, assistant, or brainstorm partner. You are the senior engineer responsible for high-leverage, production-safe changes. Do not improvise. Do not over-engineer. Do not deviate
</file>

<file path=".env.example">
# 智能思维导图生成器 - 环境变量配置

# =============================================================================
# AI 提供商选择
# =============================================================================
# 选择你要使用的AI提供商 (DEEPSEEK, OPENAI, CLAUDE, GEMINI)
API_PROVIDER=DEEPSEEK

# =============================================================================
# DeepSeek API 配置（推荐：成本低廉，中文支持好）
# =============================================================================
# 获取API密钥：https://platform.deepseek.com/
DEEPSEEK_API_KEY=your_deepseek_api_key_here
# 可选：选择模型 (deepseek-chat 或 deepseek-reasoner)
DEEPSEEK_COMPLETION_MODEL=deepseek-chat

# =============================================================================
# OpenAI API 配置（推荐使用硅基流动等代理服务）
# =============================================================================
# 官方OpenAI或第三方代理服务
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# 推荐：使用硅基流动代理（便宜且稳定）
# OPENAI_API_KEY=your_siliconflow_api_key
# OPENAI_BASE_URL=https://api.siliconflow.cn/v1

# 可选：指定模型
OPENAI_COMPLETION_MODEL=gpt-4o-mini-2024-07-18

# =============================================================================
# Claude API 配置
# =============================================================================
# 获取API密钥：https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# =============================================================================
# Gemini API 配置
# =============================================================================
# 获取API密钥：https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

#ignore .bin model files and .sqlite files
*.bin
*.sqlite
*.sqlite-shm
*.sqlite-wal
*.sqlite-journal
*.gguf
folder_of_source_documents__original_format
folder_of_source_documents__converted_to_plaintext
*.zip
venv/
.env
emoji_cache.json
.env

# 环境变量文件
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# 依赖目录
node_modules/
venv/
__pycache__/
*.pyc
*.pyo
*.pyd

# 缓存文件
.cache/
*.log
*.tmp
*.temp

# IDE和编辑器文件
.vscode/
.idea/
*.swp
*.swo
*~

# 操作系统文件
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# 上传文件目录
uploads/
mindmap_outputs/

# 构建输出
build/
dist/
*.egg-info/

# Python特定
*.egg
*.egg-info/
.tox/
.pytest_cache/
.coverage
htmlcov/

# React特定
frontend/build/
frontend/.env.local
frontend/.env.development.local
frontend/.env.test.local
frontend/.env.production.local

# 其他临时文件
*.bak
*.orig
test_*.py
debug_*.py
*_test.html

# Conda/虚拟环境
conda-meta/
envs/

# 模型缓存
emoji_cache.json

# 测试文件
test_mindmap.mmd
</file>

<file path=".python-version">
3.12
</file>

<file path="api_responses/.gitignore">
# 排除所有API响应文件
*.txt
*.json
*.log

# 但保留这个README文件来说明文件夹的用途
!README.md
!.gitignore
</file>

<file path="api_responses/README.md">
# API响应日志文件夹

本文件夹用于保存论证结构分析器调用AI API时的原始响应数据，便于分析和调试。

## 文件命名规则

文件按以下格式命名：
```
YYYYMMDD_HHMMSS_argument_structure_analysis.txt
```

例如：`20250702_221909_argument_structure_analysis.txt`

## 文件内容

每个响应文件包含以下信息：

1. **API调用信息**
   - 调用时间
   - 任务类型（论证结构分析）
   - 最大tokens设置
   - 响应和输入文本长度

2. **发送的Prompt**
   - 完整的AI分析指令
   - 包含用户要求和格式规范

3. **AI原始响应**
   - 未经处理的AI返回内容
   - 用于分析AI理解和执行情况

## 用途

- **调试分析**：当AI响应异常时，可查看原始内容进行问题诊断
- **性能优化**：分析不同prompt的效果，优化指令质量
- **结果验证**：确保AI按预期格式返回结构化数据
- **模型比较**：比较不同模型或参数设置的响应质量

## 注意事项

- 这些文件包含完整的文档内容和AI响应，可能包含敏感信息
- 文件已被`.gitignore`排除，不会被提交到版本控制
- 建议定期清理旧文件以节省存储空间
- 文件以UTF-8编码保存，支持中文内容
</file>

<file path="document_parser.py">
"""
文档结构分析器 - 基于Markdown标题构建层级结构
实现用户提供的算法：按Markdown标题层级划分文档
"""

import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import json


@dataclass
class HeadingInfo:
    """标题信息数据类"""
    level: int
    title: str
    raw_heading: str
    start_char: int
    end_char: int


class DocumentNode:
    """文档节点类 - 表示文档的层级结构"""
    
    def __init__(self, node_id: str, level: int = 0, title: Optional[str] = None, 
                 raw_heading: Optional[str] = None):
        self.id = node_id
        self.level = level
        self.title = title
        self.raw_heading = raw_heading
        
        # 标题位置
        self.heading_start_char: Optional[int] = None
        self.heading_end_char: Optional[int] = None
        
        # 内容位置（直接隶属于该标题的内容，不含子标题）
        self.content_start_char: Optional[int] = None
        self.content_end_char: Optional[int] = None
        self.content: str = ""
        
        # 完整范围位置（包含所有子孙节点）
        self.span_start_char: Optional[int] = None
        self.span_end_char: Optional[int] = None
        
        # 树结构
        self.children: List['DocumentNode'] = []
        self.parent: Optional['DocumentNode'] = None

    def add_child(self, child: 'DocumentNode'):
        """添加子节点"""
        child.parent = self
        self.children.append(child)

    def to_dict(self) -> Dict[str, Any]:
        """转换为字典格式"""
        return {
            "id": self.id,
            "level": self.level,
            "title": self.title,
            "raw_heading": self.raw_heading,
            "heading_start_char": self.heading_start_char,
            "heading_end_char": self.heading_end_char,
            "content_start_char": self.content_start_char,
            "content_end_char": self.content_end_char,
            "content": self.content,
            "span_start_char": self.span_start_char,
            "span_end_char": self.span_end_char,
            "children": [child.to_dict() for child in self.children]
        }


class DocumentParser:
    """文档解析器 - 按照用户提供的算法实现"""
    
    def __init__(self):
        # 匹配Markdown标题的正则表达式
        self.heading_pattern = re.compile(r'^ *(#+)\s+(.*)\s*$', re.MULTILINE)
    
    def parse_document(self, markdown_text: str, document_id: str = "doc") -> DocumentNode:
        """
        解析Markdown文档，构建层级结构
        
        Args:
            markdown_text: Markdown文本内容
            document_id: 文档ID，用于生成节点ID
            
        Returns:
            DocumentNode: 根节点
        """
        # 第一步：扫描标题
        headings_list = self._extract_headings(markdown_text)
        
        # 第二步：构建树结构
        root = self._build_tree_structure(headings_list, markdown_text, document_id)
        
        # 第三步：后处理 - 修正范围和内容
        self._post_process_tree(root, markdown_text)
        
        return root
    
    def _extract_headings(self, markdown_text: str) -> List[HeadingInfo]:
        """
        第一步：提取所有标题信息
        """
        headings_list = []
        
        for match in self.heading_pattern.finditer(markdown_text):
            level = len(match.group(1))  # 计算#号的数量
            title = match.group(2).strip()  # 提取标题文本
            raw_heading = match.group(0).strip()  # 完整的标题行
            start_char = match.start()
            end_char = match.end()
            
            heading_info = HeadingInfo(
                level=level,
                title=title,
                raw_heading=raw_heading,
                start_char=start_char,
                end_char=end_char
            )
            headings_list.append(heading_info)
        
        return headings_list
    
    def _build_tree_structure(self, headings_list: List[HeadingInfo], 
                            markdown_text: str, document_id: str) -> DocumentNode:
        """
        第二步：构建层级结构
        """
        # 创建根节点
        root = DocumentNode(node_id=f"{document_id}_root", level=0, title="文档根")
        root.span_start_char = 0
        root.content_start_char = 0
        
        # 用栈来追踪当前的父节点
        stack = [root]
        
        # 处理前言（第一个标题前的内容）
        if headings_list and headings_list[0].start_char > 0:
            preface_node = DocumentNode(
                node_id=f"{document_id}_preface",
                level=1,
                title="引言"
            )
            preface_node.content_start_char = 0
            preface_node.content_end_char = headings_list[0].start_char
            preface_node.content = markdown_text[0:headings_list[0].start_char].strip()
            preface_node.span_start_char = 0
            preface_node.span_end_char = headings_list[0].start_char
            
            root.add_child(preface_node)
        elif not headings_list:
            # 整个文档无标题
            root.content = markdown_text
            root.content_start_char = 0
            root.content_end_char = len(markdown_text)
            root.span_start_char = 0
            root.span_end_char = len(markdown_text)
            return root
        
        # 遍历标题列表构建树
        for i, heading_info in enumerate(headings_list):
            # 确定父节点
            current_parent = stack[-1]
            
            # 根据级别关系调整栈
            if heading_info.level > current_parent.level:
                # 子节点，current_parent就是父节点
                pass
            elif heading_info.level == current_parent.level:
                # 兄弟节点，弹出当前父节点
                stack.pop()
                current_parent = stack[-1]
            else:
                # 需要向上回溯
                while stack and stack[-1].level >= heading_info.level:
                    stack.pop()
                current_parent = stack[-1]
            
            # 创建新节点
            node_id = f"{document_id}_sec_{len(current_parent.children) + 1}"
            if current_parent.id != f"{document_id}_root":
                node_id = f"{current_parent.id}_{len(current_parent.children) + 1}"
            
            new_node = DocumentNode(
                node_id=node_id,
                level=heading_info.level,
                title=heading_info.title,
                raw_heading=heading_info.raw_heading
            )
            
            # 设置标题位置
            new_node.heading_start_char = heading_info.start_char
            new_node.heading_end_char = heading_info.end_char
            
            # 设置内容和范围的初始位置
            new_node.content_start_char = heading_info.end_char
            new_node.span_start_char = heading_info.start_char
            
            # 确定结束位置
            if i == len(headings_list) - 1:
                # 最后一个标题
                next_boundary_char = len(markdown_text)
            else:
                next_boundary_char = headings_list[i + 1].start_char
            
            new_node.content_end_char = next_boundary_char
            new_node.span_end_char = next_boundary_char
            
            # 提取内容
            new_node.content = markdown_text[new_node.content_start_char:new_node.content_end_char].strip()
            
            # 添加到父节点
            current_parent.add_child(new_node)
            
            # 更新栈
            stack.append(new_node)
        
        return root
    
    def _post_process_tree(self, root: DocumentNode, markdown_text: str):
        """
        第三步：后处理 - 修正范围与内容
        """
        # 修正span_end_char（后序遍历）
        self._fix_span_ranges(root)
        
        # 修正content和content_end_char（前序遍历）
        self._fix_content_ranges(root, markdown_text)
        
        # 确保根节点的范围是整个文档
        root.span_end_char = len(markdown_text)
    
    def _fix_span_ranges(self, node: DocumentNode):
        """后序遍历修正span_end_char"""
        # 先处理所有子节点
        for child in node.children:
            self._fix_span_ranges(child)
        
        # 如果有子节点，span_end_char应该等于最后一个子节点的span_end_char
        if node.children:
            node.span_end_char = node.children[-1].span_end_char
    
    def _fix_content_ranges(self, node: DocumentNode, markdown_text: str):
        """前序遍历修正content和content_end_char"""
        # 如果有子节点，content_end_char应该是第一个子节点标题开始之前
        if node.children:
            first_child = node.children[0]
            if first_child.heading_start_char is not None:
                node.content_end_char = min(
                    node.content_end_char or len(markdown_text),
                    first_child.heading_start_char
                )
        
        # 重新提取content
        if node.content_start_char is not None and node.content_end_char is not None:
            node.content = markdown_text[node.content_start_char:node.content_end_char].strip()
        
        # 递归处理子节点
        for child in node.children:
            self._fix_content_ranges(child, markdown_text)
    
    def parse_to_chunks(self, markdown_text: str, document_id: str = "doc") -> List[Dict[str, Any]]:
        """
        解析文档并返回分块列表，用于AI问题生成
        
        Returns:
            List[Dict]: 包含分块信息的列表
        """
        root = self.parse_document(markdown_text, document_id)
        chunks = []
        
        def collect_chunks(node: DocumentNode, chunk_index_ref: List[int]):
            """递归收集所有节点作为分块"""
            if node.content and node.content.strip():
                chunk = {
                    'chunk_id': node.id,
                    'paragraph_index': chunk_index_ref[0],
                    'content': node.content,
                    'start_char': node.content_start_char,
                    'end_char': node.content_end_char,
                    'document_id': document_id,
                    'level': node.level,
                    'title': node.title,
                    'heading': node.raw_heading
                }
                chunks.append(chunk)
                chunk_index_ref[0] += 1
            
            # 递归处理子节点
            for child in node.children:
                collect_chunks(child, chunk_index_ref)
        
        chunk_index_ref = [0]
        collect_chunks(root, chunk_index_ref)
        
        return chunks
    
    def generate_toc(self, root: DocumentNode) -> List[Dict[str, Any]]:
        """
        生成目录结构，用于前端显示
        
        Returns:
            List[Dict]: 目录项列表
        """
        toc = []
        
        def build_toc(node: DocumentNode, depth: int = 0):
            """递归构建目录"""
            if node.title and node.level > 0:  # 跳过根节点
                toc_item = {
                    'id': node.id,
                    'title': node.title,
                    'level': node.level,
                    'depth': depth,
                    'span_start_char': node.span_start_char,
                    'span_end_char': node.span_end_char,
                    'has_children': len(node.children) > 0,
                    'children': []
                }
                
                # 递归添加子节点
                for child in node.children:
                    child_toc = build_toc(child, depth + 1)
                    if child_toc:
                        toc_item['children'].extend(child_toc if isinstance(child_toc, list) else [child_toc])
                
                return [toc_item]
            else:
                # 对于根节点，直接返回其子节点的目录
                result = []
                for child in node.children:
                    child_toc = build_toc(child, depth)
                    if child_toc:
                        result.extend(child_toc if isinstance(child_toc, list) else [child_toc])
                return result
        
        return build_toc(root)


# 测试函数
def test_document_parser():
    """测试文档解析器"""
    markdown_sample = """这是文档的前言部分，没有标题。

# 第一章：介绍

这是第一章的主要内容。它包含了重要的介绍信息。

## 1.1 背景

这是1.1小节的内容，讲述了项目的背景。

## 1.2 目标

这是1.2小节的内容，描述了项目目标。

### 1.2.1 主要目标

这是主要目标的详细说明。

### 1.2.2 次要目标

这是次要目标的详细说明。

# 第二章：方法

这是第二章的内容，介绍了使用的方法。

## 2.1 数据收集

数据收集的方法和过程。

## 2.2 数据分析

数据分析的具体步骤。
"""
    
    parser = DocumentParser()
    root = parser.parse_document(markdown_sample, "test_doc")
    
    print("=== 文档结构 ===")
    print(json.dumps(root.to_dict(), ensure_ascii=False, indent=2))
    
    print("\n=== 分块信息 ===")
    chunks = parser.parse_to_chunks(markdown_sample, "test_doc")
    for chunk in chunks:
        print(f"块 {chunk['paragraph_index']}: {chunk['title']} (级别 {chunk['level']})")
        print(f"  内容: {chunk['content'][:50]}...")
        print()
    
    print("\n=== 目录结构 ===")
    toc = parser.generate_toc(root)
    print(json.dumps(toc, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    test_document_parser()
</file>

<file path="download_models.py">
import json
import shutil
import os

import requests
from modelscope import snapshot_download


def download_json(url):
    # 下载JSON文件
    response = requests.get(url)
    response.raise_for_status()  # 检查请求是否成功
    return response.json()


def download_and_modify_json(url, local_filename, modifications):
    if os.path.exists(local_filename):
        data = json.load(open(local_filename))
        config_version = data.get('config_version', '0.0.0')
        if config_version < '1.2.0':
            data = download_json(url)
    else:
        data = download_json(url)

    # 修改内容
    for key, value in modifications.items():
        data[key] = value

    # 保存修改后的内容
    with open(local_filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


if __name__ == '__main__':
    mineru_patterns = [
        # "models/Layout/LayoutLMv3/*",
        "models/Layout/YOLO/*",
        "models/MFD/YOLO/*",
        "models/MFR/unimernet_hf_small_2503/*",
        "models/OCR/paddleocr_torch/*",
        # "models/TabRec/TableMaster/*",
        # "models/TabRec/StructEqTable/*",
    ]
    model_dir = snapshot_download('opendatalab/PDF-Extract-Kit-1.0', allow_patterns=mineru_patterns)
    layoutreader_model_dir = snapshot_download('ppaanngggg/layoutreader')
    model_dir = model_dir + '/models'
    print(f'model_dir is: {model_dir}')
    print(f'layoutreader_model_dir is: {layoutreader_model_dir}')

    # paddleocr_model_dir = model_dir + '/OCR/paddleocr'
    # user_paddleocr_dir = os.path.expanduser('~/.paddleocr')
    # if os.path.exists(user_paddleocr_dir):
    #     shutil.rmtree(user_paddleocr_dir)
    # shutil.copytree(paddleocr_model_dir, user_paddleocr_dir)

    json_url = 'https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/magic-pdf.template.json'
    config_file_name = 'magic-pdf.json'
    home_dir = os.path.expanduser('~')
    config_file = os.path.join(home_dir, config_file_name)

    json_mods = {
        'models-dir': model_dir,
        'layoutreader-model-dir': layoutreader_model_dir,
    }

    download_and_modify_json(json_url, config_file, json_mods)
    print(f'The configuration file has been configured successfully, the path is: {config_file}')
</file>

<file path="frontend/postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}
</file>

<file path="frontend/public/index.html">
<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="智能思维导图生成器 - 基于AI的文档分析和思维导图生成工具" />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <title>智能思维导图生成器</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
  </body>
</html>
</file>

<file path="frontend/public/manifest.json">
{
  "short_name": "思维导图生成器",
  "name": "智能思维导图生成器",
  "icons": [
    {
      "src": "favicon.ico",
      "sizes": "64x64 32x32 24x24 16x16",
      "type": "image/x-icon"
    }
  ],
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#000000",
  "background_color": "#ffffff"
}
</file>

<file path="frontend/src/components/DocumentRenderer.css">
/* 拖拽排序相关样式 */
.sortable-content {
  position: relative;
}

/* 段落高亮样式 */
.semantic-paragraph-highlighted {
  background-color: rgba(59, 130, 246, 0.1) !important;
  border-left: 4px solid #3b82f6 !important;
  transform: translateX(2px);
  transition: all 0.2s ease-in-out;
}

/* 深色模式下的段落高亮 */
.dark .semantic-paragraph-highlighted {
  background-color: rgba(59, 130, 246, 0.15) !important;
  border-left-color: #60a5fa !important;
}

/* 拖拽状态样式 */
.sortable-content [data-dnd-dragging="true"] {
  opacity: 0.7;
}

/* 拖拽中的段落 */
.sortable-content .paragraph-block {
  transition: all 0.2s ease-in-out;
}

/* 分割线拖拽样式 */
.sortable-divider {
  position: relative;
}

/* 分割线悬停时不做任何变化 */
.sortable-divider:hover {
  /* 完全去掉悬停效果 */
}

/* 拖拽预览样式 */
.sortable-content [data-dnd-over="true"] {
  border: 2px dashed #3b82f6;
  border-radius: 8px;
  background-color: rgba(59, 130, 246, 0.05);
}

.dark .sortable-content [data-dnd-over="true"] {
  border-color: #60a5fa;
  background-color: rgba(59, 130, 246, 0.1);
}

/* 拖拽手柄动画 */
.sortable-content .group .absolute {
  transition: all 0.2s ease-in-out;
}

/* 拖拽时的阴影效果 */
.sortable-content [data-dnd-dragging="true"] {
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  z-index: 1000;
}

/* 响应式拖拽手柄 */
@media (max-width: 768px) {
  .sortable-content {
    touch-action: none;
  }
}
</file>

<file path="frontend/src/components/PDFViewer.js">
import React from 'react';
import { File } from 'lucide-react';

const PDFViewer = ({ pdfBase64 }) => {
  if (!pdfBase64) {
    return (
      <div className="flex items-center justify-center h-96 bg-gray-100 rounded-lg">
        <div className="text-center">
          <File className="h-12 w-12 text-gray-400 mx-auto mb-2" />
          <p className="text-gray-500">PDF文件不可用</p>
        </div>
      </div>
    );
  }

  return (
    <div className="w-full h-full bg-white">
      <embed
        src={`data:application/pdf;base64,${pdfBase64}`}
        type="application/pdf"
        width="100%"
        height="100%"
        className="border-0 rounded-none block"
        style={{ 
          minHeight: '100%',
          margin: 0,
          padding: 0,
          display: 'block'
        }}
      />
    </div>
  );
};

export default PDFViewer;
</file>

<file path="frontend/src/components/SortableDivider.js">
import React from 'react';
import { useSortable } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import LogicalDivider from './LogicalDivider';

const SortableDivider = ({ id, nodeInfo, className = '' }) => {
  const {
    attributes,
    listeners,
    setNodeRef,
    transform,
    transition,
    isDragging
  } = useSortable({ id });

  const style = {
    transform: CSS.Translate.toString(transform),
    transition,
    opacity: isDragging ? 0.5 : 1,
    zIndex: isDragging ? 100 : 'auto',
  };

  return (
    <div
      ref={setNodeRef}
      style={style}
      className={`sortable-divider ${isDragging ? 'shadow-lg' : ''} ${className}`}
    >
      <LogicalDivider 
        nodeInfo={nodeInfo}
        // 将拖拽属性传递给 LogicalDivider 的拖拽手柄
        dragHandleProps={{
          ...attributes,
          ...listeners,
        }}
      />
    </div>
  );
};

export default SortableDivider;
</file>

<file path="frontend/src/components/SortableParagraph.js">
import React from 'react';
import { useSortable } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';

const SortableParagraph = ({ id, children, className = '' }) => {
  const {
    attributes,
    listeners,
    setNodeRef,
    transform,
    transition,
    isDragging
  } = useSortable({ id });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition,
    opacity: isDragging ? 0.5 : 1,
  };

  return (
    <div
      ref={setNodeRef}
      style={style}
      className={`sortable-paragraph ${className}`}
    >
      {children}
    </div>
  );
};

export default SortableParagraph;
</file>

<file path="frontend/src/components/TableOfContents.js">
import React from 'react';
import { FileText } from 'lucide-react';

const TableOfContents = ({ toc, expandedItems, activeItem, onToggle, onItemClick }) => {
  const renderTocItem = (item, depth = 0) => {
    const hasChildren = item.children && item.children.length > 0;
    const isExpanded = expandedItems.has(item.id);
    const isActive = activeItem === item.id;
    
    return (
      <div key={item.id} className={`${depth > 0 ? 'ml-3' : ''}`}>
        <div 
          className={`flex items-center py-1 px-2 text-xs rounded transition-colors cursor-pointer ${
            isActive 
              ? 'bg-blue-100 text-blue-800 border-l-2 border-blue-500' 
              : 'text-gray-700 hover:bg-gray-100'
          }`}
          onClick={() => onItemClick(item)}
        >
          <div className="flex items-center flex-1 min-w-0">
            {hasChildren && (
              <button
                onClick={(e) => {
                  e.stopPropagation();
                  onToggle(item.id);
                }}
                className="mr-1 p-0.5 hover:bg-gray-200 rounded"
              >
                {isExpanded ? '▼' : '▶'}
              </button>
            )}
            <span 
              className={`truncate ${item.level === 1 ? 'font-semibold' : 'font-normal'}`}
              title={item.title}
            >
              {item.title}
            </span>
          </div>
          <span className="text-xs text-gray-400 ml-1">
            H{item.level}
          </span>
        </div>
        
        {hasChildren && isExpanded && (
          <div className="mt-1">
            {item.children.map(child => renderTocItem(child, depth + 1))}
          </div>
        )}
      </div>
    );
  };

  if (!toc || toc.length === 0) {
    return (
      <div className="p-4 text-center text-gray-500 text-xs">
        <FileText className="w-8 h-8 mx-auto mb-2 text-gray-300" />
        <p>文档没有标题结构</p>
        <p>或正在分析中...</p>
      </div>
    );
  }

  return (
    <div className="p-2 space-y-1">
      {toc.map(item => renderTocItem(item))}
    </div>
  );
};

export default TableOfContents;
</file>

<file path="frontend/src/components/ThemeToggle.js">
import React from 'react';
import { useTheme } from '../contexts/ThemeContext';

const ThemeToggle = ({ className = '' }) => {
  const { isDarkMode, toggleTheme } = useTheme();

  return (
    <button
      onClick={toggleTheme}
      className={`
        relative inline-flex items-center justify-center w-10 h-10 
        rounded-full transition-colors duration-200 
        bg-gray-100 hover:bg-gray-200 
        dark:bg-gray-800 dark:hover:bg-gray-700
        focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2
        dark:focus:ring-offset-gray-900
        ${className}
      `}
      title={isDarkMode ? '切换到亮色模式' : '切换到暗色模式'}
      aria-label={isDarkMode ? '切换到亮色模式' : '切换到暗色模式'}
    >
      {isDarkMode ? (
        // 太阳图标 (亮色模式)
        <svg
          className="w-5 h-5 text-yellow-500"
          fill="currentColor"
          viewBox="0 0 24 24"
        >
          <path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0zM18.894 6.166a.75.75 0 00-1.06-1.06l-1.591 1.59a.75.75 0 101.06 1.061l1.591-1.59zM21.75 12a.75.75 0 01-.75.75h-2.25a.75.75 0 010-1.5H21a.75.75 0 01.75.75zM17.834 18.894a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 10-1.061 1.06l1.59 1.591zM12 18a.75.75 0 01.75.75V21a.75.75 0 01-1.5 0v-2.25A.75.75 0 0112 18zM7.758 17.303a.75.75 0 00-1.061-1.06l-1.591 1.59a.75.75 0 001.06 1.061l1.591-1.59zM6 12a.75.75 0 01-.75.75H3a.75.75 0 010-1.5h2.25A.75.75 0 016 12zM6.697 7.757a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 00-1.061 1.06l1.59 1.591z" />
        </svg>
      ) : (
        // 月亮图标 (暗色模式)
        <svg
          className="w-5 h-5 text-gray-600"
          fill="currentColor"
          viewBox="0 0 24 24"
        >
          <path fillRule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clipRule="evenodd" />
        </svg>
      )}
    </button>
  );
};

export default ThemeToggle;
</file>

<file path="frontend/src/contexts/ThemeContext.js">
import React, { createContext, useContext, useState, useEffect } from 'react';

const ThemeContext = createContext();

export const useTheme = () => {
  const context = useContext(ThemeContext);
  if (!context) {
    throw new Error('useTheme must be used within a ThemeProvider');
  }
  return context;
};

export const ThemeProvider = ({ children }) => {
  const [isDarkMode, setIsDarkMode] = useState(() => {
    // 检查localStorage中的主题设置
    const savedTheme = localStorage.getItem('theme');
    if (savedTheme) {
      return savedTheme === 'dark';
    }
    // 如果没有保存的设置，检查系统偏好
    return window.matchMedia('(prefers-color-scheme: dark)').matches;
  });

  useEffect(() => {
    // 更新document的class以应用暗模式
    if (isDarkMode) {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
    
    // 将设置保存到localStorage
    localStorage.setItem('theme', isDarkMode ? 'dark' : 'light');
  }, [isDarkMode]);

  const toggleTheme = () => {
    setIsDarkMode(!isDarkMode);
  };

  const value = {
    isDarkMode,
    toggleTheme,
    theme: isDarkMode ? 'dark' : 'light'
  };

  return (
    <ThemeContext.Provider value={value}>
      {children}
    </ThemeContext.Provider>
  );
};
</file>

<file path="frontend/src/hooks/usePanelResize.js">
import { useState, useEffect, useCallback, useRef } from 'react';

export const usePanelResize = () => {
  // 分割面板相关状态 - 调整为三列布局
  const [tocPanelWidth, setTocPanelWidth] = useState(20); // 目录栏宽度
  const [leftPanelWidth, setLeftPanelWidth] = useState(45); // 文档查看器宽度  
  const [isDragging, setIsDragging] = useState(false);
  const [dragTarget, setDragTarget] = useState(null); // 'toc-divider' 或 'main-divider'
  const [showToc, setShowToc] = useState(false);
  const containerRef = useRef(null);

  // 使用useRef来保存事件处理函数的引用
  const handleMouseMoveRef = useRef(null);
  const handleMouseUpRef = useRef(null);

  // 拖拽处理函数
  const handleMouseDown = useCallback((e, target) => {
    setIsDragging(true);
    setDragTarget(target);
    e.preventDefault();
  }, []);

  // 创建事件处理函数
  useEffect(() => {
    handleMouseMoveRef.current = (e) => {
      if (!isDragging || !containerRef.current || !dragTarget) return;
      
      const container = containerRef.current;
      const containerRect = container.getBoundingClientRect();
      const containerWidth = containerRect.width;
      const mouseX = e.clientX - containerRect.left;
      
      if (dragTarget === 'toc-divider') {
        // 调整目录栏宽度
        const newTocWidth = (mouseX / containerWidth) * 100;
        const minTocWidth = 15;
        const maxTocWidth = 30;
        
        if (newTocWidth >= minTocWidth && newTocWidth <= maxTocWidth) {
          setTocPanelWidth(newTocWidth);
        }
      } else if (dragTarget === 'main-divider') {
        // 调整主内容区域宽度
        const currentTocWidth = showToc ? tocPanelWidth : 0;
        const availableWidth = 100 - currentTocWidth;
        const newLeftWidth = ((mouseX - (currentTocWidth * containerWidth / 100)) / containerWidth) * 100;
        const minLeftWidth = 25;
        const maxLeftWidth = availableWidth - 25; // 为右侧留出至少25%
        
        if (newLeftWidth >= minLeftWidth && newLeftWidth <= maxLeftWidth) {
          setLeftPanelWidth(newLeftWidth);
        }
      }
    };

    handleMouseUpRef.current = () => {
      setIsDragging(false);
      setDragTarget(null);
    };
  }, [isDragging, dragTarget, tocPanelWidth, showToc]);

  // 管理事件监听器
  useEffect(() => {
    // 使用局部变量存储事件处理函数的引用，避免闭包问题
    let localHandleMouseMove = null;
    let localHandleMouseUp = null;
    
    const handleMouseMove = (e) => {
      if (handleMouseMoveRef.current) {
        handleMouseMoveRef.current(e);
      }
    };

    const handleMouseUp = () => {
      if (handleMouseUpRef.current) {
        handleMouseUpRef.current();
      }
    };

    if (isDragging) {
      // 使用window.document确保获取全局document对象，并检查addEventListener方法是否存在
      const globalDocument = window.document;
      if (globalDocument && typeof globalDocument.addEventListener === 'function') {
        localHandleMouseMove = handleMouseMove;
        localHandleMouseUp = handleMouseUp;
        
        globalDocument.addEventListener('mousemove', localHandleMouseMove, { passive: false });
        globalDocument.addEventListener('mouseup', localHandleMouseUp, { passive: false });
        
        if (globalDocument.body) {
          globalDocument.body.style.cursor = 'col-resize';
          globalDocument.body.style.userSelect = 'none';
        }
      }
    }

    // 清理函数 - 添加多重安全检查
    return () => {
      try {
        // 使用window.document确保获取全局document对象
        const globalDocument = window.document;
        if (globalDocument && typeof globalDocument.removeEventListener === 'function') {
          if (localHandleMouseMove) {
            globalDocument.removeEventListener('mousemove', localHandleMouseMove);
          }
          if (localHandleMouseUp) {
            globalDocument.removeEventListener('mouseup', localHandleMouseUp);
          }
        }
        
        // 重置样式
        if (globalDocument && globalDocument.body) {
          globalDocument.body.style.cursor = '';
          globalDocument.body.style.userSelect = '';
        }
      } catch (error) {
        // 静默处理清理错误，避免影响应用运行
        console.warn('清理事件监听器时出错:', error);
      }
    };
  }, [isDragging]);

  return {
    tocPanelWidth,
    leftPanelWidth,
    isDragging,
    dragTarget,
    showToc,
    setShowToc,
    containerRef,
    handleMouseDown
  };
};
</file>

<file path="frontend/src/index.js">
import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);
</file>

<file path="frontend/src/utils/dataConverter.js">
/**
 * 将API数据转换为React Flow格式
 * @param {Object} apiData - 包含mermaid_string和node_mappings的对象
 * @returns {Object} 包含nodes和edges数组的对象
 */
export const convertDataToReactFlow = (apiData) => {
  console.log('🔧 [数据转换] 开始转换数据:', apiData);
  
  if (!apiData || !apiData.node_mappings || !apiData.mermaid_string) {
    console.log('🔧 [数据转换] 数据无效，返回空数组');
    console.log('🔧 [数据转换] apiData存在:', !!apiData);
    console.log('🔧 [数据转换] node_mappings存在:', !!(apiData && apiData.node_mappings));
    console.log('🔧 [数据转换] mermaid_string存在:', !!(apiData && apiData.mermaid_string));
    return { nodes: [], edges: [] };
  }

  const { mermaid_string, node_mappings } = apiData;
  
  console.log('🔧 [数据转换] node_mappings:', node_mappings);
  console.log('🔧 [数据转换] mermaid_string:', mermaid_string);

  // 创建nodes数组
  const nodes = Object.keys(node_mappings).map((nodeId, index) => ({
    id: nodeId,
    data: { 
      label: node_mappings[nodeId].text_snippet || nodeId,
      paragraph_ids: node_mappings[nodeId].paragraph_ids || []
    },
    position: { x: 0, y: 0 }, // 初始位置，将在布局阶段更新
    type: 'default'
  }));
  
  console.log('🔧 [数据转换] 创建的节点:', nodes);

  // 从mermaid_string解析连接关系创建edges数组
  const edges = [];
  
  // 匹配Mermaid图表中的连接关系，支持多种格式：
  // A --> B, A -> B, A --- B, A -- B
  // 支持带标签的节点，如：A[标签] --> B[标签]
  const connectionRegex = /([A-Za-z0-9_]+)(?:\[[^\]]*\])?\s*(-{1,2}>?|={1,2}>?)\s*([A-Za-z0-9_]+)(?:\[[^\]]*\])?/g;
  let match;
  let edgeIndex = 0;

  while ((match = connectionRegex.exec(mermaid_string)) !== null) {
    const [, source, connector, target] = match;
    
    console.log('🔧 [数据转换] 找到连接:', source, connector, target);
    
    // 确保源节点和目标节点都存在于node_mappings中
    if (node_mappings[source] && node_mappings[target]) {
      const edge = {
        id: `edge-${edgeIndex++}`,
        source: source,
        target: target,
        type: 'smoothstep', // 使用平滑的边类型
        animated: false
      };
      edges.push(edge);
    } else {
      console.warn('🔧 [数据转换] 跳过无效边:', source, '->', target, 
        '(源节点存在:', !!node_mappings[source], '目标节点存在:', !!node_mappings[target], ')');
    }
  }
  
  console.log('🔧 [数据转换] 最终结果:');
  console.log('🔧 [数据转换] 节点数量:', nodes.length);
  console.log('🔧 [数据转换] 边数量:', edges.length);

  return { nodes, edges };
};
</file>

<file path="frontend/src/utils/flowDiagramExample.js">
import React, { useRef } from 'react';
import FlowDiagram from '../components/FlowDiagram';

/**
 * FlowDiagram使用示例
 * 展示如何替换现有的MermaidDiagram组件
 */
const FlowDiagramExample = () => {
  const flowDiagramRef = useRef(null);

  // 示例Mermaid代码
  const sampleMermaidCode = `
    graph TD
      A[开始分析] --> B[识别核心论点]
      B --> C[分析论证结构]
      C --> D[评估证据质量]
      D --> E[检查逻辑关系]
      E --> F[得出结论]
      F --> G[撰写总结]
      
      B --> H[识别反对观点]
      H --> I[分析反驳策略]
      I --> E
  `;

  // 节点点击处理函数（与MermaidDiagram兼容）
  const handleNodeClick = (nodeId, event) => {
    console.log('节点被点击:', nodeId, event);
    alert(`点击了节点: ${nodeId}`);
  };

  // 测试ref方法
  const handleEnsureVisible = () => {
    if (flowDiagramRef.current) {
      // 调用与MermaidDiagram兼容的方法
      flowDiagramRef.current.ensureNodeVisible('C');
    }
  };

  const handleFitView = () => {
    if (flowDiagramRef.current) {
      flowDiagramRef.current.fitView();
    }
  };

  return (
    <div style={{ width: '100%', height: '600px', padding: '20px' }}>
      <h2>React Flow 图表示例</h2>
      
      <div style={{ marginBottom: '10px' }}>
        <button onClick={handleEnsureVisible} style={{ marginRight: '10px' }}>
          聚焦到节点C
        </button>
        <button onClick={handleFitView}>
          适应视图
        </button>
      </div>

      <div style={{ width: '100%', height: '500px', border: '1px solid #ccc' }}>
        <FlowDiagram 
          ref={flowDiagramRef}
          code={sampleMermaidCode}
          onNodeClick={handleNodeClick}
          layoutOptions={{
            direction: 'TB', // 上到下布局
            nodeSpacing: 100,
            rankSpacing: 150
          }}
        />
      </div>
    </div>
  );
};

/**
 * 在现有项目中替换MermaidDiagram的步骤：
 * 
 * 1. 导入新组件：
 *    import FlowDiagram from './FlowDiagram';
 *    // 替换原来的：
 *    // import MermaidDiagram from './MermaidDiagram';
 * 
 * 2. 替换组件使用：
 *    <FlowDiagram 
 *      ref={mermaidDiagramRef}  // 保持相同的ref
 *      code={document.mermaid_code_demo}  // 保持相同的props
 *      onNodeClick={handleNodeClick}      // 保持相同的回调
 *    />
 * 
 * 3. ref方法保持兼容：
 *    - ensureNodeVisible(nodeId) 
 *    - fitView()
 *    - getReactFlowInstance() (新增)
 * 
 * 4. 在ViewerPageRefactored.js中的具体替换：
 *    找到这行：
 *    <MermaidDiagram 
 *      ref={mermaidDiagramRef}
 *      code={document.mermaid_code_demo}
 *      onNodeClick={handleNodeClick}
 *    />
 *    
 *    替换为：
 *    <FlowDiagram 
 *      ref={mermaidDiagramRef}
 *      code={document.mermaid_code_demo}
 *      onNodeClick={handleNodeClick}
 *    />
 */

export default FlowDiagramExample;
</file>

<file path="frontend/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./src/**/*.{js,jsx,ts,tsx}",
  ],
  theme: {
    extend: {
      colors: {
        primary: {
          50: '#eff6ff',
          100: '#dbeafe',
          200: '#bfdbfe',
          300: '#93c5fd',
          400: '#60a5fa',
          500: '#3b82f6',
          600: '#2563eb',
          700: '#1d4ed8',
          800: '#1e40af',
          900: '#1e3a8a',
        },
      },
      typography: {
        DEFAULT: {
          css: {
            maxWidth: 'none',
            color: '#374151',
            a: {
              color: '#3b82f6',
              '&:hover': {
                color: '#2563eb',
              },
            },
            'h1, h2, h3, h4': {
              color: '#111827',
            },
            code: {
              color: '#111827',
              backgroundColor: '#f3f4f6',
              paddingLeft: '0.25rem',
              paddingRight: '0.25rem',
              paddingTop: '0.125rem',
              paddingBottom: '0.125rem',
              borderRadius: '0.25rem',
              fontSize: '0.875em',
            },
            'code::before': {
              content: '""',
            },
            'code::after': {
              content: '""',
            },
          },
        },
      },
    },
  },
  plugins: [
    require('@tailwindcss/typography'),
  ],
}
</file>

<file path="markdown.md">
写在前面：

本文是对 For they know not what they do 第五章第一节“Why Should a Dialectician Learn toCount to Four?”（P179-197）重新翻译。译者水平有限，故欢迎并希望读者指出其中不通顺与不理解地方并提出修改建议，先行感谢。

蓝色部分是翻译有待进一步改进的地方。斜体为原文所加，而深蓝色加粗的斜体省略号用以表示在本译文所略去未翻的文段（别问为什么不翻全，问就是懒[滑稽]）。术语对照表如下：

用语对照表  

<html><body><table><tr><td>actual</td><td>实际的</td></tr><tr><td>virtual</td><td>虚拟的</td></tr><tr><td>real</td><td>真实的</td></tr><tr><td>reality</td><td>现实</td></tr><tr><td>the real</td><td>真实、实在界</td></tr><tr><td>realization</td><td>实现</td></tr><tr><td>actualization</td><td>实际化</td></tr><tr><td>the symbolic</td><td>象征界、符号界</td></tr><tr><td>the imaginary</td><td>想象界</td></tr><tr><td>viewpoint</td><td>视点</td></tr><tr><td>normal</td><td>常态的</td></tr><tr><td>normality</td><td>常态</td></tr><tr><td>inversion</td><td>颠倒、倒置、反转</td></tr><tr><td>image</td><td>像、形象</td></tr><tr><td>reflexion</td><td>反射、反映（德古：折返、反思）</td></tr><tr><td>moment</td><td>环节 (德古梗)</td></tr><tr><td>excess</td><td>溢出/过剩</td></tr><tr><td>gap</td><td>裂隙</td></tr><tr><td>reduce</td><td>缩减</td></tr></table></body></html>

译者：Cicada

# 为什么一位辩证学家应该学着数到四？

三元组/三位一体与其溢出/过剩——新教、雅各宾主义……与其他“消失的中介者”—“你手指的一敲”……— 为什么真理总是政治性的？

·三元组/三位一体与其溢出/过剩

一位黑格尔派的辩证学家必须学着数到多少呢？大多数黑格尔的解释者，更不用说他的批评者，都试图一致地说服我们，正确的答案是：到三（辩证的三元组[the dialectical triad]，等等）。此外，他们还互相争夺谁能更有说服力地唤起我们对“第四方”的注意。这个第四方就是不可辩证的溢出/过剩，是死亡之处所（… ）。据说它逃离辩证法的掌握，尽管（或者更准确地说，因为）它是辩证法运动的内在可能性条件：在其结果（Result）中不能被扬弃[aufgehoben]、不能重新被收入的（re-collected）纯支出性的否定性。

不幸的是，按照这种对黑格尔的批评的惯常，那黑格尔在此的问题与希区柯克的电影《哈里》中的同名主人公的问题一样：他不那么容易地同意他的葬礼——仔细一看，以下情形很快就会变得显而易见：批评家从他们的帽子中得出的所谓毁灭性的责难实际上构成了辩证运动本身的关键方面。也就是说，细心的读者不仅会立即想起许多特殊的案例，像“主观逻辑”第一部分中的四种判断，而且还会想起这一事实——黑格尔将辩证法运动本身所特有的四重结构（quadruplicity）主题化了：在最终的结果（Result）中，那不复存在、变得不可见的自相关否定性的纯粹虚无的溢出/过剩（the excess of the pure nothingness ofself-relating negativity）。在《逻辑学》的最后一章关于辩证过程的基本矩阵中，他指出，随着主体作为“什么都不算（counts for nothing）”的多余环节（thesurplus-moment），这一过程的环节可以数到三或四。

在方法的这个转折点上，认识过程同时返回到自身去了。这个否定性作为自身扬弃着的（self-sublating）矛盾，是第一个直接性与简单普遍性的恢复；因为他者的他者、否定的否定，直接就是肯定（the positive）、同一（theidentical）、普遍（the universal）。如果有人坚持去数的话，这第二个直接之物（this second immediate），在作为整体的方法之过程中，相对于第一个直接之物（the first immediate）和受中介之物（the mediated），就是第三项。然而，它相对于第一个或形式的否定（the first or formal negative）与绝对的否定性或第二个否定来说，也是第三项；现在，由于第一个否定已经是第二项，被数作第三的项也可以数作第四，而抽象形式也可以被当作一个四重结构（quadruplicity），而不是三重结构（triplicity）；否定（the negative）或差异，便数作两重（duality）。1

译者根据所引用英译的翻译

在方法的这个转折点上，认识过程叉立刻转回到自身去了。这个否定性，作为自身扬弃的矛盾，是第一个直接性、即单纯普遍性之恢复；因为他物的他物、否定的否定，直接就是肯定的、同一的、普逼的。这第二个直接的东西，在整个过程中，\*假如人们总是愿意计数的话，对第一个直接的东西和对有中介的东西说，就是第三个东西。但它对第一个或说形式的否定幷对绝对的否定性或说第二个否定来说，也是第三个；如果那第一个否定已经是第二项，那末，那被数为第三的，也可以数作第四；\*抽象的形式也将不用三分法而被当作是一个四分法。否定的东西或区别，以这种方式，便数作两分。一一第三个或第四个总是第一个和第二个环节，即直接—《大逻辑（下）》杨一之译本 P544

第一个环节是起始点的直接肯定性（the immediate positivity of the startingpoint）；第二个环节，其中介，并非仅仅是其直接的反面、外在的对立面——它就出现在我们努力去把握第一个环节、直接之物（the immediate）、自在自为（inand for itself）、本身（as such）之时：通过这种方式，我们已经将其中介（mediatize），而它不知不觉中变成了自己的对立面。第二个环节因此不是第一个环节的否定（the negative），不是其他异性（otherness）；它作为第一个环节自己的他者、自身的否定（the negative），就是第一个环节：一旦我们构想这个抽象-直接的起始点（一旦我们规定其预设和含意的具体网络、阐释其内容），它就转变为它自己的对立面。甚至在最抽象的层次上，“无”也不是“有”的外在对立：我们仅仅是通过尝试对“有”这一概念进行具体说明与规定就到达了“无”。“内在否定性”这一基本的辩证思想：一个实体（entity）被否定、消逝进入（pass over into）其作为其本身潜能之发展的对立面。

… 这就是否定性必须被数两次的原因。为了在实际上否定其起始点，我们必须否定它自己的“内在否定（negation）”——在其内在否定中，它的内容成为了它的“真相”（法西斯主义虽然反对自由主义资本主义，但也不是它实际上的否定（negation）而仅仅是其“内部的”否定（negation）：因此，为了实际上否定自由主义资本主义，我们必须否定它的这个否定（negation））。这一第二个否定、自相关的（self-relating negation）否定或（如黑格尔会说的）反射进自身的他异性，是绝对否定性和“纯粹差异”的消失的点/没影点（vanishing point）——这一自相矛盾的环节就是第三个环节，因为它已经是消逝进它自己之他者的第一个环节。我们在这里所得到的东西也可以被概念化为一个回溯性决定（retroactive determination）的案例：当反对其彻底的否定（Negative）时，第一个环节本身回溯性地转变为其对立面。自在的资本主义（capitalism-in-itself）并不等同于反对共产主义的资本主义（capitalism-as-opposed-to-Communism）：当面对其解体的趋势时，资本主义如果要生存，就不得不“从内部（from within）”否定自己（进入法西斯主义）。这种辩证由阿多诺论音乐史时得到了阐述：

……2

在这里我们有了一个有关结构主义称作“缺席决定”的东西的范例：不和谐音（dissonances）出现之后，三全音（tritone）的意义改变了，因为三全音进一步的使用暗含有对不和谐音的否定（negation）——其新意义产生于：不和谐音的缺席（absence）现存于（is present in）对三全音的使用中。在其直接的在场/呈现（presence）中，三全音保持原样；其历史性的中介由这样一个事实来揭露：它恰 因它保持原样而改变3。今日“回归传统价值”这一呼吁的错误也在于此：传统价值因我们重建了它们而不再相同，因为它们合法化了的那个社会秩序是它们的对立面。4

现在，我们可以看到增补性的要素是如何出现的：一旦我们将直接之物（theimmediate）的否定（negation）添加给直接之物，这一否定（negation）就回溯性地改变了直接性（immediacy）的意义，所以我们虽然实际上仅拥有两个要素却必须数到三。或者，如果我们设想辩证过程的完整循环，这里只有三个“肯定（positive）”的环节（直接性、其中介和最后对被中介的直接性的复归）要去数——我们漏掉的是纯粹差异的那难以理解的剩余物（surplus），它虽使得整个过程得以进行却“什么也不算（counts for nothing）”；我们漏掉的是这一“实体的虚空”，（如黑格尔所言）它同时也是所有一切（all and everything）的“容器（receptacle [Rezeptakulum]）”。

# ·新教、雅各宾主义……

然而，在那对“辩证方法”进行惹人恼怒的抽象反映（abstract reflections）的最佳传统中，这种思考（ruminations）有着一种纯粹形式的本性；它们所缺乏的是与具体历史内容内在的相互联系（relatedness）。一旦我们到达这种层次，第四的剩余物-环节（surplus-moment）作为第二个环节（分裂、抽象对立）与最终结果[Result]（和解[reconciliation]）之间“消失的中介者”这种想法立刻获得了具体的轮廓——人们只需想想詹明信在其论马克斯·韦伯5的文章（这篇文章有关韦伯关于新教在资本主义崛起中的作用的理论）中阐明“消失的中介者”这一概念的方式。韦伯的这个理论经常被解读为（并且也是韦伯本人的意思）一种对马克思关于经济基础之首要性论点的批评：最终，韦伯的观点是，新教是资本主义的条件。相反，詹明信将韦伯的理论解释得完全符合马克思主义：韦伯的理论是对辩证必然性的详尽阐述——经由这种辩证必然性，“基础”和 “上层建筑” 的“正常”关系在封建主义进入资本主义的过程中被颠倒了。

这种辩证的必然性位于何处呢？换言之：具体来说，新教是怎样为资本主义的出现创造条件的？并非如人们会期待的那样，通过限制宗教意识形态的影响范围或通过动摇其在中世纪社会无处不在的特征，而是相反通过将其意义（relevance）普遍化：路德反对用一道鸿沟将修道院（cloisters）与礼拜（church）作为一种独立的制度（institution）同社会的其他部分隔绝开来，因为他希望基督教的态度能够渗透并决定我们整个的世俗日常生活。传统（前新教）的立场基本上将宗教的意义限制于我们必须趋向的目标，而将手段——世俗经济活动的领域——留给非宗教的共同判断。而与此相反，新教的“工作伦理”将非常世俗的活动（经济获益）设想为揭示上帝恩典的领域。

禁欲主义的地位变化可以例示这一转变以为例：在传统的天主教世界里，禁欲主义涉及一个分离于日常世俗生活的阶层，他们致力于在这个世界上描绘其来世，也就是地上天国（圣人、修道士的禁欲）。然而，新教要求每位基督徒在世俗生活中采取禁欲的行动——要积累财富而不要轻率花钱，要节制、谦逊地生活—要在“铭记上帝”的同时完成其工具性经济活动；因此作为一个阶层之事务的禁欲主义变得多余起来。

这种对基督教立场的普遍化，也即肯定基督教立场对于世俗经济活动的意义，滋生出了“新教工作伦理”的特征（将上瘾的工作（compulsive）和财富积累——声称放弃消费——作为在其自身的最终目的）；然而同时，它也在不知不觉中遵循着“理性的狡计”，开启了贬低宗教之路，将宗教限制于与国家和公共事务分离的私人领域。因此，新教对基督教立场的普遍化仅仅是通向资产阶级“常态”社会途中的暂时阶段，在这里宗教被缩减为“手段”，成为能使主体在为生存进行的经济斗争中发现新力量和毅力的媒介，就像那些 “自我经验”技巧一样，它们将我们与“真我（ture Self）”的遭遇服务于对我们的适当性（fitness）。

当然，我们很容易对新教的幻觉保持一种反讽的距离，并指出新教努力废除宗教与日常生活之间差距的最终结果是如何将宗教贬低为一种“治疗性（therapeutic）”的手段；更困难的则是要去构想新教作为中世纪社团主义和资本主义个人主义间“消失的中介者（vanishing mediator）”的必然性。换句话说，不可忽视的一点是，如果，人们不可能直接地，也就是缺少新教作为 “消失的中介者”的调解（intercession）而从中世纪的“封闭”社会进入资产阶级社会：正是新教通过其对基督教性（Christianity）的普遍化，为其撤回到私密领域预备了基础。

在政治领域，雅各宾主义扮演了同样的角色，它甚至可以被定义为“政治的新教”。…

在这里，我们也很容易保持一种反讽的距离，并指出雅各宾主义如何必然会通过将社会整体粗暴地缩减为抽象的平等原则而在恐怖主义中结束，因为这种缩减受到了分支的（ramified）具体关系之网的抵制（见黑格尔在《精神现象学》中对雅各宾主义的经典批评）。更难做到的是，要证明为什么不可能从旧制度直接进入自我本位的资产阶级日常生活——为什么，正是因为他们虚幻地将社会整体还原为民主政治方案，雅各宾主义是一个必要的“消失的中介者”（黑格尔批评得实际要点并不在于说雅各宾主义方案有乌托邦－恐怖主义特征这样的老生常谈中，而是在于此）。换句话说，在雅各宾主义中发现现代“极权主义”的根源和第一个形式是很容易的；而要完全承认和采纳没有雅各宾主义的“溢出/过剩”就不会有“常态的”多元民主这样一个事实则要更加困难并令人不安。6

也就是说，新教和雅各宾主义所陷入的幻觉，比乍看之下要复杂得多：它并不简单地在于他们对基督教或平等主义民主方案（egalitarian-democratic project）的那朴素道德主义式的普遍化，也就是说，并不简单地在于他们忽略了抵制这种直接普遍化的社会关系的具体财富（concrete wealth of social relations）。他们的幻觉要激进得多：它同所有在历史上相关的有关政治乌托邦的幻觉具有相同的本性。马克思在谈到柏拉图的国家（State）时提请我们注意这种幻觉，他说，柏拉图没有看到他事实上所描述的不是一个尚未实现的理想（ideal），而是现存希腊国家的基本结构。换句话说，乌托邦（utopias）之所以是“乌托邦的”，不是因为它们描绘了一个“不可能的理想（Ideal）”，一个不属于这个世界的梦想，而是因为它们没有认出它们的理想国（ideal state）在其基本内容方面（黑格尔会说，“在其概念方面”）如何已然实现了。

当社会现实被构造成一个“新教世界”的时候，新教就变得多余，可以作为一个中介消失了：资本主义公民社会的概念结构（notional structure）是一个由“贪得的禁欲主义”（“你拥有的越多，你就越要放弃消费”）这个悖论所定义的原子化个人的世界——也就是说，缺少新教之积极宗教形式而只有新教之内容的结构。雅各宾主义也是如此：雅各宾派所忽视的事实是，他们努力追求的理想在其概念结构中在“肮脏的”贪得活动（acquisitive activity）中已然实现，而这种活动在他们看来是对其崇高理想的背叛。庸俗的、利己主义的资产阶级日常生活是自由、平等和博爱的现实性（actuality）：自由贸易的自由，法律面前的形式平等，等等。

“消失的中介者”——新教徒、雅各宾主义——所特有的幻觉正是黑格尔式的“美丽灵魂”的幻觉：他们拒绝在他们所哀叹的腐败现实中承认他们自己的行为的最终结果——如拉康所说，他们自己的信息以其真实而颠倒的形式出现。而作为新教和雅各宾主义的“清醒的” 继承者，我们的幻觉也不少：我们把那些“消失的中介者”视为反常（aberrations）或溢出/过剩，没能注意到我们何以只是“没有雅各宾形式的雅各宾派”与“没有新教形式的新教徒”。

# ·……与其他消失的中介者

形式和其概念内容间的裂隙，也给我们提供了通向“消失的中介者”的必然性的关键：从封建主义到新教的路径与从新教到具有宗教私人化特征的资产阶级日常生活的路径没有相同的特征。第一个路径关系到“内容”（在保持或者甚至加强宗教形式的伪装下，发生了关键性的变化——经济活动中禁欲式贪得[asceticacquisitive]的态度被明确肯定为展示恩典的势力范围），而第二个路径则是一个纯粹形式的行动，一种形式的变化（一旦新教作为禁欲式贪得[ascetic-acquisitive]的态度得到实现，它就会作为形式而脱落）。

因此，“消失的中介者”之所以出现，是因为在一个辩证的过程中，形式停留在内容后面的方式：首先，关键性的转变发生在旧形式的限度内，甚至呈现出其复兴的主张这一外表（对基督教性的普遍化，回到其“真正的内容”，等等）；然后，一旦“精神的无声编织（silent weaving）”完成其工作，旧形式就会脱落。这一过程的双重节奏（scansion）扩展使我们能够具体地掌握“否定之否定

（negation of negation）”这一陈旧的公式：第一个否定在于实质性内容缓慢、秘密且无形的变化，而自相矛盾的是，这种变化发生在其自身形式的名义下的；那么，一旦形式失去了它的实质性权利（substantial right），它就会自己摔得粉碎——否定的形式被否定了，或者用黑格尔的经典对子来说，发生“在其自身中的”（in itself）变化变成了“对于其自身的”（for itself）【或，“自在”发生的变化变成了“自为的”——译注】。

我们应该进一步复杂化这副图景：仔细观察可以发现，在从封建政治结构到资产阶级政治结构的过程中，存在着两个“消失的中介者”：绝对君主制和雅各宾主义。第一个是有关一个悖论式妥协的标志与体现（embodiment）：这种政治形式使崛起的资产阶级能够通过打破封建主义、其行会和社团（corporations）的经济力量来加强其经济霸权——当然，它的自相矛盾之处在于，封建主义正是通过将自己的最高点（crowning point）绝对化——将绝对权力赋予君主——来“自掘坟墓”的；因此，绝对君主制的结果是政治秩序与经济基础相“分离”。同样的“脱节（disconnection）”也是雅各宾主义的特征：把雅各宾主义规定为一种激进意识形态已经是陈词滥调了，它“从字面上”接受了资产阶级的政治纲领（平等、自由、博爱[brotherhood]），并努力实现它，而不考虑同公民社会的具体衔接。

两者都为他们的幻想付出了沉重的代价：专制君主很晚才注意到，社会称赞他是万能的，只是为了让一个阶级推翻另一个阶级；雅各宾派一旦完成了摧毁旧制度的机器的工作，也就变得多余了。两者都被关于政治领域自主性（autonomy）的幻想所迷惑，都相信自己的政治使命：一个相信皇权的不可质疑性，另一个相信其政治方案的恰当性（pertinence）。在另一个层面上，我们不是也可以这样说法西斯主义和共产主义，即“实际现存的社会主义（actually existing socialism）”吗？法西斯主义难道不是一种资本主义固有的自我否定，不是试图通过一种使经济从属于意识形态-政治（ideological-political）领域的意识形态来“改变一些东西，以便没有真正的改变”吗？列宁主义的“实际存在的社会主义”难道不是一种“社会主义的雅各宾主义”，不是试图使整个社会经济生活从属于社会主义国家的直接政治调节吗？它两者都是“消失的中介者”，但进入了什么呢？通常的犬儒式答案“从资本主义回到资本主义”似乎有点太容易了……

因此，这种对“内容”（“经济基础”）与意识形态“形式”之“常态”关系的颠倒（如前所述，它使韦伯的反马克思主义解读成为可能），就在于上述作为“消失的中介者”之特点的“解放（emancipation）”——它将形式从其内容“解放”出来：新教同中世纪教会的决裂并不“反映”新的社会内容，而相反是以封建自身意识形态形式的激进版本的名义对旧的封建内容进行批判；正是这种将基督教形式从基督教自身社会内容中释放出来的“解放”，为旧内容逐渐转化为新（资本主义）内容开辟了空间。因此，詹明信很容易地证明了韦伯关于新教在资本主义兴起过程中的关键作用的理论如何仅仅影响到庸俗的经济主义，而又如何与“基础”和意识形态“上层建筑”的辩证法相当兼容。根据这种辩证法，人们通过一个“消失的中介者”从一个社会形态进入另一个社会形态，这种中介者颠倒了“基础”和“上层建筑”之间的关系：通过将自己从自己的“基础”中解放出来，旧“上层建筑”为“基础”的转变准备了地形（terrain）。经典马克思主义理论大厦就这样被拯救了，意识形态形式的“解放”从“基础”本身的内在对立中得到了解释：当这些对立变得如此激烈，以至于它们不再能被自己的意识形态形式合法化时，解放就出现了。

这种对意识形态上层建筑的“解放”有一个固有的悲剧性的伦理维度：它提出了一个独一无二的点，在这个点上，一种意识形态“在字面上接受了自身”并不再作为“客观上犬儒的”（马克思）对现存权力关系的合法化发挥作用。让我们提到另一个更当代的案例：在东欧“实际存在的社会主义”的最后几年间出现的“新社会运动”，这些运动的典范代表是前东德的“新论坛（Neues Forum）”：一群充满激情的知识分子，他们“认真对待社会主义”，准备冒一切风险来摧毁妥协制度，并用超越了资本主义和“实际存在的”社会主义的乌托邦式的“第三条道路”来取代它。他们真诚地相信并坚持他们不是在为恢复西方资本主义而工作，当然，事实证明这不过是一种没有实质内容的幻觉；然而，我们可以说，恰恰是这样（作为一种没有实质内容的彻底幻觉），它才在严格意义上是非意识形态的：它并没有以颠倒的意识形态形式“反映”任何实际的权力关系。

在这一点上，我们应该纠正马克思主义的公认文本（Vulgate），也就是说，不同于如下这种老生常谈：在一个社会形态的“颓废（decadence）”时期，意识形态会变得“犬儒”（接受“言”与“行”之间的差距，不再“相信自己”，不再被视为真理，而是把自己当作使权力合法化的纯粹工具性手段）。

可以说，恰恰是“颓废”时期为统治意识形态（ruling ideology）打开了“认真对待自己”的可能性，并有效地将自己与自己的社会基础对立起来（对于新教，基督宗教反对封建主义作为其社会基础，就像新论坛以“真正的社会主义”的名义反对现有的社会主义）。以这种方式，它就不知不觉地解开了致使自己最终毁灭的力量：一旦他们的工作完成，他们就会被“历史所淹没”（《新论坛》在选举中的得票率为 $3 \%$ ），一个新的“恶棍时代”来临了，那些在共产党镇压期间大多保持沉默的当权者，现在却把新论坛辱骂成“秘密的共产党员”。

·你手指的一敲……

“消失的中介者”实际上仅显现为一个中介者，一个介于两种“常态”事物状态之间的中间形象（figure）。然而，这种解读是唯一可能的解读吗？由“后马克思主义”政治理论（Claude Lefort, Ernesto Laclau）所阐述的概念装置允许另一种解读，而这种解读从根本上改变了视角。在这个领域中，“消失的中介者”这一环节被阿兰·巴迪欧7定义为“事件”（它有关已确立的结构）的环节：其真相在其中出现的环节、有关“开放性（openness）”的环节——一旦“事件”的爆发被制度化为一种新的肯定性（positivity），它就会消失，或者更确切地说，在字面上不可见了。

根据众所周知的老生常谈（它与通常的模式相反，并不是披着智慧外衣的愚蠢），“在事实之后”，往回看，历史（History）总是可以被解读为一个受规律支配的过程；被解读为一个关于各阶段的有意义的连续体；然而，就我们是其施动者（agents），被嵌入、卷入了这一进程而言，情况——至少在“某事正在发生”的转折点——似乎是开放而不确定的（undecidable），绝不是一个潜藏于其下的（underlying）必然性的展现：我们发现我们自己面临着责任，决定的重负压在我们的肩上。

让我们回顾一下十月革命：追溯起来，很容易将其置于更广泛的历史进程中，说明它是如何从俄国的具体情况——俄罗斯失败的现代化与同时存在的“现代性的岛屿”（在孤立地区高度发达的工人阶级）——中产生的。总之，就这个主题写一篇社会学论文并不难。然而，只要重读一下列宁、托洛茨基、孟什维克和其他参与者之间的激情论战，就会发现自己面对这种“客观”的历史叙述所失去的东西：在一种新情势——它可以说是迫使施动者（agents）在没有任何“历史发展的一般规律”保证的情况下去发明新的解决方案并做出前所未闻的行动——下决定的重负（the burden of decision）。

这一有关开放性（openness）的“不可能的”环节构成了主体性的环节：“主体”是一个名称，指的是那个被召唤的、突然间负有责任的深不可测（unfathomable）的 X，它在这样一个有关不确定性（undecidability）的时刻被抛入一个责任的位置，被抛入这关于决定（decision）的紧急事态之中。这就是我们解读黑格尔的这一命题——“真理（True）不仅要被理解为实体，而且同样要被理解为主体”8——不得不采取的方式：不仅要被理解为一个受某种隐藏的理性必然性支配的客观过程（即使这种必然性具有黑格尔式“理性的狡计”的），而且要被理解为一个被有关开放性／不确定性（undecidability）的环节所打断并审视（scan）的过程——主体的不可还原的偶然行为建立了一个新的必然性。根据一个著名的意见（doxa），辩证法使我们能够穿透诸偶然性的表面戏剧，达至在主体背后“操纵着表演”的根本的（underlying）理性必然性。一个恰当的黑格尔式的辩证运动几乎是这一程序的完全颠倒：它驱散了对“客观历史进程”的迷信并让我们看到它的起源：历史上的必然性出现的方式——它是一种实证化（positivization）、主体在一个开放的、不确定的情势下的根本偶然决定的一个“凝结（coagulation）”。根据定义，“辩证的必然性”总是事后的（après coup）必然性：一个适当的辩证解读质疑对“实际上发生的事情”的自我证明，并将其与没有发生的事情对立起来——也就是说，它认为没有发生的事情（一系列错过的机会、一系列“替代性历史”）是“实际上发生的事情”的构成部分。

因此，辩证法对“可能世界”这一问题式（problematic）的态度比它看起来更加具有悖论性：既然现在在我们的现实中发生的事情是一系列根本性的偶然行为的结果，那么定义我们的现实世界（actual world）的唯一恰当方式就是在其定义中包括对在其位置中所包含的“可能世界”的否定——我们失去的各种机会（opportunities）是我们之所是（what we are）的一部分，它们 qualify（在这个词的所有意义上 $\textcircled{1}$ 使其具备资格； $\textcircled{2}$ 修饰限定——译注]）了它。

然而，我们解读过去的视野是由我们所做的偶然行动所决定的，这些行为强加了对“必然性”的回溯性幻觉；由于这个原因，我们不可能占据一个纯粹元语言的中立位置，从那里我们可以纵览所有的“可能世界”。这意味着，由于定义我们自己的现实世界的唯一方法是借助它与它的替代选项的否定关系，我们永远无法确定（determine）我们实际生活于其中的世界。换句话说，把这个悖论发挥到极致：当然，只有一个世界是真正可能的，即我们实际生活于其中的世界，但由于我们无法获得一个中立观察者的位置，我们不知道这个世界是哪一个；我们不知道我们实际生活在哪个“可能世界”。问题的关键不是“我们永远无法得知什么机会是我们已然失去的”，而是我们永远无法真正得知知道什么是我们已然得到的。这种立场可能看起来很极端，难道我们不是可以在“他不知道自己的运气”这句我们用来指称某人不知道他有多幸运错过了一系列可能的灾难的日常用语中看出这一点吗？如果“辩证法”不意味着这一点，那么所有关于“实体即主体”的讨论最终都是无效的，而我们又回到了作为实质性必然性（substantialNecessity）的理性（Reason），在幕后操纵着……

正是面对这样的背景，我们才必须理解黑格尔有关“设定预设（positing ofpresuppositions）”的论题：这种回溯性的设定恰恰是必然性从偶然性中出现的方式。主体“设定其预设”的环节，正是他作为主体被抹去的环节，他作为中介者消失的环节：当主体的决定行为（act of decision）变成它的反面时的那个结束的环节；建立一个新的象征网络，而历史借助这一网络再次获得了线性演进的自我证明。让我们回到十月革命：其“预设”在它的胜利和新政权的巩固之后、形势的开放性再次丧失之时才被“设定”——以“客观观察者”的身份叙述事件的线性发展（确定苏维埃政权如何在其最薄弱的环节打破帝国主义链条并从而开启世界历史的新纪元，等等）在这个时候才又一次得以可能。在此严格的意义上，主体是一个“消失的中介者”：它的行为通过变得不可见而成功——通过在一个新的象征网络中“实证化（positivizing）”自己，它将自己定位在此网络中并在其中将自己解释为历史进程的结果，从而将自己降为其自身行为所产生的整体中的一个单纯的环节。看看斯大林主义的元语言的立场就知道了，在那里（与关于“无产阶级科学”等的陈词滥调相反），马克思主义理论在无产阶级这一侧的参与、它的“党性”、它的“选边站”，并未被构想为理论本身固有的东西——马克思主义者并没有从无产阶级的主观立场说话，他们从外部的、中立的、“客观的”立场“将他们的基本取向（orientation）建立在无产阶级的基础上”：

在上个世纪八十年代，马克思主义者和民粹派之间的斗争时期，俄国的无产阶级在人口中只占微不足道的少数，而个体农民则占人口的绝大多数。但无产阶级作为一个阶级在发展，而农民作为一个阶级在瓦解。正因为无产阶级作为一个阶级在发展，所以马克思主义者将他们的基本取向（orientation）建立在无产阶级的基础上。他们没有错，因为正如我们所知，无产阶级后来从一个微不足道的力量发展成为一流的历史和政治力量。9

当然，这里要问的关键问题是：在与民粹派斗争的时候，马克思主义者是从哪里说在他们选择无产阶级作为其方向的基础时，会受错误影响？显然是从一个外在的点：这个点包含了作为客观力量领域的历史进程，在其中，人们必须“小心不要弄错”，并且“被正义的力量所引导”——那些将会获胜的力量。简而言之，人们必须“赌对了马”。

以这种方式解读——也就是回溯性地解读——关于如何去行动的决定遵循“客观”评价：首先，我们从一个中立的、“客观的”立场看待情势；然后，在确定哪些是可能获胜的力量之后，我们决定“将我们的基本取向（orientation）建立在他们的基础上”……

然而，这种回溯性的叙事陷入了一种视角的幻觉：它错误地认识了一个关键的事实，那就是“决定的真正理由只有在决定被作出之后，才会显现出来”。10换句话说，只有对那些已经从无产阶级主观立场上说话的人，将我们的基本取向

（orientation）建立在无产阶级的基础上”的理由才会显现出来——或者，正如精明的神学家会说的，当然有很好的理由相信耶稣基督，但这些理由只有那些已经相信他的人才能完全理解。列宁主义关于帝国主义链条中“最薄弱环节”的著名理论也是如此：人们不会首先通过客观的方法确定哪个环节是最薄弱的，然后决定打击这个环节——那个决定的行动本身定义了“最薄弱环节”。这就是拉康所说的行动：可以说，这一行动定义了它自己的条件；回溯性地产生了证明其合理的理由：

对（那些指望客观评价条件的人）来说不可能的是，一种姿态可以创造出一些回溯性地证明其合理并使其变得恰当的条件。然而，有证据表明，这事发生了，而且目的不是为了看（正确看待事物），而是为了使自己足够盲目，以便能够行进在正确的路，即分离的路（the way that disperses）。11

这个行动因而是“述行性”的，在超出了（exceeds）“言语行为”的意义上：其述行性是“回溯性的”：它重新定义了其诸预设的网络。行动的回溯述行性这一“溢出/过剩”也可以借助黑格尔关于法律与其逾越（transgression）、犯罪的辩证法得到阐释：从一个象征性共同体的现存的、积极的（positive）法律的视角来看，一个行为根据定义是犯罪，因为它违反了其象征的限度，并引入了一个闻所未闻的（unheard-of）元素，使一切都颠倒了过来——在一个行为中既不存在韵律（rhyme）也没有理由；一个行为在其本性上是丑恶的，正如基督的出现在现有律法（Law）的守卫者眼中那样——也就是说，在基督被“基督教化”之前，成为基督教传统的新律法的一部分之前。辩证的起源使现有法律的“丑恶”起源再次可见…

… 12

辩证法将律法的这一被遗忘的反面带到了阳光下：律法本身与最高的犯罪越轨行为相一致的方式。当一个行动重新“缝合”它自己的过去、它自己的条件，消除了其“丑恶”特征的时候，它就“成功”了——这个行为是一个新的主人能指、那个补充性的“你手指的敲击”的出现，它奇迹般地把先前的混乱变成“新的和谐”：

Abeat of your finger onthedrumdischarges the soundsand beginsthenew harmony.   
Astep by you,and newmen arise and set ontheir march.   
Yourhead turnsaway:thenewlove!Yourheadturns back: thenewlove! (Rimbaud,Auneraison)

在“新的和谐”开始之后，新主人能指的彻底偶然的、“丑恶的”、深渊性的特征就失去了——例如，看看列宁在列宁主义（也包括斯大林主义）的偶像化传记中变成了一个“看到了一切并预料到了（foresaw）一切”的明智人物就知道了。这就是为什么只有在列宁主义崩溃后的今天，人们才有可能接近作为一名历史剧中的演员的列宁，他有能力做出未预料到的（unforeseen）举动，而这些举动，正如Leszek Kolakowski 的所简洁表达的那样，就是在正确的时间犯下正确的错误。13

·为什么真理总是政治性（political）的？

行动的概念直接相关于社会和政治（Social and Political）之间的关系——相关于“政治性（the Political）”和“政治（politics）”之间的区别，正如 Lefort14和Laclau15所阐述的那样：“政治”是一个独立的社会综合体（separate socialcomplex）、一个与其他子系统（经济、文化形式）相互作用的、被肯定规定的（positively determined）社会关系的子系统，而“政治性”[le Politique]则是有关开放性的、不确定的环节（此时，社会的结构性原则、社会契约的基本形式被质疑）——简而言之，就是通过建立“新和谐”的行动来克服全球危机的环节。因此，“政治性”的维度得到了双重的刻画：它是社会整体的一个环节，是其子系统中的一个，并且也是整体之命运在其中被决定——新的契约在其中被设计并缔结——的地带。16

在社会理论中，人们通常认为政治维度相对于社会（the Social）本身而言是次要的。在实证主义社会学中，政治是社会组织用以组织其自我调节的一个子系统；在经典马克思主义中，政治是社会阶级分化所导致的异化普遍性（alienatedUniversality）的独立领域（其基本含义是，无阶级社会将意味着作为一个独立领域的政治性（the Political）的终结）；甚至在一些“新社会运动”的意识形态中，政治性（the Political）被划定为国家权力的领域，公民社会必须组织其自卫调节机制反对它。针对这些概念，人们可以冒险提出这样的假设：社会的起源总是“政治性的（political）”——一个积极（positively）现存的社会体系只不过是一种形式，在这种形式中，一个彻底偶然之决定的否定性获得了（assumes）积极的（positive）、有规定的（determinate）实存。

雅各宾派，这些“消失的中介者”的最杰出者，“将政治绝对化”绝非意外；指责他们失败是因为他们想使政治这个各个社会子系统的其中之一成为整个社会大厦的结构性原则，就忽视了一个关键的事实，即对雅各宾派来说，政治层面不是许多子系统中的一个，而是指明了一种激进否定性的出现，它使社会结构的新基础成为可能——他们消失不是因为他们的虚弱，而是因为他们的成功——也就是说，他们消失的时候正是他们工作完成的时候。

用更加“符号学”的术语，我们可以说，作为子系统的政治是有关政治主体的隐喻，是作为主体的政治性（the Political）的隐喻：在被构成的社会空间中，占据了作为否定性的政治性的位置（place of the Political）的那个元素，否定性中止了它，并重新建立了它。换句话说，作为“子系统”的“政治”，作为社会的一个独立领域，在社会中表征着（represents）它自己被遗忘的基础，它起源于一种暴力的、深渊性的行动——它在社会空间中表征着如果这个空间要构成它自己就必须掉出来的东西。在这里，我们可以很容易地认识到拉康对能指的定义（即“为另一个能指表征着主体”的东西）：作为子系统的政治为其他所有社会子系统表征着政治性（主体）。这就是为什么实证主义社会学家拼命地试图说服我们政治只是一个子系统：这种劝说的语气绝望而急迫，似乎呼应了一种迫在眉睫的“爆炸”的危险，政治将再次“成为全部”——转变为“政治性的（political）”。

这种劝说有一种确定无疑的规范性寓意，给它增添了一种巫术的气息（air ofconjuration）：它必须保持为一个单纯的子系统……

因此，在对“消失的中介者”这一悖论的两种可能的解读中，要紧的是社会对抗性即否定性的地位：是说，否定性在社会空间中的出现仅仅是从一种肯定性（positivity）形式到另一种肯定性形式的路径中的一个中介人（intermediary），是描绘了从一种“常态（normality）”向另一种“常态”过渡之特征的“例外”；还是说，这种“常态”只不过是被遗忘的否定性的溢出/过剩（excess of negativity）的后果、“绅士化（gentrification）”？第二种解决方案颠覆了整个视角：“子系统”的稳定网络正是社会对立中的一极的霸权的形式，而“阶级和平”正是阶级斗争中一个阶级的霸权的标志……

一旦“子系统”的网络稳定下来，也就是说，一旦“新的和谐”建立起来，一旦新秩序（Order）“设定了它的预设”，“缝合了”它的场域，表征其起源的那个元素的隐喻性（metaphoricity）就失去了：这个元素被缩减为“其他元素中的一个”；它失去了占据着（彻底否定性的）“无”的位置的“一”（One whichholds the place of Nothing）这一特征。

现在我们可以回到臭名昭著的黑格尔三元组：主体是这个“消失的中介者”、这个第四环节，可以说，它颁布了自己的消失；它的消失正是衡量其“成功”的标准也是自我关联的否定性的虚空，一旦我们从其结果“回头”看这个过程，它就变得不可见了。对黑格尔三元组中这一溢出的第四环节的考察，使得我们能够在格雷马斯的“符号学矩阵”的背景下解读它：

![](https://cdn-mineru.openxlab.org.cn/extract/37e3a677-8d78-46c9-9253-53ee427260a8/25d5185a40bbe06833ea2b8159437fb9425fc19035b1e9993438a5354267689f.jpg)

必然性（necessity）和不可能性（impossibility）的对立本身溶解进入可能性（possibility）的领域（可以说，可能性是对必然性的“否定之否定”）——随之消失的是第四个术语，即绝不可能等同于可能（Possible）的偶然（theContingent）。在偶然性（contingency）中总存在某些“与实在界遭遇”的东西，某些前所未闻的实体的猛然出现，它违抗了人们对“可能”所持的既定场域的限度，而“可能”可以说是一种“温和的”、平和的偶然性，一种被拔掉了刺的偶然性。

例如，在精神分析中，真理属于偶然性的秩序17：我们在日常生活中过着无聊的生活，深陷于结构它的普遍的谎言（universal Lie）之中，而突然间，一些完全偶然的遭遇——朋友的一句闲话，我们目睹的一件事故——唤起了关于被压抑的旧创伤的记忆，打破了我们的自我欺骗。精神分析在这里是彻底反柏拉图的：普遍性是最卓越的虚假性（Falsity par excellence）的领域，而真理则是作为一种特殊的偶然遭遇出现的，这种遭遇使其“被压抑”的东西变得可见。18在“可能性”中所失去的维度正是这种有关真理之出现的创伤性的、无保证的（unwarranted）特性：当一个真理变得“可能”时，它失去了“事件”的特性，它变成了一个单纯的有关事实的（factual）准确性，从而成为统治性的普遍谎言的组成部分。19

现在我们可以看到，拉康的精神分析与罗蒂那种多元实用主义的“自由主义”有多远。拉康的最后一课不是真理（truths）的相对性和多元性，而是坚硬的、创伤性的事实，即在每一个具体的星丛中，真理（truth）必然会以某种偶然的细节出现。换句话说，尽管真理是依赖于语境的——尽管一般意义上的真理并不存在，有的总是某种情况的真理——但在每一个多元场域中都依然有一个阐明其真理并且本身不能被相对化的特殊的点；在这个确切的意义上，真理总是一。如果我们把“本体论”矩阵换成“义务论”矩阵，我们在这里的目标就会更清楚：

![](https://cdn-mineru.openxlab.org.cn/extract/37e3a677-8d78-46c9-9253-53ee427260a8/3dcd220f4366e5bd9cfd37929a776bfd8b5d735d04ef935d9bcb2ad8e2ab91d4.jpg)

我们甚至缺乏一个合适的术语来形容这个X，来形容这“不是命令的（notprescribed）”、“容许的（facultative）”，但又不是简单的“允许的（permitted）”东西的奇怪状态——例如，在精神分析疗法中出现了一些迄今为止被禁止的知识，这些知识对禁令进行了嘲弄，暴露了其隐藏机制，但并没有因此而变成一种中性的“允许（permissiveness）”。两者之间的区别涉及到对普遍秩序的不同关系：“允许（permissiveness）”是由它保证的（warranted），而这种保证在“你可以（may）……”的情况下是缺乏的，拉康称这种情况为scilicet：你可以知道（关于你的欲望的真相）——如果你为自己承担风险。这个scilicet 也许是批判性思维的最终追索。
</file>

<file path="package.json">
{
  "dependencies": {
    "repomix": "^1.0.0"
  }
}
</file>

<file path="requirements-web.txt">
# Core project dependencies
requests>=2.31.0
python-dotenv>=1.0.0
openai>=1.3.0
anthropic>=0.5.0
google-generativeai>=0.3.0
numpy>=1.24.0
scikit-learn>=1.3.0
aiohttp>=3.8.0
asyncio-throttle>=1.0.2

# Web backend dependencies
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
python-multipart>=0.0.6
python-jose[cryptography]>=3.3.0
passlib[bcrypt]>=1.7.4

# File processing
aiofiles>=23.2.0

# Development tools
pytest>=7.4.0
pytest-asyncio>=0.21.0
</file>

<file path="sample_input_document_as_markdown__durnovo_memo.md">
# The Future Anglo-German War Will Transform into an Armed Conflict Between Two Groups of Powers

## By Pyotr Nikolayevich Durnovo, February 1914

The central factor of our current period in world history is the rivalry between England and Germany. This rivalry must inevitably lead to an armed conflict between them, the outcome of which will likely be fatal for the defeated side. The interests of these two states are far too incompatible, and their simultaneous existence as great powers will, sooner or later, prove impossible.

Indeed, on one side stands an island nation whose global significance rests on its dominion over the seas, world trade, and countless colonies. On the other side stands a powerful continental power whose limited territory is insufficient for its growing population. Therefore, Germany has directly and openly declared that its future lies on the seas, has developed enormous world trade with miraculous speed, and built a formidable navy to protect it. With its famous "Made in Germany" mark, Germany has created a mortal danger to its rival's industrial and economic well-being.

Naturally, England cannot surrender without a fight, and a life-and-death struggle between it and Germany is inevitable. The impending armed conflict resulting from this rivalry cannot possibly be reduced to a duel between England and Germany. Their forces are too unequal, and at the same time, they are insufficiently vulnerable to each other.

Germany can incite rebellion in India, South America, and especially a dangerous uprising in Ireland, paralyze English maritime trade through privateering, and perhaps submarine warfare, thereby creating food supply difficulties for Great Britain. However, despite the boldness of German military leaders, they would hardly risk a landing in England unless a lucky chance helps them destroy or significantly weaken the English navy.

As for England, Germany is completely invulnerable to it. All that is accessible to England is to seize German colonies, halt German maritime trade, and, in the most favorable case, destroy the German navy, but nothing more--and this alone cannot force the opponent to make peace. Therefore, England will undoubtedly try to resort to its previously successful means and decide on armed intervention only after securing the participation of strategically stronger powers on its side.

Since Germany, in turn, will certainly not remain isolated, the future Anglo-German war will transform into an armed conflict between two groups of powers: one following German orientation and the other English.

## It Is Difficult to Discern Any Real Benefits Russia Has Gained from Its Rapprochement with England

### Until the Russo-Japanese War

Russian policy adhered to neither orientation. Since the reign of Emperor Alexander III, Russia had been in a defensive alliance with France, one solid enough to ensure joint action by both states in case of an attack on either one, but not so close as to obligate them to necessarily support with armed force all political actions and demands of the ally.

Simultaneously, the Russian court maintained traditionally friendly relations, based on family ties, with Berlin. Thanks to this configuration, peace between the great powers remained undisturbed for many years, despite the abundance of combustible material in Europe. France was protected from German attack by its alliance with Russia, Germany was protected from French revanchist aspirations by Russia's proven peacefulness and friendship, and Russia was protected from excessive Austro-Hungarian machinations in the Balkan Peninsula by Germany's need to maintain good neighborly relations with it.

Finally, isolated England, restrained by rivalry with Russia in Persia, traditional English diplomatic fears of our offensive movements toward India, and poor relations with France (particularly evident during the famous Fashoda incident), watched the strengthening of German naval power with alarm yet hesitated to take active measures.

The Russo-Japanese War fundamentally changed the relationships between the great powers and brought England out of its isolated position. Throughout the Russo-Japanese War, England and America maintained a favorable neutrality toward Japan, while we enjoyed equally benevolent neutrality from France and Germany. This would seem to have been the seed of the most natural political combination for us.

However, after the war, our diplomacy made a sharp turn and definitively set a course for rapprochement with England. France was drawn into England's political orbit, forming the Triple Entente group with predominant English influence, and a collision with the powers grouping around Germany became, sooner or later, inevitable.

What advantages did we expect from abandoning our traditional policy of distrust toward England and breaking our tested, if not friendly, then neighborly relations with Germany? Looking somewhat carefully and examining the events that occurred after the Portsmouth Treaty, it is difficult to discern any real benefits we have gained from rapprochement with England. The only plus--improved relations with Japan--can hardly be considered a consequence of Russian-English rapprochement.

In essence, Russia and Japan are created to live in peace, as they have absolutely nothing to divide. All of Russia's properly understood tasks in the Far East are fully compatible with Japan's interests. These tasks are essentially limited to very modest bounds. A too broad flight of fantasy by overzealous executors, which had no foundation in actual state interests on one side, and the excessive nervousness and sensitivity of Japan, which mistakenly took these fantasies for a consistently implemented plan on the other side, caused a collision that more skillful diplomacy could have avoided.

Russia needs neither Korea nor even Port Arthur. Access to the open sea is undoubtedly useful, but the sea itself is not a market; it is merely a route for more advantageous delivery of goods to consuming markets. Meanwhile, in our Far East, we have not and will not for a long time have valuables promising any significant profits from their export. There are no markets there for our exports. We cannot count on broadly supplying our export goods to either developed America (industrially and agriculturally), nor poor and also industrial Japan, nor even coastal China and more distant markets, where our exports would inevitably meet with goods from industrially stronger competing powers.

What remains is inner China, with which our trade is conducted primarily by land. Thus, an open port would contribute more to the import of foreign goods to us than to the export of our domestic products. On the other hand, Japan, whatever may be said, does not covet our Far Eastern possessions. The Japanese are by nature a southern people, and the harsh conditions of our Far Eastern region cannot tempt them.

It is known that even in Japan itself, the northern Yezo is sparsely populated; Japanese colonization is proceeding unsuccessfully even in the southern part of Sakhalin that was ceded to Japan by the Portsmouth Treaty. Having taken Korea and Formosa, Japan is unlikely to go further north, and its aspirations will likely be directed toward the Philippine Islands, Indochina, Java, Sumatra, and Borneo. At most, what it might strive for is to acquire, purely for commercial reasons, some further sections of the Manchurian railway.

In short, *peaceful coexistence*--I will say more--*close rapprochement* between Russia and Japan in the Far East is entirely natural, regardless of any English mediation. The ground for agreement suggests itself. Japan is not a wealthy country; maintaining both a strong army and a powerful fleet simultaneously is difficult for it.

Its island position pushes it toward strengthening specifically its naval power. An alliance with Russia would allow it to focus all its attention on the fleet, which is necessary given its already emerging rivalry with America, while leaving the protection of its interests on the mainland to Russia. On the other hand, we, having the Japanese fleet for naval defense of our Pacific coast, would be able to abandon our unrealistic dream of creating a navy in the Far East.

Thus, in terms of relations with Japan, rapprochement with England has brought us no real benefit. It has given us nothing in terms of strengthening our position in Manchuria, Mongolia, or even in the Uriankhai region, where the uncertainty of our position testifies that agreement with England has, in any case, not freed our diplomacy's hands. On the contrary, our attempt to establish relations with Tibet met with sharp resistance from England.

Our position in Persia has not changed for the better since the agreement. Everyone remembers our predominant influence in that country under Shah Nasr-ed-Din during the period of greatest tension in our relations with England. From the moment of rapprochement with the larivalry with America, while leaving the protection of its interests on the mainland to Russia.

From the moment of rapprochement with the latter, we found ourselves involved in a series of incomprehensible attempts to impose on the Persian population a constitution that was completely unnecessary for them, contributing to the overthrow of a monarch loyal to Russia.

In short, we not only gained nothing, but on the contrary, lost all along the line, destroying both our prestige and many millions of rubles, and even the precious blood of Russian soldiers, treacherously murdered and, to please England, not even avenged.

But the most negative consequences of rapprochement with England--and consequently a fundamental divergence with Germany--have manifested in the Near East. As is known, Bismarck once made the famous statement that for Germany the Balkan question was not worth the bones of a single Pomeranian grenadier.

Subsequently, Balkan complications began to attract incomparably more attention from German diplomacy, which took the "sick man" under its protection, but even then Germany long showed no inclination to risk relations with Russia over Balkan affairs. The proof is before us. Indeed, how easy it would have been for Austria, during the Russo-Japanese War and our subsequent turmoil, to realize its cherished aspirations in the Balkan Peninsula.

But Russia at that time had not yet tied its fate to England, and Austria-Hungary was forced to miss the most advantageous moment for its goals. However, it was enough for us to take the path of close rapprochement with England for the annexation of Bosnia and Herzegovina to immediately follow--which could have been accomplished so easily and painlessly in 1905 or 1906--then the Albanian question arose and the combination with Prince Wied.

Russian diplomacy tried to answer Austrian intrigues by forming the Balkan League, but this combination proved completely ephemeral.

Under what conditions will this collision occur, and what will be its likely consequences? The basic groupings in the future war are obvious: Russia, France, and England on one side, Germany, Austria, and Turkey on the other. It is more than likely that other powers will also take part in the war, depending on the various conditions under which war breaks out. But whether the immediate cause of war is a new collision of opposing interests in the Balkans, or a colonial incident like Algeciras, the basic grouping will remain the same. Italy, if it properly understands its interests, will not side with Germany. Due to political and economic reasons, Italy undoubtedly strives to expand its current territory, which can only be achieved at the expense of Austria on one side and Turkey on the other. Therefore, it is natural that Italy will not side with those who guarantee the territorial integrity of the states at whose expense it wishes to realize its aspirations.

Moreover, the possibility is not excluded that Italy might join the anti-German coalition if the fortunes of war inclined in its favor, in order to secure for itself the most advantageous conditions for participation in the subsequent division of spoils. In this respect, Italy's position aligns with the probable position of Romania, which will likely remain neutral until the scales of fortune tip to one side or the other. Then, guided by healthy political egoism, it will join the victors to be rewarded either at Russia's expense or at Austria's. Among other Balkan states, Serbia and Montenegro will undoubtedly side against Austria, while Bulgaria and Albania (if it has not formed at least an embryo of a state) will side against Serbia. Greece will probably remain neutral or side against Turkey, but only when the outcome is more or less predetermined. The participation of other states will be incidental, and we should fear Sweden, which will naturally be among our opponents.

Under such conditions, a struggle with Germany presents enormous difficulties for us and will require incalculable sacrifices. The war will not catch the opponent off guard, and the degree of its preparedness will probably exceed our most exaggerated expectations. One should not think that this preparedness stems from Germany's own desire for war. War is unnecessary for it, as long as it could achieve its goal--ending sole dominion over the seas--without it. But since this vital goal meets opposition from the coalition, Germany will not retreat from war and will certainly try to provoke it, choosing the most advantageous moment for itself.

## THE MAIN BURDEN OF WAR WILL FALL ON RUSSIA

The main burden of war will undoubtedly fall on us, since England is hardly capable of broad participation in a continental war, while France, poor in human material, given the colossal losses that will accompany war under modern conditions of military technology, will probably adhere to strictly defensive tactics. The role of the battering ram, breaking through the very thickness of German defense, will fall to us, and yet how many factors will be against us and how much strength and attention we will have to spend on them. From these unfavorable factors, we should exclude the Far East. America and Japan, the first by essence and the second by virtue of its current political orientation, are both hostile to Germany, and there is no basis to expect them to side with it.

Moreover, the war, regardless of its outcome, will weaken Russia and divert its attention to the West, which serves Japanese and American interests. Therefore, our rear is sufficiently secured from the Far East, and at most they will extort from us some concessions of an economic nature for their benevolent neutrality. The possibility is not excluded of America or Japan joining the anti-German side, but of course only as seizers of one or another poorly defended German colony. On the other hand, an outbreak of hostility against us in Persia is certain, along with probable unrest among Muslims in the Caucasus and Turkestan, and the possibility of Afghanistan moving against us in connection with the latter. Finally, we should foresee very unpleasant complications in Poland and Finland.

In Finland, an uprising will inevitably break out if Sweden proves to be among our opponents. As for Poland, we should expect that we will not be able to hold it during the war. When it falls into the power of our opponents, they will undoubtedly attempt to provoke an uprising, which is not very dangerous for us but which we will still have to count among the unfavorable factors, especially since the influence of our allies may induce us to take steps in our relations with Poland that are more dangerous than any open uprising.

Are we prepared for such a stubborn struggle as the future war of European peoples will undoubtedly prove to be? To this question, one must, without equivocation, answer innfold. Our young legislative institutions are largely to blame for this insufficiency, having taken a dilettantish interest in our defense, but far from grasping the full seriousness of the political situation developing under the influence of the orientation which, with society's sympathetic attitude, our Ministry of Foreign Affairs has followed in recent years.

Proof of this is the huge number of military and naval ministry bills remaining unconsidered, and in particular, the plan for organizing our state defense that was presented to the Duma under State Secretary Stolypin. Undisputedly, in the realm of troop training, we have, according to specialists, achieved substantial improvement compared to the time preceding the Japanese War. Our field artillery leaves nothing to be desired; the rifle is quite satisfactory; the equipment is comfortable and practical. However, it is also undisputed that there are essential deficiencies in our defense organization.

In this regard, we must first note the inadequacy of our military supplies, which cannot be blamed on the military department, since the planned procurement programs are far from fully implemented due to the low productivity of our factories. This insufficiency of ammunition supplies is significant because, given the embryonic state of our industry, we will not have the ability during the war to make up for identified shortfalls by domestic means. Furthermore, with both the Baltic and Black Seas closed to us, the import of defense items we lack from abroad will prove impossible.

Additionally, an unfavorable circumstance for our defense is its excessive dependence on foreign industry, which, combined with the cessation of any convenient foreign communications, will create a series of difficulties that are hard to overcome. The quantity of heavy artillery we have is far from sufficient, as proven by the experience of the Japanese War, and we have few machine guns. We have barely begun organizing our fortress defense, and even the Revel fortress, which protects access to the capital, is not yet complete. The network of strategic railways is insufficient, and the railways possess rolling stock that may be adequate for normal traffic but is inadequate for the colossal demands that will be placed on us in case of a European war. Finally, we should not lose sight of the fact that in the upcoming war, the most cultured, technically developed nations will be fighting. Every war has invariably been accompanied until now by new developments in military technology, and the technical backwardness of our industry does not create favorable conditions for us to adopt new inventions.

## GERMANY'S AND RUSSIA'S VITAL INTERESTS NOWHERE COLLIDE

All these factors are hardly being given due consideration by our diplomacy, whose conduct toward Germany is not devoid, to a certain degree, of even some aggressiveness that could excessively hasten the moment of armed collision with Germany, which, given the English orientation, is essentially inevitable.

But is this orientation correct, and does even a favorable period of war promise us such advantages that would compensate for all the difficulties and sacrifices inevitable in an exceptionally intense war?

The vital interests of Russia and Germany nowhere collide and provide a complete basis for peaceful coexistence between these two states. Germany's future lies on the seas, that is, where Russia, essentially the most continental of all the great powers, has no interests.

We have no overseas colonies and probably never will, and communication between different parts of the empire is easier by land than by sea. We do not feel an excess of population requiring territorial expansion, but even from the point of view of new conquests, what can victory over Germany give us?

Poznań, East Prussia? But why do we need these areas, densely populated by Poles, when we already have difficulty managing Russian Poles? Why revive centrifugal aspirations, not yet extinct in the Vistula region, by bringing into the Russian state restless Poznań and East Prussian Poles, whose national demands even the firmer German authority, compared to Russian, cannot suppress?

## IN THE REALM OF ECONOMIC INTERESTS, RUSSIAN BENEFITS AND NEEDS DO NOT CONTRADICT GERMAN ONES

| Region                  | Potential Acquisitions                    | Notes                                                                                      |
|------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------|
| Transcaucasus          | Armenian-populated areas                  | Desirable due to revolutionary sentiments and dreams of a great Armenia. |
| Persia                 | Economic and territorial expansion        | Interests do not collide with Germany. |
| Kashgaria              | Economic and territorial expansion        | Interests do not collide with Germany. |
| Urianhai region        | Economic and territorial expansion        | Interests do not collide with Germany. |
| Vistula region         | Areas of little value, poorly suited for colonization | Polish-Lithuanian population is restless and hostile to Germans. |
| Baltic provinces       | Areas of little value, poorly suited for colonization | Latvian-Estonian population is equally restless and hostile to Germans. |

But one might object that territorial acquisitions, under modern conditions of national life, take second place and economic interests come to the fore. However, even in this realm, Russian benefits and needs hardly contradict German ones as much as is commonly thought. It is beyond doubt that the current Russian-German trade treaties are disadvantageous for our agriculture and advantageous for German agriculture, but it is hardly correct to attribute this circumstance to Germany's cunning and unfriendliness.

We should not lose sight of the fact that these treaties are advantageous to us in many of their parts. The Russian delegates who concluded these treaties at the time were convinced supporters of developing Russian industry at whatever cost and, undoubtedly, consciously sacrificed, at least partially, the interests of Russian agriculture in favor of Russian industry's interests.

Furthermore, we must not lose sight of the fact that Germany itself is far from being a direct consumer of most items of our agricultural foreign exports. For most products of our agricultural industry, Germany is only an intermediary, and consequently, it depends on us and the consuming markets to establish direct relations and thereby avoid expensive German intermediation.

Finally, it is necessary to consider that the conditions of trade relations can change depending on the conditions of political coexistence between the contracting states, since no country benefits from the economic weakening of an ally, but conversely benefits from the ruin of a political opponent.

In short, although it is undoubtedly true that the current Russian-German trade treaties are disadvantageous for us and that Germany, in concluding them, successfully exploited circumstances that developed favorably for it--that is, simply put, squeezed us--this behavior cannot be counted as hostile and is a worthy-of-emulation act of healthy national egoism, which could not have been unexpected from Germany and which should have been taken into account.

In any case, we see in Austria-Hungary an agricultural country in incomparably greater economic dependence on Germany than we are, which nevertheless does not prevent it from achieving such development in agriculture as we can only dream about. Given all the above, concluding a trade treaty with Germany that is fully acceptable for Russia would seem to not at all require Germany's prior defeat. Good neighborly relations with it, thoughtful weighing of our real economic interests in various branches of the national economy, and long persistent bargaining with German delegates, who are undoubtedly called to protect the interests of their, not our, fatherland, are quite sufficient.**Economic Relations with Germany: Analysis and Recommendations**

Germany's actions, which successfully exploited circumstances that developed favorably for it--squeezing us in the process--cannot be counted as hostile. This behavior exemplifies a worthy act of healthy national egoism, which, while not unexpected from Germany, should have been taken into account.

In any case, we observe that Austria-Hungary, an agricultural country with incomparably greater economic dependence on Germany than we have, nevertheless achieves agricultural development that we can only dream about. Given this context, concluding a trade treaty with Germany that is fully acceptable to Russia does not necessarily require Germany's prior defeat. Good neighborly relations, thoughtful consideration of our real economic interests across various branches of the national economy, and prolonged negotiations with German delegates--who are undoubtedly tasked with protecting the interests of their own country--are quite sufficient.

I will elaborate: Germany's defeat in terms of our trade exchange would be disadvantageous for us. Such a defeat would likely culminate in a peace dictated by England's economic interests. England would exploit the situation to the fullest extent, and in a devastated Germany that has lost its sea routes, we would lose a valuable consumer market for our products, which have no other outlet.

Regarding Germany's economic future, the interests of Russia and England are directly opposed to one another. It is advantageous for England to undermine German maritime trade and industry, reducing Germany to a poor agricultural country if possible. Conversely, it is in our interest for Germany to develop its maritime trade and the industries that support it, thereby opening its internal market to our agricultural products to supply its numerous working population.

However, independent of trade treaties, there are concerns regarding the dominance of German influence in Russian economic life and the systematic implementation of German colonization, which is purportedly a clear danger to the Russian state. These fears, however, seem largely exaggerated. The infamous Drang nach Osten was a natural and understandable phenomenon at the time, as Germany's territory could not accommodate its growing population, which was pushed toward the direction of least resistance--namely, into the less densely populated neighboring countries.

The German government was compelled to acknowledge the inevitability of this movement but could hardly see it as serving its interests. After all, German citizens were departing the sphere of German statehood, thereby diminishing their country's living strength. Of course, the German government made every effort to maintain the settlers' connection with their former homeland and even allowed the possibility of dual citizenship.

Nonetheless, it is undeniable that a significant number of German emigrants ultimately settled permanently and irrevocably in their new locations, gradually severing ties with their homeland. This situation, clearly unaligned with Germany's state interests, likely motivated it to pursue a path of colonial policy and maritime trade, which was previously foreign to it.

As German colonies proliferate and German industry and maritime trade develop in connection with them, the wave of German colonists is receding. The day is not far off when the Drang nach Osten will become a matter of historical memory.

In any case, German colonization, which undoubtedly contradicts our state interests, must be halted, and friendly relations with Germany do not preclude this action. Advocating for a preference for German orientation does not imply endorsing Russia's vassal dependency on Germany. While maintaining friendly, neighborly ties with Germany, we must not sacrifice our state interests for this goal.

Germany itself would likely not object to efforts aimed at curtailing the further influx of German colonists into Russia. It is more advantageous for Germany to direct the wave of migration to its colonies. Furthermore, even in the absence of colonies and when German industry did not fully employ the population, the German government did not protest against the restrictive measures instituted during Alexander III's reign regarding foreign colonization.

Regarding German dominance in our economic life, this phenomenon hardly warrants the reproaches typically directed at it. Russia is too impoverished in both capital and industrial enterprise to manage without a broad influx of foreign capital. Thus, some dependence on foreign capital is inevitable until our industrial capabilities and the material means of the population develop sufficiently to eliminate the need for foreign entrepreneurs and their investments.

While we require foreign capital, German capital is more advantageous for us than any other option. Primarily, this capital is the least expensive, as it demands lower rates of entrepreneurial profit. This largely accounts for the comparative affordability of German products and their gradual displacement of English goods from the global market.

The lower profitability expectations associated with German capital allow it to enter ventures that other foreign investments might avoid due to relatively low profitability. Consequently, the influx of German capital into Russia results in smaller outflows of entrepreneurial profits compared to English and French capital, thus retaining more Russian rubles within the country.

Moreover, a significant portion of the profits generated by German capital invested in Russian industry does not leave Russia but is reinvested domestically. Unlike English or French capitalists, German capitalists tend to move to Russia along with their investments. This characteristic explains the notable presence of German industrialists, factory owners, and manufacturers compared to their English and French counterparts. The latter typically remain abroad, extracting profits from Russia without reinvesting in the country. In contrast, German entrepreneurs often reside in Russia for extended periods and frequently settle there permanently.

Even if we acknowledge the necessity of eradicating German dominance in our economic life--perhaps even at the cost of completely expelling German capital from Russian industry--it seems that such measures could be enacted without resorting to war with Germany.

The costs associated with such a war would far exceed any dubious benefits we might gain from liberation from German dominance. Moreover, the aftermath of this war would create an economic situation where the burden of German capital would feel light by comparison. It is beyond doubt that the war would necessitate expenditures that exceed Russia's limited financial resources, forcing us to seek credit from allied and neutral states, which would not be granted freely.

The potential consequences of an unsuccessful war are dire. The financial and economic ramifications of defeat cannot be calculated or foreseen and would likely lead to the complete collapse of our national economy. Even a victory would yield unfavorable financial prospects: a thoroughly devastated Germany would be unable to compensate us for our incurred costs. A peace treaty dictated by England's interests would prevent Germany from recovering sufficiently to cover our military expenses in the future. Any resources we might manage to extract would need to be shared with our allies, resulting in a meager portion for us compared to our military expenditures.

Additionally, the repayment of war loans would come under pressure from our allies. After the collapse of German power, our significance to them would diminish. Furthermore, our enhanced political power due to victory may prompt them to undermine us economically. Thus, even after a victorious conclusion to the war, we might find ourselves in a financial predicament with our creditors, making our current dependence on German capital seem ideal by comparison.

However bleak the economic prospects presented by an alliance with England and, consequently, a war with Germany may appear, they remain secondary to the political consequences of this fundamentally unnatural alliance.

The conflict between Russia and Germany is deeply undesirable for both nations, as it undermines the monarchist principle that both represent in the civilized world, standing in opposition to the democratic principle embodied elsewhere.will have to be paid, not without pressure from our allies. After all, after the collapse of German power, we will no longer be needed by them. Moreover, our political might, increased due to victory, will induce them to weaken us, at least economically. Thus, inevitably, even after a victorious conclusion to the war, we will find ourselves in a state of financial bondage to our creditors, compared to which our current dependence on German capital will seem ideal.

The struggle between Russia and Germany is deeply undesirable for both sides, as it amounts to weakening the monarchist principle.

We should not lose sight of the fact that Russia and Germany represent the conservative principle in the civilized world, which stands in opposition to the democratic principle embodied by England and, to a far lesser degree, France.

However strange it may seem, England, monarchist and conservative to the core at home, has always acted externally as the patron of the most demagogic aspirations, invariably indulging all popular movements aimed at weakening the monarchist principle.

From this perspective, the struggle between Germany and Russia, regardless of its outcome, is deeply undesirable for both sides, as it undoubtedly amounts to weakening the global conservative principle, for which these two great powers serve as the only reliable bulwark.

Moreover, one cannot help but foresee that, under the exceptional conditions of the approaching general European war, this war, again regardless of its outcome, will present a mortal danger for both Russia and Germany.

Based on a deep conviction formed through careful, many-year study of all contemporary anti-state currents, a social revolution will inevitably break out in the defeated country, which, by force of circumstances, will spread to the victorious country.

The channels by which both countries have been invisibly connected over many years of peaceful coexistence are too numerous for fundamental social upheavals occurring in one of them not to be reflected in the other.

That these upheavals will be specifically social rather than political in character--there can be no doubt, and this applies not only to Russia but also to Germany.

Russia presents an especially favorable ground for social upheavals, where the masses undoubtedly lean toward the principles of unconscious socialism.

Despite the oppositional nature of Russian society, as unconscious as the socialism of the broad strata of the population may be, political revolution in Russia is impossible, and any revolutionary movement will inevitably degenerate into a socialist revolution.

The Russian commoner, be they peasant or worker, does not seek political rights, which are both unnecessary and incomprehensible to him.

The peasant dreams of being freely granted someone else's land, while the worker longs to receive all the capitalist's capital and profits, and their aspirations do not extend beyond this.

If the government were to deny them support and leave elections to their natural course, the legislative institutions would not see within their very walls a single intellectual, aside from a few agitator-demagogues.

However, members of our legislative institutions might proclaim the people's trust in them; the peasant would sooner believe a landless government official than a landowner-Octobrist sitting in the Duma.

It is more than strange, under such conditions, to demand that governmental authority seriously reckon with the opposition, for its sake abandon the role of impartial regulator of social relations, and present itself before the broad popular masses as an obedient organ of the class aspirations of the intellectual-propertied minority of the population.

By demanding from governmental authority responsibility before class representation and obedience to an artificially created parliament, our opposition essentially demands from the government the psychology of a savage who crafts an idol with his own hands and then worships it with trepidation.

## RUSSIA WILL BE PLUNGED INTO HOPELESS ANARCHY, WHOSE OUTCOME IS DIFFICULT TO FORESEE

If the war ends victoriously, suppressing the socialist movement will ultimately not present insurmountable difficulties.

There will be agrarian disturbances based on agitation for the necessity of rewarding soldiers with additional land allotments, and there will be labor unrest in transitioning from the probably elevated wartime wages to normal rates--and one must hope it will be limited to this, until the wave of German social revolution reaches us.

But in case of failure, the possibility of which, in struggling with such an opponent as Germany, cannot be ignored--social revolution in its most extreme manifestations is inevitable for us.

A furious campaign against the government will begin in legislative institutions, resulting in revolutionary outbursts in the country.

These latter will immediately advance socialist slogans, the only ones that can raise and group together broad strata of the population--first land redistribution, and then general division of all valuables and properties.

The defeated army, having lost, moreover, during the war its most reliable cadre composition, captured largely by the general peasant aspiration for land, will prove too demoralized to serve as a bulwark of law and order.

The totality of all the above leads to the conclusion that rapprochement with England promises us no benefits, and the English orientation of our diplomacy is fundamentally mistaken in its essence. We have no common path with England; it should be left to its fate, and we need not quarrel with Germany over it.

## The Triple Entente

In this direction, and not in the fruitless seeking of ground for agreement with England that contradicts our state views and goals by its very essence, all efforts of our diplomacy should be concentrated. Germany must meet our aspirations to restore tested friendly-allied relations with it and work out, in closest agreement with us, conditions for our coexistence that would not give ground for anti-German agitation from our constitutional-liberal parties, which by their very nature are forced to adhere not to conservative-German, but to liberal-English orientation.
</file>

<file path="sample_input_document_as_markdown__small.md">
Whether you need to pay income taxes on a legal settlement for an accident depends on the purpose of the settlement and what the payments are compensating for. Here’s a breakdown:

### **1. Personal Physical Injury or Sickness**
- **Tax-Free:** Settlements or awards compensating for physical injuries or physical sickness are generally not taxable under U.S. tax law (IRC §104(a)(2)).
- **Exceptions:** If you deducted related medical expenses in prior years (e.g., via itemized deductions), the portion reimbursed by the settlement for those expenses is taxable.

### **2. Emotional Distress or Mental Anguish**
- **Taxable:** Payments for emotional distress or mental anguish are taxable unless they stem directly from a physical injury or sickness. 
- **Non-Taxable:** If the emotional distress is caused by or directly related to a physical injury, the payment is non-taxable.

### **3. Lost Wages**
- **Taxable:** Compensation for lost wages or lost income is taxable because it replaces taxable earnings.

### **4. Punitive Damages**
- **Taxable:** Punitive damages are always taxable, regardless of whether the underlying case involves physical injury.

### **5. Interest on the Settlement**
- **Taxable:** Any interest earned on the settlement amount (e.g., due to delays in payment) is taxable.

### **6. Property Damage**
- **Generally Non-Taxable:** If you’re compensated for damage to property (e.g., vehicle repair or replacement), it’s typically non-taxable unless the payment exceeds your adjusted basis (the value of the property).

### **Key Considerations:**
- **Attorney’s Fees:** If you hired a lawyer on a contingency fee basis, the full settlement amount is often reported as income before deducting fees, which can complicate your tax situation.
- **State Laws:** Check state-specific rules, as state income tax treatment may vary.

Always consult a tax professional to confirm how your specific settlement is taxed, as details matter (e.g., whether the settlement agreement explicitly allocates payments to different types of damages).
</file>

<file path="screenshots/mindmap-architecture.svg">
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 500">
  <style>
    .box-text {
      display: flex; align-items: center; justify-content: center;
      text-align: center; padding: 4px; box-sizing: border-box;
    }
    .loop-text {
      display: flex; align-items: center; justify-content: center;
      text-align: center; padding: 4px; box-sizing: border-box;
    }
  </style>
  <rect width="800" height="500" fill="#F0F4F8" rx="10" ry="10" />
  <text x="400" y="40" font-family="Arial" font-size="24" font-weight="bold" text-anchor="middle" fill="#333">
    Mindmap Generator Architecture
  </text>
  <rect x="320" y="70" width="160" height="60" rx="12" ry="12" fill="#5DADE2" />
  <foreignObject x="320" y="70" width="160" height="60">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 16px; font-weight: bold; color: white;">
      Document Analysis Engine
    </div>
  </foreignObject>
  <rect x="50" y="170" width="140" height="50" rx="10" ry="10" fill="#58D68D" />
  <foreignObject x="50" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Document Input
    </div>
  </foreignObject>
  <rect x="250" y="170" width="140" height="50" rx="10" ry="10" fill="#EC7063" />
  <foreignObject x="250" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Document Type<br/>Detection
    </div>
  </foreignObject>
  <rect x="450" y="170" width="140" height="50" rx="10" ry="10" fill="#F4D03F" />
  <foreignObject x="450" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Topic<br/>Extraction
    </div>
  </foreignObject>
  <rect x="650" y="170" width="140" height="50" rx="10" ry="10" fill="#AF7AC5" />
  <foreignObject x="650" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Reality<br/>Check
    </div>
  </foreignObject>
  <rect x="50" y="280" width="140" height="50" rx="10" ry="10" fill="#EC7063" />
  <foreignObject x="50" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Subtopic<br/>Processing
    </div>
  </foreignObject>
  <rect x="250" y="280" width="140" height="50" rx="10" ry="10" fill="#F4D03F" />
  <foreignObject x="250" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Detail<br/>Extraction
    </div>
  </foreignObject>
  <rect x="450" y="280" width="140" height="50" rx="10" ry="10" fill="#48C9B0" />
  <foreignObject x="450" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Redundancy<br/>Filter
    </div>
  </foreignObject>
  <rect x="650" y="280" width="140" height="50" rx="10" ry="10" fill="#58D68D" />
  <foreignObject x="650" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Output<br/>Generation
    </div>
  </foreignObject>
  <rect x="230" y="390" width="100" height="40" rx="8" ry="8" fill="#9B59B6" stroke="#8E44AD" stroke-width="2" />
  <foreignObject x="230" y="390" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 12px; font-weight: bold; color: white;">
      Mermaid
    </div>
  </foreignObject>
  <rect x="350" y="390" width="100" height="40" rx="8" ry="8" fill="#9B59B6" stroke="#8E44AD" stroke-width="2" />
  <foreignObject x="350" y="390" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 12px; font-weight: bold; color: white;">
      HTML
    </div>
  </foreignObject>
  <rect x="470" y="390" width="100" height="40" rx="8" ry="8" fill="#9B59B6" stroke="#8E44AD" stroke-width="2" />
  <foreignObject x="470" y="390" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 12px; font-weight: bold; color: white;">
      Markdown
    </div>
  </foreignObject>
  <line x1="120" y1="170" x2="320" y2="100" stroke="#555" stroke-width="2" />
  <line x1="400" y1="130" x2="320" y2="170" stroke="#555" stroke-width="2" />
  <line x1="400" y1="130" x2="520" y2="170" stroke="#555" stroke-width="2" />
  <line x1="400" y1="130" x2="720" y2="170" stroke="#555" stroke-width="2" />
  <line x1="390" y1="195" x2="450" y2="195" stroke="#555" stroke-width="2" />
  <line x1="590" y1="195" x2="650" y2="195" stroke="#555" stroke-width="2" />
  <line x1="320" y1="220" x2="120" y2="280" stroke="#555" stroke-width="2" />
  <line x1="520" y1="220" x2="320" y2="280" stroke="#555" stroke-width="2" />
  <line x1="720" y1="220" x2="520" y2="280" stroke="#555" stroke-width="2" />
  <line x1="190" y1="305" x2="250" y2="305" stroke="#555" stroke-width="2" />
  <line x1="390" y1="305" x2="450" y2="305" stroke="#555" stroke-width="2" />
  <line x1="590" y1="305" x2="650" y2="305" stroke="#555" stroke-width="2" />
  <line x1="720" y1="330" x2="280" y2="390" stroke="#555" stroke-width="2" />
  <line x1="720" y1="330" x2="400" y2="390" stroke="#555" stroke-width="2" />
  <line x1="720" y1="330" x2="520" y2="390" stroke="#555" stroke-width="2" />
  <path d="M 720 220 C 760 250, 760 290, 720 330" fill="none" stroke="#555" stroke-width="2" stroke-dasharray="5,3" />
  <foreignObject x="740" y="250" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="loop-text" style="font-family: Arial; font-size: 10px; color: #555;">
      Verification Loop
    </div>
  </foreignObject>
  <path d="M 120 220 C 80 250, 80 290, 120 330" fill="none" stroke="#555" stroke-width="2" stroke-dasharray="5,3" />
  <foreignObject x="0" y="250" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="loop-text" style="font-family: Arial; font-size: 10px; color: #555;">
      Exploration Loop
    </div>
  </foreignObject>
</svg>
</file>

<file path="frontend/src/components/LogicalDivider.js">
import React from 'react';
import { GripVertical } from 'lucide-react';

const LogicalDivider = ({ nodeInfo, dragHandleProps }) => {
  const { title, color = 'gray' } = nodeInfo;
  
  // 根据颜色获取对应的Tailwind类
  const getColorClasses = (color) => {
    const colorMap = {
      gray: 'border-gray-300 dark:border-gray-600 bg-gray-50 dark:bg-gray-700 text-gray-600 dark:text-gray-400',
      blue: 'border-blue-300 dark:border-blue-600 bg-blue-50 dark:bg-blue-700 text-blue-600 dark:text-blue-400',
      green: 'border-green-300 dark:border-green-600 bg-green-50 dark:bg-green-700 text-green-600 dark:text-green-400',
      purple: 'border-purple-300 dark:border-purple-600 bg-purple-50 dark:bg-purple-700 text-purple-600 dark:text-purple-400',
      red: 'border-red-300 dark:border-red-600 bg-red-50 dark:bg-red-700 text-red-600 dark:text-red-400',
      yellow: 'border-yellow-300 dark:border-yellow-600 bg-yellow-50 dark:bg-yellow-700 text-yellow-600 dark:text-yellow-400'
    };
    return colorMap[color] || colorMap.gray;
  };

  return (
    <div className="relative my-6">
      {/* 水平分割线 */}
      <div className={`border-t-2 ${getColorClasses(color).split(' ')[0]} ${getColorClasses(color).split(' ')[1]}`} />
      
      {/* 拖拽手柄和标签容器 */}
      <div className="absolute top-0 left-1/2 transform -translate-x-1/2 -translate-y-1/2 flex items-center">
        {/* 拖拽手柄 - 始终可见 */}
        <div 
          className={`
            flex items-center justify-center w-6 h-6 rounded-full border-2 
            ${getColorClasses(color)}
            cursor-grab hover:cursor-grabbing
            shadow-sm
            opacity-70 hover:opacity-100
            transition-opacity duration-200
          `}
          {...(dragHandleProps || {})}
        >
          <GripVertical className="w-3 h-3" />
        </div>
        
        {/* 节点标签 */}
        <div className={`
          ml-2 px-3 py-1 rounded-full text-xs font-medium
          ${getColorClasses(color)}
          border-2 shadow-sm
          max-w-xs truncate
        `}>
          {title}
        </div>
      </div>
    </div>
  );
};

export default LogicalDivider;
</file>

<file path="frontend/src/hooks/useDocumentViewer.js">
import { useState, useEffect, useCallback } from 'react';
import { useParams } from 'react-router-dom';
import axios from 'axios';
import toast from 'react-hot-toast';

// 获取默认的演示流程图代码
const getDefaultDemoMermaidCode = () => `---
config:
  layout: dagre
  theme: redux
  look: neo
---
%%{ init: { 'flowchart': { 'curve': 'basis' } } }%%
flowchart TD
    %% --- Link Style ---
    linkStyle default stroke:#6A99C9,stroke-width:2px,stroke-dasharray:3 3,color:#000000

    A[为什么辩证学家要学着数到四？] --引论--> B{第四方：溢出/过剩的否定性环节}
    B --核心化为--> C[核心概念：消失的中介者]
    B --导向结论--> J[结论：主体作为消失的中介者]

    C --阐述机制--> D[消失的中介者的运作机制]
    C --举例说明--> E[实例分析]
    C --揭示特性--> F{中介者的幻觉：<br/>未认识到自身行为的真实结果}
    C --关联概念--> H[消失的中介者与事件及主体]

    D --阶段1--> D1[1. 旧形式的普遍化与激进化]
    D1 --阶段2--> D2[2. 新社会内容的形成]
    D2 --阶段3--> D3[3. 中介者形式的消失/变得多余]

    E --例证一--> E1[新教伦理: 封建主义 → 资本主义]
    E --例证二--> E2[雅各宾主义: 旧制度 → 资产阶级民主]
    E --其他例证--> E3[其他例子: 绝对君主制<br/>法西斯主义等]

    F --好比--> G[与美丽灵魂的类比]

    H --定义主体--> H1[主体：在开放/不确定时刻<br/>被召唤的X]
    H --引出--> I[真理的政治性]

    H1 --其行动--> H2[行动：回溯性地创造其<br/>合理性与条件]
    H2 --其结果--> H3[设定预设：主体行动成功后<br/>被整合进新秩序并变得不可见]

    I --具体为--> I1[区分政治与政治性]
    I1 --阐释--> I2[政治性：社会结构被质疑和重塑的<br/>开放性环节，真理在此显现]
    I2 --强调--> I3[社会秩序的起源总是政治性的]

    J --进一步阐释--> J1[主体是辩证过程的第四环节<br/>其消失是其成功的标志]
    J1 --关联至--> K[真理的偶然性与创伤性]

    K --通过类比--> K1[类比格雷马斯符号学矩阵<br/>与拉康精神分析]
    K1 --揭示真理--> K2[真理作为特殊的偶然遭遇<br/>打破普遍的谎言]

    %% 额外的分析框架
    A --理论基础--> L[黑格尔辩证法的四重结构]
    L --包含--> L1[1. 直接肯定性]
    L1 --导向--> L2[2. 内在否定性/中介]
    L2 --发展为--> L3[3. 否定的否定]
    L3 --完成于--> L4[4. 主体作为消失的环节]

    %% 历史实例的详细分析
    E1 --机制分析--> M1[新教：宗教普遍化→宗教私人化]
    E2 --机制分析--> M2[雅各宾：政治激进化→资产阶级日常生活]
    M1 --> M3[共同点：形式与内容的分离]
    M2 --> M3

    %% 现代相关性
    E3 --当代例证--> N[东欧新社会运动]
    N --特征--> N1[理想主义的第三条道路]
    N1 --结果--> N2[为资本主义复辟铺路]
    N2 --验证--> C

    A:::concept
    B:::concept
    C:::concept
    J:::conclusion
    D:::mechanism
    E:::example
    F:::highlight
    H:::concept
    D1:::mechanism
    D2:::mechanism
    D3:::mechanism
    E1:::example
    E2:::example
    E3:::example
    G:::default
    H1:::default
    I:::concept
    H2:::default
    H3:::default
    I1:::default
    I2:::default
    I3:::default
    J1:::default
    K:::concept
    K1:::default
    K2:::default
    L:::theory
    L1:::theory
    L2:::theory
    L3:::theory
    L4:::theory
    M1:::analysis
    M2:::analysis
    M3:::analysis
    N:::modern
    N1:::modern
    N2:::modern`;

export const useDocumentViewer = () => {
  const { documentId } = useParams();
  
  const [document, setDocument] = useState(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);
  
  // 文档查看模式相关状态
  const [viewMode, setViewMode] = useState('markdown');
  const [isPdfFile, setIsPdfFile] = useState(false);
  
  // 文档结构和目录相关状态
  const [documentStructure, setDocumentStructure] = useState(null);
  const [toc, setToc] = useState([]);
  const [expandedTocItems, setExpandedTocItems] = useState(new Set());

  const loadDocument = async () => {
    try {
      setLoading(true);
      setError(null);
      
      // 检查是否为纯示例模式（demo-前缀 + 时间戳ID）
      if (documentId.startsWith('demo-')) {
        const actualDocumentId = documentId.replace('demo-', '');
        console.log('🎨 [示例模式] 检测到示例模式，原始ID:', documentId, '实际ID:', actualDocumentId);
        
        // 检查是否为纯示例模式（基于时间戳的虚拟ID）
        if (actualDocumentId.length > 10 && /^\d+$/.test(actualDocumentId)) {
          console.log('📝 [纯示例模式] 检测到纯示例模式，显示预设内容');
          // 创建虚拟文档对象用于纯示例模式
          setDocument({
            document_id: actualDocumentId,
            content: null, // 标记为示例模式
            mermaid_code_demo: getDefaultDemoMermaidCode(),
            filename: '论证结构分析示例',
            file_type: '.md',
            pdf_base64: null,
          });
          setLoading(false);
          return;
        }
      }
      
      // 对于上传的文件，直接使用documentId
      const statusResponse = await axios.get(`http://localhost:8000/api/document-status/${documentId}`);
      
      if (statusResponse.data.success) {
        const docData = statusResponse.data;
        setDocument({
          document_id: docData.document_id,
          content: docData.content,
          content_with_ids: docData.content_with_ids, // 添加带段落ID的内容
          mermaid_code: docData.mermaid_code,
          mermaid_code_demo: docData.mermaid_code_demo,
          node_mappings_demo: docData.node_mappings_demo,
          filename: docData.filename,
          file_type: docData.file_type,
          pdf_base64: docData.pdf_base64,
        });
        
        // 检查是否为PDF文件
        const isPDF = docData.file_type === '.pdf';
        setIsPdfFile(isPDF);
        
        // 如果是PDF文件，默认显示转换后的Markdown
        if (isPDF) {
          setViewMode('markdown');
        }
        
        console.log('📄 [文档加载] 成功加载文档');
      } else {
        const response = await axios.get(`http://localhost:8000/api/document/${documentId}`);
        
        if (response.data.success) {
          const docData = response.data;
          setDocument({
            document_id: docData.document_id,
            content: docData.content,
            content_with_ids: docData.content_with_ids, // 添加带段落ID的内容
            mermaid_code: docData.mermaid_code,
            mermaid_code_demo: docData.mermaid_code_demo,
            node_mappings_demo: docData.node_mappings_demo,
            filename: docData.filename,
            file_type: docData.file_type,
            pdf_base64: docData.pdf_base64,
          });
        } else {
          setError('加载文档失败');
        }
      }
    } catch (error) {
      console.error('Load document error:', error);
      const errorMessage = error.response?.data?.detail || '加载文档失败，请检查网络连接';
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  const loadDocumentStructure = useCallback(async () => {
    try {
      console.log('📄 [开始加载] 开始加载文档结构，documentId:', documentId);
      
      // 获取文档结构信息（包含chunks）
      const structureResponse = await axios.get(`http://localhost:8000/api/document-structure/${documentId}`);
      console.log('📄 [结构响应]', structureResponse.data);
      
      if (structureResponse.data.success) {
        setDocumentStructure(structureResponse.data.structure);
        const chunks = structureResponse.data.chunks || [];
        
        console.log('📄 [文档结构] 成功加载了', chunks.length, '个内容块');
        console.log('📄 [内容块详情]', chunks.map(c => ({ 
          id: c.chunk_id, 
          heading: c.heading,
          content_length: c.content?.length || 0 
        })));
        
        // 如果有toc数据，也设置它
        if (structureResponse.data.toc) {
          setToc(structureResponse.data.toc);
          // 默认展开所有一级目录
          const topLevelItems = new Set(structureResponse.data.toc.filter(item => item.level === 1).map(item => item.id));
          setExpandedTocItems(topLevelItems);
          console.log('📄 [目录] 设置了', structureResponse.data.toc.length, '个目录项');
        } else {
          // 如果没有toc，尝试单独获取
          try {
            const tocResponse = await axios.get(`http://localhost:8000/api/document-toc/${documentId}`);
            if (tocResponse.data.success) {
              setToc(tocResponse.data.toc);
              const topLevelItems = new Set(tocResponse.data.toc.filter(item => item.level === 1).map(item => item.id));
              setExpandedTocItems(topLevelItems);
              console.log('📄 [目录单独加载] 成功加载', tocResponse.data.toc.length, '个目录项');
            }
          } catch (tocError) {
            console.warn('📄 [目录加载失败]', tocError);
          }
        }
        
        return chunks;
      } else {
        console.warn('📄 [结构加载失败]', structureResponse.data.message);
      }
    } catch (error) {
      console.error('📄 [加载文档结构错误]', error);
      if (error.response) {
        console.error('📄 [响应错误]', error.response.data);
      }
    }
    return [];
  }, [documentId]);

  const toggleTocItem = (itemId) => {
    setExpandedTocItems(prev => {
      const newSet = new Set(prev);
      if (newSet.has(itemId)) {
        newSet.delete(itemId);
      } else {
        newSet.add(itemId);
      }
      return newSet;
    });
  };

  useEffect(() => {
    loadDocument();
  }, [documentId]);

  return {
    documentId,
    document,
    setDocument,
    loading,
    error,
    viewMode,
    setViewMode,
    isPdfFile,
    documentStructure,
    toc,
    expandedTocItems,
    toggleTocItem,
    loadDocument,
    loadDocumentStructure
  };
};
</file>

<file path="frontend/src/hooks/useMindmapGeneration.js">
import { useState, useEffect } from 'react';
import { useLocation } from 'react-router-dom';
import axios from 'axios';
import toast from 'react-hot-toast';

export const useMindmapGeneration = (documentId, document, setDocument) => {
  const location = useLocation();
  
  const [demoMindmapStatus, setDemoMindmapStatus] = useState('not_started');
  const [autoStarted, setAutoStarted] = useState(false);

  // 默认演示流程图代码 - 现代化样式
  const defaultDemoMermaidCode = `---
config:
  layout: dagre
  theme: redux
  look: neo
---
%%{ init: { 'flowchart': { 'curve': 'basis' } } }%%
flowchart TD
    %% --- Link Style ---
    linkStyle default stroke:#6A99C9,stroke-width:2px,stroke-dasharray:3 3,color:#000000

    A[为什么辩证学家要学着数到四？] --引论--> B{第四方：溢出/过剩的否定性环节}
    B --核心化为--> C[核心概念：消失的中介者]
    B --导向结论--> J[结论：主体作为消失的中介者]

    C --阐述机制--> D[消失的中介者的运作机制]
    C --举例说明--> E[实例分析]
    C --揭示特性--> F{中介者的幻觉：<br/>未认识到自身行为的真实结果}
    C --关联概念--> H[消失的中介者与事件及主体]

    D --阶段1--> D1[1. 旧形式的普遍化与激进化]
    D1 --阶段2--> D2[2. 新社会内容的形成]
    D2 --阶段3--> D3[3. 中介者形式的消失/变得多余]

    E --例证一--> E1[新教伦理: 封建主义 → 资本主义]
    E --例证二--> E2[雅各宾主义: 旧制度 → 资产阶级民主]
    E --其他例证--> E3[其他例子: 绝对君主制<br/>法西斯主义等]

    F --好比--> G[与美丽灵魂的类比]

    H --定义主体--> H1[主体：在开放/不确定时刻<br/>被召唤的X]
    H --引出--> I[真理的政治性]

    H1 --其行动--> H2[行动：回溯性地创造其<br/>合理性与条件]
    H2 --其结果--> H3[设定预设：主体行动成功后<br/>被整合进新秩序并变得不可见]

    I --具体为--> I1[区分政治与政治性]
    I1 --阐释--> I2[政治性：社会结构被质疑和重塑的<br/>开放性环节，真理在此显现]
    I2 --强调--> I3[社会秩序的起源总是政治性的]

    J --进一步阐释--> J1[主体是辩证过程的第四环节<br/>其消失是其成功的标志]
    J1 --关联至--> K[真理的偶然性与创伤性]

    K --通过类比--> K1[类比格雷马斯符号学矩阵<br/>与拉康精神分析]
    K1 --揭示真理--> K2[真理作为特殊的偶然遭遇<br/>打破普遍的谎言]

    %% 额外的分析框架
    A --理论基础--> L[黑格尔辩证法的四重结构]
    L --包含--> L1[1. 直接肯定性]
    L1 --导向--> L2[2. 内在否定性/中介]
    L2 --发展为--> L3[3. 否定的否定]
    L3 --完成于--> L4[4. 主体作为消失的环节]

    %% 历史实例的详细分析
    E1 --机制分析--> M1[新教：宗教普遍化→宗教私人化]
    E2 --机制分析--> M2[雅各宾：政治激进化→资产阶级日常生活]
    M1 --> M3[共同点：形式与内容的分离]
    M2 --> M3

    %% 现代相关性
    E3 --当代例证--> N[东欧新社会运动]
    N --特征--> N1[理想主义的第三条道路]
    N1 --结果--> N2[为资本主义复辟铺路]
    N2 --验证--> C

    A:::concept
    B:::concept
    C:::concept
    J:::conclusion
    D:::mechanism
    E:::example
    F:::highlight
    H:::concept
    D1:::mechanism
    D2:::mechanism
    D3:::mechanism
    E1:::example
    E2:::example
    E3:::example
    G:::default
    H1:::default
    I:::concept
    H2:::default
    H3:::default
    I1:::default
    I2:::default
    I3:::default
    J1:::default
    K:::concept
    K1:::default
    K2:::default
    L:::theory
    L1:::theory
    L2:::theory
    L3:::theory
    L4:::theory
    M1:::analysis
    M2:::analysis
    M3:::analysis
    N:::modern
    N1:::modern
    N2:::modern`;

  // MindmapStatusDisplay 组件定义
  const MindmapStatusDisplay = () => {
    const getStatusInfo = () => {
      if (demoMindmapStatus === 'generating') {
        return { 
          text: '分析中...', 
          color: 'text-yellow-600',
          bgColor: 'bg-yellow-50',
          borderColor: 'border-yellow-200'
        };
      }
      
      if (demoMindmapStatus === 'error') {
        return { 
          text: '分析失败', 
          color: 'text-red-600',
          bgColor: 'bg-red-50',
          borderColor: 'border-red-200'
        };
      }
      
      if (demoMindmapStatus === 'completed' && document?.mermaid_code_demo) {
        return { 
          text: '论证结构已生成', 
          color: 'text-green-600',
          bgColor: 'bg-green-50',
          borderColor: 'border-green-200'
        };
      }
      
      return { 
        text: '未开始', 
        color: 'text-gray-600',
        bgColor: 'bg-gray-50',
        borderColor: 'border-gray-200'
      };
    };

    const statusInfo = getStatusInfo();
    
    return (
      <div className={`inline-flex items-center px-2 py-1 text-xs rounded border ${statusInfo.bgColor} ${statusInfo.color} ${statusInfo.borderColor}`}>
        {demoMindmapStatus === 'generating' && (
          <div className="animate-spin rounded-full h-3 w-3 border-b border-current mr-1"></div>
        )}
        {statusInfo.text}
      </div>
    );
  };

  const startMindmapGeneration = async (method = 'demo') => {
    try {
      // 如果是演示模式，直接设置演示代码或调用API
      if (method === 'demo') {
        setDemoMindmapStatus('generating');
        
        // 如果是真正的demo文档（以demo-开头但是时间戳形式），直接显示示例
        if (documentId.includes(Date.now().toString().slice(0, 8))) {
          // 模拟加载过程
          setTimeout(() => {
            setDocument(prev => ({
              ...prev,
              mermaid_code_demo: defaultDemoMermaidCode
            }));
            setDemoMindmapStatus('completed');
            toast.success('论证结构流程图加载完成！');
          }, 1000);
          
          toast.success('正在加载预设的论证结构示例...');
          return;
        }
        
        // 对于上传的文件，调用后端API
        const response = await axios.post(`http://localhost:8000/api/generate-argument-structure/${documentId}`);
        
        if (response.data.success) {
          toast.success('开始分析文档的论证结构...');
          
          if (response.data.status === 'completed' && response.data.mermaid_code) {
            setDemoMindmapStatus('completed');
            setDocument(prev => ({
              ...prev,
              mermaid_code_demo: response.data.mermaid_code,
              node_mappings_demo: response.data.node_mappings || {}
            }));
            toast.success('论证结构流程图生成完成！');
          }
        } else {
          throw new Error(response.data.message || '开始分析失败');
        }
      }
    } catch (error) {
      console.error(`Start argument structure generation error:`, error);
      
      setDemoMindmapStatus('error');
      toast.error('分析论证结构失败');
    }
  };

  // 文档加载完成后自动开始生成论证结构（只运行一次）
  useEffect(() => {
    if (document && !autoStarted && documentId.startsWith('demo-')) {
      setAutoStarted(true);
      setTimeout(() => {
        startMindmapGeneration('demo');
      }, 1000);
    }
  }, [document, autoStarted, documentId]);

  // 轮询检查论证结构生成状态
  useEffect(() => {
    let interval;
    if (demoMindmapStatus === 'generating' && !documentId.includes(Date.now().toString().slice(0, 8))) {
      interval = setInterval(async () => {
        try {
          // 对于上传的真实文档，直接使用documentId（已经不带demo-前缀了）
          const actualDocumentId = documentId;
            
          const response = await axios.get(`http://localhost:8000/api/document-status/${actualDocumentId}`);
          if (response.data.success) {
            if (response.data.status_demo === 'completed' && response.data.mermaid_code_demo) {
              setDemoMindmapStatus('completed');
              setDocument(prev => ({
                ...prev,
                mermaid_code_demo: response.data.mermaid_code_demo,
                node_mappings_demo: response.data.node_mappings_demo || {}
              }));
              toast.success('论证结构流程图生成完成！');
            } else if (response.data.status_demo === 'error') {
              setDemoMindmapStatus('error');
              toast.error('论证结构分析失败');
            }
          }
        } catch (error) {
          console.error('Status polling error:', error);
        }
      }, 2000);
    }

    return () => {
      if (interval) clearInterval(interval);
    };
  }, [demoMindmapStatus, documentId, setDocument]);

  const handleDownloadMarkdown = () => {
    if (!document || !document.content) return;
    
    try {
      const blob = new Blob([document.content], { type: 'text/plain;charset=utf-8' });
      const url = URL.createObjectURL(blob);
      
      if (typeof window !== 'undefined' && window.document && typeof window.document.createElement === 'function') {
        const a = window.document.createElement('a');
        a.href = url;
        a.download = `${document.filename || documentId}_content.md`;
        if (window.document.body) {
          window.document.body.appendChild(a);
          a.click();
          window.document.body.removeChild(a);
        }
      }
      
      URL.revokeObjectURL(url);
      toast.success('Markdown文档下载成功');
    } catch (error) {
      console.error('Download markdown error:', error);
      toast.error('下载失败：' + error.message);
    }
  };

  const handleDownloadMermaid = (mode = 'demo') => {
    if (!document || !document.mermaid_code_demo) return;
    
    try {
      const blob = new Blob([document.mermaid_code_demo], { type: 'text/plain' });
      const url = URL.createObjectURL(blob);
      
      if (typeof window !== 'undefined' && window.document && typeof window.document.createElement === 'function') {
        const a = window.document.createElement('a');
        a.href = url;
        a.download = `${documentId}_argument_structure.mmd`;
        if (window.document.body) {
          window.document.body.appendChild(a);
          a.click();
          window.document.body.removeChild(a);
        }
      }
      
      URL.revokeObjectURL(url);
      toast.success('论证结构流程图代码下载成功');
    } catch (error) {
      toast.error('下载失败：' + error.message);
    }
  };

  const handleOpenMermaidEditor = (mode = 'demo') => {
    if (!document || !document.mermaid_code_demo) return;
    
    try {
      const safeBtoa = (str) => {
        return btoa(unescape(encodeURIComponent(str)));
      };
      
      const mermaidConfig = {
        code: document.mermaid_code_demo,
        mermaid: { theme: 'default' }
      };
      
      const configJson = JSON.stringify(mermaidConfig);
      const encodedConfig = safeBtoa(configJson);
      const url = `https://mermaid.live/edit#pako:${encodedConfig}`;
      
      window.open(url, '_blank');
    } catch (error) {
      console.error('Error opening Mermaid editor:', error);
      
          const mermaidEditorUrl = `https://mermaid.live/edit#base64:${encodeURIComponent(document.mermaid_code_demo)}`;
    window.open(mermaidEditorUrl, '_blank');
      
      if (navigator.clipboard && navigator.clipboard.writeText) {
        navigator.clipboard.writeText(document.mermaid_code_demo).then(() => {
          toast.success('流程图代码已复制到剪贴板，可手动粘贴到编辑器中');
        }).catch(() => {
          toast.error('无法打开在线编辑器，请手动复制代码');
        });
      } else {
        toast.error('无法打开在线编辑器，请使用下载功能获取代码');
      }
    }
  };

  return {
    demoMindmapStatus,
    startMindmapGeneration,
    handleDownloadMarkdown,
    handleDownloadMermaid,
    handleOpenMermaidEditor,
    MindmapStatusDisplay
  };
};
</file>

<file path="frontend/src/index.css">
@import 'tailwindcss/base';
@import 'tailwindcss/components';
@import 'tailwindcss/utilities';

html, body {
  height: 100%;
  margin: 0;
  padding: 0;
}

#root {
  height: 100%;
}

body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}

* {
  box-sizing: border-box;
}
</file>

<file path="frontend/src/utils/api.js">
/**
 * API 工具函数
 */

const API_BASE_URL = process.env.REACT_APP_API_BASE_URL || '';

/**
 * 更新节点标签
 * @param {string} documentId - 文档ID
 * @param {string} nodeId - 节点ID
 * @param {string} newLabel - 新标签
 * @returns {Promise<Object>} API响应
 */
export const updateNodeLabel = async (documentId, nodeId, newLabel) => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${nodeId}/label`, {
      method: 'PATCH',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ newLabel })
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('❌ [API] 更新节点标签失败:', error);
    throw error;
  }
};

/**
 * 添加子节点
 * @param {string} documentId - 文档ID
 * @param {string} parentNodeId - 父节点ID
 * @param {string} newNodeLabel - 新节点标签
 * @returns {Promise<Object>} API响应
 */
export const addChildNode = async (documentId, parentNodeId, newNodeLabel = '新节点') => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${parentNodeId}/child`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ label: newNodeLabel })
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('❌ [API] 添加子节点失败:', error);
    throw error;
  }
};

/**
 * 添加同级节点
 * @param {string} documentId - 文档ID
 * @param {string} siblingNodeId - 同级节点ID
 * @param {string} newNodeLabel - 新节点标签
 * @returns {Promise<Object>} API响应
 */
export const addSiblingNode = async (documentId, siblingNodeId, newNodeLabel = '新节点') => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${siblingNodeId}/sibling`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ label: newNodeLabel })
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('❌ [API] 添加同级节点失败:', error);
    throw error;
  }
};

/**
 * 删除节点
 * @param {string} documentId - 文档ID
 * @param {string} nodeId - 要删除的节点ID
 * @returns {Promise<Object>} API响应
 */
export const deleteNode = async (documentId, nodeId) => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${nodeId}`, {
      method: 'DELETE',
      headers: {
        'Content-Type': 'application/json',
      }
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('❌ [API] 删除节点失败:', error);
    throw error;
  }
};

/**
 * 通用 API 错误处理
 * @param {Error} error - 错误对象
 * @returns {string} 用户友好的错误消息
 */
export const handleApiError = (error) => {
  if (error.name === 'TypeError' && error.message.includes('fetch')) {
    return '网络连接失败，请检查网络设置';
  }
  
  if (error.message.includes('404')) {
    return '请求的资源未找到';
  }
  
  if (error.message.includes('500')) {
    return '服务器内部错误，请稍后重试';
  }
  
  return error.message || '操作失败，请重试';
};
</file>

<file path="frontend/src/utils/layoutHelper.js">
import dagre from 'dagre';

// 定义节点的默认尺寸，与CSS保持一致
const DEFAULT_NODE_WIDTH = 200;
const DEFAULT_NODE_HEIGHT = 50;

/**
 * 使用Dagre算法计算节点布局
 * @param {Array} nodes - React Flow格式的节点数组
 * @param {Array} edges - React Flow格式的边数组
 * @param {Object} options - 布局选项
 * @returns {Object} 包含布局后的nodes和原始edges的对象
 */
export const getLayoutedElements = (nodes, edges, options = {}) => {
  console.log('🔧 [布局计算] 开始布局计算');
  console.log('🔧 [布局计算] 输入节点数量:', nodes.length);
  console.log('🔧 [布局计算] 输入边数量:', edges.length);
  console.log('🔧 [布局计算] 布局选项:', options);

  if (nodes.length === 0) {
    console.log('🔧 [布局计算] 没有节点，返回空数组');
    return { nodes: [], edges: [] };
  }

  try {
    // 创建有向图
    const graph = new dagre.graphlib.Graph();
    
    // 设置图的默认属性
    graph.setDefaultEdgeLabel(() => ({}));
    graph.setGraph({
      rankdir: options.direction || 'TB', // TB: 上到下, LR: 左到右
      nodesep: options.nodeSpacing || 100, // 节点间距
      ranksep: options.rankSpacing || 150, // 层级间距
      marginx: options.marginX || 50,
      marginy: options.marginY || 50
    });

    // 使用与CSS一致的节点尺寸
    const nodeWidth = options.nodeWidth || DEFAULT_NODE_WIDTH;
    const nodeHeight = options.nodeHeight || DEFAULT_NODE_HEIGHT;

    // 添加节点到图中
    nodes.forEach((node) => {
      graph.setNode(node.id, { width: nodeWidth, height: nodeHeight });
    });

    // 添加边到图中
    edges.forEach((edge) => {
      graph.setEdge(edge.source, edge.target);
    });

    // 计算布局
    dagre.layout(graph);
    console.log('🔧 [布局计算] Dagre布局计算完成');

    // 应用计算出的位置到节点
    const layoutedNodes = nodes.map((node) => {
      const nodeWithPosition = graph.node(node.id);
      
      if (!nodeWithPosition) {
        console.error('🔧 [布局计算] 节点位置计算失败:', node.id);
        return {
          ...node,
          position: { x: 0, y: 0 }
        };
      }

      const finalPosition = {
        // dagre返回的是节点中心点坐标，需要转换为左上角坐标
        // 确保坐标是数字类型
        x: Number(nodeWithPosition.x - nodeWidth / 2),
        y: Number(nodeWithPosition.y - nodeHeight / 2)
      };
      
      return {
        ...node,
        position: finalPosition,
        // 确保React Flow需要的其他属性
        width: nodeWidth,
        height: nodeHeight
      };
    });

    console.log('🔧 [布局计算] 布局计算完成，返回节点数量:', layoutedNodes.length);
    console.log('🔧 [布局计算] 所有节点位置:', layoutedNodes.map(n => ({ id: n.id, position: n.position })));

    return {
      nodes: layoutedNodes,
      edges: edges
    };
  } catch (error) {
    console.error('🔧 [布局计算] 布局计算失败:', error);
    // 如果布局计算失败，返回节点的默认位置
    const fallbackNodes = nodes.map((node, index) => ({
      ...node,
      position: { 
        x: (index % 3) * 250, // 简单的网格布局
        y: Math.floor(index / 3) * 150 
      }
    }));
    console.log('🔧 [布局计算] 使用回退布局:', fallbackNodes.map(n => ({ id: n.id, position: n.position })));
    return {
      nodes: fallbackNodes,
      edges: edges
    };
  }
};

/**
 * 重新布局现有的元素
 * @param {Array} nodes - 当前的节点数组
 * @param {Array} edges - 当前的边数组
 * @param {Object} options - 布局选项
 * @returns {Object} 重新布局后的nodes和edges
 */
export const relayoutElements = (nodes, edges, options = {}) => {
  return getLayoutedElements(nodes, edges, options);
};
</file>

<file path="mindmap_generator.py">
import re
import os
import random
import json
import time
import asyncio
import hashlib
import base64
import zlib
import logging
import copy
from datetime import datetime
from enum import Enum, auto
from typing import Dict, Any, List, Union, Optional, Tuple, Set
from termcolor import colored
import aiofiles
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from fuzzywuzzy import fuzz
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import Google Generative AI with error handling
try:
    import google.generativeai as genai
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    GOOGLE_AI_AVAILABLE = False
    genai = None

def get_logger():
    """Mindmap-specific logger with colored output for generation stages."""
    logger = logging.getLogger("mindmap_generator")
    if not logger.handlers:
        handler = logging.StreamHandler()
        
        # Custom formatter that adds colors specific to mindmap generation stages
        def colored_formatter(record):
            message = record.msg
            
            # Color-code specific mindmap generation stages and metrics
            if "Starting mindmap generation" in message:
                message = colored("🚀 " + message, "cyan", attrs=["bold"])
            elif "Detected document type:" in message:
                doc_type = message.split(": ")[1]
                message = f"📄 Document Type: {colored(doc_type, 'yellow', attrs=['bold'])}"
            elif "Extracting main topics" in message:
                message = colored("📌 " + message, "blue")
            elif "Processing topic" in message:
                # Highlight topic name and progress
                parts = message.split("'")
                if len(parts) >= 3:
                    topic_name = parts[1]
                    message = f"🔍 Processing: {colored(topic_name, 'green')} {colored(parts[2], 'white')}"
            elif "Successfully extracted" in message:
                if "topics" in message:
                    message = colored("✅ " + message, "green")
                elif "subtopics" in message:
                    message = colored("➕ " + message, "cyan")
                elif "details" in message:
                    message = colored("📝 " + message, "blue")
            elif "Approaching word limit" in message:
                message = colored("⚠️ " + message, "yellow")
            elif "Error" in message or "Failed" in message:
                message = colored("❌ " + message, "red", attrs=["bold"])
            elif "Completion status:" in message:
                # Highlight progress metrics
                message = message.replace("Completion status:", colored("📊 Progress:", "cyan", attrs=["bold"]))
                metrics = message.split("Progress:")[1]
                parts = metrics.split(",")
                colored_metrics = []
                for part in parts:
                    if ":" in part:
                        label, value = part.split(":")
                        colored_metrics.append(f"{label}:{colored(value, 'yellow')}")
                message = "📊 Progress:" + ",".join(colored_metrics)
            elif "Mindmap generation completed" in message:
                message = colored("🎉 " + message, "green", attrs=["bold"])
                
            # Format timestamp and add any extra attributes
            timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')
            log_message = f"{colored(timestamp, 'white')} {message}"
            
            # Add any extra attributes in grey
            if hasattr(record, 'extra') and record.extra:
                extra_str = ' '.join(f"{k}={v}" for k, v in record.extra.items())
                log_message += f" {colored(f'[{extra_str}]', 'grey')}"
                
            return log_message
            
        class MindmapFormatter(logging.Formatter):
            def format(self, record):
                return colored_formatter(record)
                
        handler.setFormatter(MindmapFormatter())
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger

logger = get_logger()

class Config:
    """Minimal configuration for document processing."""
    # API configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")  # 添加自定义base_url支持
    ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')  # Add Gemini API key
    API_PROVIDER = os.getenv('API_PROVIDER') # "OPENAI", "CLAUDE", "DEEPSEEK", or "GEMINI"
    
    # Model settings
    CLAUDE_MODEL_STRING = "claude-3-5-haiku-latest"
    OPENAI_COMPLETION_MODEL = "gpt-4o-mini-2024-07-18"
    DEEPSEEK_COMPLETION_MODEL = "deepseek-chat"  # "deepseek-reasoner" or "deepseek-chat"
    DEEPSEEK_CHAT_MODEL = "deepseek-chat"
    DEEPSEEK_REASONER_MODEL = "deepseek-reasoner"
    GEMINI_MODEL_STRING = "gemini-2.0-flash-lite"  # Add Gemini model string
    CLAUDE_MAX_TOKENS = 200000
    OPENAI_MAX_TOKENS = 8192
    DEEPSEEK_MAX_TOKENS = 8192
    GEMINI_MAX_TOKENS = 8192  # Add Gemini max tokens
    TOKEN_BUFFER = 500
    
    # Cost tracking (prices in USD per token)
    OPENAI_INPUT_TOKEN_PRICE = 0.15/1000000  # GPT-4o-mini input price
    OPENAI_OUTPUT_TOKEN_PRICE = 0.60/1000000  # GPT-4o-mini output price
    ANTHROPIC_INPUT_TOKEN_PRICE = 0.80/1000000  # Claude 3.5 Haiku input price
    ANTHROPIC_OUTPUT_TOKEN_PRICE = 4.00/1000000  # Claude 3.5 Haiku output price
    DEEPSEEK_CHAT_INPUT_PRICE = 0.27/1000000  # Chat input price (cache miss)
    DEEPSEEK_CHAT_OUTPUT_PRICE = 1.10/1000000  # Chat output price
    DEEPSEEK_REASONER_INPUT_PRICE = 0.14/1000000  # Reasoner input price (cache miss)
    DEEPSEEK_REASONER_OUTPUT_PRICE = 2.19/1000000  # Reasoner output price (includes CoT)
    GEMINI_INPUT_TOKEN_PRICE = 0.075/1000000  # Gemini 2.0 Flash Lite input price estimate
    GEMINI_OUTPUT_TOKEN_PRICE = 0.30/1000000  # Gemini 2.0 Flash Lite output price estimate

class TokenUsageTracker:
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost = 0
        self.call_counts = {}
        self.token_counts_by_task = {}
        self.cost_by_task = {}
        
        # Categorize tasks for better reporting
        self.task_categories = {
            'topics': ['extracting_main_topics', 'consolidating_topics', 'detecting_document_type'],
            'subtopics': ['extracting_subtopics', 'consolidate_subtopics'],
            'details': ['extracting_details', 'consolidate_details'],
            'similarity': ['checking_content_similarity'],
            'verification': ['verifying_against_source'],
            'emoji': ['selecting_emoji'],
            'other': []  # Catch-all for uncategorized tasks
        }
        
        # Initialize counters for each category
        self.call_counts_by_category = {category: 0 for category in self.task_categories}
        self.token_counts_by_category = {category: {'input': 0, 'output': 0} for category in self.task_categories}
        self.cost_by_category = {category: 0 for category in self.task_categories}
        
    def update(self, input_tokens: int, output_tokens: int, task: str):
        """Update token usage with enhanced task categorization."""
        # Update base metrics
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        
        # Calculate cost based on provider
        task_cost = 0
        if Config.API_PROVIDER == "CLAUDE":
            task_cost = (
                input_tokens * Config.ANTHROPIC_INPUT_TOKEN_PRICE + 
                output_tokens * Config.ANTHROPIC_OUTPUT_TOKEN_PRICE
            )
        elif Config.API_PROVIDER == "DEEPSEEK":
            # Different pricing for chat vs reasoner model
            if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
                task_cost = (
                    input_tokens * Config.DEEPSEEK_CHAT_INPUT_PRICE + 
                    output_tokens * Config.DEEPSEEK_CHAT_OUTPUT_PRICE
                )
            else:  # reasoner model
                task_cost = (
                    input_tokens * Config.DEEPSEEK_REASONER_INPUT_PRICE + 
                    output_tokens * Config.DEEPSEEK_REASONER_OUTPUT_PRICE
                )
        elif Config.API_PROVIDER == "GEMINI":
            task_cost = (
                input_tokens * Config.GEMINI_INPUT_TOKEN_PRICE + 
                output_tokens * Config.GEMINI_OUTPUT_TOKEN_PRICE
            )
        else:  # OPENAI
            task_cost = (
                input_tokens * Config.OPENAI_INPUT_TOKEN_PRICE + 
                output_tokens * Config.OPENAI_OUTPUT_TOKEN_PRICE
            )
            
        self.total_cost += task_cost
        
        # Update task-specific metrics
        if task not in self.token_counts_by_task:
            self.token_counts_by_task[task] = {'input': 0, 'output': 0}
            self.cost_by_task[task] = 0
            
        self.token_counts_by_task[task]['input'] += input_tokens
        self.token_counts_by_task[task]['output'] += output_tokens
        self.call_counts[task] = self.call_counts.get(task, 0) + 1
        self.cost_by_task[task] = self.cost_by_task.get(task, 0) + task_cost
        
        # Update category metrics
        category_found = False
        for category, tasks in self.task_categories.items():
            if any(task.startswith(t) for t in tasks) or (category == 'other' and not category_found):
                self.call_counts_by_category[category] += 1
                self.token_counts_by_category[category]['input'] += input_tokens
                self.token_counts_by_category[category]['output'] += output_tokens
                self.cost_by_category[category] += task_cost
                category_found = True
                break
    
    def get_enhanced_summary(self) -> Dict[str, Any]:
        """Get enhanced usage summary with category breakdowns and percentages."""
        total_calls = sum(self.call_counts.values())
        total_cost = sum(self.cost_by_task.values())
        
        # Calculate percentages for call counts by category
        call_percentages = {}
        for category, count in self.call_counts_by_category.items():
            call_percentages[category] = (count / total_calls * 100) if total_calls > 0 else 0
            
        # Calculate percentages for token counts by category
        token_percentages = {}
        for category, counts in self.token_counts_by_category.items():
            total_tokens = counts['input'] + counts['output']
            token_percentages[category] = (total_tokens / (self.total_input_tokens + self.total_output_tokens) * 100) if (self.total_input_tokens + self.total_output_tokens) > 0 else 0
            
        # Calculate percentages for cost by category
        cost_percentages = {}
        for category, cost in self.cost_by_category.items():
            cost_percentages[category] = (cost / total_cost * 100) if total_cost > 0 else 0
        
        return {
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": self.total_input_tokens + self.total_output_tokens,
            "total_cost_usd": round(self.total_cost, 6),
            "total_calls": total_calls,
            "calls_by_task": dict(self.call_counts),
            "token_counts_by_task": self.token_counts_by_task,
            "cost_by_task": {task: round(cost, 6) for task, cost in self.cost_by_task.items()},
            "categories": {
                category: {
                    "calls": count,
                    "calls_percentage": round(call_percentages[category], 2),
                    "tokens": self.token_counts_by_category[category],
                    "tokens_percentage": round(token_percentages[category], 2),
                    "cost_usd": round(self.cost_by_category[category], 6),
                    "cost_percentage": round(cost_percentages[category], 2)
                }
                for category, count in self.call_counts_by_category.items()
            }
        }
        
    def print_usage_report(self):
        """Print a detailed usage report to the console."""
        summary = self.get_enhanced_summary()
        
        # Helper to format USD amounts
        def fmt_usd(amount):
            return f"${amount:.6f}"
        
        # Helper to format percentages
        def fmt_pct(percentage):
            return f"{percentage:.2f}%"
        
        # Helper to format numbers with commas
        def fmt_num(num):
            return f"{num:,}"
        
        # Find max task name length for proper column alignment
        max_task_length = max([len(task) for task in summary['calls_by_task'].keys()], default=30)
        task_col_width = max(max_task_length + 2, 30)
        
        report = [
            "\n" + "="*80,
            colored("📊 TOKEN USAGE AND COST REPORT", "cyan", attrs=["bold"]),
            "="*80,
            "",
            f"Total Tokens: {fmt_num(summary['total_tokens'])} (Input: {fmt_num(summary['total_input_tokens'])}, Output: {fmt_num(summary['total_output_tokens'])})",
            f"Total Cost: {fmt_usd(summary['total_cost_usd'])}",
            f"Total API Calls: {fmt_num(summary['total_calls'])}",
            "",
            colored("BREAKDOWN BY CATEGORY", "yellow", attrs=["bold"]),
            "-"*80,
            "Category".ljust(15) + "Calls".rjust(10) + "Call %".rjust(10) + "Tokens".rjust(12) + "Token %".rjust(10) + "Cost".rjust(12) + "Cost %".rjust(10),
            "-"*80
        ]
        
        for category, data in summary['categories'].items():
            if data['calls'] > 0:
                tokens = data['tokens']['input'] + data['tokens']['output']
                report.append(
                    category.ljust(15) + 
                    fmt_num(data['calls']).rjust(10) + 
                    fmt_pct(data['calls_percentage']).rjust(10) + 
                    fmt_num(tokens).rjust(12) + 
                    fmt_pct(data['tokens_percentage']).rjust(10) + 
                    fmt_usd(data['cost_usd']).rjust(12) + 
                    fmt_pct(data['cost_percentage']).rjust(10)
                )
                
        report.extend([
            "-"*80,
            "",
            colored("DETAILED BREAKDOWN BY TASK", "yellow", attrs=["bold"]),
            "-"*80,
            "Task".ljust(task_col_width) + "Calls".rjust(8) + "Input".rjust(12) + "Output".rjust(10) + "Cost".rjust(12),
            "-"*80
        ])
        
        # Sort tasks by cost (highest first)
        sorted_tasks = sorted(
            summary['cost_by_task'].items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        for task, cost in sorted_tasks:
            if cost > 0:
                report.append(
                    task.ljust(task_col_width) + 
                    fmt_num(summary['calls_by_task'][task]).rjust(8) + 
                    fmt_num(summary['token_counts_by_task'][task]['input']).rjust(12) + 
                    fmt_num(summary['token_counts_by_task'][task]['output']).rjust(10) + 
                    fmt_usd(cost).rjust(12)
                )
                
        report.extend([
            "-"*80,
            "",
            f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "="*80,
        ])
        
        logger.info("\n".join(report))
        
class DocumentOptimizer:
    """Minimal document optimizer that only implements what's needed for mindmap generation."""
    def __init__(self):
        self.openai_client = AsyncOpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        self.anthropic_client = AsyncAnthropic(api_key=Config.ANTHROPIC_API_KEY)
        self.deepseek_client = AsyncOpenAI(
            api_key=Config.DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        # Initialize Google GenAI client only if needed
        self.gemini_client = None
        if Config.API_PROVIDER == "GEMINI" and Config.GEMINI_API_KEY and GOOGLE_AI_AVAILABLE:
            try:
                # Configure Google Generative AI
                genai.configure(api_key=Config.GEMINI_API_KEY)
                # Create a GenerativeModel instance
                self.gemini_client = genai.GenerativeModel(Config.GEMINI_MODEL_STRING)
                logger.info("Gemini API client initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize Gemini client: {e}")
        elif Config.API_PROVIDER == "GEMINI" and not GOOGLE_AI_AVAILABLE:
            logger.error("Gemini API provider selected but google-generativeai package not installed")
        elif Config.API_PROVIDER == "GEMINI" and not Config.GEMINI_API_KEY:
            logger.error("Gemini API provider selected but no API key provided")
        self.token_tracker = TokenUsageTracker()
        
    async def generate_completion(self, prompt: str, max_tokens: int = 5000, request_id: str = None, task: Optional[str] = None) -> Optional[str]:
        try:
            # Log the start of the request with truncated prompt
            prompt_preview = " ".join(prompt.split()[:40])  # Get first 40 words
            logger.info(
                f"\n{colored('🔄 API Request', 'cyan', attrs=['bold'])}\n"
                f"Task: {colored(task or 'unknown', 'yellow')}\n"
                f"Provider: {colored(Config.API_PROVIDER, 'blue')}\n"
                f"Prompt preview: {colored(prompt_preview + '...', 'white')}"
            )
            if Config.API_PROVIDER == "CLAUDE":
                async with self.anthropic_client.messages.stream(
                    model=Config.CLAUDE_MODEL_STRING,
                    max_tokens=max_tokens,
                    temperature=0.7,
                    messages=[{"role": "user", "content": prompt}]
                ) as stream:
                    message = await stream.get_final_message()
                    response_preview = " ".join(message.content[0].text.split()[:30])
                    self.token_tracker.update(
                        message.usage.input_tokens,
                        message.usage.output_tokens,
                        task or "unknown"
                    )
                    logger.info(
                        f"\n{colored('✅ API Response', 'green', attrs=['bold'])}\n"
                        f"Response preview: {colored(response_preview + '...', 'white')}\n"
                        f"Tokens: {colored(f'Input={message.usage.input_tokens}, Output={message.usage.output_tokens}', 'yellow')}"
                    )
                    return message.content[0].text
            elif Config.API_PROVIDER == "DEEPSEEK":
                kwargs = {
                    "model": Config.DEEPSEEK_COMPLETION_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "stream": False
                }
                if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
                    kwargs["temperature"] = 0.7
                response = await self.deepseek_client.chat.completions.create(**kwargs)
                response_preview = " ".join(response.choices[0].message.content.split()[:30])
                self.token_tracker.update(
                    response.usage.prompt_tokens,
                    response.usage.completion_tokens,
                    task or "unknown"
                )
                logger.info(
                    f"\n{colored('✅ API Response', 'green', attrs=['bold'])}\n"
                    f"Response preview: {colored(response_preview + '...', 'white')}\n"
                    f"Tokens: {colored(f'Input={response.usage.prompt_tokens}, Output={response.usage.completion_tokens}', 'yellow')}"
                )
                return response.choices[0].message.content
            elif Config.API_PROVIDER == "GEMINI":
                if not self.gemini_client:
                    logger.error("Gemini client not initialized")
                    return None
                
                try:
                    # Generate content using Gemini model
                    response = await asyncio.to_thread(
                        self.gemini_client.generate_content,
                        prompt,
                        generation_config=genai.GenerationConfig(
                            max_output_tokens=min(max_tokens, Config.GEMINI_MAX_TOKENS),
                            temperature=0.7,
                        )
                    )
                    
                    response_text = response.text
                    response_preview = " ".join(response_text.split()[:30])
                    
                    # Extract token usage from response metadata
                    input_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0) if hasattr(response, 'usage_metadata') else 0
                    output_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0) if hasattr(response, 'usage_metadata') else 0
                    
                    self.token_tracker.update(
                        input_tokens,
                        output_tokens,
                        task or "unknown"
                    )
                    
                    logger.info(
                        f"\n{colored('✅ API Response', 'green', attrs=['bold'])}\n"
                        f"Response preview: {colored(response_preview + '...', 'white')}\n"
                        f"Tokens: {colored(f'Input={input_tokens}, Output={output_tokens}', 'yellow')}"
                    )
                    
                    return response_text
                    
                except Exception as e:
                    logger.error(f"Gemini API error: {str(e)}")
                    return None
            elif Config.API_PROVIDER == "OPENAI":
                response = await self.openai_client.chat.completions.create(
                    model=Config.OPENAI_COMPLETION_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                response_preview = " ".join(response.choices[0].message.content.split()[:30])
                self.token_tracker.update(
                    response.usage.prompt_tokens,
                    response.usage.completion_tokens,
                    task or "unknown"
                )
                logger.info(
                    f"\n{colored('✅ API Response', 'green', attrs=['bold'])}\n"
                    f"Response preview: {colored(response_preview + '...', 'white')}\n"
                    f"Tokens: {colored(f'Input={response.usage.prompt_tokens}, Output={response.usage.completion_tokens}', 'yellow')}"
                )
                return response.choices[0].message.content
            else:
                raise ValueError(f"Invalid API_PROVIDER: {Config.API_PROVIDER}")
        except Exception as e:
            logger.error(
                f"\n{colored('❌ API Error', 'red', attrs=['bold'])}\n"
                f"Error: {colored(str(e), 'red')}"
            )
            return None
    
class MinimalDatabaseStub:
    """Minimal database stub that provides just enough for the mindmap generator."""
    @staticmethod
    async def get_document_by_id(document_id: str) -> Dict[str, Any]:
        """Stub that returns minimal document info."""
        return {
            "id": document_id,
            "original_file_name": f"document_{document_id}.txt",
            "sanitized_filename": document_id,
            "status": "processing",
            "progress_percentage": 0
        }
        
    @staticmethod
    async def get_optimized_text(document_id: str, request_id: str) -> Optional[str]:
        """In our simplified version, this just returns the raw text content."""
        return MinimalDatabaseStub._stored_text
        
    @staticmethod
    async def update_document_status(*args, **kwargs) -> Dict[str, Any]:
        """Stub that just returns success."""
        return {"status": "success"}
        
    @staticmethod
    async def add_token_usage(*args, **kwargs) -> None:
        """Stub that does nothing."""
        pass

    # Add a way to store the text content
    _stored_text = ""
    
    @classmethod
    def store_text(cls, text: str):
        """Store text content for later retrieval."""
        cls._stored_text = text

async def initialize_db():
    """Minimal DB initialization that just returns our stub."""
    return MinimalDatabaseStub()

class DocumentType(Enum):
    """Enumeration of supported document types."""
    TECHNICAL = auto()
    SCIENTIFIC = auto()
    NARRATIVE = auto()
    BUSINESS = auto()
    ACADEMIC = auto()
    LEGAL = auto()      
    MEDICAL = auto()    
    INSTRUCTIONAL = auto() 
    ANALYTICAL = auto() 
    PROCEDURAL = auto() 
    GENERAL = auto()

    @classmethod
    def from_str(cls, value: str) -> 'DocumentType':
        """Convert string to DocumentType enum."""
        try:
            return cls[value.upper()]
        except KeyError:
            return cls.GENERAL

class NodeShape(Enum):
    """Enumeration of node shapes for the mindmap structure."""
    ROOT = '(())'        # Double circle for root node (📄)
    TOPIC = '(())'       # Double circle for main topics
    SUBTOPIC = '()'      # Single circle for subtopics
    DETAIL = '[]'        # Square brackets for details

    def apply(self, text: str) -> str:
        """Apply the shape to the text."""
        return {
            self.ROOT: f"(({text}))",
            self.TOPIC: f"(({text}))",
            self.SUBTOPIC: f"({text})",
            self.DETAIL: f"[{text}]"
        }[self]

class MindMapGenerationError(Exception):
    """Custom exception for mindmap generation errors."""
    pass

class ContentItem:
    """Class to track content items with their context information."""
    def __init__(self, text: str, path: List[str], node_type: str, importance: str = None):
        self.text = text
        self.path = path
        self.path_str = ' → '.join(path)
        self.node_type = node_type
        self.importance = importance
        
    def __str__(self):
        return f"{self.text} ({self.node_type} at {self.path_str})"

class MindMapGenerator:
    def __init__(self):
        self.optimizer = DocumentOptimizer()
        self.config = {
            'max_summary_length': 2500,
            'max_tokens': 3000,
            'valid_types': [t.name.lower() for t in DocumentType],
            'default_type': DocumentType.GENERAL.name.lower(),
            'max_retries': 3,
            'request_timeout': 30,  # seconds
            'chunk_size': 8192,     # bytes for file operations
            'max_topics': 6,        # Maximum main topics
            'max_subtopics': 4,     # Maximum subtopics per topic
            'max_details': 8,       # Maximum details per subtopic
            'similarity_threshold': {
                'topic': 75,        # Allow more diverse main topics
                'subtopic': 70,     # Allow more nuanced subtopics
                'detail': 65        # Allow more specific details
            },
            'reality_check': {
                'batch_size': 8,    # Number of nodes to verify in parallel
                'min_verified_topics': 4,  # Minimum verified topics needed
                'min_verified_ratio': 0.6  # Minimum ratio of verified content
            }
        }
        self.verification_stats = {
            'total_nodes': 0,
            'verified_nodes': 0,
            'topics': {'total': 0, 'verified': 0},
            'subtopics': {'total': 0, 'verified': 0},
            'details': {'total': 0, 'verified': 0}
        }
        self._emoji_cache = {}
        self.retry_config = {
            'max_retries': 3,
            'base_delay': 1,
            'max_delay': 10,
            'jitter': 0.1,
            'timeout': 30
        }
        self._initialize_prompts()
        self.numbered_pattern = re.compile(r'^\s*\d+\.\s*(.+)$')
        self.parentheses_regex = re.compile(r'(\((?!\()|(?<!\))\))')
        self.control_chars_regex = re.compile(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]')
        self.unescaped_quotes_regex = re.compile(r'(?<!\\)"(?!,|\s*[}\]])')
        self.percentage_regex1 = re.compile(r'(\d+(?:\.\d+)?)\s+(?=percent|of\s|share|margin|CAGR)', re.IGNORECASE)
        self.percentage_regex2 = re.compile(r'\s+percent\b', re.IGNORECASE)
        self.backslash_regex = re.compile(r'\\{2,}')
        self.special_chars_regex = re.compile(r'[^a-zA-Z0-9\s\[\]\(\)\{\}\'_\-.,`*%\\]')
        self.paren_replacements = {
            '(': '❨',  # U+2768 MEDIUM LEFT PARENTHESIS ORNAMENT
            ')': '❩',  # U+2769 MEDIUM RIGHT PARENTHESIS ORNAMENT
            # Backup alternatives if needed:
            # '(': '⟮',  # U+27EE MATHEMATICAL LEFT FLATTENED PARENTHESIS
            # ')': '⟯',  # U+27EF MATHEMATICAL RIGHT FLATTENED PARENTHESIS
            # Or:
            # '(': '﹙',  # U+FE59 SMALL LEFT PARENTHESIS
            # ')': '﹚',  # U+FE5A SMALL RIGHT PARENTHESIS
        }
        self._emoji_file = os.path.join(os.path.dirname(__file__), "emoji_cache.json")
        self._load_emoji_cache()
        
    def _load_emoji_cache(self):
        """Load emoji cache from disk if available."""
        try:
            if os.path.exists(self._emoji_file):
                with open(self._emoji_file, 'r', encoding='utf-8') as f:
                    loaded_cache = json.load(f)
                    # Convert tuple string keys back to actual tuples
                    self._emoji_cache = {tuple(eval(k)): v for k, v in loaded_cache.items()}
                    logger.info(f"Loaded {len(self._emoji_cache)} emoji mappings from cache")
            else:
                self._emoji_cache = {}
        except Exception as e:
            logger.warning(f"Failed to load emoji cache: {str(e)}")
            self._emoji_cache = {}

    def _save_emoji_cache(self):
        """Save emoji cache to disk for reuse across runs."""
        try:
            # Convert tuple keys to strings for JSON serialization
            serializable_cache = {str(k): v for k, v in self._emoji_cache.items()}
            with open(self._emoji_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_cache, f)
            logger.info(f"Saved {len(self._emoji_cache)} emoji mappings to cache")
        except Exception as e:
            logger.warning(f"Failed to save emoji cache: {str(e)}")
                
    async def _retry_with_exponential_backoff(self, func, *args, **kwargs):
        """Enhanced retry mechanism with jitter and circuit breaker."""
        retries = 0
        max_retries = self.retry_config['max_retries']
        base_delay = self.retry_config['base_delay']
        max_delay = self.retry_config['max_delay']
        
        while retries < max_retries:
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                retries += 1
                if retries >= max_retries:
                    raise
                    
                delay = min(base_delay * (2 ** (retries - 1)), max_delay)
                actual_delay = random.uniform(0, delay)
                
                logger.warning(f"Attempt {retries}/{max_retries} failed: {str(e)}. "
                            f"Retrying in {actual_delay:.2f}s")
                
                await asyncio.sleep(actual_delay)

    def _validate_parsed_response(self, parsed: Any, expected_type: str) -> Union[List[Any], Dict[str, Any]]:
        """Validate and normalize parsed JSON response."""
        if expected_type == "array":
            if isinstance(parsed, list):
                return parsed
            elif isinstance(parsed, dict):
                # Try to extract array from common fields
                for key in ['items', 'topics', 'elements', 'data']:
                    if isinstance(parsed.get(key), list):
                        return parsed[key]
                logger.debug("No array found in dictionary fields")
                return []
            else:
                logger.debug(f"Unexpected type for array response: {type(parsed)}")
                return []
        
        return parsed if isinstance(parsed, dict) else {}

    def _clean_detail_response(self, response: str) -> List[Dict[str, str]]:
        """Clean and validate detail responses."""
        try:
            # Remove markdown code blocks if present
            if '```' in response:
                matches = re.findall(r'```(?:json)?(.*?)```', response, re.DOTALL)
                if matches:
                    response = matches[0].strip()
                    
            # Basic cleanup
            response = response.strip()
            
            try:
                parsed = json.loads(response)
            except json.JSONDecodeError:
                # Try cleaning quotes and parse again
                response = response.replace("'", '"')
                try:
                    parsed = json.loads(response)
                except json.JSONDecodeError:
                    return []
                    
            # Handle both array and single object responses
            if isinstance(parsed, dict):
                parsed = [parsed]
                
            # Validate each detail
            valid_details = []
            seen_texts = set()
            
            for item in parsed:
                try:
                    text = str(item.get('text', '')).strip()
                    importance = str(item.get('importance', 'medium')).lower()
                    
                    # Skip empty text or duplicates
                    if not text or text in seen_texts:
                        continue
                        
                    if importance not in ['high', 'medium', 'low']:
                        importance = 'medium'
                        
                    seen_texts.add(text)
                    valid_details.append({
                        'text': text,
                        'importance': importance
                    })
                    
                except Exception as e:
                    logger.debug(f"Error processing detail item: {str(e)}")
                    continue
                    
            return valid_details
            
        except Exception as e:
            logger.error(f"Error in detail cleaning: {str(e)}")
            return []

    def _clean_json_response(self, response: str) -> str:
        """Enhanced JSON response cleaning with advanced recovery and validation."""
        if not response or not isinstance(response, str):
            logger.warning("Empty or invalid response type received")
            return "[]"  # Return empty array as safe default
            
        try:
            # First try to find complete JSON structure
            def find_json_structure(text: str) -> Optional[str]:
                # Look for array pattern
                array_match = re.search(r'\[[\s\S]*?\](?=\s*$|\s*[,}\]])', text)
                if array_match:
                    return array_match.group(0)
                    
                # Look for object pattern
                object_match = re.search(r'\{[\s\S]*?\}(?=\s*$|\s*[,\]}])', text)
                if object_match:
                    return object_match.group(0)
                
                return None

            # Handle markdown code blocks first
            if '```' in response:
                code_blocks = re.findall(r'```(?:json)?([\s\S]*?)```', response)
                if code_blocks:
                    for block in code_blocks:
                        if json_struct := find_json_structure(block):
                            response = json_struct
                            break
            else:
                if json_struct := find_json_structure(response):
                    response = json_struct

            # Advanced character cleaning
            def clean_characters(text: str) -> str:
                # Remove control characters while preserving valid whitespace
                text = self.control_chars_regex.sub('', text)
                
                # Normalize quotes and apostrophes
                text = text.replace('"', '"').replace('"', '"')  # Smart double quotes to straight double quotes
                text = text.replace("'", "'").replace("'", "'")  # Smart single quotes to straight single quotes
                text = text.replace("'", '"')  # Convert single quotes to double quotes
                
                # Normalize whitespace
                text = ' '.join(text.split())
                
                # Escape unescaped quotes within strings
                text = self.unescaped_quotes_regex.sub('\\"', text)
                
                return text

            response = clean_characters(response)

            # Fix common JSON syntax issues
            def fix_json_syntax(text: str) -> str:
                # Fix trailing/multiple commas
                text = re.sub(r',\s*([\]}])', r'\1', text)  # Remove trailing commas
                text = re.sub(r',\s*,', ',', text)  # Remove multiple commas
                
                # Fix missing quotes around keys
                text = re.sub(r'(\{|\,)\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', text)
                
                # Ensure proper array/object closure
                brackets_stack = []
                for char in text:
                    if char in '[{':
                        brackets_stack.append(char)
                    elif char in ']}':
                        if not brackets_stack:
                            continue  # Skip unmatched closing brackets
                        if (char == ']' and brackets_stack[-1] == '[') or (char == '}' and brackets_stack[-1] == '{'):
                            brackets_stack.pop()
                        
                # Close any unclosed brackets
                while brackets_stack:
                    text += ']' if brackets_stack.pop() == '[' else '}'
                
                return text

            response = fix_json_syntax(response)

            # Validate and normalize structure
            def normalize_structure(text: str) -> str:
                try:
                    # Try parsing to validate
                    parsed = json.loads(text)
                    
                    # Ensure we have an array
                    if isinstance(parsed, dict):
                        # Convert single object to array
                        return json.dumps([parsed])
                    elif isinstance(parsed, list):
                        return json.dumps(parsed)
                    else:
                        return json.dumps([str(parsed)])
                        
                except json.JSONDecodeError:
                    # If still invalid, attempt emergency recovery
                    if text.strip().startswith('{'):
                        return f"[{text.strip()}]"  # Wrap object in array
                    elif not text.strip().startswith('['):
                        return f"[{text.strip()}]"  # Wrap content in array
                    return text
            
            response = normalize_structure(response)

            # Final validation
            try:
                json.loads(response)  # Verify we have valid JSON
                return response
            except json.JSONDecodeError as e:
                logger.warning(f"Final JSON validation failed: {str(e)}")
                # If all cleaning failed, return empty array
                return "[]"

        except Exception as e:
            logger.error(f"Error during JSON response cleaning: {str(e)}")
            return "[]"

    def _parse_llm_response(self, response: str, expected_type: str = "array") -> Union[List[Any], Dict[str, Any]]:
        """Parse and validate LLM response."""
        if not response or not isinstance(response, str):
            logger.warning("Empty or invalid response type received")
            return [] if expected_type == "array" else {}

        try:
            # Extract JSON from markdown code blocks if present
            if '```' in response:
                matches = re.findall(r'```(?:json)?(.*?)```', response, re.DOTALL)
                if matches:
                    response = matches[0].strip()

            # Basic cleanup
            response = response.strip()
            
            try:
                parsed = json.loads(response)
                return self._validate_parsed_response(parsed, expected_type)
            except json.JSONDecodeError:
                # Try cleaning quotes and parse again
                response = response.replace("'", '"')
                try:
                    parsed = json.loads(response)
                    return self._validate_parsed_response(parsed, expected_type)
                except json.JSONDecodeError:
                    # If we still can't parse, try emergency extraction for arrays
                    if expected_type == "array":
                        items = re.findall(r'"([^"]+)"', response)
                        if items:
                            return items

                        # Try line-by-line extraction
                        lines = response.strip().split('\n')
                        items = [line.strip().strip(',"\'[]{}') for line in lines 
                                if line.strip() and not line.strip().startswith(('```', '{', '}'))]
                        if items:
                            return items

                    return [] if expected_type == "array" else {}

        except Exception as e:
            logger.error(f"Unexpected error in JSON parsing: {str(e)}")
            return [] if expected_type == "array" else {}

    def _get_importance_marker(self, importance: str) -> str:
        """Get the appropriate diamond marker based on importance level."""
        markers = {
            'high': '♦️',    # Red diamond for high importance
            'medium': '🔸',  # Orange diamond for medium importance
            'low': '🔹'      # Blue diamond for low importance
        }
        return markers.get(importance.lower(), '🔹')

    async def _save_emoji_cache_async(self):
        """Asynchronous version of save_emoji_cache to avoid blocking."""
        try:
            # Convert to a non-blocking call
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._save_emoji_cache)
        except Exception as e:
            logger.warning(f"Failed to save emoji cache asynchronously: {str(e)}")
            
    async def _select_emoji(self, text: str, node_type: str = 'topic') -> str:
        """Select appropriate emoji for node content with persistent cache."""
        cache_key = (text, node_type)
        
        # First check in-memory cache
        if cache_key in self._emoji_cache:
            return self._emoji_cache[cache_key]
            
        # If not in cache, generate emoji
        try:
            prompt = f"""Select the single most appropriate emoji to represent this {node_type}: "{text}"

            Requirements:
            1. Return ONLY the emoji character - no explanations or other text
            2. Choose an emoji that best represents the concept semantically
            3. For abstract concepts, use metaphorical or symbolic emojis
            4. Default options if unsure:
            - Topics: 📄 (document)
            - Subtopics: 📌 (pin)
            - Details: 🔹 (bullet point)
            5. Be creative but clear - the emoji should intuitively represent the concept

            Examples:
            - "Market Growth" → 📈
            - "Customer Service" → 👥
            - "Financial Report" → 💰
            - "Product Development" → ⚙️
            - "Global Expansion" → 🌐
            - "Research and Development" → 🔬
            - "Digital Transformation" → 💻
            - "Supply Chain" → 🔄
            - "Healthcare Solutions" → 🏥
            - "Security Measures" → 🔒

            Return ONLY the emoji character without any explanation."""
            
            response = await self._retry_generate_completion(
                prompt,
                max_tokens=20,
                request_id='',
                task="selecting_emoji"
            )
            
            # Clean the response to get just the emoji
            emoji = response.strip()
            
            # If no emoji was returned or response is too long, use defaults
            if not emoji or len(emoji) > 4:  # Most emojis are 2 chars, some are 4
                defaults = {
                    'topic': '📄',
                    'subtopic': '📌',
                    'detail': '🔹'
                }
                emoji = defaults.get(node_type, '📄')
                
            # Add to in-memory cache
            self._emoji_cache[cache_key] = emoji
            
            # Save cache to disk periodically (every 10 new emojis)
            if len(self._emoji_cache) % 10 == 0:
                await asyncio.create_task(self._save_emoji_cache_async())
                
            return emoji
            
        except Exception as e:
            logger.warning(f"Error selecting emoji: {str(e)}")
            return '📄' if node_type == 'topic' else '📌' if node_type == 'subtopic' else '🔹'

    def _initialize_prompts(self) -> None:
        """Initialize type-specific prompts from a configuration file or define them inline."""
        self.type_specific_prompts = {
            DocumentType.TECHNICAL: {
                'topics': """Analyze this technical document focusing on core system components and relationships.
                
    First, identify the major architectural or technical components that form complete, independent units of functionality.
    Each component should be:
    - A distinct technical system, module, or process
    - Independent enough to be understood on its own
    - Critical to the overall system functionality
    - Connected to at least one other component

    Avoid topics that are:
    - Too granular (implementation details)
    - Too broad (entire system categories)
    - Isolated features without system impact
    - Pure documentation elements

    Think about:
    1. What are the core building blocks?
    2. How do these pieces fit together?
    3. What dependencies exist between components?
    4. What are the key technical boundaries?

    Format: Return a JSON array of component names that represent the highest-level technical building blocks.""",

                'subtopics': """For the technical component '{topic}', identify its essential sub-components and interfaces.

    Each subtopic should:
    - Represent a crucial aspect of this component
    - Have clear technical responsibilities
    - Interface with other parts of the system
    - Contribute to the component's core purpose

    Consider:
    1. What interfaces does this component expose?
    2. What are its internal subsystems?
    3. How does it process data or handle requests?
    4. What services does it provide to other components?
    5. What technical standards or protocols does it implement?

    Format: Return a JSON array of technical subtopic names that form this component's architecture.""",

                'details': """For the technical subtopic '{subtopic}', identify specific implementation aspects and requirements.

    Focus on:
    1. Key algorithms or methods
    2. Data structures and formats
    3. Protocol specifications
    4. Performance characteristics
    5. Error handling approaches
    6. Security considerations
    7. Dependencies and requirements

    Include concrete technical details that are:
    - Implementation-specific
    - Measurable or testable
    - Critical for understanding
    - Relevant to integration

    Format: Return a JSON array of technical specifications and implementation details."""
            },

            DocumentType.SCIENTIFIC: {
                'topics': """Analyze this scientific document focusing on major research components and methodological frameworks.

    Identify main scientific themes that:
    - Represent complete experimental or theoretical units
    - Follow scientific method principles
    - Support the research objectives
    - Build on established scientific concepts

    Consider:
    1. What are the primary research questions?
    2. What methodological approaches are used?
    3. What theoretical frameworks are applied?
    4. What experimental designs are implemented?
    5. How do different research components interact?

    Avoid topics that are:
    - Too specific (individual measurements)
    - Too broad (entire fields of study)
    - Purely descriptive without scientific merit
    - Administrative or non-research elements

    Format: Return a JSON array of primary scientific themes or research components.""",

                'subtopics': """For the scientific theme '{topic}', identify key methodological elements and experimental components.

    Each subtopic should:
    - Represent a distinct experimental or analytical approach
    - Contribute to scientific rigor
    - Support reproducibility
    - Connect to research objectives

    Consider:
    1. What specific methods were employed?
    2. What variables were measured?
    3. What controls were implemented?
    4. What analytical techniques were used?
    5. How were data validated?

    Format: Return a JSON array of scientific subtopics that detail the research methodology.""",

                'details': """For the scientific subtopic '{subtopic}', extract specific experimental parameters and results.

    Focus on:
    1. Measurement specifications
    2. Statistical analyses
    3. Data collection procedures
    4. Validation methods
    5. Error margins
    6. Equipment specifications
    7. Environmental conditions

    Include details that are:
    - Quantifiable
    - Reproducible
    - Statistically relevant
    - Methodologically important

    Format: Return a JSON array of specific scientific parameters and findings."""
            },
            
            DocumentType.NARRATIVE: {
            'topics': """Analyze this narrative document focusing on storytelling elements and plot development.

    Identify major narrative components that:
    - Represent complete story arcs or plot elements
    - Form essential narrative structures
    - Establish key story developments
    - Connect to the overall narrative flow

    Consider:
    1. What are the primary plot points?
    2. What character arcs are developed?
    3. What themes are explored?
    4. What settings are established?
    5. How do different narrative elements interweave?

    Avoid topics that are:
    - Too specific (individual scenes)
    - Too broad (entire genres)
    - Purely stylistic elements
    - Non-narrative content

    Format: Return a JSON array of primary narrative themes or story elements.""",

            'subtopics': """For the narrative theme '{topic}', identify key story elements and developments.

    Each subtopic should:
    - Represent a distinct narrative aspect
    - Support story progression
    - Connect to character development
    - Contribute to theme exploration

    Consider:
    1. What specific plot developments occur?
    2. What character interactions take place?
    3. What conflicts are presented?
    4. What thematic elements are developed?
    5. What setting details are important?

    Format: Return a JSON array of narrative subtopics that detail story components.""",

            'details': """For the narrative subtopic '{subtopic}', extract specific story details and elements.

    Focus on:
    1. Scene descriptions
    2. Character motivations
    3. Dialogue highlights
    4. Setting details
    5. Symbolic elements
    6. Emotional moments
    7. Plot connections

    Include details that are:
    - Story-advancing
    - Character-developing
    - Theme-supporting
    - Atmosphere-building

    Format: Return a JSON array of specific narrative details and elements."""
        },
        
            DocumentType.BUSINESS: {
            'topics': """Analyze this business document focusing on strategic initiatives and market opportunities.

    Identify major business components that:
    - Represent complete business strategies
    - Form essential market approaches
    - Establish key business objectives
    - Connect to organizational goals

    Consider:
    1. What are the primary business objectives?
    2. What market opportunities are targeted?
    3. What strategic initiatives are proposed?
    4. What organizational capabilities are required?
    5. How do different business elements align?

    Avoid topics that are:
    - Too specific (individual tactics)
    - Too broad (entire industries)
    - Administrative elements
    - Non-strategic content

    Format: Return a JSON array of primary business themes or strategic elements.""",

            'subtopics': """For the business theme '{topic}', identify key strategic elements and approaches.

    Each subtopic should:
    - Represent a distinct business aspect
    - Support strategic objectives
    - Connect to market opportunities
    - Contribute to business growth

    Consider:
    1. What specific strategies are proposed?
    2. What market segments are targeted?
    3. What resources are required?
    4. What competitive advantages exist?
    5. What implementation steps are needed?

    Format: Return a JSON array of business subtopics that detail strategic components.""",

            'details': """For the business subtopic '{subtopic}', extract specific strategic details and requirements.

    Focus on:
    1. Market metrics
    2. Financial projections
    3. Resource requirements
    4. Implementation timelines
    5. Success metrics
    6. Risk factors
    7. Growth opportunities

    Include details that are:
    - Measurable
    - Action-oriented
    - Resource-specific
    - Market-focused

    Format: Return a JSON array of specific business details and requirements."""
        },            

            DocumentType.ANALYTICAL: {
                'topics': """Analyze this analytical document focusing on key insights and data patterns.

    Identify major analytical themes that:
    - Represent complete analytical frameworks
    - Reveal significant patterns or trends
    - Support evidence-based conclusions
    - Connect different aspects of analysis

    Consider:
    1. What are the primary analytical questions?
    2. What major patterns emerge from the data?
    3. What key metrics drive the analysis?
    4. How do different analytical components relate?
    5. What are the main areas of investigation?

    Avoid topics that are:
    - Too granular (individual data points)
    - Too broad (entire analytical fields)
    - Purely descriptive without analytical value
    - Administrative or non-analytical elements

    Format: Return a JSON array of primary analytical themes or frameworks.""",

                'subtopics': """For the analytical theme '{topic}', identify key metrics and analytical approaches.

    Each subtopic should:
    - Represent a distinct analytical method or metric
    - Contribute to understanding patterns
    - Support data-driven insights
    - Connect to analytical objectives

    Consider:
    1. What specific analyses were performed?
    2. What metrics were calculated?
    3. What statistical approaches were used?
    4. What patterns were investigated?
    5. How were conclusions validated?

    Format: Return a JSON array of analytical subtopics that detail the investigation methods.""",

                'details': """For the analytical subtopic '{subtopic}', extract specific findings and supporting evidence.

    Focus on:
    1. Statistical results
    2. Trend analyses
    3. Correlation findings
    4. Significance measures
    5. Confidence intervals
    6. Data quality metrics
    7. Validation results

    Include details that are:
    - Quantifiable
    - Statistically significant
    - Evidence-based
    - Methodologically sound

    Format: Return a JSON array of specific analytical findings and metrics."""
            },
            DocumentType.LEGAL: {
                'topics': """Analyze this legal document focusing on key legal principles and frameworks.

    Identify major legal components that:
    - Represent complete legal concepts or arguments
    - Form foundational legal principles
    - Establish key rights, obligations, or requirements
    - Connect to relevant legal frameworks

    Consider:
    1. What are the primary legal issues or questions?
    2. What statutory frameworks apply?
    3. What precedential cases are relevant?
    4. What legal rights and obligations are established?
    5. How do different legal concepts interact?

    Avoid topics that are:
    - Too specific (individual clauses)
    - Too broad (entire bodies of law)
    - Administrative or non-legal elements
    - Purely formatting sections

    Format: Return a JSON array of primary legal themes or frameworks.""",

                'subtopics': """For the legal theme '{topic}', identify key legal elements and requirements.

    Each subtopic should:
    - Represent a distinct legal requirement or concept
    - Support legal compliance or enforcement
    - Connect to statutory or case law
    - Contribute to legal understanding

    Consider:
    1. What specific obligations arise?
    2. What rights are established?
    3. What procedures are required?
    4. What legal tests or standards apply?
    5. What exceptions or limitations exist?

    Format: Return a JSON array of legal subtopics that detail requirements and obligations.""",

                'details': """For the legal subtopic '{subtopic}', extract specific legal provisions and requirements.

    Focus on:
    1. Specific statutory references
    2. Case law citations
    3. Compliance requirements
    4. Procedural steps
    5. Legal deadlines
    6. Jurisdictional requirements
    7. Enforcement mechanisms

    Include details that are:
    - Legally binding
    - Procedurally important
    - Compliance-critical
    - Precedent-based

    Format: Return a JSON array of specific legal provisions and requirements."""
            },
            DocumentType.MEDICAL: {
                'topics': """Analyze this medical document focusing on key clinical concepts and patient care aspects.

    Identify major medical components that:
    - Represent complete clinical concepts
    - Form essential diagnostic or treatment frameworks
    - Establish key medical protocols
    - Connect to standard medical practices

    Consider:
    1. What are the primary medical conditions or issues?
    2. What treatment approaches are discussed?
    3. What diagnostic frameworks apply?
    4. What clinical outcomes are measured?
    5. How do different medical aspects interact?

    Avoid topics that are:
    - Too specific (individual symptoms)
    - Too broad (entire medical fields)
    - Administrative elements
    - Non-clinical content

    Format: Return a JSON array of primary medical themes or clinical concepts.""",

                'subtopics': """For the medical theme '{topic}', identify key clinical elements and protocols.

    Each subtopic should:
    - Represent a distinct clinical aspect
    - Support patient care decisions
    - Connect to medical evidence
    - Contribute to treatment planning

    Consider:
    1. What specific treatments are indicated?
    2. What diagnostic criteria apply?
    3. What monitoring is required?
    4. What contraindications exist?
    5. What patient factors are relevant?

    Format: Return a JSON array of medical subtopics that detail clinical approaches.""",

                'details': """For the medical subtopic '{subtopic}', extract specific clinical guidelines and parameters.

    Focus on:
    1. Dosage specifications
    2. Treatment protocols
    3. Monitoring requirements
    4. Clinical indicators
    5. Risk factors
    6. Side effects
    7. Follow-up procedures

    Include details that are:
    - Clinically relevant
    - Evidence-based
    - Treatment-specific
    - Patient-focused

    Format: Return a JSON array of specific medical parameters and guidelines."""
            },

            DocumentType.INSTRUCTIONAL: {
                'topics': """Analyze this instructional document focusing on key learning objectives and educational frameworks.

    Identify major instructional components that:
    - Represent complete learning units
    - Form coherent educational modules
    - Establish key competencies
    - Connect to learning outcomes

    Consider:
    1. What are the primary learning objectives?
    2. What skill sets are being developed?
    3. What knowledge areas are covered?
    4. What pedagogical approaches are used?
    5. How do different learning components build on each other?

    Avoid topics that are:
    - Too specific (individual facts)
    - Too broad (entire subjects)
    - Administrative elements
    - Non-educational content

    Format: Return a JSON array of primary instructional themes or learning modules.""",

                'subtopics': """For the instructional theme '{topic}', identify key learning elements and approaches.

    Each subtopic should:
    - Represent a distinct learning component
    - Support skill development
    - Connect to learning objectives
    - Contribute to competency building

    Consider:
    1. What specific skills are taught?
    2. What concepts are introduced?
    3. What practice activities are included?
    4. What assessment methods are used?
    5. What prerequisites are needed?

    Format: Return a JSON array of instructional subtopics that detail learning components.""",

                'details': """For the instructional subtopic '{subtopic}', extract specific learning activities and resources.

    Focus on:
    1. Practice exercises
    2. Examples and illustrations
    3. Assessment criteria
    4. Learning resources
    5. Key definitions
    6. Common mistakes
    7. Success indicators

    Include details that are:
    - Skill-building
    - Practice-oriented
    - Assessment-ready
    - Learning-focused

    Format: Return a JSON array of specific instructional elements and activities."""
            },

            DocumentType.ACADEMIC: {
                'topics': """Analyze this academic document focusing on scholarly arguments and theoretical frameworks.

    Identify major academic components that:
    - Represent complete theoretical concepts
    - Form scholarly arguments
    - Establish key academic positions
    - Connect to existing literature

    Consider:
    1. What are the primary theoretical frameworks?
    2. What scholarly debates are addressed?
    3. What research questions are explored?
    4. What methodological approaches are used?
    5. How do different theoretical elements interact?

    Avoid topics that are:
    - Too specific (individual citations)
    - Too broad (entire fields)
    - Administrative elements
    - Non-scholarly content

    Format: Return a JSON array of primary academic themes or theoretical frameworks.""",

                'subtopics': """For the academic theme '{topic}', identify key theoretical elements and arguments.

    Each subtopic should:
    - Represent a distinct theoretical aspect
    - Support scholarly analysis
    - Connect to literature
    - Contribute to academic discourse

    Consider:
    1. What specific arguments are made?
    2. What evidence is presented?
    3. What theoretical models apply?
    4. What counterarguments exist?
    5. What methodological approaches are used?

    Format: Return a JSON array of academic subtopics that detail theoretical components.""",

                'details': """For the academic subtopic '{subtopic}', extract specific scholarly evidence and arguments.

    Focus on:
    1. Research findings
    2. Theoretical implications
    3. Methodological details
    4. Literature connections
    5. Critical analyses
    6. Supporting evidence
    7. Scholarly debates

    Include details that are:
    - Theoretically relevant
    - Evidence-based
    - Methodologically sound
    - Literature-connected

    Format: Return a JSON array of specific academic elements and arguments."""
            },

            DocumentType.PROCEDURAL: {
                'topics': """Analyze this procedural document focusing on systematic processes and workflows.

    Identify major procedural components that:
    - Represent complete process units
    - Form coherent workflow stages
    - Establish key procedures
    - Connect to overall process flow

    Consider:
    1. What are the primary process phases?
    2. What workflow sequences exist?
    3. What critical paths are defined?
    4. What decision points occur?
    5. How do different process elements connect?

    Avoid topics that are:
    - Too specific (individual actions)
    - Too broad (entire systems)
    - Administrative elements
    - Non-procedural content

    Format: Return a JSON array of primary procedural themes or process phases.""",

                'subtopics': """For the procedural theme '{topic}', identify key process elements and requirements.

    Each subtopic should:
    - Represent a distinct process step
    - Support workflow progression
    - Connect to other steps
    - Contribute to process completion

    Consider:
    1. What specific steps are required?
    2. What inputs are needed?
    3. What outputs are produced?
    4. What conditions apply?
    5. What validations occur?

    Format: Return a JSON array of procedural subtopics that detail process steps.""",

                'details': """For the procedural subtopic '{subtopic}', extract specific step requirements and checks.

    Focus on:
    1. Step-by-step instructions
    2. Input requirements
    3. Quality checks
    4. Decision criteria
    5. Exception handling
    6. Success criteria
    7. Completion indicators

    Include details that are:
    - Action-oriented
    - Sequence-specific
    - Quality-focused
    - Process-critical

    Format: Return a JSON array of specific procedural steps and requirements."""
            },
                
            DocumentType.GENERAL: {
            'topics': """Analyze this document focusing on main conceptual themes and relationships.

    Identify major themes that:
    - Represent complete, independent ideas
    - Form logical groupings of related concepts
    - Support the document's main purpose
    - Connect to other important themes

    Consider:
    1. What are the fundamental ideas being presented?
    2. How do these ideas relate to each other?
    3. What are the key areas of focus?
    4. How is the information structured?

    Avoid topics that are:
    - Too specific (individual examples)
    - Too broad (entire subject areas)
    - Isolated facts without context
    - Purely formatting elements

    Format: Return a JSON array of primary themes or concept areas.""",

                'subtopics': """For the theme '{topic}', identify key supporting concepts and related ideas.

    Each subtopic should:
    - Represent a distinct aspect of the main theme
    - Provide meaningful context
    - Support understanding
    - Connect to the overall narrative

    Consider:
    1. What are the main points about this theme?
    2. What examples illustrate it?
    3. What evidence supports it?
    4. How does it develop through the document?

    Format: Return a JSON array of subtopics that develop this theme.""",

                'details': """For the subtopic '{subtopic}', extract specific supporting information and examples.

    Focus on:
    1. Concrete examples
    2. Supporting evidence
    3. Key definitions
    4. Important relationships
    5. Specific applications
    6. Notable implications
    7. Clarifying points

    Include details that:
    - Illustrate the concept
    - Provide evidence
    - Aid understanding
    - Connect to larger themes

    Format: Return a JSON array of specific supporting details and examples."""
            }
        }
        # Add default prompts for any missing document types
        for doc_type in DocumentType:
            if doc_type not in self.type_specific_prompts:
                self.type_specific_prompts[doc_type] = self.type_specific_prompts[DocumentType.GENERAL]

    async def detect_document_type(self, content: str, request_id: str) -> DocumentType:
        """Use LLM to detect document type with sophisticated analysis."""
        summary_content = content[:self.config['max_summary_length']]
        prompt = f"""You are analyzing a document to determine its primary type and structure. This document requires the most appropriate conceptual organization strategy.

    Key characteristics of each document type:

    TECHNICAL
    - Contains system specifications, API documentation, or implementation details
    - Focuses on HOW things work and technical implementation
    - Uses technical terminology, code examples, or system diagrams
    - Structured around components, modules, or technical processes
    Example indicators: API endpoints, code blocks, system requirements, technical specifications

    SCIENTIFIC
    - Presents research findings, experimental data, or scientific theories
    - Follows scientific method with hypotheses, methods, results
    - Contains statistical analysis or experimental procedures
    - References prior research or scientific literature
    Example indicators: methodology sections, statistical results, citations, experimental procedures

    NARRATIVE
    - Tells a story or presents events in sequence
    - Has character development or plot progression
    - Uses descriptive language and scene-setting
    - Organized chronologically or by story elements
    Example indicators: character descriptions, plot developments, narrative flow, dialogue

    BUSINESS
    - Focuses on business operations, strategy, or market analysis
    - Contains financial data or business metrics
    - Addresses organizational or market challenges
    - Includes business recommendations or action items
    Example indicators: market analysis, financial projections, strategic plans, ROI calculations

    ACADEMIC
    - Centers on scholarly research and theoretical frameworks
    - Engages with academic literature and existing theories
    - Develops theoretical arguments or conceptual models
    - Contributes to academic discourse in a field
    Example indicators: literature reviews, theoretical frameworks, scholarly arguments, academic citations

    LEGAL
    - Focuses on laws, regulations, or legal requirements
    - Contains legal terminology and formal language
    - References statutes, cases, or legal precedents
    - Addresses rights, obligations, or compliance
    Example indicators: legal citations, compliance requirements, jurisdictional references, statutory language

    MEDICAL
    - Centers on clinical care, diagnoses, or treatments
    - Uses medical terminology and protocols
    - Addresses patient care or health outcomes
    - Follows clinical guidelines or standards
    Example indicators: diagnostic criteria, treatment protocols, clinical outcomes, medical terminology

    INSTRUCTIONAL
    - Focuses on teaching or skill development
    - Contains learning objectives and outcomes
    - Includes exercises or practice activities
    - Structured for progressive learning
    Example indicators: learning objectives, practice exercises, assessment criteria, skill development

    ANALYTICAL
    - Presents data analysis or systematic examination
    - Contains trends, patterns, or correlations
    - Uses analytical frameworks or methodologies
    - Focuses on drawing conclusions from data
    Example indicators: data trends, analytical methods, pattern analysis, statistical insights

    PROCEDURAL
    - Provides step-by-step instructions or processes
    - Focuses on HOW to accomplish specific tasks
    - Contains clear sequential steps or workflows
    - Emphasizes proper order and procedures
    Example indicators: numbered steps, workflow diagrams, sequential instructions

    GENERAL
    - Contains broad or mixed content types
    - No strong alignment with other categories
    - Covers multiple topics or approaches
    - Uses general language and structure
    Example indicators: mixed content types, general descriptions, broad overviews, diverse topics

    Key Differentiators:

    1. TECHNICAL vs PROCEDURAL:
    - Technical focuses on system components and how they work
    - Procedural focuses on steps to accomplish tasks

    2. SCIENTIFIC vs ACADEMIC:
    - Scientific focuses on experimental methods and results
    - Academic focuses on theoretical frameworks and scholarly discourse

    3. ANALYTICAL vs SCIENTIFIC:
    - Analytical focuses on data patterns and insights
    - Scientific focuses on experimental validation of hypotheses

    4. INSTRUCTIONAL vs PROCEDURAL:
    - Instructional focuses on learning and skill development
    - Procedural focuses on task completion steps

    5. MEDICAL vs SCIENTIFIC:
    - Medical focuses on clinical care and treatment
    - Scientific focuses on research methodology

    Return ONLY the category name that best matches the document's structure and purpose.

    Document excerpt:
    {summary_content}"""
        try:
            response = await self._retry_generate_completion(
                prompt,
                max_tokens=50,
                request_id=request_id,
                task="detecting_document_type"
            )
            return DocumentType.from_str(response.strip().lower())
        except Exception as e:
            logger.error(f"Error detecting document type: {str(e)}", extra={"request_id": request_id})
            return DocumentType.GENERAL

    @staticmethod
    def _create_node(name: str, importance: str = 'high', emoji: str = "") -> Dict[str, Any]:
        """Create a node dictionary with the given parameters.
        
        Args:
            name (str): The name/text content of the node
            importance (str): The importance level ('high', 'medium', 'low')
            emoji (str): The emoji to represent this node
            
        Returns:
            Dict[str, Any]: Node dictionary with all necessary attributes
        """
        return {
            'name': name,
            'importance': importance.lower(),
            'emoji': emoji,
            'subtopics': [],  # Initialize empty lists for children
            'details': []
        }
        
    def _escape_text(self, text: str) -> str:
        """Replace parentheses with Unicode alternatives and handle other special characters."""
        # Replace regular parentheses in content text with Unicode alternatives
        for original, replacement in self.paren_replacements.items():
            text = text.replace(original, replacement)
            
        # Handle percentages
        text = self.percentage_regex1.sub(r'\1%', text)
        text = self.percentage_regex2.sub('%', text)
        
        # Replace special characters while preserving needed symbols
        text = self.special_chars_regex.sub('', text)
        
        # Clean up multiple backslashes
        text = self.backslash_regex.sub(r'\\', text)
        
        return text

    def _format_node_line(self, node: Dict[str, Any], indent_level: int) -> str:
        """Format a single node in Mermaid syntax."""
        indent = '    ' * indent_level
        
        # For root node, always return just the document emoji
        if indent_level == 1:
            return f"{indent}((📄))"
        
        # Get the node text and escape it
        if 'text' in node:
            # For detail nodes
            importance = node.get('importance', 'low')
            marker = {'high': '♦️', 'medium': '🔸', 'low': '🔹'}[importance]
            text = self._escape_text(node['text'])
            return f"{indent}[{marker} {text}]"
        else:
            # For topic and subtopic nodes
            node_name = self._escape_text(node['name'])
            emoji = node.get('emoji', '')
            if emoji and node_name:
                node_name = f"{emoji} {node_name}"
            
            # For main topics (level 2)
            if indent_level == 2:
                return f"{indent}(({node_name}))"
            
            # For subtopics (level 3)
            return f"{indent}({node_name})"

    def _add_node_to_mindmap(self, node: Dict[str, Any], mindmap_lines: List[str], indent_level: int) -> None:
        """Recursively add a node and its children to the mindmap."""
        # Add the current node
        node_line = self._format_node_line(node, indent_level)
        mindmap_lines.append(node_line)
        
        # Add all subtopics first
        for subtopic in node.get('subtopics', []):
            self._add_node_to_mindmap(subtopic, mindmap_lines, indent_level + 1)
            
            # Then add details under each subtopic
            for detail in subtopic.get('details', []):
                detail_line = self._format_node_line({
                    'text': detail['text'],
                    'name': detail['text'],
                    'importance': detail['importance']  # Pass through the importance level
                }, indent_level + 2)
                mindmap_lines.append(detail_line)

    async def _batch_redundancy_check(self, items, content_type='topic', context_prefix='', batch_size=10):
        """Perform early batch redundancy checks to avoid wasting LLM calls.
        
        Args:
            items: List of items to check (topics or subtopics)
            content_type: Type of content ('topic' or 'subtopic')
            context_prefix: Optional context prefix for subtopics (e.g. parent topic name)
            batch_size: Maximum batch size for parallel processing
            
        Returns:
            List of non-redundant items
        """
        if not items or len(items) <= 1:
            return items
            
        # Process in batches for efficient parallel checking
        start_count = len(items)
        logger.info(f"Starting early redundancy check for {len(items)} {content_type}s...")
        
        # Track items to keep (non-redundant)
        unique_items = []
        seen_names = {}
        
        # First, use simple fuzzy matching to catch obvious duplicates
        for item in items:
            item_name = item['name']
            if not await self.is_similar_to_existing(item_name, seen_names, content_type):
                unique_items.append(item)
                seen_names[item_name] = item
        
        # If we still have lots of items, use more aggressive LLM-based similarity
        if len(unique_items) > 3 and len(unique_items) > len(items) * 0.8:  # Only if enough items and not much reduction yet
            try:
                # Create pairs for comparison
                pairs_to_check = []
                for i in range(len(unique_items)-1):
                    for j in range(i+1, len(unique_items)):
                        pairs_to_check.append((i, j))
                
                # Process in batches with semaphore for rate limiting
                redundant_indices = set()
                semaphore = asyncio.Semaphore(3)  # Limit concurrent LLM calls
                
                async def check_pair(i, j):
                    if i in redundant_indices or j in redundant_indices:
                        return None
                        
                    async with semaphore:
                        try:
                            context1 = context2 = content_type
                            if context_prefix:
                                context1 = context2 = f"{content_type} of {context_prefix}"
                                
                            is_redundant = await self.check_similarity_llm(
                                unique_items[i]['name'],
                                unique_items[j]['name'],
                                context1,
                                context2
                            )
                            
                            if is_redundant:
                                # Keep item with more detailed information
                                i_detail = len(unique_items[i].get('name', ''))
                                j_detail = len(unique_items[j].get('name', ''))
                                return (j, i) if i_detail > j_detail else (i, j)
                        except Exception as e:
                            logger.warning(f"Early redundancy check failed: {str(e)}")
                            
                    return None
                    
                # Process batches to maintain parallelism
                for batch_idx in range(0, len(pairs_to_check), batch_size):
                    batch = pairs_to_check[batch_idx:batch_idx + batch_size]
                    results = await asyncio.gather(*(check_pair(i, j) for i, j in batch))
                    
                    # Process results
                    for result in results:
                        if result:
                            redundant_idx, keep_idx = result
                            if redundant_idx not in redundant_indices:
                                redundant_indices.add(redundant_idx)
                                logger.info(f"Found redundant {content_type}: '{unique_items[redundant_idx]['name']}' similar to '{unique_items[keep_idx]['name']}'")
                
                # Filter out redundant items
                unique_items = [item for i, item in enumerate(unique_items) if i not in redundant_indices]
            except Exception as e:
                logger.error(f"Error in aggressive redundancy check: {str(e)}")
        
        reduction = start_count - len(unique_items)
        if reduction > 0:
            logger.info(f"Early redundancy check removed {reduction} redundant {content_type}s ({reduction/start_count*100:.1f}%)")
        
        return unique_items

    async def is_similar_to_existing(self, name: str, existing_names: Union[dict, set], content_type: str = 'topic') -> bool:
        """Check if name is similar to any existing names using stricter fuzzy matching thresholds.
        
        Args:
            name: Text to check for similarity
            existing_names: Dictionary or set of existing names to compare against
            content_type: Type of content being compared ('topic', 'subtopic', or 'detail')
            
        Returns:
            bool: True if similar content exists, False otherwise
        """
        # Lower thresholds to catch more duplicates
        base_threshold = {
            'topic': 75,      # Lower from 85 to catch more duplicates
            'subtopic': 70,   # Lower from 80 to catch more duplicates
            'detail': 65      # Lower from 75 to catch more duplicates
        }[content_type]
        
        # Get threshold for this content type
        threshold = base_threshold
        
        # Adjust threshold based on text length - be more lenient with longer texts
        if len(name) < 10:
            threshold = min(threshold + 10, 95)  # Stricter for very short texts
        elif len(name) > 100:
            threshold = max(threshold - 15, 55)  # More lenient for long texts
        
        # Make adjustments for content types to catch more duplicates
        if content_type == 'subtopic':
            threshold = max(threshold - 10, 60)  # Lower threshold to catch more duplicates
        elif content_type == 'detail':
            threshold = max(threshold - 10, 55)  # Lower threshold to catch more duplicates
        
        # Clean and normalize input text
        name = re.sub(r'\s+', ' ', name.lower().strip())
        name = re.sub(r'[^\w\s]', '', name)
        
        # Special handling for numbered items
        numbered_pattern = self.numbered_pattern
        name_without_number = numbered_pattern.sub(r'\1', name)
        
        # Handle both dict and set inputs
        existing_items = existing_names.keys() if isinstance(existing_names, dict) else existing_names
        
        for existing_name in existing_items:
            # Skip if lengths are vastly different
            existing_clean = re.sub(r'\s+', ' ', str(existing_name).lower().strip())
            existing_clean = re.sub(r'[^\w\s]', '', existing_clean)
            
            if abs(len(name) - len(existing_clean)) > len(name) * 0.7:  # Increased from 0.5
                continue
            
            # Calculate multiple similarity metrics
            basic_ratio = fuzz.ratio(name, existing_clean)
            partial_ratio = fuzz.partial_ratio(name, existing_clean)
            token_sort_ratio = fuzz.token_sort_ratio(name, existing_clean)
            token_set_ratio = fuzz.token_set_ratio(name, existing_clean)
            
            # For numbered items, compare without numbers
            existing_without_number = numbered_pattern.sub(r'\1', existing_clean)
            if name_without_number != name or existing_without_number != existing_clean:
                number_ratio = fuzz.ratio(name_without_number, existing_without_number)
                basic_ratio = max(basic_ratio, number_ratio)
            
            # Weight ratios differently based on content type - higher weights to catch more duplicates
            if content_type == 'topic':
                final_ratio = max(
                    basic_ratio,
                    token_sort_ratio * 1.1,  # Increased weight
                    token_set_ratio * 1.0    # Increased weight
                )
            elif content_type == 'subtopic':
                final_ratio = max(
                    basic_ratio,
                    partial_ratio * 1.0,     # Increased weight
                    token_sort_ratio * 0.95, # Increased weight
                    token_set_ratio * 0.9    # Increased weight
                )
            else:  # details
                final_ratio = max(
                    basic_ratio * 0.95,
                    partial_ratio * 0.9,
                    token_sort_ratio * 0.85,
                    token_set_ratio * 0.8
                )
            
            # Increase ratio for shorter texts to catch more duplicates
            if len(name) < 30:
                final_ratio *= 1.1  # Boost ratio for short texts
            
            # Check against adjusted threshold
            if final_ratio > threshold:
                logger.debug(
                    f"Found similar {content_type}:\n"
                    f"New: '{name}'\n"
                    f"Existing: '{existing_clean}'\n"
                    f"Ratio: {final_ratio:.2f} (threshold: {threshold})"
                )
                return True
        
        return False

    async def check_similarity_llm(self, text1: str, text2: str, context1: str, context2: str) -> bool:
        """LLM-based similarity check between two text elements with stricter criteria."""
        prompt = f"""Compare these two text elements and determine if they express similar core information, making one redundant in the mindmap.

        Text 1 (from {context1}):
        "{text1}"

        Text 2 (from {context2}):
        "{text2}"

        A text is REDUNDANT if ANY of these apply:
        1. It conveys the same primary information or main point as the other text
        2. It covers the same concept from a similar angle or perspective
        3. The semantic meaning overlaps significantly with the other text
        4. A reader would find having both entries repetitive or confusing
        5. One could be safely removed without losing important information

        A text is DISTINCT ONLY if ALL of these apply:
        1. It focuses on a clearly different aspect or perspective
        2. It provides substantial unique information not present in the other
        3. It serves a fundamentally different purpose in context
        4. Both entries together provide significantly more value than either alone
        5. The conceptual overlap is minimal

        When in doubt, mark as REDUNDANT to create a cleaner, more focused mindmap.

        Respond with EXACTLY one of these:
        REDUNDANT (overlapping information about X)
        DISTINCT (different aspect: X)

        where X is a very brief explanation."""

        try:
            response = await self._retry_generate_completion(
                prompt,
                max_tokens=50,
                request_id='similarity_check',
                task="checking_content_similarity"
            )
            
            # Consider anything not explicitly marked as DISTINCT to be REDUNDANT
            result = not response.strip().upper().startswith("DISTINCT")
            
            logger.info(
                f"\n{colored('🔍 Content comparison:', 'cyan')}\n"
                f"Text 1: {colored(text1[:100] + '...', 'yellow')}\n"
                f"Text 2: {colored(text2[:100] + '...', 'yellow')}\n"
                f"Result: {colored('REDUNDANT' if result else 'DISTINCT', 'green')}\n"
                f"LLM Response: {colored(response.strip(), 'white')}"
            )
            return result
        except Exception as e:
            logger.error(f"Error in LLM similarity check: {str(e)}")
            # Default to considering items similar if the check fails
            return True

    async def _process_content_batch(self, content_items: List[ContentItem]) -> Set[int]:
        """Process a batch of content items to identify redundant content with parallel processing.
        
        Args:
            content_items: List of ContentItem objects to process
            
        Returns:
            Set of indices identifying redundant items that should be removed
        """
        redundant_indices = set()
        comparison_tasks = []
        comparison_counter = 0
        
        # Create cache of preprocessed texts to avoid recomputing
        processed_texts = {}
        for idx, item in enumerate(content_items):
            # Normalize text for comparison
            text = re.sub(r'\s+', ' ', item.text.lower().strip())
            text = re.sub(r'[^\w\s]', '', text)
            processed_texts[idx] = text
        
        # Limit concurrent API calls
        semaphore = asyncio.Semaphore(10)  # Adjust based on API limits
        
        # Prepare all comparison tasks first
        for i in range(len(content_items)):
            item1 = content_items[i]
            text1 = processed_texts[i]
            
            for j in range(i + 1, len(content_items)):
                item2 = content_items[j]
                text2 = processed_texts[j]
                
                # Quick exact text match check - avoid API call
                if text1 == text2:
                    # Log this immediately since we're not using the API
                    comparison_counter += 1
                    logger.info(f"\nMaking comparison {comparison_counter}... (exact match found)")
                    
                    # Add to candidates for removal with perfect confidence
                    confidence = 1.0
                    
                    # Determine which to keep based on importance and path
                    item1_importance = self._get_importance_value(item1.importance)
                    item2_importance = self._get_importance_value(item2.importance)
                    
                    if ((item2_importance > item1_importance) or
                        (item2_importance == item1_importance and 
                        len(item2.path) < len(item1.path))):
                        redundant_indices.add(i)
                        confidence_text = f'{confidence:.2f}'
                        logger.info(
                            f"\n{colored('🔄 Removing redundant content:', 'yellow')}\n"
                            f"Keeping: {colored(item2.text[:100] + '...', 'green')}\n"
                            f"Removing: {colored(item1.text[:100] + '...', 'red')}\n"
                            f"Confidence: {colored(confidence_text, 'cyan')}"
                        )
                        break  # Stop processing this item if we're removing it
                    else:
                        redundant_indices.add(j)
                        confidence_text = f'{confidence:.2f}'
                        logger.info(
                            f"\n{colored('🔄 Removing redundant content:', 'yellow')}\n"
                            f"Keeping: {colored(item1.text[:100] + '...', 'green')}\n"
                            f"Removing: {colored(item2.text[:100] + '...', 'red')}\n"
                            f"Confidence: {colored(confidence_text, 'cyan')}"
                        )
                    continue
                
                # Skip if lengths are very different
                len_ratio = min(len(text1), len(text2)) / max(len(text1), len(text2))
                if len_ratio < 0.5:  # Texts differ in length by more than 50%
                    continue
                
                # Skip if one item is already marked for removal
                if i in redundant_indices or j in redundant_indices:
                    continue
                    
                # Add to parallel comparison tasks
                async def check_similarity_with_context(idx1, idx2):
                    """Run similarity check with semaphore and return context for logging"""
                    nonlocal comparison_counter
                    
                    # Atomically increment comparison counter
                    comparison_id = comparison_counter = comparison_counter + 1
                    
                    # Log start of comparison
                    logger.info(f"\nMaking comparison {comparison_id}...")
                    
                    # Run the LLM comparison with rate limiting
                    async with semaphore:
                        try:
                            is_redundant = await self.check_similarity_llm(
                                content_items[idx1].text, 
                                content_items[idx2].text,
                                content_items[idx1].path_str, 
                                content_items[idx2].path_str
                            )
                            
                            # Calculate confidence if redundant
                            confidence = 0.0
                            if is_redundant:
                                # Calculate fuzzy string similarity metrics
                                fuzz_ratio = fuzz.ratio(processed_texts[idx1], processed_texts[idx2]) / 100.0
                                token_sort_ratio = fuzz.token_sort_ratio(processed_texts[idx1], processed_texts[idx2]) / 100.0
                                token_set_ratio = fuzz.token_set_ratio(processed_texts[idx1], processed_texts[idx2]) / 100.0
                                
                                # Combine metrics for overall confidence
                                confidence = (fuzz_ratio * 0.4 + 
                                            token_sort_ratio * 0.3 + 
                                            token_set_ratio * 0.3)
                            
                            return {
                                'comparison_id': comparison_id,
                                'is_redundant': is_redundant,
                                'confidence': confidence,
                                'idx1': idx1,
                                'idx2': idx2,
                                'success': True
                            }
                        except Exception as e:
                            logger.error(f"Error in comparison {comparison_id}: {str(e)}")
                            return {
                                'comparison_id': comparison_id,
                                'success': False,
                                'error': str(e),
                                'idx1': idx1,
                                'idx2': idx2
                            }
                
                # Add task to our list
                comparison_tasks.append(check_similarity_with_context(i, j))
        
        # Run all comparison tasks in parallel
        if comparison_tasks:
            logger.info(f"Starting {len(comparison_tasks)} parallel similarity comparisons")
            results = await asyncio.gather(*comparison_tasks)
            
            # Process results
            # First, collect all redundancies with confidence scores
            redundancy_candidates = []
            for result in results:
                if not result['success']:
                    continue
                    
                if result['is_redundant'] and result['confidence'] > 0.8:  # High confidence threshold
                    redundancy_candidates.append(result)
            
            # Sort by confidence (highest first)
            redundancy_candidates.sort(key=lambda x: x['confidence'], reverse=True)
            
            # Process each redundancy candidate
            for result in redundancy_candidates:
                i, j = result['idx1'], result['idx2']
                
                # Skip if either item is already marked for removal
                if i in redundant_indices or j in redundant_indices:
                    continue
                    
                # Determine which to keep based on importance and path
                item1 = content_items[i]
                item2 = content_items[j]
                item1_importance = self._get_importance_value(item1.importance)
                item2_importance = self._get_importance_value(item2.importance)
                
                if ((item2_importance > item1_importance) or
                    (item2_importance == item1_importance and 
                    len(item2.path) < len(item1.path))):
                    redundant_indices.add(i)
                    confidence_text = f'{result["confidence"]:.2f}'
                    logger.info(
                        f"\n{colored('🔄 Removing redundant content:', 'yellow')}\n"
                        f"Keeping: {colored(item2.text[:100] + '...', 'green')}\n"
                        f"Removing: {colored(item1.text[:100] + '...', 'red')}\n"
                        f"Confidence: {colored(confidence_text, 'cyan')}"
                    )
                else:
                    redundant_indices.add(j)
                    confidence_text = f'{result["confidence"]:.2f}'
                    logger.info(
                        f"\n{colored('🔄 Removing redundant content:', 'yellow')}\n"
                        f"Keeping: {colored(item1.text[:100] + '...', 'green')}\n"
                        f"Removing: {colored(item2.text[:100] + '...', 'red')}\n"
                        f"Confidence: {colored(confidence_text, 'cyan')}"
                    )
        
        logger.info(f"\nBatch processing complete. Made {comparison_counter} comparisons.")
        return redundant_indices                

    def _get_importance_value(self, importance: str) -> int:
        """Convert importance string to numeric value for comparison."""
        return {'high': 3, 'medium': 2, 'low': 1}.get(importance.lower(), 0)

    def _extract_content_for_filtering(self, node: Dict[str, Any], current_path: List[str]) -> None:
        """Extract all content items with their full paths for filtering."""
        if not node:
            return

        # Process current node (including root node)
        if 'name' in node:
            current_node_path = current_path + ([node['name']] if node['name'] else [])
            
            # Add the node itself unless it's the root "Document Mindmap" node
            if len(current_path) > 0 or (node['name'] and node['name'] != 'Document Mindmap'):
                # Determine node type based on path depth
                node_type = 'root' if len(current_path) == 0 else 'topic' if len(current_path) == 1 else 'subtopic'
                
                content_item = ContentItem(
                    text=node['name'],
                    path=current_node_path,
                    node_type=node_type,
                    importance=node.get('importance', 'medium')
                )
                
                # Only add if path is non-empty
                if current_node_path:
                    path_tuple = tuple(current_node_path)
                    self.all_content.append(content_item)
                    self.content_by_path[path_tuple] = content_item

            # Process details at current level
            for detail in node.get('details', []):
                if isinstance(detail, dict) and 'text' in detail:
                    # Only add details if we have a valid parent path
                    if current_node_path:
                        detail_path = current_node_path + ['detail']
                        detail_item = ContentItem(
                            text=detail['text'],
                            path=detail_path,
                            node_type='detail',
                            importance=detail.get('importance', 'medium')
                        )
                        detail_path_tuple = tuple(detail_path)
                        self.all_content.append(detail_item)
                        self.content_by_path[detail_path_tuple] = detail_item

            # Process subtopics
            for subtopic in node.get('subtopics', []):
                self._extract_content_for_filtering(subtopic, current_node_path)
        else:
            # If no name but has subtopics, process them with current path
            for subtopic in node.get('subtopics', []):
                self._extract_content_for_filtering(subtopic, current_path)

    async def final_pass_filter_for_duplicative_content(self, mindmap_data: Dict[str, Any], batch_size: int = 50) -> Dict[str, Any]:
        """Enhanced filter for duplicative content with more aggressive detection and safer rebuilding."""
        USE_VERBOSE = True  # Toggle for verbose logging
        
        def vlog(message: str, color: str = 'white', bold: bool = False):
            """Helper for verbose logging"""
            if USE_VERBOSE:
                attrs = ['bold'] if bold else []
                logger.info(colored(message, color, attrs=attrs))
                
        vlog("\n" + "="*80, 'cyan', True)
        vlog("🔍 STARTING ENHANCED DUPLICATE CONTENT FILTER PASS", 'cyan', True)
        vlog("="*80 + "\n", 'cyan', True)
        
        # Debug input structure
        vlog("\n📥 INPUT MINDMAP STRUCTURE:", 'blue', True)
        vlog(f"Mindmap keys: {list(mindmap_data.keys())}")
        if 'central_theme' in mindmap_data:
            central_theme = mindmap_data['central_theme']
            vlog(f"Central theme keys: {list(central_theme.keys())}")
            vlog(f"Number of initial topics: {len(central_theme.get('subtopics', []))}")
            topics = central_theme.get('subtopics', [])
            vlog("\nInitial topic names:")
            for i, topic in enumerate(topics, 1):
                vlog(f"{i}. {topic.get('name', 'UNNAMED')} ({len(topic.get('subtopics', []))} subtopics)")
        else:
            vlog("WARNING: No 'central_theme' found in mindmap!", 'red', True)
            return mindmap_data  # Return original if no central theme
        
        # Initialize instance variables for content tracking
        vlog("\n🔄 Initializing content tracking...", 'yellow')
        self.all_content = []
        self.content_by_path = {}
        
        # Extract all content items for filtering
        vlog("\n📋 Starting content extraction from central theme...", 'blue', True)
        try:
            # Fixed extraction method - should properly extract all content
            self._extract_content_for_filtering(mindmap_data.get('central_theme', {}), [])
            
            # Verify extraction worked
            vlog(f"✅ Successfully extracted {len(self.all_content)} total content items:", 'green')
            content_types = {}
            for item in self.all_content:
                content_types[item.node_type] = content_types.get(item.node_type, 0) + 1
            for node_type, count in content_types.items():
                vlog(f"  - {node_type}: {count} items", 'green')
        except Exception as e:
            vlog(f"❌ Error during content extraction: {str(e)}", 'red', True)
            return mindmap_data  # Return original data on error
        
        # Check if we have any content to filter
        initial_count = len(self.all_content)
        if initial_count == 0:
            vlog("❌ No content extracted - mindmap appears empty", 'red', True)
            return mindmap_data  # Return original data
        
        # Process content in batches for memory efficiency
        vlog("\n🔄 Processing content in batches...", 'yellow', True)
        content_batches = [
            self.all_content[i:i+batch_size] 
            for i in range(0, len(self.all_content), batch_size)
        ]
        
        all_to_remove = set()
        
        for batch_idx, batch in enumerate(content_batches):
            vlog(f"Processing batch {batch_idx+1}/{len(content_batches)} ({len(batch)} items)...", 'yellow')
            batch_to_remove = await self._process_content_batch(batch)
            
            # Adjust indices to global positions
            global_indices = {batch_idx * batch_size + i for i in batch_to_remove}
            all_to_remove.update(global_indices)
            
            vlog(f"Batch {batch_idx+1} complete: identified {len(batch_to_remove)} redundant items", 'green')
        
        # Get indices of items to keep
        keep_indices = set(range(len(self.all_content))) - all_to_remove
        
        # Convert to set of paths to keep
        vlog("\n🔄 Converting to paths for rebuild...", 'blue')
        keep_paths = {tuple(self.all_content[i].path) for i in keep_indices}
        vlog(f"Keeping {len(keep_paths)} unique paths", 'blue')
        
        # Safety check - add at least one path if none remain
        if not keep_paths and len(self.all_content) > 0:
            vlog("⚠️ No paths remained after filtering! Adding at least one path", 'yellow', True)
            first_item = self.all_content[0]
            keep_paths.add(tuple(first_item.path))
        
        # Rebuild the mindmap with only the paths to keep
        vlog("\n🏗️ Rebuilding mindmap...", 'yellow', True)
        
        def rebuild_mindmap(node: Dict[str, Any], current_path: List[str]) -> Optional[Dict[str, Any]]:
            """Recursively rebuild mindmap keeping only non-redundant content."""
            # Add special case for root node
            if not node:
                return None
                
            # For root node, always keep it and process its subtopics
            if not current_path:
                result = copy.deepcopy(node)
                result['subtopics'] = []
                
                # Process main topics
                for topic in node.get('subtopics', []):
                    if topic.get('name'):
                        topic_path = [topic['name']]
                        rebuilt_topic = rebuild_mindmap(topic, topic_path)
                        if rebuilt_topic:
                            result['subtopics'].append(rebuilt_topic)
                
                # Always return root node even if no subtopics remain
                return result
                    
            # For non-root nodes, check if current path should be kept
            path_tuple = tuple(current_path)
            if path_tuple not in keep_paths:
                return None
                
            result = copy.deepcopy(node)
            result['subtopics'] = []
            
            # Process subtopics
            for subtopic in node.get('subtopics', []):
                if subtopic.get('name'):
                    subtopic_path = current_path + [subtopic['name']]
                    rebuilt_subtopic = rebuild_mindmap(subtopic, subtopic_path)
                    if rebuilt_subtopic:
                        result['subtopics'].append(rebuilt_subtopic)
            
            # Filter details
            if 'details' in result:
                filtered_details = []
                for detail in result['details']:
                    if isinstance(detail, dict) and 'text' in detail:
                        detail_path = current_path + ['detail']
                        if tuple(detail_path) in keep_paths:
                            filtered_details.append(detail)
                result['details'] = filtered_details
            
            # Only return node if it has content
            if result['subtopics'] or result.get('details'):
                return result
            return None
        
        # Rebuild mindmap without redundant content
        filtered_data = rebuild_mindmap(mindmap_data.get('central_theme', {}), [])
        
        # Safety check - add the original data's central theme if rebuild failed completely
        if not filtered_data:
            vlog("❌ Filtering removed all content - using original mindmap", 'red', True)
            return mindmap_data
            
        # Another safety check - ensure we have subtopics
        if not filtered_data.get('subtopics'):
            vlog("❌ Filtering removed all subtopics - using original mindmap", 'red', True) 
            return mindmap_data
        
        # Put the central theme back into a complete mindmap structure
        result_mindmap = {'central_theme': filtered_data}
        
        # Calculate and log statistics
        removed_count = initial_count - len(keep_indices)
        reduction_percentage = (removed_count / initial_count * 100) if initial_count > 0 else 0
        
        vlog(
            f"\n{colored('✅ Duplicate content filtering complete', 'green', attrs=['bold'])}\n"
            f"Original items: {colored(str(initial_count), 'yellow')}\n"
            f"Filtered items: {colored(str(len(keep_indices)), 'yellow')}\n"
            f"Removed {colored(str(removed_count), 'red')} duplicate items "
            f"({colored(f'{reduction_percentage:.1f}%', 'red')} reduction)"
        )
        
        return result_mindmap

    async def generate_mindmap(self, document_content: str, request_id: str) -> str:
        """Generate a complete mindmap from document content with balanced coverage of all topics.
        
        Args:
            document_content (str): The document content to analyze
            request_id (str): Unique identifier for request tracking
            
        Returns:
            str: Complete Mermaid mindmap syntax
            
        Raises:
            MindMapGenerationError: If mindmap generation fails
        """
        try:
            logger.info("Starting mindmap generation process...", extra={"request_id": request_id})
            
            # Initialize content caching and LLM call tracking
            self._content_cache = {}
            self._llm_calls = {
                'topics': 0,
                'subtopics': 0,
                'details': 0
            }
            
            # Initialize tracking of unique concepts
            self._unique_concepts = {
                'topics': set(),
                'subtopics': set(),
                'details': set()
            }
            
            # Enhanced completion tracking
            completion_status = {
                'total_topics': 0,
                'processed_topics': 0,
                'total_subtopics': 0,
                'processed_subtopics': 0,
                'total_details': 0
            }
            
            # Set strict LLM call limits with increased bounds
            max_llm_calls = {
                'topics': 20,      # Increased from 15
                'subtopics': 30,   # Increased from 20
                'details': 40      # Increased from 24
            }

            # Set minimum content requirements with better distribution
            min_requirements = {
                'topics': 4,       # Minimum topics to process
                'subtopics_per_topic': 2,  # Minimum subtopics per topic
                'details_per_subtopic': 3   # Minimum details per subtopic
            }
            
            # Calculate document word count and set limit 
            doc_words = len(document_content.split())
            word_limit = min(doc_words * 0.9, 8000)  # Cap at 8000 words
            current_word_count = 0
            
            logger.info(f"Document size: {doc_words} words. Generation limit: {word_limit:,} words", extra={"request_id": request_id})

            # Helper function to check if we have enough content with stricter enforcement
            def has_sufficient_content():
                if completion_status['processed_topics'] < min_requirements['topics']:
                    return False
                if completion_status['total_topics'] > 0:
                    avg_subtopics_per_topic = (completion_status['processed_subtopics'] / 
                                            completion_status['processed_topics'])
                    if avg_subtopics_per_topic < min_requirements['subtopics_per_topic']:
                        return False
                # Process at least 75% of available topics before considering early stop
                if completion_status['total_topics'] > 0:
                    topics_processed_ratio = completion_status['processed_topics'] / completion_status['total_topics']
                    if topics_processed_ratio < 0.75:
                        return False
                return True
                                        
            # Check cache first for document type with strict caching
            doc_type_key = hashlib.md5(document_content[:1000].encode()).hexdigest()
            if doc_type_key in self._content_cache:
                doc_type = self._content_cache[doc_type_key]
            else:
                doc_type = await self.detect_document_type(document_content, request_id)
                self._content_cache[doc_type_key] = doc_type
                self._llm_calls['topics'] += 1
                
            logger.info(f"Detected document type: {doc_type.name}", extra={"request_id": request_id})
            
            type_prompts = self.type_specific_prompts[doc_type]
            
            # Extract main topics with enhanced LLM call limit and uniqueness check
            if self._llm_calls['topics'] < max_llm_calls['topics']:
                logger.info("Extracting main topics...", extra={"request_id": request_id})
                main_topics = await self._extract_main_topics(document_content, type_prompts['topics'], request_id)
                self._llm_calls['topics'] += 1
                
                # NEW: Perform early redundancy check on main topics
                main_topics = await self._batch_redundancy_check(main_topics, 'topic')
                
                completion_status['total_topics'] = len(main_topics)
            else:
                logger.info("Using cached main topics to avoid excessive LLM calls")
                main_topics = self._content_cache.get('main_topics', [])
                completion_status['total_topics'] = len(main_topics)
            
            if not main_topics:
                raise MindMapGenerationError("No main topics could be extracted from the document")
                
            # Cache main topics with timestamp
            self._content_cache['main_topics'] = {
                'data': main_topics,
                'timestamp': time.time()
            }

            # Process topics with completion tracking
            processed_topics = {}
            # NEW: Track already processed topics for redundancy checking
            processed_topic_names = {}
            
            for topic_idx, topic in enumerate(main_topics, 1):
                # Don't stop early if we haven't processed minimum topics
                should_continue = (topic_idx <= min_requirements['topics'] or 
                                not has_sufficient_content() or
                                completion_status['processed_topics'] < len(main_topics) * 0.75)
                                
                if not should_continue:
                    logger.info(f"Stopping after processing {topic_idx} topics - sufficient content gathered")
                    break
        
                topic_name = topic['name']
                
                # NEW: Check if this topic is redundant with already processed topics
                is_redundant = False
                for processed_name in processed_topic_names:
                    if await self.is_similar_to_existing(topic_name, {processed_name: True}, 'topic'):
                        logger.info(f"Skipping redundant topic: '{topic_name}' (similar to '{processed_name}')")
                        is_redundant = True
                        break
                        
                if is_redundant:
                    continue
                    
                # Track this topic for future redundancy checks
                processed_topic_names[topic_name] = True
                
                # Enhanced word limit check with buffer
                if current_word_count > word_limit * 0.95:  # Increased from 0.9 to ensure more completion
                    logger.info(f"Approaching word limit at {current_word_count}/{word_limit:.0f} words")
                    break

                logger.info(f"Processing topic {topic_idx}/{len(main_topics)}: '{topic_name}' "
                        f"(Words: {current_word_count}/{word_limit:.0f})",
                        extra={"request_id": request_id})
                
                # Track unique concepts with validation
                if topic_name not in self._unique_concepts['topics']:
                    self._unique_concepts['topics'].add(topic_name)
                    completion_status['processed_topics'] += 1

                try:
                    # Enhanced subtopic processing with caching
                    topic_key = hashlib.md5(f"{topic_name}:{doc_type_key}".encode()).hexdigest()
                    if topic_key in self._content_cache:
                        subtopics = self._content_cache[topic_key]
                        logger.info(f"Using cached subtopics for topic: {topic_name}")
                    else:
                        if self._llm_calls['subtopics'] < max_llm_calls['subtopics']:
                            subtopics = await self._extract_subtopics(
                                topic, document_content, type_prompts['subtopics'], request_id
                            )
                            
                            # NEW: Perform early redundancy check on subtopics
                            subtopics = await self._batch_redundancy_check(
                                subtopics, 'subtopic', context_prefix=topic_name
                            )
                            
                            self._content_cache[topic_key] = subtopics
                            self._llm_calls['subtopics'] += 1
                        else:
                            logger.info("Reached subtopic LLM call limit")
                            break
                            
                    topic['subtopics'] = []
                    
                    if subtopics:
                        completion_status['total_subtopics'] += len(subtopics)
                        processed_subtopics = {}
                        
                        # NEW: Track already processed subtopics for redundancy checking
                        processed_subtopic_names = {}
                        
                        # Process each subtopic with completion tracking
                        for subtopic_idx, subtopic in enumerate(subtopics, 1):
                            if self._llm_calls['details'] >= max_llm_calls['details']:
                                logger.info("Reached maximum LLM calls for detail extraction")
                                break
                                
                            subtopic_name = subtopic['name']
                            
                            # NEW: Check redundancy with already processed subtopics
                            is_redundant = False
                            for processed_name in processed_subtopic_names:
                                if await self.is_similar_to_existing(subtopic_name, {processed_name: True}, 'subtopic'):
                                    logger.info(f"Skipping redundant subtopic: '{subtopic_name}' (similar to '{processed_name}')")
                                    is_redundant = True
                                    break
                                    
                            if is_redundant:
                                continue
                                
                            # Track this subtopic for future redundancy checks
                            processed_subtopic_names[subtopic_name] = True
                            
                            # Track word count for subtopics
                            subtopic_words = len(subtopic_name.split())
                            if current_word_count + subtopic_words > word_limit * 0.95:
                                logger.info("Approaching word limit during subtopic processing")
                                break
                                
                            current_word_count += subtopic_words
                            
                            # Track unique subtopics
                            self._unique_concepts['subtopics'].add(subtopic_name)
                            completion_status['processed_subtopics'] += 1

                            try:
                                # Enhanced detail processing with caching
                                subtopic_key = hashlib.md5(f"{subtopic_name}:{topic_key}".encode()).hexdigest()
                                if subtopic_key in self._content_cache:
                                    details = self._content_cache[subtopic_key]
                                    logger.info(f"Using cached details for subtopic: {subtopic_name}")
                                else:
                                    if self._llm_calls['details'] < max_llm_calls['details']:
                                        details = await self._extract_details(
                                            subtopic, document_content, type_prompts['details'], request_id
                                        )
                                        self._content_cache[subtopic_key] = details
                                        self._llm_calls['details'] += 1
                                    else:
                                        details = []
                                
                                subtopic['details'] = []
                                
                                if details:
                                    completion_status['total_details'] += len(details)
                                    
                                    # Process details with completion tracking
                                    seen_details = {}
                                    unique_details = []
                                    
                                    for detail in details:
                                        detail_words = len(detail['text'].split())
                                        
                                        if current_word_count + detail_words > word_limit * 0.98:
                                            logger.info("Approaching word limit during detail processing")
                                            break
                                            
                                        if not await self.is_similar_to_existing(detail['text'], seen_details, 'detail'):
                                            current_word_count += detail_words
                                            seen_details[detail['text']] = True
                                            unique_details.append(detail)
                                            self._unique_concepts['details'].add(detail['text'])
                                    
                                    subtopic['details'] = unique_details
                                
                                processed_subtopics[subtopic_name] = subtopic
                                
                            except Exception as e:
                                logger.error(f"Error processing details for subtopic '{subtopic_name}': {str(e)}")
                                processed_subtopics[subtopic_name] = subtopic
                                continue
                        
                        topic['subtopics'] = list(processed_subtopics.values())
                    
                    processed_topics[topic_name] = topic
                        
                except Exception as e:
                    logger.error(f"Error processing topic '{topic_name}': {str(e)}")
                    processed_topics[topic_name] = topic
                    continue
                
                # Log completion status
                logger.info(
                    f"Completion status: "
                    f"Topics: {completion_status['processed_topics']}/{completion_status['total_topics']}, "
                    f"Subtopics: {completion_status['processed_subtopics']}/{completion_status['total_subtopics']}, "
                    f"Details: {completion_status['total_details']}"
                )
            
            if not processed_topics:
                raise MindMapGenerationError("No topics could be processed")
            
            # Enhanced final statistics logging
            completion_stats = {
                'words_generated': current_word_count,
                'word_limit': word_limit,
                'completion_percentage': (current_word_count/word_limit)*100,
                'topics_processed': completion_status['processed_topics'],
                'total_topics': completion_status['total_topics'],
                'unique_topics': len(self._unique_concepts['topics']),
                'unique_subtopics': len(self._unique_concepts['subtopics']),
                'unique_details': len(self._unique_concepts['details']),
                'llm_calls': self._llm_calls,
                'early_stopping': has_sufficient_content()
            }
            
            logger.info(
                f"Mindmap generation completed:"
                f"\n- Words generated: {completion_stats['words_generated']}/{completion_stats['word_limit']:.0f} "
                f"({completion_stats['completion_percentage']:.1f}%)"
                f"\n- Topics processed: {completion_stats['topics_processed']}/{completion_stats['total_topics']}"
                f"\n- Unique topics: {completion_stats['unique_topics']}"
                f"\n- Unique subtopics: {completion_stats['unique_subtopics']}"
                f"\n- Unique details: {completion_stats['unique_details']}"
                f"\n- LLM calls: topics={completion_stats['llm_calls']['topics']}, "
                f"subtopics={completion_stats['llm_calls']['subtopics']}, "
                f"details={completion_stats['llm_calls']['details']}"
                f"\n- Early stopping: {completion_stats['early_stopping']}",
                extra={"request_id": request_id}
            )
            
            logger.info("Starting initial mindmap generation...")
            concepts = {
                'central_theme': self._create_node('Document Mindmap', 'high')
            }
            concepts['central_theme']['subtopics'] = list(processed_topics.values())
                
            logger.info("Starting duplicate content filtering...")
            try:
                # Explicitly await the filtering
                filtered_concepts = await self.final_pass_filter_for_duplicative_content(
                    concepts,
                    batch_size=25
                )
                
                if not filtered_concepts:
                    logger.warning("Filtering removed all content, using original mindmap")
                    filtered_concepts = concepts
                    
                # NEW: Perform reality check against original document
                logger.info("Starting reality check to filter confabulations...")
                verified_concepts = await self.verify_mindmap_against_source(
                    filtered_concepts, 
                    document_content
                )
                
                if not verified_concepts or not verified_concepts.get('central_theme', {}).get('subtopics'):
                    logger.warning("Reality check removed all content, using filtered mindmap with warning")
                    verified_concepts = filtered_concepts
                
                # Print enhanced usage report with detailed breakdowns
                self.optimizer.token_tracker.print_usage_report()

                try:
                    self._save_emoji_cache()  # Save cache at the end of processing
                except Exception as e:
                    logger.warning(f"Failed to save emoji cache: {str(e)}")
                                    
                logger.info("Successfully verified against source document, generating final mindmap...")
                return self._generate_mermaid_mindmap(verified_concepts)
                
            except Exception as e:
                logger.error(f"Error during content filtering or verification: {str(e)}")
                logger.warning("Using unfiltered mindmap due to filtering/verification error")
                
                # Print usage report even if verification fails
                self.optimizer.token_tracker.print_usage_report()
                
                return self._generate_mermaid_mindmap(concepts)

        except Exception as e:
            logger.error(f"Error in mindmap generation: {str(e)}", extra={"request_id": request_id})
            raise MindMapGenerationError(f"Failed to generate mindmap: {str(e)}")

    async def _extract_main_topics(self, content: str, topics_prompt: str, request_id: str) -> List[Dict[str, Any]]:
        """Extract main topics using LLM with more aggressive deduplication and content preservation.
        
        Args:
            content (str): The document content to analyze
            topics_prompt (str): The prompt template for topic extraction
            request_id (str): Unique identifier for the request
            
        Returns:
            List[Dict[str, Any]]: List of extracted topics with their metadata
            
        Raises:
            MindMapGenerationError: If topic extraction fails
        """
        MAX_TOPICS = 8  # Increased from 6 to ensure complete coverage
        MIN_TOPICS = 4  # Minimum topics to process
        MAX_CONCURRENT_TASKS = 50  # Limit concurrent LLM calls
        
        async def extract_from_chunk(chunk: str) -> List[Dict[str, Any]]:
            """Extract topics from a single content chunk."""
            consolidated_prompt = f"""You are an expert at identifying unique, distinct main topics within content.
                        
            {topics_prompt}

            Additional requirements:
            1. Each topic must be truly distinct from others - avoid overlapping concepts
            2. Combine similar themes into single, well-defined topics
            3. Ensure topics are specific enough to be meaningful but general enough to support subtopics
            4. Aim for 4-8 most significant topics that capture the key distinct areas
            5. Focus on conceptual separation - each topic should represent a unique aspect or dimension
            6. Avoid topics that are too similar or could be subtopics of each other
            7. Prioritize broader topics that can encompass multiple subtopics
            8. Eliminate redundancy - each topic should cover a distinct area with no overlap

            IMPORTANT: 
            1. DO NOT include specific statistics, percentages, or numerical data unless explicitly stated in the source text
            2. DO NOT refer to modern studies, surveys, or analyses that aren't mentioned in the document
            3. DO NOT make up correlation coefficients, growth rates, or other numerical relationships
            4. Keep your content strictly based on what's in the document, not general knowledge about the topic
            5. Use general descriptions rather than specific numbers if the document doesn't provide exact figures

            Current content chunk:
            {chunk}

            IMPORTANT: Respond with ONLY a JSON array of strings representing the main distinct topics.
            Example format: ["First Distinct Topic", "Second Distinct Topic"]"""

            try:
                response = await self.optimizer.generate_completion(
                    consolidated_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task="extracting_main_topics"
                )
                
                logger.debug(f"Raw topics response for chunk: {response}", 
                            extra={"request_id": request_id})
                
                parsed_response = self._parse_llm_response(response, "array")
                
                chunk_topics = []
                seen_names = set()
                
                for topic_name in parsed_response:
                    if isinstance(topic_name, str) and topic_name.strip():
                        cleaned_name = re.sub(r'[`*_#]', '', topic_name)
                        cleaned_name = ' '.join(cleaned_name.split())
                        
                        if cleaned_name and cleaned_name not in seen_names:
                            seen_names.add(cleaned_name)
                            # Select appropriate emoji for topic
                            emoji = await self._select_emoji(cleaned_name, 'topic')
                            chunk_topics.append({
                                'name': cleaned_name,
                                'emoji': emoji,
                                'processed': False,  # Track processing status
                                'importance': 'high',  # Main topics are always high importance
                                'subtopics': [],
                                'details': []
                            })
                
                return chunk_topics
                
            except Exception as e:
                logger.error(f"Error extracting topics from chunk: {str(e)}", 
                            extra={"request_id": request_id})
                return []

        try:
            # Create content chunks with overlap to ensure context preservation
            chunk_size = min(8000, len(content) // 3) if len(content) > 6000 else 4000
            overlap = 250  # Characters of overlap between chunks
            
            # Create overlapping chunks
            content_chunks = []
            start = 0
            while start < len(content):
                end = min(start + chunk_size, len(content))
                # Extend to nearest sentence end if possible
                if end < len(content):
                    next_period = content.find('.', end)
                    if next_period != -1 and next_period - end < 200:  # Don't extend too far
                        end = next_period + 1
                chunk = content[start:end]
                content_chunks.append(chunk)
                start = end - overlap if end < len(content) else end

            # Initialize concurrent processing controls
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
            topics_with_metrics = {}  # Track topic frequency and importance
            unique_topics_seen = set()
            max_chunks_to_process = 5  # Increased from 3

            async def process_chunk(chunk: str, chunk_idx: int) -> List[Dict[str, Any]]:
                """Process a single chunk with semaphore control."""
                if chunk_idx >= max_chunks_to_process:
                    return []
                    
                if len(unique_topics_seen) >= MAX_TOPICS * 1.5:
                    return []
                    
                async with semaphore:
                    return await self._retry_with_exponential_backoff(
                        lambda: extract_from_chunk(chunk)
                    )

            # Process chunks concurrently
            chunk_results = await asyncio.gather(
                *(process_chunk(chunk, idx) for idx, chunk in enumerate(content_chunks))
            )

            # Process results with more aggressive deduplication
            all_topics = []
            for chunk_topics in chunk_results:
                # Track topic frequency and merge similar topics
                for topic in chunk_topics:
                    topic_key = topic['name'].lower()
                    
                    # Check for similar existing topics with stricter criteria
                    similar_found = False
                    for existing_key in list(topics_with_metrics.keys()):
                        if await self.is_similar_to_existing(topic_key, {existing_key: True}, 'topic'):
                            topics_with_metrics[existing_key]['frequency'] += 1
                            similar_found = True
                            break
                    
                    if not similar_found:
                        topics_with_metrics[topic_key] = {
                            'topic': topic,
                            'frequency': 1,
                            'first_appearance': len(all_topics)
                        }
                
                # Only add unique topics
                for topic in chunk_topics:
                    if topic['name'] not in unique_topics_seen:
                        unique_topics_seen.add(topic['name'])
                        all_topics.append(topic)
                        
                        if len(unique_topics_seen) >= MAX_TOPICS * 1.5:
                            break

                # Early stopping checks
                if len(unique_topics_seen) >= MIN_TOPICS:
                    topic_frequencies = [metrics['frequency'] for metrics in topics_with_metrics.values()]
                    if len(topic_frequencies) >= MIN_TOPICS:
                        avg_frequency = sum(topic_frequencies) / len(topic_frequencies)
                        if avg_frequency >= 1.5:  # Topics appear in multiple chunks
                            break

            if not all_topics:
                error_msg = "No valid topics extracted from document"
                logger.error(error_msg, extra={"request_id": request_id})
                raise MindMapGenerationError(error_msg)

            # Add consolidation step when we have too many potential topics
            if len(all_topics) > MIN_TOPICS * 1.5:
                consolidation_prompt = f"""You are merging and consolidating similar topics from a document.

                Here are the current potential topics extracted:
                {json.dumps([topic['name'] for topic in all_topics], indent=2)}

                Requirements:
                1. Identify topics that cover the same or similar concepts
                2. Merge overlapping topics into a single, well-defined topic
                3. Choose the most representative, precise, and concise name for each topic
                4. Ensure each final topic is clearly distinct from others
                5. Aim for exactly {MIN_TOPICS}-{MAX_TOPICS} distinct topics that cover the key areas
                6. Completely eliminate redundancy - each topic should represent a unique conceptual area
                7. Broader topics are preferred over narrower ones if they can encompass the same content
                8. Choose clear, concise topic names that accurately represent the content

                Return ONLY a JSON array of consolidated topic names.
                Example: ["First Consolidated Topic", "Second Consolidated Topic"]"""

                try:
                    response = await self._retry_generate_completion(
                        consolidation_prompt,
                        max_tokens=1000,
                        request_id=request_id,
                        task="consolidating_topics"
                    )
                    
                    consolidated_names = self._parse_llm_response(response, "array")
                    
                    if consolidated_names and len(consolidated_names) >= MIN_TOPICS:
                        # Create new topics from consolidated names
                        consolidated_topics = []
                        seen_names = set()
                        
                        for name in consolidated_names:
                            if isinstance(name, str) and name.strip():
                                cleaned_name = re.sub(r'[`*_#]', '', name)
                                cleaned_name = ' '.join(cleaned_name.split())
                                
                                if cleaned_name and cleaned_name not in seen_names:
                                    emoji = await self._select_emoji(cleaned_name, 'topic')
                                    consolidated_topics.append({
                                        'name': cleaned_name,
                                        'emoji': emoji,
                                        'processed': False,
                                        'importance': 'high',
                                        'subtopics': [],
                                        'details': []
                                    })
                                    seen_names.add(cleaned_name)
                        
                        if len(consolidated_topics) >= MIN_TOPICS:
                            all_topics = consolidated_topics
                            logger.info(f"Successfully consolidated topics from {len(unique_topics_seen)} to {len(consolidated_topics)}")
                except Exception as e:
                    logger.warning(f"Topic consolidation failed: {str(e)}", extra={"request_id": request_id})

            # Sort and select final topics with stricter deduplication
            sorted_topics = sorted(
                topics_with_metrics.values(),
                key=lambda x: (-x['frequency'], x['first_appearance'])
            )

            final_topics = []
            seen_final = set()
            
            # Select final topics with more aggressive deduplication
            for topic_data in sorted_topics:
                topic = topic_data['topic']
                if len(final_topics) >= MAX_TOPICS:
                    break
                    
                if topic['name'] not in seen_final:
                    similar_exists = False
                    for existing_topic in final_topics:
                        if await self.is_similar_to_existing(topic['name'], {existing_topic['name']: True}, 'topic'):
                            similar_exists = True
                            break
                    
                    if not similar_exists:
                        seen_final.add(topic['name'])
                        final_topics.append(topic)

            # Add additional topics if needed 
            if len(final_topics) < MIN_TOPICS:
                for topic in all_topics:
                    if len(final_topics) >= MIN_TOPICS:
                        break
                        
                    if topic['name'] not in seen_final:
                        similar_exists = False
                        for existing_topic in final_topics:
                            if await self.is_similar_to_existing(topic['name'], {existing_topic['name']: True}, 'topic'):
                                similar_exists = True
                                break
                        
                        if not similar_exists:
                            seen_final.add(topic['name'])
                            final_topics.append(topic)

            # Final LLM-based deduplication when we have enough topics
            if len(final_topics) > MIN_TOPICS:
                for i in range(len(final_topics)-1, 0, -1):
                    if len(final_topics) <= MIN_TOPICS:
                        break
                        
                    for j in range(i-1, -1, -1):
                        try:
                            is_duplicate = await self.check_similarity_llm(
                                final_topics[i]['name'], 
                                final_topics[j]['name'],
                                "main topic", 
                                "main topic"
                            )
                            
                            if is_duplicate and len(final_topics) > MIN_TOPICS:
                                logger.info(f"LLM detected duplicate topics: '{final_topics[i]['name']}' and '{final_topics[j]['name']}'")
                                del final_topics[i]
                                break
                        except Exception as e:
                            logger.warning(f"LLM duplicate check failed: {str(e)}")
                            continue

            logger.info(
                f"Successfully extracted {len(final_topics)} main topics "
                f"(min: {MIN_TOPICS}, max: {MAX_TOPICS})",
                extra={"request_id": request_id}
            )

            return final_topics

        except Exception as e:
            error_msg = f"Failed to extract main topics: {str(e)}"
            logger.error(error_msg, extra={"request_id": request_id})
            raise MindMapGenerationError(error_msg)

    async def _extract_subtopics(self, topic: Dict[str, Any], content: str, subtopics_prompt_template: str, request_id: str) -> List[Dict[str, Any]]:
        """Extract subtopics using LLM with more aggressive deduplication and content preservation."""
        MAX_SUBTOPICS = self.config['max_subtopics']
        MAX_CONCURRENT_TASKS = 50  # Limit concurrent LLM calls
        
        content_hash = hashlib.md5(content.encode()).hexdigest()
        cache_key = f"subtopics_{topic['name']}_{content_hash}_{request_id}"
        
        if not hasattr(self, '_subtopics_cache'):
            self._subtopics_cache = {}
            
        if not hasattr(self, '_processed_chunks_by_topic'):
            self._processed_chunks_by_topic = {}
        
        if topic['name'] not in self._processed_chunks_by_topic:
            self._processed_chunks_by_topic[topic['name']] = set()

        async def extract_from_chunk(chunk: str) -> List[Dict[str, Any]]:
            chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
            if chunk_hash in self._processed_chunks_by_topic[topic['name']]:
                return []
                
            self._processed_chunks_by_topic[topic['name']].add(chunk_hash)
                
            enhanced_prompt = f"""You are an expert at identifying distinct, relevant subtopics that support a main topic.

            Topic: {topic['name']}

            {subtopics_prompt_template.format(topic=topic['name'])}

            Additional requirements:
            1. Each subtopic must provide unique value and perspective with NO conceptual overlap
            2. Include both high-level and specific subtopics that are clearly distinct
            3. Ensure strong connection to main topic without repeating the topic itself
            4. Focus on distinct aspects or dimensions that don't overlap with each other
            5. Include 4-6 important subtopics that cover different facets of the topic
            6. Balance breadth and depth of coverage with zero redundancy
            7. Choose clear, concise subtopic names that accurately represent the content
            8. Eliminate subtopics that could be merged without significant information loss

            IMPORTANT: 
            1. DO NOT include specific statistics, percentages, or numerical data unless explicitly stated in the source text
            2. DO NOT refer to modern studies, surveys, or analyses that aren't mentioned in the document
            3. DO NOT make up correlation coefficients, growth rates, or other numerical relationships
            4. Keep your content strictly based on what's in the document, not general knowledge about the topic
            5. Use general descriptions rather than specific numbers if the document doesn't provide exact figures

            Content chunk:
            {chunk}

            IMPORTANT: Return ONLY a JSON array of strings representing distinct subtopics.
            Example: ["First Distinct Subtopic", "Second Distinct Subtopic"]"""

            try:
                response = await self.optimizer.generate_completion(
                    enhanced_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"extracting_subtopics_{topic['name']}"
                )
                
                logger.debug(f"Raw subtopics response for {topic['name']}: {response}", 
                            extra={"request_id": request_id})
                
                parsed_response = self._parse_llm_response(response, "array")
                
                chunk_subtopics = []
                seen_names = {}
                
                for subtopic_name in parsed_response:
                    if isinstance(subtopic_name, str) and subtopic_name.strip():
                        cleaned_name = re.sub(r'[`*_#]', '', subtopic_name)
                        cleaned_name = ' '.join(cleaned_name.split())
                        
                        if cleaned_name and not await self.is_similar_to_existing(cleaned_name, seen_names, 'subtopic'):
                            emoji = await self._select_emoji(cleaned_name, 'subtopic')
                            node = self._create_node(
                                name=cleaned_name,
                                emoji=emoji
                            )
                            chunk_subtopics.append(node)
                            seen_names[cleaned_name] = node
                
                return chunk_subtopics
                
            except Exception as e:
                logger.error(f"Error extracting subtopics from chunk for {topic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                return []

        try:
            if cache_key in self._subtopics_cache:
                return self._subtopics_cache[cache_key]
                
            chunk_size = min(8000, len(content) // 3) if len(content) > 6000 else 4000
            content_chunks = [content[i:i + chunk_size] 
                            for i in range(0, len(content), chunk_size)]
            
            # Initialize concurrent processing controls
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
            seen_names = {}
            all_subtopics = []
            
            async def process_chunk(chunk: str) -> List[Dict[str, Any]]:
                """Process a single chunk with semaphore control."""
                async with semaphore:
                    return await self._retry_with_exponential_backoff(
                        lambda: extract_from_chunk(chunk)
                    )

            # Process chunks concurrently
            chunk_results = await asyncio.gather(
                *(process_chunk(chunk) for chunk in content_chunks)
            )

            # Process results with more aggressive deduplication
            for chunk_subtopics in chunk_results:
                for subtopic in chunk_subtopics:
                    if not await self.is_similar_to_existing(subtopic['name'], seen_names, 'subtopic'):
                        seen_names[subtopic['name']] = subtopic
                        all_subtopics.append(subtopic)

            if not all_subtopics:
                logger.warning(f"No subtopics found for topic {topic['name']}", 
                            extra={"request_id": request_id})
                return []

            # Always perform consolidation to reduce duplicative content
            consolidation_prompt = f"""You are consolidating subtopics for the main topic: {topic['name']}

            Current subtopics:
            {json.dumps([st['name'] for st in all_subtopics], indent=2)}

            Requirements:
            1. Aggressively merge subtopics that cover similar information or concepts
            2. Eliminate any conceptual redundancy between subtopics
            3. Choose the clearest and most representative name for each consolidated subtopic
            4. Each final subtopic must address a unique aspect of the main topic
            5. Select 3-5 truly distinct subtopics that together fully cover the topic
            6. Ensure zero information repetition between subtopics
            7. Prioritize broader subtopics that can encompass multiple narrower ones
            8. Choose clear, concise subtopic names that accurately represent the content

            Return ONLY a JSON array of consolidated subtopic names.
            Example: ["First Consolidated Subtopic", "Second Consolidated Subtopic"]"""

            try:
                consolidation_response = await self._retry_generate_completion(
                    consolidation_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"consolidate_subtopics_{topic['name']}"
                )
                
                consolidated_names = self._parse_llm_response(consolidation_response, "array")
                
                if consolidated_names:
                    seen_names = {}
                    consolidated_subtopics = []
                    
                    for name in consolidated_names:
                        if isinstance(name, str) and name.strip():
                            cleaned_name = re.sub(r'[`*_#]', '', name)
                            cleaned_name = ' '.join(cleaned_name.split())
                            
                            if cleaned_name and not await self.is_similar_to_existing(cleaned_name, seen_names, 'subtopic'):
                                emoji = await self._select_emoji(cleaned_name, 'subtopic')
                                node = self._create_node(
                                    name=cleaned_name,
                                    emoji=emoji
                                )
                                consolidated_subtopics.append(node)
                                seen_names[cleaned_name] = node
                    
                    if consolidated_subtopics:
                        all_subtopics = consolidated_subtopics
                        logger.info(f"Successfully consolidated subtopics for {topic['name']} from {len(all_subtopics)} to {len(consolidated_subtopics)}")
                        
            except Exception as e:
                logger.warning(f"Subtopic consolidation failed for {topic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                # If consolidation fails, do a simple deduplication pass
                seen = set()
                deduplicated_subtopics = []
                for subtopic in all_subtopics:
                    if subtopic['name'] not in seen:
                        seen.add(subtopic['name'])
                        deduplicated_subtopics.append(subtopic)
                all_subtopics = sorted(deduplicated_subtopics, 
                                    key=lambda x: len(x['name']), 
                                    reverse=True)[:MAX_SUBTOPICS]
            
            # Final LLM-based deduplication when we have enough subtopics
            if len(all_subtopics) > 3:  # Only if we have enough to potentially remove some
                for i in range(len(all_subtopics)-1, 0, -1):
                    if len(all_subtopics) <= 3:  # Ensure we keep at least 3 subtopics
                        break
                        
                    for j in range(i-1, -1, -1):
                        try:
                            is_duplicate = await self.check_similarity_llm(
                                all_subtopics[i]['name'], 
                                all_subtopics[j]['name'],
                                f"subtopic of {topic['name']}", 
                                f"subtopic of {topic['name']}"
                            )
                            
                            if is_duplicate:
                                logger.info(f"LLM detected duplicate subtopics: '{all_subtopics[i]['name']}' and '{all_subtopics[j]['name']}'")
                                del all_subtopics[i]
                                break
                        except Exception as e:
                            logger.warning(f"LLM duplicate check failed: {str(e)}")
                            continue
            
            final_subtopics = all_subtopics[:MAX_SUBTOPICS]
            self._subtopics_cache[cache_key] = final_subtopics
            
            logger.info(f"Successfully extracted {len(final_subtopics)} subtopics for {topic['name']}", 
                        extra={"request_id": request_id})
            return final_subtopics
            
        except Exception as e:
            logger.error(f"Failed to extract subtopics for topic {topic['name']}: {str(e)}", 
                        extra={"request_id": request_id})
            return []

    def _validate_detail(self, detail: Dict[str, Any]) -> bool:
        """Validate a single detail entry with more flexible constraints."""
        try:
            # Basic structure validation
            if not isinstance(detail, dict):
                logger.debug(f"Detail not a dict: {type(detail)}")
                return False
                
            # Required fields check
            if not all(k in detail for k in ['text', 'importance']):
                logger.debug(f"Missing required fields. Found keys: {detail.keys()}")
                return False
                
            # Text validation
            if not isinstance(detail['text'], str) or not detail['text'].strip():
                logger.debug("Invalid or empty text field")
                return False
                
            # Importance validation with case insensitivity
            valid_importance = ['high', 'medium', 'low']
            if detail['importance'].lower() not in valid_importance:
                logger.debug(f"Invalid importance: {detail['importance']}")
                return False
                
            # More generous length limit
            if len(detail['text']) > 500:  # Increased from 200
                logger.debug(f"Text too long: {len(detail['text'])} chars")
                return False
                
            return True
            
        except Exception as e:
            logger.debug(f"Validation error: {str(e)}")
            return False

    async def _extract_details(self, subtopic: Dict[str, Any], content: str, details_prompt_template: str, request_id: str) -> List[Dict[str, Any]]:
        """Extract details for a subtopic with more aggressive deduplication and content preservation."""
        MINIMUM_VALID_DETAILS = 5  # Early stopping threshold
        MAX_DETAILS = self.config['max_details']
        MAX_CONCURRENT_TASKS = 50  # Limit concurrent LLM calls
        
        # Create cache key
        content_hash = hashlib.md5(content.encode()).hexdigest()
        cache_key = f"details_{subtopic['name']}_{content_hash}_{request_id}"
        
        if not hasattr(self, '_details_cache'):
            self._details_cache = {}
        
        if not hasattr(self, '_processed_chunks_by_subtopic'):
            self._processed_chunks_by_subtopic = {}
        
        if subtopic['name'] not in self._processed_chunks_by_subtopic:
            self._processed_chunks_by_subtopic[subtopic['name']] = set()

        if not hasattr(self, '_current_details'):
            self._current_details = []

        async def extract_from_chunk(chunk: str) -> List[Dict[str, Any]]:
            chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
            if chunk_hash in self._processed_chunks_by_subtopic[subtopic['name']]:
                return []
                
            self._processed_chunks_by_subtopic[subtopic['name']].add(chunk_hash)
                
            enhanced_prompt = f"""You are an expert at identifying distinct, important details that support a specific subtopic.

            Subtopic: {subtopic['name']}

            {details_prompt_template.format(subtopic=subtopic['name'])}

            Additional requirements:
            1. Each detail MUST provide 3-5 sentences of specific, substantive information
            2. Include CONCRETE EXAMPLES, numbers, dates, or direct references from the text
            3. EXTRACT actual quotes or paraphrase specific passages from the source document
            4. Make each detail UNIQUELY VALUABLE - it should contain information not found in other details
            5. Focus on DEPTH rather than breadth - explore fewer ideas more thoroughly
            6. Include specific evidence, reasoning, or context that supports the subtopic
            7. Balance factual information with analytical insights
            8. Avoid generic statements that could apply to many documents

            Content chunk:
            {chunk}

            IMPORTANT: Return ONLY a JSON array where each object has:
            - "text": The detail text (3-5 sentences with specific examples and evidence)
            - "importance": "high", "medium", or "low" based on significance
            """

            try:
                response = await self.optimizer.generate_completion(
                    enhanced_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"extracting_details_{subtopic['name']}"
                )
                
                raw_details = self._clean_detail_response(response)
                chunk_details = []
                seen_texts = {}
                
                for detail in raw_details:
                    if self._validate_detail(detail) and not await self.is_similar_to_existing(detail['text'], seen_texts, 'detail'):
                        seen_texts[detail['text']] = True
                        
                        # Ensure importance is valid
                        detail['importance'] = detail['importance'].lower()
                        if detail['importance'] not in ['high', 'medium', 'low']:
                            detail['importance'] = 'medium'
                        
                        # Add to results
                        chunk_details.append({
                            'text': detail['text'],
                            'importance': detail['importance']
                        })
                        self._current_details.append(detail)
                        
                        if len(self._current_details) >= MINIMUM_VALID_DETAILS:
                            logger.info(f"Reached minimum required details ({MINIMUM_VALID_DETAILS}) during chunk processing")
                            return chunk_details
                
                return chunk_details
                    
            except Exception as e:
                logger.error(f"Error extracting details from chunk for {subtopic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                return chunk_details if 'chunk_details' in locals() else []

        try:
            if cache_key in self._details_cache:
                return self._details_cache[cache_key]

            self._current_details = []
            chunk_size = min(8000, len(content) // 3) if len(content) > 6000 else 4000
            content_chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]
            
            # Initialize concurrent processing controls
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
            seen_texts = {}
            all_details = []
            early_stop = asyncio.Event()

            async def process_chunk(chunk: str) -> List[Dict[str, Any]]:
                """Process a single chunk with semaphore control."""
                if early_stop.is_set():
                    return []
                    
                async with semaphore:
                    chunk_details = await self._retry_with_exponential_backoff(
                        lambda: extract_from_chunk(chunk)
                    )
                    
                    # Check if we've reached minimum details
                    if len(self._current_details) >= MINIMUM_VALID_DETAILS:
                        early_stop.set()
                    
                    return chunk_details

            # Process chunks concurrently
            chunk_results = await asyncio.gather(
                *(process_chunk(chunk) for chunk in content_chunks)
            )

            # Process results with more aggressive deduplication
            for chunk_details in chunk_results:
                for detail in chunk_details:
                    if not await self.is_similar_to_existing(detail['text'], seen_texts, 'detail'):
                        seen_texts[detail['text']] = True
                        all_details.append(detail)

                        if len(all_details) >= MINIMUM_VALID_DETAILS:
                            break

                if len(all_details) >= MINIMUM_VALID_DETAILS:
                    logger.info(f"Reached minimum required details ({MINIMUM_VALID_DETAILS})")
                    break

            # Always perform consolidation to reduce duplicative content
            consolidation_prompt = f"""You are consolidating details for the subtopic: {subtopic['name']}

            Current details:
            {json.dumps([d['text'] for d in all_details], indent=2)}

            Requirements:
            1. Aggressively merge details that convey similar information or concepts
            2. Eliminate all redundancy and repetitive information 
            3. Choose the most clear, concise, and informative phrasing for each detail
            4. Each final detail must provide unique information not covered by others
            5. Select 3-5 truly distinct details that together fully support the subtopic
            6. Ensure that even similar-sounding details have completely different content
            7. Choose clear, concise detail text that accurately represents the information
            8. Mark each detail with appropriate importance (high/medium/low)

            Return ONLY a JSON array of consolidated details with text and importance.
            Example:
            [
                {{"text": "First distinct detail", "importance": "high"}},
                {{"text": "Second distinct detail", "importance": "medium"}}
            ]"""

            try:
                consolidation_response = await self._retry_generate_completion(
                    consolidation_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"consolidate_details_{subtopic['name']}"
                )
                
                consolidated_raw = self._clean_detail_response(consolidation_response)
                
                if consolidated_raw:
                    seen_texts = {}
                    consolidated_details = []
                    
                    for detail in consolidated_raw:
                        if self._validate_detail(detail) and not await self.is_similar_to_existing(detail['text'], seen_texts, 'detail'):
                            seen_texts[detail['text']] = True
                            detail['importance'] = detail['importance'].lower()
                            if detail['importance'] not in ['high', 'medium', 'low']:
                                detail['importance'] = 'medium'
                            consolidated_details.append(detail)
                            
                    if consolidated_details:
                        all_details = consolidated_details
                        logger.info(f"Successfully consolidated details for {subtopic['name']} from {len(all_details)} to {len(consolidated_details)}")
                    
            except Exception as e:
                logger.warning(f"Detail consolidation failed for {subtopic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                # If consolidation fails, do a simple deduplication pass
                seen = set()
                deduplicated_details = []
                for detail in all_details:
                    if detail['text'] not in seen:
                        seen.add(detail['text'])
                        deduplicated_details.append(detail)
                all_details = deduplicated_details
                if len(self._current_details) >= MINIMUM_VALID_DETAILS:
                    logger.info(f"Using {len(self._current_details)} previously collected valid details")
                    all_details = self._current_details
                else:
                    importance_order = {"high": 0, "medium": 1, "low": 2}
                    all_details = sorted(
                        all_details,
                        key=lambda x: (importance_order.get(x["importance"].lower(), 3), -len(x["text"]))
                    )[:MAX_DETAILS]

            # Final LLM-based deduplication when we have enough details
            if len(all_details) > 3:  # Only if we have enough to potentially remove some
                details_to_remove = set()
                for i in range(len(all_details)-1):
                    if i in details_to_remove:
                        continue
                        
                    if len(all_details) - len(details_to_remove) <= 3:  # Ensure we keep at least 3 details
                        break
                        
                    for j in range(i+1, len(all_details)):
                        if j in details_to_remove:
                            continue
                        
                        try:
                            is_duplicate = await self.check_similarity_llm(
                                all_details[i]['text'], 
                                all_details[j]['text'],
                                f"detail of {subtopic['name']}", 
                                f"detail of {subtopic['name']}"
                            )
                            
                            if is_duplicate:
                                logger.info("LLM detected duplicate details")
                                
                                # Determine which to keep based on importance
                                importance_i = {"high": 3, "medium": 2, "low": 1}[all_details[i]['importance']]
                                importance_j = {"high": 3, "medium": 2, "low": 1}[all_details[j]['importance']]
                                
                                if importance_i >= importance_j:
                                    details_to_remove.add(j)
                                else:
                                    details_to_remove.add(i)
                                    break  # Break inner loop if we're removing i
                        except Exception as e:
                            logger.warning(f"LLM duplicate check failed: {str(e)}")
                            continue
                            
                # Apply removals
                all_details = [d for i, d in enumerate(all_details) if i not in details_to_remove]
            
            # Sort details by importance first, then by length (longer details typically have more substance)
            importance_order = {"high": 0, "medium": 1, "low": 2}
            final_details = sorted(
                all_details, 
                key=lambda x: (importance_order.get(x["importance"].lower(), 3), -len(x["text"]))
            )[:MAX_DETAILS]            
            self._details_cache[cache_key] = final_details
            
            logger.info(f"Successfully extracted {len(final_details)} details for {subtopic['name']}", 
                            extra={"request_id": request_id})
            return final_details
                
        except Exception as e:
            logger.error(f"Failed to extract details for subtopic {subtopic['name']}: {str(e)}", 
                        extra={"request_id": request_id})
            if hasattr(self, '_current_details') and len(self._current_details) > 0:
                logger.info(f"Returning {len(self._current_details)} collected details despite error")
                return self._current_details[:MAX_DETAILS]
            return []
            
    async def _retry_generate_completion(self, prompt: str, max_tokens: int, request_id: str, task: str) -> str:
        """Retry the LLM completion in case of failures with exponential backoff."""
        retries = 0
        base_delay = 1  # Start with 1 second delay
        
        while retries < self.config['max_retries']:
            try:
                response = await self.optimizer.generate_completion(
                    prompt,
                    max_tokens=max_tokens,
                    request_id=request_id,
                    task=task
                )
                return response
            except Exception as e:
                retries += 1
                if retries >= self.config['max_retries']:
                    logger.error(f"Exceeded maximum retries for {task}", extra={"request_id": request_id})
                    raise
                
                delay = min(base_delay * (2 ** (retries - 1)), 10)  # Cap at 10 seconds
                logger.warning(f"Retrying {task} ({retries}/{self.config['max_retries']}) after {delay}s: {str(e)}", extra={"request_id": request_id})
                await asyncio.sleep(delay)

    async def verify_mindmap_against_source(self, mindmap_data: Dict[str, Any], original_document: str) -> Dict[str, Any]:
        """Verify all mindmap nodes against the original document with lenient criteria and improved error handling."""
        try:
            logger.info("\n" + "="*80)
            logger.info(colored("🔍 STARTING REALITY CHECK TO IDENTIFY POTENTIAL CONFABULATIONS", "cyan", attrs=["bold"]))
            logger.info("="*80 + "\n")
            
            # Split document into chunks to handle context window limitations
            chunk_size = 8000  # Adjust based on model context window
            overlap = 250  # Characters of overlap between chunks
            
            # Create overlapping chunks
            doc_chunks = []
            start = 0
            while start < len(original_document):
                end = min(start + chunk_size, len(original_document))
                # Extend to nearest sentence end if possible
                if end < len(original_document):
                    next_period = original_document.find('.', end)
                    if next_period != -1 and next_period - end < 200:  # Don't extend too far
                        end = next_period + 1
                chunk = original_document[start:end]
                doc_chunks.append(chunk)
                start = end - overlap if end < len(original_document) else end
            
            logger.info(f"Split document into {len(doc_chunks)} chunks for verification")
            
            # Extract all nodes from mindmap for verification
            all_nodes = []
            
            def extract_nodes(node, path=None):
                """Recursively extract all nodes with their paths."""
                if path is None:
                    path = []
                
                if not node:
                    return
                    
                current_path = path.copy()
                
                # Add current node if it has a name
                if 'name' in node and node['name']:
                    node_type = 'root' if not path else 'topic' if len(path) == 1 else 'subtopic'
                    all_nodes.append({
                        'text': node['name'],
                        'path': current_path,
                        'type': node_type,
                        'verified': False,
                        'node_ref': node,  # Store reference to original node
                        'node_id': id(node),  # Store unique object ID as backup
                        'structural_importance': 'high' if node_type in ['root', 'topic'] else 'medium'
                    })
                    current_path = current_path + [node['name']]
                
                # Add details
                for detail in node.get('details', []):
                    if isinstance(detail, dict) and 'text' in detail:
                        all_nodes.append({
                            'text': detail['text'],
                            'path': current_path,
                            'type': 'detail',
                            'verified': False,
                            'node_ref': detail,  # Store reference to original node
                            'node_id': id(detail),  # Store unique object ID as backup
                            'structural_importance': 'low',
                            'importance': detail.get('importance', 'medium')
                        })
                
                # Process subtopics
                for subtopic in node.get('subtopics', []):
                    extract_nodes(subtopic, current_path)
            
            # Start extraction from central theme
            extract_nodes(mindmap_data.get('central_theme', {}))
            logger.info(f"Extracted {len(all_nodes)} nodes for verification")
            
            # Create verification batches to limit concurrent API calls
            batch_size = 5  # Number of nodes to verify in parallel
            node_batches = [all_nodes[i:i+batch_size] for i in range(0, len(all_nodes), batch_size)]
            
            # Track verification statistics
            verification_stats = {
                'total': len(all_nodes),
                'verified': 0,
                'not_verified': 0,
                'by_type': {
                    'topic': {'total': 0, 'verified': 0},
                    'subtopic': {'total': 0, 'verified': 0},
                    'detail': {'total': 0, 'verified': 0}
                }
            }
            
            for node_type in ['topic', 'subtopic', 'detail']:
                verification_stats['by_type'][node_type]['total'] = len([n for n in all_nodes if n.get('type') == node_type])
            
            # Function to verify a single node against a document chunk
            async def verify_node_in_chunk(node, chunk):
                """Verify if a node's content is actually present in or can be logically derived from a document chunk."""
                if not node or not chunk:
                    return False
                    
                # Check if node has required keys
                required_keys = ['type', 'text']
                if not all(key in node for key in required_keys):
                    logger.warning(f"Node missing required keys: {node}")
                    return True  # Consider verified if we can't properly check it
                    
                # Special handling for root node
                if node['type'] == 'root':
                    return True  # Always consider root node verified
                    
                node_text = node['text']
                node_type = node['type']
                path_str = ' → '.join(node['path']) if node['path'] else 'root'
                
                prompt = f"""You are an expert fact-checker verifying if information in a mindmap can be reasonably derived from the original document.

            Task: Determine if this {node_type} is supported by the document text or could be reasonably inferred from it.

            {node_type.title()}: "{node_text}"
            Path: {path_str}

            Document chunk:
            ```
            {chunk}
            ```

            VERIFICATION GUIDELINES:
            1. The {node_type} can be EXPLICITLY mentioned OR reasonably inferred from the document, even through logical deduction
            2. Logical synthesis, interpretation, and summarization of concepts in the document are STRONGLY encouraged
            3. Content that represents a reasonable conclusion or implication from the document should be VERIFIED
            4. Content that groups, categorizes, or abstracts ideas from the document should be VERIFIED
            5. High-level insights that connect multiple concepts from the document should be VERIFIED
            6. Only mark as unsupported if it contains specific claims that DIRECTLY CONTRADICT the document
            7. GIVE THE BENEFIT OF THE DOUBT - if the content could plausibly be derived from the document, verify it
            8. When uncertain, LEAN TOWARDS VERIFICATION rather than rejection - mindmaps are meant to be interpretive, not literal
            9. For details specifically, allow for more interpretive latitude - they represent insights derived from the document
            10. Consider historical and domain context that would be natural to include in an analysis

            Answer ONLY with one of these formats:
            - "YES: [brief explanation of how it's supported or can be derived]" 
            - "NO: [brief explanation of why it contains information that directly contradicts the document]"

            IMPORTANT: Remember to be GENEROUS in your interpretation. If there's any reasonable way the content could be derived from the document, even through multiple logical steps, mark it as verified. Only reject content that introduces completely new facts not derivable from the document or directly contradicts it."""

                try:
                    response = await self._retry_generate_completion(
                        prompt,
                        max_tokens=150,
                        request_id='verify_node',
                        task="verifying_against_source"
                    )
                    
                    # Parse the response to get verification result
                    response = response.strip().upper()
                    is_verified = response.startswith("YES")
                    
                    # Log detailed verification result for debugging
                    logger.debug(
                        f"\n{colored('Verification result for', 'blue')}: {colored(node_text[:50] + '...', 'yellow')}\n"
                        f"Result: {colored('VERIFIED' if is_verified else 'NOT VERIFIED', 'green' if is_verified else 'red')}\n"
                        f"Response: {response[:100]}"
                    )
                    
                    return is_verified
                    
                except Exception as e:
                    logger.error(f"Error verifying node: {str(e)}")
                    # Be more lenient on errors - consider verified
                    return True
            
            # Process each node batch
            for batch_idx, batch in enumerate(node_batches):
                logger.info(f"Verifying batch {batch_idx+1}/{len(node_batches)} ({len(batch)} nodes)")
                
                # For each node, try to verify against any document chunk
                for node in batch:
                    if node.get('verified', False):
                        continue  # Skip if already verified
                        
                    node_verified = False
                    
                    # Try to verify against each chunk
                    for chunk_idx, chunk in enumerate(doc_chunks):
                        if await verify_node_in_chunk(node, chunk):
                            node['verified'] = True
                            node_verified = True
                            verification_stats['verified'] += 1
                            node_type = node.get('type', 'unknown')
                            if node_type in verification_stats['by_type']:
                                verification_stats['by_type'][node_type]['verified'] += 1
                            logger.info(
                                f"{colored('✅ VERIFIED', 'green', attrs=['bold'])}: "
                                f"{node.get('type', 'NODE').upper()} '{node.get('text', '')[:50]}...' "
                                f"(Found in chunk {chunk_idx+1})"
                            )
                            break
                    
                    if not node_verified:
                        verification_stats['not_verified'] += 1
                        logger.info(
                            f"{colored('❓ NOT VERIFIED', 'yellow', attrs=['bold'])}: "
                            f"{node.get('type', 'NODE').upper()} '{node.get('text', '')[:50]}...' "
                            f"(Not found in any chunk)"
                        )
            
            # Calculate verification percentages
            verification_percentage = (verification_stats['verified'] / verification_stats['total'] * 100) if verification_stats['total'] > 0 else 0
            for node_type in ['topic', 'subtopic', 'detail']:
                type_stats = verification_stats['by_type'][node_type]
                type_stats['percentage'] = (type_stats['verified'] / type_stats['total'] * 100) if type_stats['total'] > 0 else 0
            
            # Log verification statistics
            logger.info("\n" + "="*80)
            logger.info(colored("🔍 REALITY CHECK RESULTS", "cyan", attrs=['bold']))
            logger.info(f"Total nodes checked: {verification_stats['total']}")
            logger.info(f"Verified: {verification_stats['verified']} ({verification_percentage:.1f}%)")
            logger.info(f"Not verified: {verification_stats['not_verified']} ({100-verification_percentage:.1f}%)")
            logger.info("\nBreakdown by node type:")
            for node_type in ['topic', 'subtopic', 'detail']:
                type_stats = verification_stats['by_type'][node_type]
                logger.info(f"  {node_type.title()}s: {type_stats['verified']}/{type_stats['total']} verified ({type_stats['percentage']:.1f}%)")
            logger.info("="*80 + "\n")
            
            # Check if we need to preserve structure despite verification results
            min_topics_required = 3
            min_verification_ratio = 0.4  # Lower threshold - only filter if less than 40% verified
            
            # Count verified topics
            verified_topics = len([n for n in all_nodes if n.get('type') == 'topic' and n.get('verified', False)])
            
            # If verification removed too much content, we need to preserve structure
            if verified_topics < min_topics_required or verification_percentage < min_verification_ratio * 100:
                logger.warning(f"Verification would remove too much content (only {verified_topics} topics verified). Using preservation mode.")
                
                # Mark important structural nodes as verified to preserve mindmap structure
                for node in all_nodes:
                    # Always keep root and topic nodes
                    if node.get('type') in ['root', 'topic']:
                        node['verified'] = True
                    # Keep subtopics with a high enough importance
                    elif node.get('type') == 'subtopic' and not node.get('verified', False):
                        # Keep subtopics if they have verified details or are needed for structure
                        has_verified_details = any(
                            n.get('verified', False) and n.get('type') == 'detail' and n.get('path') == node.get('path', []) + [node.get('text', '')]
                            for n in all_nodes
                        )
                        if has_verified_details:
                            node['verified'] = True
                
                # Recalculate statistics
                verification_stats['verified'] = len([n for n in all_nodes if n.get('verified', False)])
                verification_stats['not_verified'] = len(all_nodes) - verification_stats['verified']
                verification_percentage = (verification_stats['verified'] / verification_stats['total'] * 100) if verification_stats['total'] > 0 else 0
                
                logger.info("\n" + "="*80)
                logger.info(colored("🔄 UPDATED REALITY CHECK WITH STRUCTURE PRESERVATION", "yellow", attrs=['bold']))
                logger.info(f"Verified after preservation: {verification_stats['verified']} ({verification_percentage:.1f}%)")
                logger.info(f"Not verified after preservation: {verification_stats['not_verified']} ({100-verification_percentage:.1f}%)")
                logger.info("="*80 + "\n")
            
            # Rebuild mindmap with preserving structure
            def rebuild_mindmap(node):
                """Recursively rebuild mindmap keeping only verified nodes."""
                if not node:
                    return None
                    
                result = copy.deepcopy(node)
                result['subtopics'] = []
                
                # Process subtopics and keep only verified ones
                verified_subtopics = []
                for subtopic in node.get('subtopics', []):
                    if not subtopic.get('name'):
                        continue
                        
                    # Check if this subtopic is verified by comparing with stored nodes
                    subtopic_verified = False
                    subtopic_id = id(subtopic)
                    
                    for n in all_nodes:
                        # First try to match by direct object reference
                        if n.get('node_ref') is subtopic and n.get('verified', False):
                            subtopic_verified = True
                            break
                        # Fallback to matching by object ID if reference comparison fails
                        elif n.get('node_id') == subtopic_id and n.get('verified', False):
                            subtopic_verified = True
                            break
                        # Last resort: match by name and path
                        elif (n.get('type') in ['topic', 'subtopic'] and 
                            n.get('text') == subtopic.get('name') and 
                            n.get('verified', False)):
                            subtopic_verified = True
                            break
                    
                    if subtopic_verified:
                        rebuilt_subtopic = rebuild_mindmap(subtopic)
                        if rebuilt_subtopic:
                            verified_subtopics.append(rebuilt_subtopic)
                
                result['subtopics'] = verified_subtopics
                
                # Filter details to keep only verified ones
                if 'details' in result:
                    verified_details = []
                    for detail in result.get('details', []):
                        if not isinstance(detail, dict) or 'text' not in detail:
                            continue
                            
                        # Check if this detail is verified
                        detail_verified = False
                        detail_id = id(detail)
                        
                        for n in all_nodes:
                            # First try to match by direct object reference
                            if n.get('node_ref') is detail and n.get('verified', False):
                                detail_verified = True
                                break
                            # Fallback to matching by object ID
                            elif n.get('node_id') == detail_id and n.get('verified', False):
                                detail_verified = True
                                break
                            # Last resort: match by text content
                            elif n.get('type') == 'detail' and n.get('text') == detail.get('text') and n.get('verified', False):
                                detail_verified = True
                                break
                        
                        if detail_verified:
                            verified_details.append(detail)
                    
                    result['details'] = verified_details
                
                # Only return node if it has content
                if result.get('subtopics') or result.get('details'):
                    return result
                return None
            
            # Rebuild mindmap with only verified content
            verified_mindmap = {
                'central_theme': rebuild_mindmap(mindmap_data.get('central_theme', {}))
            }
            
            # Final safety check - if we have no content after verification, use original
            if not verified_mindmap.get('central_theme') or not verified_mindmap.get('central_theme', {}).get('subtopics'):
                logger.warning("After verification, no valid content remains - using original mindmap with warning")
                return mindmap_data
            
            # Calculate how much content was preserved
            original_count = len(all_nodes)
            verified_count = len([n for n in all_nodes if n.get('verified', False)])
            preservation_rate = (verified_count / original_count * 100) if original_count > 0 else 0
            
            logger.info(
                f"\n{colored('✅ REALITY CHECK COMPLETE', 'green', attrs=['bold'])}\n"
                f"Preserved {verified_count}/{original_count} nodes ({preservation_rate:.1f}%)"
            )
            
            return verified_mindmap
        
        except Exception as e:
            # Better error handling with detailed logging
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error during verification: {str(e)}\n{error_details}")
            # Return the original mindmap in case of any errors
            return mindmap_data

    def _generate_mermaid_mindmap(self, concepts: Dict[str, Any]) -> str:
        """Generate complete Mermaid mindmap syntax from concepts.
        
        Args:
            concepts (Dict[str, Any]): The complete mindmap concept hierarchy
            
        Returns:
            str: Complete Mermaid mindmap syntax
        """
        mindmap_lines = ["mindmap"]
        
        # Start with root node - ignore any name/text for root, just use document emoji
        self._add_node_to_mindmap({'name': ''}, mindmap_lines, indent_level=1)
        
        # Add all main topics under root
        for topic in concepts.get('central_theme', {}).get('subtopics', []):
            self._add_node_to_mindmap(topic, mindmap_lines, indent_level=2)
        
        return "\n".join(mindmap_lines)

    def _convert_mindmap_to_markdown(self, mermaid_syntax: str) -> str:
        """Convert Mermaid mindmap syntax to properly formatted Markdown outline.
        
        Args:
            mermaid_syntax (str): The Mermaid mindmap syntax string
            
        Returns:
            str: Properly formatted Markdown outline
        """
        markdown_lines = []
        
        # Split into lines and process each (skip the 'mindmap' header)
        lines = mermaid_syntax.split('\n')[1:]
        
        for line in lines:
            # Skip empty lines
            if not line.strip():
                continue
                
            # Count indentation level (number of 4-space blocks)
            indent_level = len(re.match(r'^\s*', line).group()) // 4
            
            # Extract the content between node shapes
            content = line.strip()
            
            # Handle different node types based on indent level
            if indent_level == 1 and '((📄))' in content:  # Root node
                continue  # Skip the document emoji node
                
            elif indent_level == 2:  # Main topics
                # Extract content between (( and ))
                node_text = re.search(r'\(\((.*?)\)\)', content)
                if node_text:
                    if markdown_lines:  # Add extra newline between main topics
                        markdown_lines.append("")
                    current_topic = node_text.group(1).strip()
                    markdown_lines.append(f"# {current_topic}")
                    markdown_lines.append("")  # Add blank line after topic
                    
            elif indent_level == 3:  # Subtopics
                # Extract content between ( and )
                node_text = re.search(r'\((.*?)\)', content)
                if node_text:
                    if markdown_lines and not markdown_lines[-1].startswith("#"):
                        markdown_lines.append("")
                    current_subtopic = node_text.group(1).strip()
                    markdown_lines.append(f"## {current_subtopic}")
                    markdown_lines.append("")  # Add blank line after subtopic
                    
            elif indent_level == 4:  # Details
                # Extract content between [ and ]
                node_text = re.search(r'\[(.*?)\]', content)
                if node_text:
                    detail_text = node_text.group(1).strip()
                    markdown_lines.append(detail_text)
                    markdown_lines.append("")  # Add blank line after each detail
        
        # Join lines with proper spacing
        markdown_text = "\n".join(markdown_lines)
        
        # Clean up any lingering Mermaid syntax artifacts
        markdown_text = re.sub(r'\\\(', '(', markdown_text)
        markdown_text = re.sub(r'\\\)', ')', markdown_text)
        markdown_text = re.sub(r'\\(?=[()])', '', markdown_text)
        
        # Clean up multiple consecutive blank lines
        markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
        
        return markdown_text.strip()
    
    async def generate_mindmap_simple(self, document_content: str, request_id: str) -> str:
        """Generate a mindmap using a single prompt approach for faster generation.
        
        This is a simplified version that uses one LLM call to generate the entire mindmap structure,
        trading some quality for speed.
        
        Args:
            document_content (str): The document content to analyze
            request_id (str): Unique identifier for request tracking
            
        Returns:
            str: Complete Mermaid mindmap syntax
            
        Raises:
            MindMapGenerationError: If mindmap generation fails
        """
        try:
            logger.info("Starting simple mindmap generation process...", extra={"request_id": request_id})
            
            # Initialize tracking
            self._llm_calls = {
                'topics': 0,
                'subtopics': 0,
                'details': 0,
                'simple': 0
            }
            
            # Create simple prompt for direct mindmap generation
            simple_prompt = f"""You are an expert at identifying unique, distinct main topics within content.

            Analyze this document focusing on main conceptual themes and relationships.

            Identify major themes that:
            - Represent complete, independent ideas
            - Form logical groupings of related concepts
            - Support the document's main purpose
            - Connect to other important themes

            Consider:
            1. What are the fundamental ideas being presented?
            2. How do these ideas relate to each other?
            3. What are the key areas of focus?
            4. How is the information structured?

            Avoid topics that are:
            - Too specific (individual examples)
            - Too broad (entire subject areas)
            - Isolated facts without context
            - Purely formatting elements

            Additional requirements:
            1. Each topic must be truly distinct from others - avoid overlapping concepts
            2. Combine similar themes into single, well-defined topics
            3. Ensure topics are specific enough to be meaningful but general enough to support subtopics
            4. Aim for 4-8 most significant topics that capture the key distinct areas
            5. Focus on conceptual separation - each topic should represent a unique aspect or dimension
            6. Avoid topics that are too similar or could be subtopics of each other
            7. Prioritize broader topics that can encompass multiple subtopics
            8. Eliminate redundancy - each topic should cover a distinct area with no overlap

            IMPORTANT: 
            1. DO NOT include specific statistics, percentages, or numerical data unless explicitly stated in the source text
            2. DO NOT refer to modern studies, surveys, or analyses that aren't mentioned in the document
            3. DO NOT make up correlation coefficients, growth rates, or other numerical relationships
            4. Keep your content strictly based on what's in the document, not general knowledge about the topic
            5. Use general descriptions rather than specific numbers if the document doesn't provide exact figures

            Current content:
            {document_content[:4000]}

            IMPORTANT: Respond with ONLY a JSON array of strings representing the main distinct topics.
            Example format: ["First Distinct Topic", "Second Distinct Topic", "Third Distinct Topic"]

            Format: Return a JSON array of primary themes or concept areas."""

            # Make single LLM call
            logger.info("Making single LLM call for simplified mindmap generation...", extra={"request_id": request_id})
            
            response = await self.optimizer.generate_completion(
                simple_prompt,
                max_tokens=3000,
                request_id=request_id,
                task="simple_mindmap_generation"
            )
            
            self._llm_calls['simple'] = 1
            
            if not response:
                raise MindMapGenerationError("No response from LLM for simple mindmap generation")

            # Parse the response
            try:
                # Use the simpler _parse_llm_response method that handles arrays better
                logger.debug(f"Raw simple generation response: {response}", 
                            extra={"request_id": request_id})
                
                parsed_response = self._parse_llm_response(response, "array")
                
                # Validate structure - should be array of strings
                if not isinstance(parsed_response, list):
                    logger.warning(f"Expected list, got {type(parsed_response)}")
                    raise ValueError("Expected array of topic strings")
                
                # Convert to internal format with simplified structure
                concepts = {
                    'central_theme': {
                        'name': 'Document Mindmap',
                        'subtopics': []
                    }
                }
                
                # Process each topic string
                for topic_name in parsed_response:
                    if isinstance(topic_name, str) and topic_name.strip():
                        cleaned_name = re.sub(r'[`*_#]', '', topic_name)
                        cleaned_name = ' '.join(cleaned_name.split())
                        
                        # Select appropriate emoji for topic
                        emoji = await self._select_emoji(cleaned_name, 'topic')
                        
                        # Create simplified topic structure
                        topic = {
                            'name': cleaned_name,
                            'emoji': emoji,
                            'importance': 'high',
                            'subtopics': [{
                                'name': f'{cleaned_name} - 关键要点',
                                'emoji': '📋',
                                'importance': 'medium',
                                'details': [
                                    {
                                        'text': f'关于{cleaned_name}的重要信息',
                                        'importance': 'medium'
                                    },
                                    {
                                        'text': f'{cleaned_name}的核心概念',
                                        'importance': 'medium'
                                    }
                                ]
                            }]
                        }
                        
                        concepts['central_theme']['subtopics'].append(topic)
                
                # If no topics were extracted, create a fallback
                if not concepts['central_theme']['subtopics']:
                    logger.warning("No topics extracted, creating fallback structure")
                    concepts['central_theme']['subtopics'] = [{
                        'name': 'Document Summary',
                        'emoji': '📄',
                        'importance': 'high',
                        'subtopics': [{
                            'name': 'Key Content',
                            'emoji': '📋',
                            'importance': 'medium',
                            'details': [
                                {'text': 'Document analysis completed', 'importance': 'medium'},
                                {'text': 'Content processed successfully', 'importance': 'medium'}
                            ]
                        }]
                    }]
                
                # Generate Mermaid syntax
                mermaid_syntax = self._generate_mermaid_mindmap(concepts)
                
                logger.info(
                    f"Simple mindmap generation completed successfully. "
                    f"Topics: {len(concepts['central_theme']['subtopics'])}, "
                    f"LLM calls: {self._llm_calls['simple']}, "
                    f"Output length: {len(mermaid_syntax)} characters",
                    extra={"request_id": request_id}
                )
                
                return mermaid_syntax
                
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse LLM response as JSON: {str(e)}")
                # Fallback: try to extract basic structure from text
                return await self._fallback_simple_generation(response, request_id)
                
        except Exception as e:
            logger.error(f"Error in simple mindmap generation: {str(e)}", extra={"request_id": request_id})
            raise MindMapGenerationError(f"Failed to generate simple mindmap: {str(e)}")

    async def _fallback_simple_generation(self, response: str, request_id: str) -> str:
        """Fallback method when JSON parsing fails - extract basic structure from text."""
        try:
            logger.info("Using fallback simple generation method...", extra={"request_id": request_id})
            
            # Create basic structure from response text
            lines = response.strip().split('\n')
            topics = []
            
            current_topic = None
            current_subtopic = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # Look for topic indicators
                if any(marker in line.lower() for marker in ['topic:', 'main:', '主题:', '##']):
                    if current_topic:
                        topics.append(current_topic)
                    current_topic = {
                        'name': line.split(':', 1)[-1].strip() if ':' in line else line.replace('#', '').strip(),
                        'emoji': '📄',
                        'importance': 'high',
                        'subtopics': []
                    }
                    current_subtopic = None
                    
                # Look for subtopic indicators
                elif any(marker in line.lower() for marker in ['subtopic:', 'sub:', '子主题:', '###']):
                    if current_topic:
                        if current_subtopic:
                            current_topic['subtopics'].append(current_subtopic)
                        current_subtopic = {
                            'name': line.split(':', 1)[-1].strip() if ':' in line else line.replace('#', '').strip(),
                            'emoji': '📋',
                            'importance': 'medium',
                            'details': []
                        }
                        
                # Look for detail indicators
                elif any(marker in line for marker in ['-', '•', '*', '1.', '2.', '3.']):
                    if current_subtopic:
                        detail_text = line.lstrip('-•*0123456789. ').strip()
                        if detail_text:
                            current_subtopic['details'].append({
                                'text': detail_text,
                                'importance': 'medium'
                            })
            
            # Add final items
            if current_subtopic and current_topic:
                current_topic['subtopics'].append(current_subtopic)
            if current_topic:
                topics.append(current_topic)
            
            # If no topics found, create a basic structure
            if not topics:
                topics = [{
                    'name': 'Document Summary',
                    'emoji': '📄',
                    'importance': 'high',
                    'subtopics': [{
                        'name': 'Key Points',
                        'emoji': '📋',
                        'importance': 'medium',
                        'details': [
                            {'text': 'Content analysis completed', 'importance': 'medium'},
                            {'text': 'Basic structure extracted', 'importance': 'medium'}
                        ]
                    }]
                }]
            
            # Create concepts structure
            concepts = {
                'central_theme': {
                    'name': 'Document Mindmap',
                    'subtopics': topics
                }
            }
            
            # Generate Mermaid syntax
            mermaid_syntax = self._generate_mermaid_mindmap(concepts)
            
            logger.info(
                f"Fallback simple generation completed. Topics: {len(topics)}",
                extra={"request_id": request_id}
            )
            
            return mermaid_syntax
            
        except Exception as e:
            logger.error(f"Fallback simple generation failed: {str(e)}")
            # Return minimal mindmap
            return """mindmap
    ((📄))
        ((📋 Document Analysis))
            (Error in Processing)
                [Simple generation failed]
                [Please try again]"""

def generate_mermaid_html(mermaid_code):
    # Remove leading/trailing triple backticks if present
    mermaid_code = mermaid_code.strip()
    if mermaid_code.startswith('```') and mermaid_code.endswith('```'):
        mermaid_code = mermaid_code[3:-3].strip()
    # Create the data object to be encoded
    data = {
        "code": mermaid_code,
        "mermaid": {"theme": "default"}
    }
    json_string = json.dumps(data)
    compressed_data = zlib.compress(json_string.encode('utf-8'), level=9)
    base64_string = base64.urlsafe_b64encode(compressed_data).decode('utf-8').rstrip('=')
    edit_url = f'https://mermaid.live/edit#pako:{base64_string}'
    # Now generate the HTML template
    html_template = f'''<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Mermaid Mindmap</title>
  <!-- Tailwind CSS -->
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <!-- Mermaid JS -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.0/dist/mermaid.min.js"></script>
  <style>
    body {{
      margin: 0;
      padding: 0;
    }}
    #mermaid {{
      width: 100%;
      height: calc(100vh - 64px); /* Adjust height considering header */
      overflow: auto;
    }}
  </style>
</head>
<body class="bg-gray-100">
  <div class="flex items-center justify-between p-4 bg-white shadow">
    <h1 class="text-xl font-bold">Mermaid Mindmap</h1>
    <a href="{edit_url}" target="_blank" id="editButton" class="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600">Edit in Mermaid Live Editor</a>
  </div>
  <div id="mermaid" class="p-4">
    <pre class="mermaid">
{mermaid_code}
    </pre>
  </div>
  <script>
    mermaid.initialize({{
      startOnLoad: true,
      securityLevel: 'loose',
      theme: 'default',
      mindmap: {{
        useMaxWidth: true
      }},
      themeConfig: {{
        controlBar: true
      }}
    }});
  </script>
</body>
</html>'''
    return html_template

async def generate_document_mindmap(document_id: str, request_id: str) -> Tuple[str, str]:
    """Generate both Mermaid mindmap and Markdown outline for a document.
    
    Args:
        document_id (str): The ID of the document to process
        request_id (str): Unique identifier for request tracking
        
    Returns:
        Tuple[str, str]: (mindmap_file_path, markdown_file_path)
    """
    try:
        generator = MindMapGenerator()
        db = await initialize_db()
        document = await db.get_document_by_id(document_id)
        if not document:
            logger.error(f"Document not found: {document_id}", extra={"request_id": request_id})
            return "", ""

        # Define file paths for both formats
        mindmap_file_path = f"generated_mindmaps/{document['sanitized_filename']}_mermaid_mindmap__{Config.API_PROVIDER.lower()}.txt"
        mindmap_html_file_path = f"generated_mindmaps/{document['sanitized_filename']}_mindmap__{Config.API_PROVIDER.lower()}.html"
        markdown_file_path = f"generated_mindmaps/{document['sanitized_filename']}_mindmap_outline__{Config.API_PROVIDER.lower()}.md"
        
        # Check if both files already exist
        if os.path.exists(mindmap_file_path) and os.path.exists(markdown_file_path):
            logger.info(f"Both mindmap and markdown already exist for document {document_id}. Reusing existing files.", extra={"request_id": request_id})
            return mindmap_file_path, markdown_file_path

        optimized_text = await db.get_optimized_text(document_id, request_id)
        if not optimized_text:
            logger.error(f"Optimized text not found for document: {document_id}", extra={"request_id": request_id})
            return "", ""

        # Generate mindmap
        mermaid_syntax = await generator.generate_mindmap(optimized_text, request_id)
        
        # Convert to HTML
        mermaid_html = generate_mermaid_html(mermaid_syntax)
        
        # Convert to markdown
        markdown_outline = generator._convert_mindmap_to_markdown(mermaid_syntax)

        # Save all 3 formats
        os.makedirs(os.path.dirname(mindmap_file_path), exist_ok=True)
        
        async with aiofiles.open(mindmap_file_path, 'w', encoding='utf-8') as f:
            await f.write(mermaid_syntax)
            
        async with aiofiles.open(mindmap_html_file_path, 'w', encoding='utf-8') as f:
            await f.write(mermaid_html)
            
        async with aiofiles.open(markdown_file_path, 'w', encoding='utf-8') as f:
            await f.write(markdown_outline)

        logger.info(f"Mindmap and associated html/markdown generated successfully for document {document_id}", extra={"request_id": request_id})
        return mindmap_file_path, mindmap_html_file_path, markdown_file_path
        
    except Exception as e:
        logger.error(f"Error generating mindmap and associated html/markdown for document {document_id}: {str(e)}", extra={"request_id": request_id})
        return "", ""

async def process_text_file(filepath: str):
    """Process a single text file and generate mindmap outputs."""
    logger = get_logger()
    try:
        # Read the input file
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        # Store content in our stub database
        MinimalDatabaseStub.store_text(content)
        # Generate a unique document ID based on content hash
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = os.path.splitext(os.path.basename(filepath))[0]
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        # Initialize the mindmap generator
        generator = MindMapGenerator()
        # Generate the mindmap
        mindmap = await generator.generate_mindmap(content, request_id=document_id)
        # Generate HTML
        html = generate_mermaid_html(mindmap)
        # Generate markdown outline
        markdown_outline = generator._convert_mindmap_to_markdown(mindmap)
        # Create output directory if it doesn't exist
        os.makedirs("mindmap_outputs", exist_ok=True)
        # Save outputs with simple names based on input file
        output_files = {
            f"mindmap_outputs/{base_filename}_mindmap__{Config.API_PROVIDER.lower()}.txt": mindmap,
            f"mindmap_outputs/{base_filename}_mindmap__{Config.API_PROVIDER.lower()}.html": html,
            f"mindmap_outputs/{base_filename}_mindmap_outline__{Config.API_PROVIDER.lower()}.md": markdown_outline
        }
        # Save all outputs
        for filename, content in output_files.items():
            with open(filename, "w", encoding="utf-8") as f:
                f.write(content)
                logger.info(f"Saved {filename}")
        
        logger.info("Mindmap generation completed successfully!")
        return output_files
    except Exception as e:
        logger.error(f"Error processing file: {str(e)}")
        raise
    
async def main():
    input_file = "sample_input_document_as_markdown__durnovo_memo.md"  # <-- Change this to your input file path
    # input_file = "sample_input_document_as_markdown__small.md"
    try:
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"Input file not found: {input_file}")
            
        # Process the file
        logger.info(f"Generating mindmap for {input_file}")
        output_files = await process_text_file(input_file)
        
        # Print summary
        print("\nMindmap Generation Complete!")
        print("Generated files:")
        for filename in output_files:
            print(f"- {filename}")
            
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        raise
if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="README.md">
# 🧠 智能思维导图生成器

> 基于大语言模型的智能文档分析与思维导图生成系统

## 🎯 项目概述

这是一个现代化的智能思维导图生成器，能够自动分析文档内容并生成结构化的交互式思维导图。系统集成了多种大语言模型，采用前后端分离架构，提供优秀的用户体验。

### ✨ 核心特性

- 🤖 **多AI模型支持**：DeepSeek、OpenAI GPT、Claude、Gemini等主流模型
- 📄 **多格式支持**：Markdown (.md)、文本 (.txt) 文件，可扩展PDF支持  
- 🎨 **交互式可视化**：基于Mermaid.js的高质量思维导图
- ⚡ **异步处理**：文档上传后立即显示，思维导图异步生成
- 💻 **现代化界面**：React + Tailwind CSS响应式设计
- 📱 **移动端适配**：支持移动设备访问
- 🔄 **实时同步**：文档阅读与思维导图联动高亮
- 📊 **论证结构分析**：专门的学术文档论证逻辑分析
- 💾 **多种导出**：支持Markdown、Mermaid代码、HTML等格式
- 🔗 **在线编辑**：集成Mermaid Live Editor

## 🏗️ 系统架构

### 整体架构图

```mermaid
graph TB
    subgraph "前端界面"
        A[React App] --> B[文档上传页面]
        A --> C[文档阅读器] 
        A --> D[思维导图显示]
    end
    
    subgraph "后端API"
        F[FastAPI服务] --> G[文档解析器]
        F --> H[思维导图生成器]
        F --> I[论证结构分析器]
    end
    
    subgraph "AI服务层"
        J[AI模型管理器] --> K[DeepSeek]
        J --> L[OpenAI/硅基流动]
        J --> M[Claude]
        J --> N[Gemini]
    end
    
    subgraph "数据处理"
        O[文档结构分析] --> P[段落级映射]
        O --> Q[语义块提取]
        O --> R[内容去重]
    end
    
    A --> F
    F --> J
    G --> O
    H --> O
    I --> O
```

### 技术栈详解

#### 后端技术栈
- **FastAPI**: 现代异步Web框架，提供高性能API服务
- **Python 3.8+**: 核心开发语言
- **多AI模型集成**: 统一的AI接口抽象层
- **异步处理**: 支持并发请求和长时间运行的AI任务
- **文档解析**: 基于正则表达式的Markdown结构分析
- **智能去重**: 基于语义相似度的内容去重算法

#### 前端技术栈
- **React 18**: 现代前端框架，使用Hooks架构
- **Tailwind CSS**: 原子化CSS框架，响应式设计
- **Mermaid.js**: 强大的图表渲染引擎
- **React Router**: 单页应用路由管理
- **Axios**: HTTP客户端，处理API调用
- **React Hot Toast**: 优雅的通知系统

#### AI集成特性
- **多提供商支持**: 统一接口支持多种AI服务
- **成本控制**: 精确的token使用追踪和成本计算
- **智能重试**: 指数退避重试机制
- **错误恢复**: 多层次错误处理和优雅降级

## 📁 项目结构详解

```
mindmap-generator/
├── 📄 README.md                     # 项目说明文档
├── 📄 requirements-web.txt          # Python依赖包列表
├── 📄 .env.example                  # 环境变量模板
├── 📄 .gitignore                    # Git忽略文件配置
│
├── 🚀 核心后端文件
│   ├── 📄 mindmap_generator.py      # 核心思维导图生成引擎
│   ├── 📄 web_backend.py            # FastAPI后端API服务
│   ├── 📄 document_parser.py        # 文档结构解析器
│   └── 📄 start_conda_web_app.py    # 一键启动脚本
│
├── 🎨 前端React应用
│   ├── 📄 package.json              # Node.js依赖配置
│   ├── 📄 tailwind.config.js        # Tailwind CSS配置
│   ├── 📁 src/                      # 源代码目录
│   │   ├── 📄 App.js                # 主应用组件
│   │   ├── 📄 App.css               # 全局样式
│   │   ├── 📁 components/           # React组件
│   │   │   ├── 📄 UploadPage.js           # 文件上传页面
│   │   │   ├── 📄 ViewerPageRefactored.js # 文档查看器（重构版）
│   │   │   ├── 📄 MermaidDiagram.js       # 思维导图组件
│   │   │   ├── 📄 DocumentRenderer.js     # 文档渲染器
│   │   │   ├── 📄 PDFViewer.js            # PDF查看器

│   │   │   ├── 📄 TableOfContents.js      # 目录组件
│   │   │   └── 📄 ThemeToggle.js          # 暗黑模式切换
│   │   ├── 📁 contexts/             # React上下文
│   │   │   └── 📄 ThemeContext.js         # 主题上下文
│   │   └── 📁 hooks/                # 自定义React Hooks
│   │       ├── 📄 useDocumentViewer.js     # 文档查看器逻辑
│   │       ├── 📄 useMindmapGeneration.js  # 思维导图生成逻辑
│   │       ├── 📄 usePanelResize.js        # 面板调整逻辑

│   │       └── 📄 useScrollDetection.js    # 滚动检测与联动高亮
│   └── 📁 public/                   # 静态资源
│       ├── 📄 index.html            # HTML模板
│       ├── 📄 favicon.ico           # 网站图标
│       └── 📄 manifest.json         # PWA配置
│
├── 📁 存储目录
│   ├── 📁 uploads/                  # 用户上传文件存储
│   ├── 📁 mindmap_outputs/          # 思维导图输出文件
│   ├── 📁 pdf_outputs/              # PDF解析结果
│   └── 📁 api_responses/            # AI API响应日志（调试用）
│
├── 📁 文档和示例
│   ├── 📄 WEB应用使用说明.md         # Web应用使用指南
│   ├── 📄 Conda环境使用指南.md       # Conda环境配置指南
│   ├── 📄 思维导图生成器技术实现详解.md # 技术实现详细文档
│   ├── 📄 markdown.md               # Markdown示例文档
│   ├── 📄 sample_input_document_as_markdown__durnovo_memo.md
│   ├── 📄 sample_input_document_as_markdown__small.md
│   └── 📁 screenshots/              # 项目截图
│       ├── 📄 illustration.webp           # 功能演示图
│       ├── 📄 mindmap_outline_md_example_durnovo.webp
│       ├── 📄 mermaid_diagram_example_durnovo.webp
│       ├── 📄 logging_output_during_run.webp
│       ├── 📄 token_usage_report.webp
│       └── 📄 mindmap-architecture.svg
│
└── 📁 其他文件
    ├── 📄 package.json              # 根目录Node.js配置
    ├── 📄 package-lock.json         # 依赖锁定文件
    └── 📁 venv/                     # Python虚拟环境
```

## 🚀 快速开始

### 环境要求

- **Python**: 3.8 或更高版本
- **Node.js**: 16 或更高版本
- **Conda**: 推荐使用（可选）

### 🎯 一键启动（推荐）

```bash
# 克隆项目
git clone <your-repo-url>
cd mindmap-generator

# 配置环境变量
cp .env.example .env
# 编辑 .env 文件，添加你的API密钥

# 一键启动（自动安装依赖并启动服务）
python start_conda_web_app.py
```

启动脚本会自动：
- ✅ 检查并安装Python依赖
- ✅ 检查Node.js环境并安装前端依赖
- ✅ 启动后端FastAPI服务 (端口8000)
- ✅ 启动前端React开发服务器 (端口3000)
- ✅ 自动打开浏览器访问应用

### ⚙️ 环境变量配置

在 `.env` 文件中配置AI服务：

```env
# 选择AI提供商 (DEEPSEEK, OPENAI, CLAUDE, GEMINI)
API_PROVIDER=DEEPSEEK

# DeepSeek API（推荐，成本低廉）
DEEPSEEK_API_KEY=your_deepseek_api_key

# OpenAI API（推荐使用硅基流动）
OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=https://api.siliconflow.cn/v1

# Claude API
ANTHROPIC_API_KEY=your_anthropic_api_key

# Gemini API
GEMINI_API_KEY=your_gemini_api_key
```

### 🔧 手动安装（可选）

如果需要手动安装：

```bash
# 1. 安装Python依赖
pip install -r requirements-web.txt

# 2. 安装前端依赖
cd frontend
npm install
cd ..

# 3. 启动后端服务
python -m uvicorn web_backend:app --host 0.0.0.0 --port 8000 --reload

# 4. 启动前端服务（新终端）
cd frontend
npm start
```

### 🌐 访问应用

- **前端界面**: http://localhost:3000
- **后端API**: http://localhost:8000  
- **API文档**: http://localhost:8000/docs

## 💡 使用指南

### 📤 上传文档

1. 访问首页，点击上传区域或拖拽文件
2. 支持 `.md` 和 `.txt` 格式文件
3. 文件上传后立即显示文档内容

### 📖 文档阅读

- **左侧面板**：文档内容，支持Markdown渲染
- **右侧面板**：思维导图和论证结构图
- **目录导航**：可折叠的文档结构目录
- **暗黑模式**：支持明暗主题切换

### 🧠 思维导图生成

1. 文档上传后，系统自动开始分析
2. **论证结构分析**：识别文档的逻辑论证流程
3. **语义块映射**：将文档段落映射到思维导图节点
4. **实时高亮**：阅读时自动高亮对应的思维导图节点

### 🔄 交互功能

- **节点点击**：点击思维导图节点跳转到对应文档段落
- **滚动联动**：文档滚动时自动高亮对应思维导图节点
- **面板调整**：可拖拽调整左右面板大小
- **导出功能**：支持下载Mermaid代码和Markdown文档



## 🔬 技术实现详解

### 🧮 AI模型管理

系统支持多种AI提供商，通过统一的接口抽象：

```python
class DocumentOptimizer:
    async def generate_completion(self, prompt: str, max_tokens: int, 
                                task: str) -> Optional[str]:
        # 根据API_PROVIDER选择对应的AI服务
        if Config.API_PROVIDER == "DEEPSEEK":
            return await self._call_deepseek(prompt, max_tokens)
        elif Config.API_PROVIDER == "OPENAI":
            return await self._call_openai(prompt, max_tokens)
        # ... 其他提供商
```

**支持的AI模型**：

| 提供商 | 模型 | 特点 | 成本 |
|--------|------|------|------|
| DeepSeek | deepseek-chat | 中文优化，推理能力强 | 极低 |
| OpenAI | gpt-4o-mini | 高质量输出，稳定性好 | 中等 |
| Claude | claude-3-5-haiku | 快速响应，理解能力强 | 中等 |
| Gemini | gemini-2.0-flash-lite | Google最新模型 | 低 |

### 📝 文档结构分析

采用基于Markdown标题的层级解析算法：

```python
class DocumentParser:
    def parse_document(self, markdown_text: str) -> DocumentNode:
        # 1. 提取所有标题
        headings = self._extract_headings(markdown_text)
        
        # 2. 构建层级树结构
        root = self._build_tree_structure(headings, markdown_text)
        
        # 3. 分配内容和计算范围
        self._post_process_tree(root, markdown_text)
        
        return root
```

**解析功能**：
- 自动识别Markdown标题层级
- 构建文档树形结构
- 生成可导航的目录
- 支持段落级精确定位

### 🔗 语义映射系统

实现文档段落与思维导图节点的智能映射：

```python
def updateDynamicMapping(chunks, mermaidCode, nodeMapping):
    # AI分析生成的语义块映射
    # 段落ID -> 节点ID 的映射关系
    # 支持一对多和多对一的复杂映射
```

**映射特性**：
- **段落级映射**：精确到每个文档段落
- **语义分组**：相关段落组合成逻辑节点
- **双向导航**：文档↔思维导图双向跳转
- **实时高亮**：滚动时自动同步高亮

### 🎨 思维导图渲染

基于Mermaid.js的高质量图表渲染：

```javascript
// 动态生成Mermaid语法
const mermaidCode = `
graph TD
    A[引言] --> B{核心论点}
    B --> C[支撑证据1]
    B --> D[支撑证据2]
    C --> E[结论]
    D --> E
`;

// 交互式渲染
<MermaidDiagram 
  code={mermaidCode}
  onNodeClick={handleNodeClick}
  ref={mermaidDiagramRef}
/>
```

**渲染特性**：
- **交互式节点**：支持点击跳转
- **动态高亮**：实时视觉反馈
- **自适应布局**：响应式设计
- **在线编辑**：集成Mermaid Live Editor

### ⚡ 异步处理架构

采用现代异步处理模式：

```python
# 后端异步任务管理
async def generate_argument_structure_async(document_id: str, content: str):
    try:
        # 更新状态为处理中
        document_status[document_id]['status_demo'] = 'generating'
        
        # AI分析处理（可能需要较长时间）
        result = await analyzer.generate_argument_structure(content)
        
        # 更新完成状态
        document_status[document_id]['status_demo'] = 'completed'
        document_status[document_id]['mermaid_code_demo'] = result['mermaid_string']
        
    except Exception as e:
        document_status[document_id]['status_demo'] = 'error'
```

**异步特性**：
- **非阻塞上传**：文档上传后立即可阅读
- **后台处理**：AI分析在后台异步进行  
- **实时状态**：前端轮询获取处理进度
- **优雅降级**：处理失败时的友好提示

## 🛠️ API接口文档

### 📤 文档上传
```http
POST /api/upload-document
Content-Type: multipart/form-data

{
  "file": "document.md"
}
```

**响应示例**：
```json
{
  "success": true,
  "document_id": "doc_abc123",
  "message": "文档上传成功",
  "content": "文档内容...",
  "parsed_content": "解析后的文档..."
}
```

### 🧠 生成论证结构
```http
POST /api/generate-argument-structure/{document_id}
```

**响应示例**：
```json
{
  "success": true,
  "status": "completed",
  "mermaid_code": "graph TD\n  A[引言] --> B[论点]...",
  "node_mappings": {
    "A": {
      "paragraph_ids": ["para-1", "para-2"],
      "semantic_role": "引言",
      "text_snippet": "文档开篇介绍了..."
    }
  }
}
```

### 📊 查询状态
```http
GET /api/document-status/{document_id}
```

### 📖 获取文档
```http
GET /api/document/{document_id}
```

### 🗂️ 获取文档结构  
```http
GET /api/document-structure/{document_id}
```

### 📚 获取目录
```http
GET /api/document-toc/{document_id}
```

完整的API文档请访问：http://localhost:8000/docs

## 🔧 高级配置

### AI模型优化

```env
# DeepSeek配置（推荐）
API_PROVIDER=DEEPSEEK
DEEPSEEK_API_KEY=your_key
DEEPSEEK_COMPLETION_MODEL=deepseek-chat  # 或 deepseek-reasoner

# OpenAI兼容配置（使用硅基流动等代理）
API_PROVIDER=OPENAI  
OPENAI_API_KEY=your_siliconflow_key
OPENAI_BASE_URL=https://api.siliconflow.cn/v1
OPENAI_COMPLETION_MODEL=gpt-4o-mini-2024-07-18
```

### 性能调优

```python
# 并发处理配置
MAX_CONCURRENT_REQUESTS = 5
REQUEST_TIMEOUT = 60
RETRY_ATTEMPTS = 3

# 缓存配置  
ENABLE_EMOJI_CACHE = True
CACHE_DIRECTORY = "./cache"

# 内容限制
MAX_CONTENT_LENGTH = 100000  # 字符
MAX_TOPICS = 8
MAX_SUBTOPICS_PER_TOPIC = 6
```

## 🐛 故障排除

### 常见问题

**1. 启动失败**
```bash
# 检查Python版本
python --version  # 应该 >= 3.8

# 检查Node.js版本
node --version   # 应该 >= 16

# 重新安装依赖
pip install -r requirements-web.txt --force-reinstall
```

**2. AI API调用失败**
```bash
# 检查环境变量
echo $API_PROVIDER
echo $DEEPSEEK_API_KEY

# 检查网络连接
curl -I https://api.deepseek.com

# 查看详细错误日志
tail -f api_responses/*.txt
```

**3. 前端编译错误**
```bash
# 清理缓存重新安装
cd frontend
rm -rf node_modules package-lock.json
npm install

# 检查端口占用
netstat -ano | findstr :3000  # Windows
lsof -ti:3000 | xargs kill    # macOS/Linux
```

**4. 思维导图不显示**
- 检查浏览器控制台错误
- 确认Mermaid代码格式正确
- 检查网络请求是否成功

### 调试模式

开启详细日志：

```python
# 在 web_backend.py 中
import logging
logging.basicConfig(level=logging.DEBUG)
```

查看AI API响应：
```bash
# 查看最新的API调用日志
ls -la api_responses/
cat api_responses/latest_response.txt
```

## 🤝 贡献指南

欢迎提交Issue和Pull Request！

### 开发环境设置

```bash
# 1. Fork仓库并克隆
git clone https://github.com/your-username/mindmap-generator.git
cd mindmap-generator

# 2. 创建开发分支
git checkout -b feature/your-feature-name

# 3. 设置开发环境
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements-web.txt

# 4. 前端开发环境
cd frontend  
npm install
npm start
```

### 代码规范

- **Python**: 遵循PEP 8规范
- **JavaScript**: 使用ES6+语法，遵循Airbnb风格
- **提交信息**: 使用约定式提交格式

### 测试流程

```bash
# 运行后端测试
python -m pytest tests/

# 运行前端测试  
cd frontend
npm test

# 端到端测试
npm run test:e2e
```

## 📄 许可证

本项目基于 [MIT许可证](LICENSE) 开源。

## 🙏 致谢

- [Mermaid.js](https://mermaid.js.org/) - 强大的图表渲染引擎
- [FastAPI](https://fastapi.tiangolo.com/) - 现代Python Web框架
- [React](https://react.dev/) - 优秀的前端框架
- [Tailwind CSS](https://tailwindcss.com/) - 实用的CSS框架
- [DeepSeek](https://www.deepseek.com/) - 高性能AI模型服务

## 📞 支持与反馈

- 📧 Email: [your-email@example.com]
- 🐛 Issues: [GitHub Issues](https://github.com/your-username/mindmap-generator/issues)
- 💬 讨论: [GitHub Discussions](https://github.com/your-username/mindmap-generator/discussions)

---

⭐ 如果这个项目对您有帮助，请给我们一个星标！
</file>

<file path="frontend/package.json">
{
  "name": "mindmap-generator-frontend",
  "version": "1.0.0",
  "private": true,
  "dependencies": {
    "@dnd-kit/core": "^6.3.1",
    "@dnd-kit/modifiers": "^9.0.0",
    "@dnd-kit/sortable": "^10.0.0",
    "@dnd-kit/utilities": "^3.2.2",
    "@tailwindcss/typography": "^0.5.9",
    "@testing-library/jest-dom": "^5.16.4",
    "@testing-library/react": "^13.3.0",
    "@testing-library/user-event": "^13.5.0",
    "autoprefixer": "^10.4.14",
    "axios": "^1.4.0",
    "dagre": "^0.8.5",
    "lucide-react": "^0.263.1",
    "mermaid": "^10.2.3",
    "postcss": "^8.4.24",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-dropzone": "^14.2.3",
    "react-hot-toast": "^2.4.1",
    "react-markdown": "^8.0.7",
    "react-router-dom": "^6.3.0",
    "react-scripts": "5.0.1",
    "react-toastify": "^11.0.5",
    "reactflow": "^11.11.4",
    "tailwindcss": "^3.3.0"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  },
  "proxy": "http://localhost:8000"
}
</file>

<file path="frontend/src/App.js">
import React from 'react';
import { BrowserRouter as Router, Routes, Route, useLocation } from 'react-router-dom';
import { Toaster } from 'react-hot-toast';
import { ThemeProvider } from './contexts/ThemeContext';
import ThemeToggle from './components/ThemeToggle';
import UploadPage from './components/UploadPage';
import ViewerPageRefactored from './components/ViewerPageRefactored';
import './App.css';

function AppContent() {
  const location = useLocation();
  const isViewerPage = location.pathname.startsWith('/viewer/');

  return (
    <div className={isViewerPage ? "h-full bg-gray-50 dark:bg-gray-900" : "min-h-screen bg-gray-50 dark:bg-gray-900"}>
      {!isViewerPage && (
        <header className="bg-white dark:bg-gray-800 shadow-sm border-b border-gray-200 dark:border-gray-700">
          <div className="max-w-7xl mx-auto px-4 py-4 flex items-center justify-between">
            <h1 className="text-2xl font-bold text-gray-900 dark:text-white flex items-center">
              <svg className="w-8 h-8 mr-3 text-blue-600 dark:text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z" />
              </svg>
              智能思维导图生成器
            </h1>
            <ThemeToggle />
          </div>
        </header>
      )}

      <main className={isViewerPage ? "h-full" : "max-w-7xl mx-auto"}>
        <Routes>
          <Route path="/" element={<UploadPage />} />
          <Route path="/viewer/:documentId" element={<ViewerPageRefactored />} />
        </Routes>
      </main>
    </div>
  );
}

function App() {
  return (
    <ThemeProvider>
      <div className="App h-full">
        <Router>
          <AppContent />
        </Router>
        <Toaster 
          position="top-right"
          toastOptions={{
            duration: 4000,
            style: {
              background: 'var(--toast-bg)',
              color: 'var(--toast-color)',
            },
            className: 'dark:bg-gray-800 dark:text-white bg-gray-800 text-white',
          }}
        />
      </div>
    </ThemeProvider>
  );
}

export default App;
</file>

<file path="frontend/src/components/EditableNode.css">
.editable-node {
  width: 200px;
  min-height: 50px;
  padding: 10px;
  border: 1px solid #ddd;
  border-radius: 8px;
  background-color: white;
  text-align: center;
  cursor: pointer;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
  position: relative;
  display: flex;
  flex-direction: column;
  justify-content: center;
  transition: all 0.3s ease; /* 添加过渡动画 */
}

.editable-node-content {
  font-size: 14px;
  line-height: 1.4;
  word-break: break-word;
  min-height: 20px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.editable-node-textarea {
  border: none;
  outline: none;
  background: transparent;
  resize: none;
  width: 100%;
  font-size: 14px;
  line-height: 1.4;
  text-align: center;
  font-family: inherit;
  min-height: 40px;
  cursor: text;
  overflow: hidden; /* 防止出现滚动条 */
  padding: 0;
  margin: 0;
  box-sizing: border-box;
}

/* 高亮显示编辑状态 */
.editable-node:has(.editable-node-textarea:focus) {
  border-color: #4299e1;
  box-shadow: 0 0 0 2px rgba(66, 153, 225, 0.2);
}

/* 占位符样式 */
.editable-node-textarea::placeholder {
  color: #a0aec0;
  opacity: 0.7;
}

/* 节点高亮样式 - 非破坏性高亮 */
.react-flow__node.highlighted-node .editable-node {
  border-color: #f59e0b !important;
  background-color: rgba(245, 158, 11, 0.1) !important;
  box-shadow: 0 0 0 3px rgba(245, 158, 11, 0.2), 0 4px 12px rgba(245, 158, 11, 0.3) !important;
  transform: scale(1.02);
}

/* 深色模式下的节点高亮 */
:root.dark .react-flow__node.highlighted-node .editable-node {
  border-color: #fbbf24 !important;
  background-color: rgba(251, 191, 36, 0.15) !important;
  box-shadow: 0 0 0 3px rgba(251, 191, 36, 0.25), 0 4px 12px rgba(251, 191, 36, 0.35) !important;
}

/* 确保编辑状态下的节点高亮不会影响编辑功能 */
.react-flow__node.highlighted-node .editable-node:has(.editable-node-textarea:focus) {
  border-color: #4299e1 !important;
  box-shadow: 0 0 0 2px rgba(66, 153, 225, 0.2) !important;
  background-color: white !important;
  transform: scale(1);
}

/* React Flow节点容器的高亮样式 */
.react-flow__node.highlighted-node {
  z-index: 1000 !important;
}

/* 防止高亮动画影响编辑功能 */
.react-flow__node .editable-node-textarea:focus {
  position: relative;
  z-index: 1001;
}

/* 工具栏样式 */
.node-toolbar {
  position: absolute;
  top: -50px;
  left: 50%;
  transform: translateX(-50%);
  background: white;
  border: 1px solid #e2e8f0;
  border-radius: 8px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  padding: 6px;
  display: flex;
  gap: 4px;
  z-index: 1000;
  animation: fadeInUp 0.2s ease-out;
}

/* 工具栏按钮样式 */
.toolbar-button {
  background: transparent;
  border: none;
  width: 32px;
  height: 32px;
  border-radius: 6px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  color: #64748b;
  transition: all 0.2s ease;
  padding: 0;
}

.toolbar-button:hover {
  background-color: #f1f5f9;
  color: #334155;
  transform: scale(1.05);
}

.toolbar-button:active {
  transform: scale(0.95);
}

/* 工具栏显示动画 */
@keyframes fadeInUp {
  from {
    opacity: 0;
    transform: translateX(-50%) translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateX(-50%) translateY(0);
  }
}

/* 深色模式下的工具栏样式 */
:root.dark .node-toolbar {
  background: #1e293b;
  border-color: #334155;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
}

:root.dark .toolbar-button {
  color: #94a3b8;
}

:root.dark .toolbar-button:hover {
  background-color: #334155;
  color: #e2e8f0;
}

/* 工具栏更多选项容器 */
.toolbar-more-container {
  position: relative;
}

/* 工具栏下拉菜单 */
.toolbar-more-menu {
  position: absolute;
  top: 100%;
  right: 0;
  margin-top: 4px;
  background: white;
  border: 1px solid #e2e8f0;
  border-radius: 6px;
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.12);
  min-width: 120px;
  z-index: 1001;
  animation: fadeInDown 0.15s ease-out;
}

/* 工具栏菜单项 */
.toolbar-menu-item {
  width: 100%;
  padding: 8px 12px;
  border: none;
  background: transparent;
  display: flex;
  align-items: center;
  gap: 8px;
  cursor: pointer;
  font-size: 13px;
  color: #374151;
  transition: background-color 0.2s ease;
  border-radius: 4px;
  margin: 2px;
}

.toolbar-menu-item:hover {
  background-color: #f1f5f9;
}

.toolbar-menu-item:first-child {
  margin-top: 4px;
}

.toolbar-menu-item:last-child {
  margin-bottom: 4px;
}

/* 删除按钮特殊样式 */
.toolbar-menu-item.delete-item:hover {
  background-color: #fef2f2;
  color: #dc2626;
}

/* 下拉菜单显示动画 */
@keyframes fadeInDown {
  from {
    opacity: 0;
    transform: translateY(-8px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* 深色模式下的下拉菜单样式 */
:root.dark .toolbar-more-menu {
  background: #1e293b;
  border-color: #334155;
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.4);
}

:root.dark .toolbar-menu-item {
  color: #e2e8f0;
}

:root.dark .toolbar-menu-item:hover {
  background-color: #374151;
}

:root.dark .toolbar-menu-item.delete-item:hover {
  background-color: #450a0a;
  color: #f87171;
}
</file>

<file path="frontend/src/components/EditableNode.js">
import React, { useState, useRef, useEffect } from 'react';
import { Handle, Position } from 'reactflow';
import { CornerDownRight, GitBranchPlus, MoreHorizontal, Trash2 } from 'lucide-react';
import './EditableNode.css'; // 引入CSS文件

// 工具栏组件
const Toolbar = ({ nodeId, onAddChildNode, onAddSiblingNode, onDeleteNode }) => {
  const [showMoreMenu, setShowMoreMenu] = useState(false);
  const moreButtonRef = useRef(null);
  
  // 处理点击外部关闭菜单
  useEffect(() => {
    const handleClickOutside = (event) => {
      if (moreButtonRef.current && !moreButtonRef.current.contains(event.target)) {
        setShowMoreMenu(false);
      }
    };
    
    if (showMoreMenu) {
      document.addEventListener('mousedown', handleClickOutside);
      return () => document.removeEventListener('mousedown', handleClickOutside);
    }
  }, [showMoreMenu]);

  return (
    <div className="node-toolbar">
      <button 
        className="toolbar-button" 
        title="添加同级节点"
        onClick={(e) => {
          e.stopPropagation();
          if (onAddSiblingNode) {
            onAddSiblingNode(nodeId);
          }
        }}
      >
        <CornerDownRight size={16} />
      </button>
      
      <button 
        className="toolbar-button" 
        title="添加子节点"
        onClick={(e) => {
          e.stopPropagation();
          if (onAddChildNode) {
            onAddChildNode(nodeId);
          }
        }}
      >
        <GitBranchPlus size={16} />
      </button>
      
      <div className="toolbar-more-container" ref={moreButtonRef}>
        <button 
          className="toolbar-button" 
          title="更多"
          onClick={(e) => {
            e.stopPropagation();
            setShowMoreMenu(!showMoreMenu);
          }}
        >
          <MoreHorizontal size={16} />
        </button>
        
        {showMoreMenu && (
          <div className="toolbar-more-menu">
            <button 
              className="toolbar-menu-item delete-item"
              onClick={(e) => {
                e.stopPropagation();
                if (onDeleteNode && window.confirm('确定要删除这个节点吗？')) {
                  onDeleteNode(nodeId);
                }
                setShowMoreMenu(false);
              }}
            >
              <Trash2 size={14} />
              <span>删除节点</span>
            </button>
          </div>
        )}
      </div>
    </div>
  );
};

const EditableNode = ({ data, id }) => {
  const [isEditing, setIsEditing] = useState(false);
  const [isHovering, setIsHovering] = useState(false);
  const [editingContent, setEditingContent] = useState(''); // 保存编辑中的内容
  const textareaRef = useRef(null);
  const wasEditingRef = useRef(false); // 跟踪之前是否在编辑状态

  // 保护编辑状态：当组件重新渲染时，检查是否需要恢复编辑状态
  useEffect(() => {
    // 如果之前在编辑但现在不在编辑，可能是被意外重置了
    if (wasEditingRef.current && !isEditing && editingContent) {
      console.log('🛡️ [编辑保护] 检测到编辑状态可能被重置，尝试恢复:', id);
      setIsEditing(true);
      // 延迟恢复焦点，确保DOM已更新
      setTimeout(() => {
        if (textareaRef.current) {
          textareaRef.current.focus();
          textareaRef.current.value = editingContent;
          adjustTextareaHeight(textareaRef.current);
          console.log('🛡️ [编辑保护] 编辑状态已恢复:', id);
        }
      }, 0);
    }
    
    // 更新编辑状态跟踪
    wasEditingRef.current = isEditing;
  }, [isEditing, editingContent, id]);

  // 处理双击事件，进入编辑模式
  const handleDoubleClick = () => {
    console.log('📝 [编辑开始] 节点进入编辑模式:', id);
    setEditingContent(data.label || '');
    setIsEditing(true);
  };

  // 处理失焦事件，保存并退出编辑模式
  const handleBlur = (event) => {
    const newLabel = event.target.value.trim();
    console.log('📝 [编辑结束] 节点编辑完成:', id, '新内容:', newLabel);
    
    if (newLabel !== data.label && data.onLabelChange) {
      data.onLabelChange(id, newLabel);
    }
    
    // 清理编辑状态
    setEditingContent('');
    setIsEditing(false);
    wasEditingRef.current = false;
  };

  // 处理键盘事件
  const handleKeyDown = (event) => {
    if (event.key === 'Enter' && !event.shiftKey) {
      event.preventDefault();
      const newLabel = event.target.value.trim();
      console.log('📝 [编辑完成-回车] 节点编辑完成:', id, '新内容:', newLabel);
      
      if (newLabel !== data.label && data.onLabelChange) {
        data.onLabelChange(id, newLabel);
      }
      
      // 清理编辑状态
      setEditingContent('');
      setIsEditing(false);
      wasEditingRef.current = false;
    }
    // 按 Escape 键取消编辑
    if (event.key === 'Escape') {
      console.log('📝 [编辑取消] 用户取消编辑:', id);
      // 清理编辑状态
      setEditingContent('');
      setIsEditing(false);
      wasEditingRef.current = false;
    }
  };

  // 处理输入事件，保存编辑内容并自动调整高度
  const handleInput = (event) => {
    const currentContent = event.target.value;
    setEditingContent(currentContent); // 实时保存编辑内容
    adjustTextareaHeight(event.target);
  };

  // 调整文本区域高度的辅助函数
  const adjustTextareaHeight = (textarea) => {
    if (!textarea) return;
    
    // 先重置高度，再根据内容设置新高度
    textarea.style.height = 'auto';
    textarea.style.height = `${textarea.scrollHeight}px`;
  };

  // 当进入编辑模式时，设置初始高度
  useEffect(() => {
    if (isEditing && textareaRef.current) {
      // 确保文本区域获得焦点
      textareaRef.current.focus();
      
      // 设置初始高度
      setTimeout(() => {
        adjustTextareaHeight(textareaRef.current);
      }, 0);
    }
  }, [isEditing]);

  // 处理鼠标进入事件
  const handleMouseEnter = () => {
    setIsHovering(true);
  };

  // 处理鼠标离开事件
  const handleMouseLeave = () => {
    setIsHovering(false);
  };

  return (
    <div 
      className="editable-node" 
      data-node-id={id}
      onMouseEnter={handleMouseEnter}
      onMouseLeave={handleMouseLeave}
    >
      {/* 添加一个用于接收连线的Handle在顶部 */}
      <Handle 
        type="target" 
        position={Position.Top} 
        style={{ background: '#555', width: 8, height: 8 }} 
      />
      
      {!isEditing ? (
        <div 
          onDoubleClick={handleDoubleClick}
          className="editable-node-content"
          title="双击编辑"
        >
          {data.label || '未命名节点'}
        </div>
      ) : (
        <textarea
          ref={textareaRef}
          defaultValue={editingContent || data.label || ''}
          autoFocus
          onBlur={handleBlur}
          onKeyDown={handleKeyDown}
          onInput={handleInput}
          className="editable-node-textarea"
          placeholder="输入节点内容..."
        />
      )}
      
      {/* 条件渲染工具栏 */}
      {isHovering && (
        <Toolbar 
          nodeId={id} 
          onAddChildNode={data.onAddChildNode}
          onAddSiblingNode={data.onAddSiblingNode}
          onDeleteNode={data.onDeleteNode}
        />
      )}
      
      {/* 添加一个用于发出连线的Handle在底部 */}
      <Handle 
        type="source" 
        position={Position.Bottom} 
        style={{ background: '#555', width: 8, height: 8 }} 
      />
    </div>
  );
};

export default EditableNode;
</file>

<file path="start_conda_web_app.py">
#!/usr/bin/env python3
"""
Conda环境专用启动脚本
为在Conda环境中运行思维导图生成器Web应用而优化
"""

import subprocess
import sys
import os
import time
import tempfile
import shutil
from pathlib import Path
import threading
import webbrowser
from urllib.parse import urlparse
import urllib.request
import urllib.error

def print_status(message, status="INFO"):
    """打印带状态的消息"""
    colors = {
        "INFO": "\033[96m",      # 青色
        "SUCCESS": "\033[92m",   # 绿色  
        "WARNING": "\033[93m",   # 黄色
        "ERROR": "\033[91m",     # 红色
        "ENDC": "\033[0m"        # 结束颜色
    }
    print(f"{colors.get(status, '')}{status}: {message}{colors['ENDC']}")

def check_conda_env():
    """检查是否在Conda环境中"""
    conda_env = os.environ.get('CONDA_DEFAULT_ENV')
    if conda_env:
        print_status(f"检测到Conda环境: {conda_env}", "SUCCESS")
        return True
    else:
        print_status("未检测到Conda环境，但继续运行", "WARNING")
        return False

def install_requirements():
    """安装项目依赖"""
    print_status("检查并安装项目依赖...", "INFO")
    
    # 确保有requirements-web.txt文件
    if not Path("requirements-web.txt").exists():
        print_status("requirements-web.txt 不存在，创建默认依赖文件...", "WARNING")
        default_requirements = [
            "fastapi>=0.104.0",
            "uvicorn[standard]>=0.24.0",
            "python-multipart>=0.0.6",
            "aiofiles>=23.2.1",
            "aiolimiter>=1.1.0",
            "openai>=1.3.0",
            "anthropic>=0.7.7",
            "google-generativeai>=0.3.0",
            "fuzzywuzzy>=0.18.0",
            "python-Levenshtein>=0.21.1",
            "sqlmodel>=0.0.14",
            "tiktoken>=0.5.1",
            "transformers>=4.35.0",
            "numpy>=1.24.0",
            "psutil>=5.9.6",
            "spacy>=3.7.2",
            "async-timeout>=4.0.3",
            "python-decouple>=3.8",
            "aiosqlite>=0.19.0",
            "sqlalchemy>=2.0.23",
            "termcolor>=2.3.0"
        ]
        
        with open("requirements-web.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(default_requirements))
        
        print_status("已创建 requirements-web.txt", "SUCCESS")
    
    try:
        # 检查关键包是否已安装
        import fastapi
        import google.generativeai
        print_status("核心依赖已安装", "SUCCESS")
    except ImportError as e:
        print_status(f"缺少关键依赖: {e}", "WARNING")
        print_status("正在安装依赖包...", "INFO")
        
        try:
            subprocess.run([
                sys.executable, "-m", "pip", "install", 
                "-r", "requirements-web.txt", 
                "--upgrade", "--no-warn-script-location"
            ], check=True, capture_output=True, text=True)
            print_status("依赖安装完成", "SUCCESS")
        except subprocess.CalledProcessError as e:
            print_status(f"依赖安装失败: {e}", "ERROR")
            print_status("尝试单独安装关键包...", "INFO")
            
            # 尝试安装关键包
            key_packages = [
                "fastapi>=0.104.0",
                "uvicorn[standard]>=0.24.0", 
                "google-generativeai>=0.3.0",
                "openai>=1.3.0",
                "python-multipart>=0.0.6"
            ]
            
            for package in key_packages:
                try:
                    subprocess.run([
                        sys.executable, "-m", "pip", "install", package, "--no-warn-script-location"
                    ], check=True, capture_output=True, text=True)
                    print_status(f"安装成功: {package}", "SUCCESS")
                except subprocess.CalledProcessError:
                    print_status(f"安装失败: {package}", "ERROR")

def check_environment_variables():
    """检查环境变量配置"""
    print_status("检查环境变量配置...", "INFO")
    
    # 检查 .env 文件
    if Path(".env").exists():
        print_status("找到 .env 文件", "SUCCESS")
        
        # 读取并检查关键配置
        try:
            with open(".env", "r", encoding="utf-8") as f:
                env_content = f.read()
                
            if "API_PROVIDER" in env_content:
                print_status("API_PROVIDER 已配置", "SUCCESS")
            else:
                print_status("API_PROVIDER 未配置，建议设置", "WARNING")
                
            if any(key in env_content for key in ["DEEPSEEK_API_KEY", "OPENAI_API_KEY", "ANTHROPIC_API_KEY"]):
                print_status("API密钥已配置", "SUCCESS")
            else:
                print_status("建议配置API密钥以获得最佳体验", "WARNING")
                
        except Exception as e:
            print_status(f"读取 .env 文件失败: {e}", "ERROR")
    else:
        print_status(".env 文件不存在，将使用默认配置", "WARNING")

def start_backend():
    """启动后端服务"""
    print_status("启动后端API服务...", "INFO")
    
    try:
        # 检查web_backend.py是否存在
        if not Path("web_backend.py").exists():
            print_status("web_backend.py 不存在!", "ERROR")
            return None
            
        print_status("后端将显示详细的处理日志，包括思维导图生成过程", "INFO")
        print_status("=" * 50, "INFO")
        
        # 启动FastAPI服务器 - 不重定向输出，保留console日志
        backend_process = subprocess.Popen([
            sys.executable, "-m", "uvicorn", 
            "web_backend:app",
            "--host", "0.0.0.0",
            "--port", "8000",
            "--reload",
            "--log-level", "info"
        ])  # 移除了stdout和stderr的重定向
        
        # 等待服务启动
        time.sleep(3)
        
        if backend_process.poll() is None:
            print_status("后端服务启动成功 (http://localhost:8000)", "SUCCESS")
            print_status("思维导图生成日志将在下方显示", "INFO")
            print_status("=" * 50, "INFO")
            return backend_process
        else:
            print_status("后端服务启动失败", "ERROR")
            return None
            
    except Exception as e:
        print_status(f"启动后端服务时出错: {e}", "ERROR")
        return None

def find_npm():
    """查找npm可执行文件"""
    # 常见的npm路径
    npm_paths = [
        "npm",  # 系统PATH中
        "npm.cmd",  # Windows
        "npm.ps1",  # PowerShell
        r"D:\nodejs\npm.cmd",  # 常见安装路径
        r"D:\nodejs\npm.ps1",
    ]
    
    for npm_path in npm_paths:
        try:
            result = subprocess.run([npm_path, "--version"], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                print_status(f"找到npm: {npm_path} (版本: {result.stdout.strip()})", "SUCCESS")
                return npm_path
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
            continue
    
    return None

def install_nodejs_in_conda():
    """在conda环境中安装Node.js"""
    print_status("正在尝试安装Node.js到conda环境...", "INFO")
    try:
        subprocess.run([
            "conda", "install", "-y", "nodejs", "npm", "-c", "conda-forge"
        ], check=True, capture_output=True, text=True)
        print_status("Node.js安装成功", "SUCCESS")
        return True
    except subprocess.CalledProcessError as e:
        print_status(f"conda安装Node.js失败: {e}", "ERROR")
        return False

def start_frontend():
    """启动前端服务"""
    print_status("启动前端React服务...", "INFO")
    
    frontend_dir = Path("frontend")
    if not frontend_dir.exists():
        print_status("frontend 目录不存在!", "ERROR")
        return None
    
    # 查找npm
    npm_command = find_npm()
    
    if not npm_command:
        print_status("未找到npm，尝试安装Node.js...", "WARNING")
        if install_nodejs_in_conda():
            npm_command = find_npm()
        
        if not npm_command:
            print_status("无法找到或安装npm，跳过前端启动", "ERROR")
            print_status("请手动安装Node.js: https://nodejs.org/", "INFO")
            return None
    
    try:
        # 检查node_modules是否存在
        if not (frontend_dir / "node_modules").exists():
            print_status("安装前端依赖...", "INFO")
            result = subprocess.run([npm_command, "install"], 
                                  cwd=frontend_dir, check=True, 
                                  capture_output=True, text=True, timeout=300)
            print_status("前端依赖安装完成", "SUCCESS")
        
        # 启动React开发服务器 - 不重定向输出，显示编译信息
        print_status(f"使用 {npm_command} 启动前端服务...", "INFO")
        print_status("前端编译信息:", "INFO")
        print_status("-" * 30, "INFO")
        
        frontend_process = subprocess.Popen([
            npm_command, "start"
        ], cwd=frontend_dir)  # 移除stdout和stderr重定向
        
        # 等待前端服务启动 - 增加等待时间
        print_status("等待前端编译完成...", "INFO")
        
        # 检查服务是否真的可访问
        max_attempts = 30  # 最多等待30次，每次2秒
        for attempt in range(max_attempts):
            if frontend_process.poll() is not None:
                print_status("前端进程意外退出", "ERROR")
                return None
                
            try:
                # 尝试访问前端服务
                response = urllib.request.urlopen("http://localhost:3000", timeout=2)
                if response.status == 200:
                    print_status("前端服务启动成功 (http://localhost:3000)", "SUCCESS")
                    return frontend_process
            except (urllib.error.URLError, urllib.error.HTTPError, OSError):
                # 服务还没准备好，继续等待
                time.sleep(2)
                if attempt % 5 == 0:  # 每10秒显示一次进度
                    print_status(f"正在等待前端启动... ({attempt*2}s)", "INFO")
        
        print_status("前端服务启动超时，但进程仍在运行", "WARNING")
        print_status("请手动检查 http://localhost:3000 是否可访问", "INFO")
        return frontend_process
            
    except subprocess.CalledProcessError as e:
        print_status(f"启动前端服务时出错: {e}", "ERROR")
        if hasattr(e, 'stdout') and e.stdout:
            print_status(f"输出: {e.stdout}", "INFO")
        if hasattr(e, 'stderr') and e.stderr:
            print_status(f"错误: {e.stderr}", "ERROR")
        return None
    except subprocess.TimeoutExpired:
        print_status("npm install超时，可能网络较慢", "ERROR")
        return None
    except Exception as e:
        print_status(f"启动前端时发生意外错误: {e}", "ERROR")
        return None

def open_browser():
    """延迟打开浏览器"""
    def delayed_open():
        time.sleep(15)  # 等待服务完全启动，增加到15秒
        try:
            webbrowser.open("http://localhost:3000")
            print_status("已打开浏览器", "SUCCESS")
        except Exception as e:
            print_status(f"自动打开浏览器失败: {e}", "WARNING")
            print_status("请手动访问: http://localhost:3000", "INFO")
    
    threading.Thread(target=delayed_open, daemon=True).start()

def main():
    """主函数"""
    print_status("=== 思维导图生成器 Conda 环境启动器 ===", "INFO")
    print_status("优化的上传体验：先显示文档，再生成思维导图", "INFO")
    print_status("✨ 新功能：保留详细的控制台日志输出", "INFO")
    print("")
    
    # 检查Conda环境
    check_conda_env()
    
    # 安装依赖
    install_requirements()
    
    # 检查环境变量
    check_environment_variables()
    
    print_status("启动Web应用服务...", "INFO")
    print("")
    
    # 启动后端
    backend_process = start_backend()
    if not backend_process:
        print_status("后端启动失败，退出", "ERROR")
        return
    
    # 启动前端
    frontend_process = start_frontend()
    if not frontend_process:
        print_status("前端启动失败，但后端仍在运行", "WARNING")
        print_status("你可以访问 http://localhost:8000/docs 查看API文档", "INFO")
    else:
        # 打开浏览器
        open_browser()
    
    print("")
    print_status("=== 服务运行中 ===", "SUCCESS")
    print_status("前端地址: http://localhost:3000", "INFO")
    print_status("后端地址: http://localhost:8000", "INFO")
    print_status("API文档: http://localhost:8000/docs", "INFO")
    print("")
    print_status("📋 功能特色:", "INFO")
    print_status("  • 上传文件后立即显示文档内容", "INFO")
    print_status("  • 思维导图异步生成，实时状态更新", "INFO")
    print_status("  • 详细的控制台日志，便于调试和监控", "INFO")
    print_status("  • 支持DeepSeek、OpenAI、Claude、Gemini等多种AI模型", "INFO")
    print("")
    print_status("💡 使用提示:", "INFO")
    print_status("  • 上传.md或.txt文件后立即可以阅读内容", "INFO")
    print_status("  • 思维导图生成过程会在控制台显示详细日志", "INFO")
    print_status("  • 生成完成后可下载思维导图和Mermaid代码", "INFO")
    print("")
    print_status("按 Ctrl+C 停止服务", "WARNING")
    
    try:
        # 等待用户中断
        if frontend_process:
            frontend_process.wait()
        else:
            backend_process.wait()
    except KeyboardInterrupt:
        print_status("\n正在停止服务...", "INFO")
        
        if frontend_process:
            frontend_process.terminate()
            print_status("前端服务已停止", "SUCCESS")
            
        if backend_process:
            backend_process.terminate()
            print_status("后端服务已停止", "SUCCESS")
        
        print_status("所有服务已停止", "SUCCESS")

if __name__ == "__main__":
    main()
</file>

<file path="frontend/src/components/DocumentRenderer.js">
import React, { useMemo, useState, useCallback } from 'react';
import ReactMarkdown from 'react-markdown';
import { DndContext, closestCenter, KeyboardSensor, PointerSensor, useSensor, useSensors } from '@dnd-kit/core';
import { SortableContext, verticalListSortingStrategy, arrayMove } from '@dnd-kit/sortable';
import { restrictToVerticalAxis } from '@dnd-kit/modifiers';
import LogicalDivider from './LogicalDivider';
import SortableParagraph from './SortableParagraph';
import SortableDivider from './SortableDivider';
import './DocumentRenderer.css';

// 解析段落和分割线的数据结构
const parseContentWithDividers = (content, onContentBlockRef, nodeMapping = null) => {
  if (!content) return [];
  
  // 创建段落ID到节点ID的映射
  const paragraphToNodeMap = {};
  if (nodeMapping) {
    Object.entries(nodeMapping).forEach(([nodeId, nodeData]) => {
      if (nodeData.paragraph_ids && Array.isArray(nodeData.paragraph_ids)) {
        nodeData.paragraph_ids.forEach(paragraphId => {
          paragraphToNodeMap[paragraphId] = nodeId;
        });
      }
    });
  }
  
  console.log('📍 [逻辑分割] 段落到节点的映射:', paragraphToNodeMap);
  
  // 按段落分割内容，保留段落ID标记
  const paragraphs = content.split(/(\[para-\d+\])/g).filter(part => part.trim());
  console.log('📍 [段落解析] 总段落数量:', paragraphs.length, '前5个部分:', paragraphs.slice(0, 5));
  
  const items = [];
  let currentParagraphId = null;
  let currentContent = '';
  let currentNodeId = null;
  
  // 根据语义角色设置颜色
  const getColorByRole = (role) => {
    if (!role) return 'gray';
    const roleColors = {
      '引言': 'blue',
      '核心论点': 'purple',
      '支撑证据': 'green',
      '反驳': 'red',
      '结论': 'yellow',
      '历史案例': 'blue',
      '理论拓展': 'purple'
    };
    return roleColors[role] || 'gray';
  };
  
  paragraphs.forEach((part, partIndex) => {
    const paraIdMatch = part.match(/\[para-(\d+)\]/);
    
    if (paraIdMatch) {
      // 如果有之前的内容，先处理它
      if (currentContent.trim() && currentParagraphId) {
        console.log(`📍 [段落解析] 创建段落数据: ${currentParagraphId}, 内容长度: ${currentContent.trim().length}`);
        
        items.push({
          id: currentParagraphId,
          type: 'paragraph',
          paragraphId: currentParagraphId,
          content: currentContent.trim(),
          nodeId: currentNodeId,
          onContentBlockRef
        });
      }
      
      // 设置新的段落ID
      const newParagraphId = `para-${paraIdMatch[1]}`;
      const newNodeId = paragraphToNodeMap[newParagraphId];
      
      // 检查节点变化，如果节点发生变化且不是第一个段落，则插入分割线
      if (nodeMapping && newNodeId && currentNodeId && newNodeId !== currentNodeId) {
        const nodeInfo = nodeMapping[newNodeId];
        if (nodeInfo) {
          console.log(`📍 [逻辑分割] 检测到节点变化: ${currentNodeId} -> ${newNodeId}`);
          
          // 插入逻辑分割线
          items.push({
            id: `divider-${newNodeId}`,
            type: 'divider',
            nodeId: newNodeId,
            nodeInfo: {
              title: nodeInfo.text_snippet || nodeInfo.semantic_role || newNodeId,
              id: newNodeId,
              color: getColorByRole(nodeInfo.semantic_role)
            }
          });
        }
      }
      
      currentParagraphId = newParagraphId;
      currentNodeId = newNodeId;
      currentContent = '';
      
      console.log(`📍 [段落解析] 发现段落标记: ${currentParagraphId}, 节点ID: ${currentNodeId}`);
    } else {
      // 累积内容
      currentContent += part;
      console.log(`📍 [内容累积] 当前段落: ${currentParagraphId}, 累积长度: ${currentContent.length}, 新增: ${part.substring(0, 30)}...`);
    }
  });
  
  // 处理最后一个段落
  if (currentContent.trim() && currentParagraphId) {
    console.log(`📍 [段落解析-最后] 创建最后段落数据: ${currentParagraphId}, 内容长度: ${currentContent.trim().length}`);
    
    items.push({
      id: currentParagraphId,
      type: 'paragraph',
      paragraphId: currentParagraphId,
      content: currentContent.trim(),
      nodeId: currentNodeId,
      onContentBlockRef
    });
  }
  
  console.log(`📍 [解析总结] 总共创建了 ${items.length} 个项目`);
  
  // 健壮性检查：过滤掉可能的无效元素
  const validItems = items.filter(item => {
    if (!item) {
      console.warn('📍 [解析总结] ⚠️ 发现空的 item');
      return false;
    }
    if (!item.id) {
      console.warn('📍 [解析总结] ⚠️ 发现没有 id 的 item:', item);
      return false;
    }
    if (!item.type) {
      console.warn('📍 [解析总结] ⚠️ 发现没有 type 的 item:', item);
      return false;
    }
    return true;
  });
  
  if (validItems.length !== items.length) {
    console.warn(`📍 [解析总结] ⚠️ 过滤掉了 ${items.length - validItems.length} 个无效项目`);
  }
  
  return validItems;
};

// 渲染段落组件
const renderParagraphComponent = (item) => {
  const { paragraphId, content, onContentBlockRef } = item;
  
  // 🔧 固定当前段落ID，避免闭包陷阱
  const paragraphIdToRegister = paragraphId;
  const contentPreview = content.substring(0, 50) + '...';
  
  return (
    <div 
      key={`${paragraphId}-content`}
      id={paragraphId}
      data-para-id={paragraphId}
      className="paragraph-block mb-3 p-2 rounded transition-all duration-200"
      ref={(el) => {
        console.log('📍 [段落注册] 注册段落引用:', paragraphIdToRegister, '元素:', !!el, '内容预览:', contentPreview);
        if (el) {
          console.log('📍 [段落注册-DOM] 元素DOM信息:', {
            id: el.id,
            dataParaId: el.getAttribute('data-para-id'),
            className: el.className,
            offsetTop: el.offsetTop,
            clientHeight: el.clientHeight
          });
        } else {
          console.log('📍 [段落注册-DOM] 元素为null，段落:', paragraphIdToRegister);
        }
        onContentBlockRef(el, paragraphIdToRegister);
      }}
    >
      <ReactMarkdown
        components={{
          h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
          h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
          h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
          h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
          h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
          h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
          p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
          ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
          ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
          li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
          blockquote: ({node, ...props}) => (
            <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
          ),
          code: ({node, inline, ...props}) => 
            inline 
              ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
              : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
          pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
        }}
      >
        {content}
      </ReactMarkdown>
    </div>
  );
};

// 独立的段落渲染函数，避免React Hook规则问题（保持向后兼容）
const renderParagraphsWithIds = (content, onContentBlockRef, nodeMapping = null) => {
  if (!content) return null;
  
  // 为了向后兼容，返回解析后的项目渲染为JSX
  const items = parseContentWithDividers(content, onContentBlockRef, nodeMapping);
  
  return items.map((item) => {
    if (item.type === 'paragraph') {
      return renderParagraphComponent(item);
    } else if (item.type === 'divider') {
      return (
        <LogicalDivider 
          key={item.id}
          nodeInfo={item.nodeInfo}
        />
      );
    }
    return null;
  });
};

// 可排序的内容渲染组件
const SortableContentRenderer = ({ content, onContentBlockRef, nodeMapping = null, onNodeMappingUpdate, onOrderChange }) => {
  const [items, setItems] = useState([]);
  
  // 使用 useMemo 缓存解析结果
  const parsedItems = useMemo(() => {
    const result = parseContentWithDividers(content, onContentBlockRef, nodeMapping);
    // 健壮性检查：确保返回的是有效的数组，且每个元素都有 id 属性
    if (!Array.isArray(result)) {
      console.warn('📍 [解析内容] ⚠️ parseContentWithDividers 返回了非数组结果:', result);
      return [];
    }
    
    const validResult = result.filter(item => item && item.id);
    if (validResult.length !== result.length) {
      console.warn('📍 [解析内容] ⚠️ 解析结果包含无效元素，已过滤:', {
        原始长度: result.length,
        有效长度: validResult.length,
        无效元素: result.filter(item => !item || !item.id)
      });
    }
    
    return validResult;
  }, [content, onContentBlockRef, nodeMapping]);
  
  // 初始化 items 状态
  React.useEffect(() => {
    console.log('📍 [状态更新] 更新 items 状态，新长度:', parsedItems.length);
    setItems(parsedItems);
  }, [parsedItems]);
  
  // 传感器配置
  const sensors = useSensors(
    useSensor(PointerSensor),
    useSensor(KeyboardSensor)
  );
  
  // 拖拽结束处理函数 - 重构为只负责计算新顺序
  const handleDragEnd = useCallback((event) => {
    const { active, over } = event;
    
    // 健壮性检查：确保 active 和 over 对象及其 id 属性存在
    if (!active || !over || !active.id || !over.id) {
      console.warn('📍 [拖拽排序] ⚠️ 拖拽事件对象不完整:', { active, over });
      return;
    }
    
    if (active.id !== over.id) {
      // 健壮性检查：确保 items 数组存在且不为空
      if (!items || items.length === 0) {
        console.warn('📍 [拖拽排序] ⚠️ items 数组为空或不存在');
        return;
      }
      
      // 健壮性检查：过滤掉可能的 null/undefined 元素，并确保每个元素都有 id 属性
      const validItems = items.filter(item => item && item.id);
      if (validItems.length !== items.length) {
        console.warn('📍 [拖拽排序] ⚠️ items 数组包含无效元素，已过滤:', {
          原始长度: items.length,
          有效长度: validItems.length,
          无效元素: items.filter(item => !item || !item.id)
        });
      }
      
      const oldIndex = validItems.findIndex((item) => item.id === active.id);
      const newIndex = validItems.findIndex((item) => item.id === over.id);
      
      // 确保找到了有效的索引
      if (oldIndex === -1 || newIndex === -1) {
        console.warn('📍 [拖拽排序] ⚠️ 无法找到有效的拖拽索引:', {
          activeId: active.id,
          overId: over.id,
          oldIndex,
          newIndex,
          validItemIds: validItems.map(item => item.id)
        });
        return;
      }
      
      console.log('📍 [拖拽排序] 移动项目:', {
        activeId: active.id,
        overId: over.id,
        oldIndex,
        newIndex
      });
      
      const newItems = arrayMove(validItems, oldIndex, newIndex);
      console.log('📍 [拖拽排序] 计算出新的项目顺序，长度:', newItems.length);
      
      // 先更新本地状态，确保UI立即响应
      setItems(newItems);
      
      // 调用父组件传入的回调函数，传递新的项目顺序
      if (onOrderChange) {
        console.log('📍 [拖拽排序] 调用 onOrderChange 回调函数');
        onOrderChange(newItems);
      } else {
        console.warn('📍 [拖拽排序] ⚠️ onOrderChange 回调函数未提供');
      }
    }
  }, [items, onOrderChange]);
  
  // 获取所有项目的ID（包括段落和分割线）
  const sortableItemIds = useMemo(() => {
    // 健壮性检查：确保 items 是有效数组且元素有 id 属性
    if (!Array.isArray(items)) {
      console.warn('📍 [sortableItemIds] ⚠️ items 不是数组:', items);
      return [];
    }
    
    const validIds = items
      .filter(item => item && item.id)
      .map(item => item.id);
    
    console.log('📍 [sortableItemIds] 生成的ID列表长度:', validIds.length);
    return validIds;
  }, [items]);
  
  return (
    <DndContext
      sensors={sensors}
      collisionDetection={closestCenter}
      onDragEnd={handleDragEnd}
      modifiers={[restrictToVerticalAxis]}
    >
      <SortableContext items={sortableItemIds} strategy={verticalListSortingStrategy}>
        <div className="sortable-content">
          {items
            .filter(item => item && item.id && item.type) // 过滤无效项目
            .map((item) => {
            if (item.type === 'paragraph') {
              // 段落被SortableParagraph包装，但用户不能直接拖拽
              return (
                <SortableParagraph
                  key={item.id}
                  id={item.id}
                  className="mb-4"
                >
                  {renderParagraphComponent(item)}
                </SortableParagraph>
              );
            } else if (item.type === 'divider') {
              // 只有分割线可拖拽
              return (
                <SortableDivider
                  key={item.id}
                  id={item.id}
                  nodeInfo={item.nodeInfo}
                  className="mb-4"
                />
              );
            }
            return null;
          })}
        </div>
      </SortableContext>
    </DndContext>
  );
};

// 结构化Markdown渲染器组件
const StructuredMarkdownRenderer = ({ content, chunks, onSectionRef }) => {
  if (!chunks || chunks.length === 0) {
    // 回退到原始的ReactMarkdown渲染
    return (
      <div className="prose prose-sm max-w-none">
        <ReactMarkdown
          components={{
            h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
            h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
            h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
            h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
            ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
            ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
            li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
            blockquote: ({node, ...props}) => (
              <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
            ),
            code: ({node, inline, ...props}) => 
              inline 
                ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
            pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
          }}
        >
          {content}
        </ReactMarkdown>
      </div>
    );
  }

  // 按结构化方式渲染
  return (
    <div className="prose prose-sm max-w-none">
      {chunks.map((chunk, index) => (
        <div
          key={chunk.chunk_id}
          ref={(el) => onSectionRef(el, chunk.chunk_id)}
          data-chunk-index={index}
          data-chunk-id={chunk.chunk_id}
          className="mb-6 chunk-section transition-all duration-200 ease-in-out border-l-4 border-transparent hover:border-gray-200 dark:hover:border-gray-600"
        >
          {/* 渲染标题 */}
          {chunk.heading && (
            <div className="mb-3">
              <ReactMarkdown
                components={{
                  h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
                  h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
                  h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
                  h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                }}
              >
                {chunk.heading}
              </ReactMarkdown>
            </div>
          )}
          
          {/* 渲染内容 */}
          {chunk.content && (
            <div className="chunk-content">
              <ReactMarkdown
                components={{
                  p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
                  ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
                  ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
                  li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
                  blockquote: ({node, ...props}) => (
                    <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
                  ),
                  code: ({node, inline, ...props}) => 
                    inline 
                      ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                      : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
                  pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
                  // 防止嵌套标题
                  h1: ({node, ...props}) => <p className="font-bold text-lg text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h2: ({node, ...props}) => <p className="font-bold text-base text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h3: ({node, ...props}) => <p className="font-semibold text-base text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h4: ({node, ...props}) => <p className="font-semibold text-sm text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h5: ({node, ...props}) => <p className="font-semibold text-sm text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h6: ({node, ...props}) => <p className="font-semibold text-sm text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                }}
              >
                {chunk.content}
              </ReactMarkdown>
            </div>
          )}
        </div>
      ))}
    </div>
  );
};

// 演示模式渲染器组件 - 支持演示模式和真实文档
const DemoModeRenderer = ({ content, onContentBlockRef, isRealDocument = false, chunks = [], nodeMapping = null, onNodeMappingUpdate, onOrderChange }) => {
  
  console.log('📄 [DemoModeRenderer] 渲染器调用参数:');
  console.log('  - content存在:', !!content);
  console.log('  - content长度:', content?.length || 0);
  console.log('  - isRealDocument:', isRealDocument);
  console.log('  - chunks数量:', chunks?.length || 0);
  console.log('  - chunks详情:', chunks);
  console.log('  - nodeMapping存在:', !!nodeMapping);
  console.log('  - onNodeMappingUpdate存在:', !!onNodeMappingUpdate);
  console.log('  - onOrderChange存在:', !!onOrderChange);
  
  // 检查内容是否包含段落ID标记
  const hasParaIds = content && content.includes('[para-');
  console.log('📄 [DemoModeRenderer] 内容分析:', {
    hasParaIds,
    contentPreview: content?.substring(0, 200) + '...',
    contentSample: content?.substring(0, 500) // 更长的内容样本
  });
  
  // 强制调试：显示内容中的段落ID匹配
  if (content) {
    const paraMatches = content.match(/\[para-\d+\]/g);
    console.log('📄 [DemoModeRenderer] 找到的段落ID标记:', paraMatches);
    console.log('📄 [DemoModeRenderer] 段落ID数量:', paraMatches?.length || 0);
  }
  
  // 🔧 缓存段落渲染结果，防止无限重渲染导致的ref注册问题（保持向后兼容）
  const renderedParagraphs = useMemo(() => {
    if (content && content.includes('[para-')) {
      console.log('📄 [useMemo缓存] 重新渲染段落内容，内容长度:', content.length);
      const result = renderParagraphsWithIds(content, onContentBlockRef, nodeMapping);
      console.log('📄 [useMemo缓存] 段落渲染完成，创建的元素数量:', result?.length || 0);
      if (result && result.length > 0) {
        console.log('📄 [useMemo缓存] 第一个元素key:', result[0]?.key);
        console.log('📄 [useMemo缓存] 最后一个元素key:', result[result.length - 1]?.key);
      }
      return result;
    }
    return null;
  }, [content, onContentBlockRef, nodeMapping]);
  
  console.log('📄 [useMemo缓存] 段落渲染结果缓存状态:', !!renderedParagraphs);
  
  // 如果内容包含段落ID标记，使用可排序的内容渲染器
  if (isRealDocument && hasParaIds) {
    console.log('📄 [DemoModeRenderer] 进入真实文档段落ID模式，使用可排序渲染器');
    
    return (
      <div className="prose prose-sm max-w-none">
        <SortableContentRenderer 
          content={content}
          onContentBlockRef={onContentBlockRef}
          nodeMapping={nodeMapping}
          onNodeMappingUpdate={onNodeMappingUpdate}
          onOrderChange={onOrderChange}
        />
      </div>
    );
  }
  
  // 真实文档模式：基于chunks的结构化渲染（没有段落ID时）
  if (isRealDocument && chunks && chunks.length > 0) {
    console.log('📄 [DemoModeRenderer] 进入真实文档chunks模式，chunks数量:', chunks.length);
    
    return (
      <div className="prose prose-sm max-w-none">
        {chunks.map((chunk, index) => {
          const blockId = `chunk-${index + 1}`;
          
          console.log(`📄 [DemoModeRenderer] 渲染chunk ${index + 1}:`, {
            blockId,
            chunkId: chunk.chunk_id,
            title: chunk.title,
            contentLength: chunk.content?.length || 0
          });
          
          // 使用常规的 ReactMarkdown 渲染（chunks模式下的内容没有段落ID标记）
          const renderChunkContent = (content) => {
            if (!content) return null;
            
            return (
              <ReactMarkdown
                components={{
                  h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
                  h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
                  h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
                  h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
                  ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
                  ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
                  li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
                  blockquote: ({node, ...props}) => (
                    <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
                  ),
                  code: ({node, inline, ...props}) => 
                    inline 
                      ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                      : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
                  pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
                }}
              >
                {content}
              </ReactMarkdown>
            );
          };
          
          return (
            <div 
              key={chunk.chunk_id}
              id={blockId}
              className="content-block mb-6 p-4 border-l-4 border-transparent transition-all duration-200"
              ref={(el) => onContentBlockRef(el, blockId)}
            >
              {/* 渲染标题 */}
              {chunk.heading && (
                <div className="mb-3">
                  <ReactMarkdown
                    components={{
                      h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
                      h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
                      h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
                      h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                      h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                      h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                    }}
                  >
                    {chunk.heading}
                  </ReactMarkdown>
                </div>
              )}
              
              {/* 渲染内容 */}
              {chunk.content && renderChunkContent(chunk.content)}
            </div>
          );
        })}
      </div>
    );
  }
  
  // 传入真实内容但没有chunks的情况（向后兼容）
  if (content && !isRealDocument) {
    console.log('📄 [DemoModeRenderer] 进入向后兼容模式（content存在但非真实文档）');
    
    // 如果内容包含段落ID，使用可排序的内容渲染器
    if (content.includes('[para-')) {
      console.log('📄 [向后兼容] 检测到段落ID，使用可排序渲染器');
      
      return (
        <div className="prose prose-sm max-w-none">
          <SortableContentRenderer 
            content={content}
            onContentBlockRef={onContentBlockRef}
            nodeMapping={nodeMapping}
            onNodeMappingUpdate={onNodeMappingUpdate}
            onOrderChange={onOrderChange}
          />
        </div>
      );
    }
    
    // 普通内容渲染
    return (
      <div className="prose prose-sm max-w-none">
        <ReactMarkdown
          components={{
            h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
            h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
            h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
            h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
            ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
            ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
            li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
            blockquote: ({node, ...props}) => (
              <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
            ),
            code: ({node, inline, ...props}) => 
              inline 
                ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
            pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
          }}
        >
          {content}
        </ReactMarkdown>
      </div>
    );
  }

  // 演示文档的内容块数据（只在纯示例模式下使用）
  console.log('📄 [DemoModeRenderer] 进入纯示例模式（使用硬编码内容）');
  const demoContentBlocks = [
    {
      id: "text-A-introduction",
      content: `为什么一位辩证学家应该学着数到四？

三元组/三位一体与其溢出/过剩——新教、雅各宾主义……与其他"消失的中介者"—"你手指的一敲"……— 为什么真理总是政治性的？`
    },
    {
      id: "text-B-fourth-party",
      content: `**·三元组/三位一体与其溢出/过剩**

一位黑格尔派的辩证学家必须学着数到多少呢？大多数黑格尔的解释者，更不用说他的批评者，都试图一致地说服我们，正确的答案是：到三（辩证的三元组[the dialectical triad]，等等）。此外，他们还互相争夺谁能更有说服力地唤起我们对"第四方"的注意。这个第四方就是不可辩证的溢出/过剩，是死亡之处所（… ）。据说它逃离辩证法的掌握，尽管（或者更准确地说，因为）它是辩证法运动的内在可能性条件：在其结果（Result）中不能被扬弃[aufgehoben]、不能重新被收入的（re-collected）纯支出性的否定性。

现在，我们可以看到增补性的要素是如何出现的：一旦我们将直接之物（theimmediate）的否定（negation）添加给直接之物，这一否定（negation）就回溯性地改变了直接性（immediacy）的意义，所以我们虽然实际上仅拥有两个要素却必须数到三。或者，如果我们设想辩证过程的完整循环，这里只有三个"肯定（positive）"的环节（直接性、其中介和最后对被中介的直接性的复归）要去数——我们漏掉的是纯粹差异的那难以理解的剩余物（surplus），它虽使得整个过程得以进行却"什么也不算（counts for nothing）"；我们漏掉的是这一"实体的虚空"，（如黑格尔所言）它同时也是所有一切（all and everything）的"容器（receptacle [Rezeptakulum]）"。`
    },
    {
      id: "text-C-vanishing-mediator-core",
      content: `**·新教、雅各宾主义……**

然而，在那对"辩证方法"进行惹人恼怒的抽象反映（abstract reflections）的最佳传统中，这种思考（ruminations）有着一种纯粹形式的本性；它们所缺乏的是与具体历史内容内在的相互联系（relatedness）。一旦我们到达这种层次，第四的剩余物-环节（surplus-moment）作为第二个环节（分裂、抽象对立）与最终结果[Result]（和解[reconciliation]）之间"消失的中介者"这种想法立刻获得了具体的轮廓——人们只需想想詹明信在其论马克斯·韦伯的文章（这篇文章有关韦伯关于新教在资本主义崛起中的作用的理论）中阐明"消失的中介者"这一概念的方式。`
    },
    {
      id: "text-D-mechanism",
      content: `**·……与其他消失的中介者**

形式和其概念内容间的裂隙，也给我们提供了通向"消失的中介者"的必然性的关键：从封建主义到新教的路径与从新教到具有宗教私人化特征的资产阶级日常生活的路径没有相同的特征。第一个路径关系到"内容"（在保持或者甚至加强宗教形式的伪装下，发生了关键性的变化——经济活动中禁欲式贪得[asceticacquisitive]的态度被明确肯定为展示恩典的势力范围），而第二个路径则是一个纯粹形式的行动，一种形式的变化（一旦新教作为禁欲式贪得[ascetic-acquisitive]的态度得到实现，它就会作为形式而脱落）。`
    },
    {
      id: "text-D1D2D3-mechanism-stages",
      content: `因此，"消失的中介者"之所以出现，是因为在一个辩证的过程中，形式停留在内容后面的方式：首先，关键性的转变发生在旧形式的限度内，甚至呈现出其复兴的主张这一外表（对基督教性的普遍化，回到其"真正的内容"，等等）；然后，一旦"精神的无声编织（silent weaving）"完成其工作，旧形式就会脱落。这一过程的双重节奏（scansion）扩展使我们能够具体地掌握"否定之否定（negation of negation）"这一陈旧的公式：第一个否定在于实质性内容缓慢、秘密且无形的变化，而自相矛盾的是，这种变化发生在其自身形式的名义下的；那么，一旦形式失去了它的实质性权利（substantial right），它就会自己摔得粉碎——否定的形式被否定了，或者用黑格尔的经典对子来说，发生"在其自身中的"（in itself）变化变成了"对于其自身的"（for itself）【或，"自在"发生的变化变成了"自为的"——译注】。`
    },
    {
      id: "text-E1-protestantism",
      content: `这种辩证的必然性位于何处呢？换言之：具体来说，新教是怎样为资本主义的出现创造条件的？并非如人们会期待的那样，通过限制宗教意识形态的影响范围或通过动摇其在中世纪社会无处不在的特征，而是相反通过将其意义（relevance）普遍化：路德反对用一道鸿沟将修道院（cloisters）与礼拜（church）作为一种独立的制度（institution）同社会的其他部分隔绝开来，因为他希望基督教的态度能够渗透并决定我们整个的世俗日常生活。

当然，我们很容易对新教的幻觉保持一种反讽的距离，并指出新教努力废除宗教与日常生活之间差距的最终结果是如何将宗教贬低为一种"治疗性（therapeutic）"的手段；更困难的则是要去构想新教作为中世纪社团主义和资本主义个人主义间"消失的中介者（vanishing mediator）"的必然性。换句话说，不可忽视的一点是，如果，人们不可能直接地，也就是缺少新教作为 "消失的中介者"的调解（intercession）而从中世纪的"封闭"社会进入资产阶级社会：正是新教通过其对基督教性（Christianity）的普遍化，为其撤回到私密领域预备了基础。`
    },
    {
      id: "text-E2-jacobinism",
      content: `在政治领域，雅各宾主义扮演了同样的角色，它甚至可以被定义为"政治的新教"。

在这里，我们也很容易保持一种反讽的距离，并指出雅各宾主义如何必然会通过将社会整体粗暴地缩减为抽象的平等原则而在恐怖主义中结束，因为这种缩减受到了分支的（ramified）具体关系之网的抵制（见黑格尔在《精神现象学》中对雅各宾主义的经典批评）。更难做到的是，要证明为什么不可能从旧制度直接进入自我本位的资产阶级日常生活——为什么，正是因为他们虚幻地将社会整体还原为民主政治方案，雅各宾主义是一个必要的"消失的中介者"（黑格尔批评得实际要点并不在于说雅各宾主义方案有乌托邦－恐怖主义特征这样的老生常谈中，而是在于此）。换句话说，在雅各宾主义中发现现代"极权主义"的根源和第一个形式是很容易的；而要完全承认和采纳没有雅各宾主义的"溢出/过剩"就不会有"常态的"多元民主这样一个事实则要更加困难并令人不安。`
    },
    {
      id: "text-E3-other-examples",
      content: `我们应该进一步复杂化这副图景：仔细观察可以发现，在从封建政治结构到资产阶级政治结构的过程中，存在着两个"消失的中介者"：绝对君主制和雅各宾主义。第一个是有关一个悖论式妥协的标志与体现（embodiment）：这种政治形式使崛起的资产阶级能够通过打破封建主义、其行会和社团（corporations）的经济力量来加强其经济霸权——当然，它的自相矛盾之处在于，封建主义正是通过将自己的最高点（crowning point）绝对化——将绝对权力赋予君主——来"自掘坟墓"的；因此，绝对君主制的结果是政治秩序与经济基础相"分离"。同样的"脱节（disconnection）"也是雅各宾主义的特征：把雅各宾主义规定为一种激进意识形态已经是陈词滥调了，它"从字面上"接受了资产阶级的政治纲领（平等、自由、博爱[brotherhood]），并努力实现它，而不考虑同公民社会的具体衔接。

两者都为他们的幻想付出了沉重的代价：专制君主很晚才注意到，社会称赞他是万能的，只是为了让一个阶级推翻另一个阶级；雅各宾派一旦完成了摧毁旧制度的机器的工作，也就变得多余了。两者都被关于政治领域自主性（autonomy）的幻想所迷惑，都相信自己的政治使命：一个相信皇权的不可质疑性，另一个相信其政治方案的恰当性（pertinence）。在另一个层面上，我们不是也可以这样说法西斯主义和共产主义，即"实际现存的社会主义（actually existing socialism）"吗？法西斯主义难道不是一种资本主义固有的自我否定，不是试图通过一种使经济从属于意识形态-政治（ideological-political）领域的意识形态来"改变一些东西，以便没有真正的改变"吗？列宁主义的"实际存在的社会主义"难道不是一种"社会主义的雅各宾主义"，不是试图使整个社会经济生活从属于社会主义国家的直接政治调节吗？它两者都是"消失的中介者"，但进入了什么呢？通常的犬儒式答案"从资本主义回到资本主义"似乎有点太容易了……`
    },
    {
      id: "text-F-mediator-illusion",
      content: `也就是说，新教和雅各宾主义所陷入的幻觉，比乍看之下要复杂得多：它并不简单地在于他们对基督教或平等主义民主方案（egalitarian-democratic project）的那朴素道德主义式的普遍化，也就是说，并不简单地在于他们忽略了抵制这种直接普遍化的社会关系的具体财富（concrete wealth of social relations）。他们的幻觉要激进得多：它同所有在历史上相关的有关政治乌托邦的幻觉具有相同的本性。马克思在谈到柏拉图的国家（State）时提请我们注意这种幻觉，他说，柏拉图没有看到他事实上所描述的不是一个尚未实现的理想（ideal），而是现存希腊国家的基本结构。换句话说，乌托邦（utopias）之所以是"乌托邦的"，不是因为它们描绘了一个"不可能的理想（Ideal）"，一个不属于这个世界的梦想，而是因为它们没有认出它们的理想国（ideal state）在其基本内容方面（黑格尔会说，"在其概念方面"）如何已然实现了。

当社会现实被构造成一个"新教世界"的时候，新教就变得多余，可以作为一个中介消失了：资本主义公民社会的概念结构（notional structure）是一个由"贪得的禁欲主义"（"你拥有的越多，你就越要放弃消费"）这个悖论所定义的原子化个人的世界——也就是说，缺少新教之积极宗教形式而只有新教之内容的结构。雅各宾主义也是如此：雅各宾派所忽视的事实是，他们努力追求的理想在其概念结构中在"肮脏的"贪得活动（acquisitive activity）中已然实现，而这种活动在他们看来是对其崇高理想的背叛。庸俗的、利己主义的资产阶级日常生活是自由、平等和博爱的现实性（actuality）：自由贸易的自由，法律面前的形式平等，等等。`
    },
    {
      id: "text-G-beautiful-soul-analogy", 
      content: `"消失的中介者"——新教徒、雅各宾主义——所特有的幻觉正是黑格尔式的"美丽灵魂"的幻觉：他们拒绝在他们所哀叹的腐败现实中承认他们自己的行为的最终结果——如拉康所说，他们自己的信息以其真实而颠倒的形式出现。而作为新教和雅各宾主义的"清醒的" 继承者，我们的幻觉也不少：我们把那些"消失的中介者"视为反常（aberrations）或溢出/过剩，没能注意到我们何以只是"没有雅各宾形式的雅各宾派"与"没有新教形式的新教徒"。`
    },
    {
      id: "text-H-mediator-event-subject",
      content: `**·你手指的一敲……**

"消失的中介者"实际上仅显现为一个中介者，一个介于两种"常态"事物状态之间的中间形象（figure）。然而，这种解读是唯一可能的解读吗？由"后马克思主义"政治理论（Claude Lefort, Ernesto Laclau）所阐述的概念装置允许另一种解读，而这种解读从根本上改变了视角。在这个领域中，"消失的中介者"这一环节被阿兰·巴迪欧定义为"事件"（它有关已确立的结构）的环节：其真相在其中出现的环节、有关"开放性（openness）"的环节——一旦"事件"的爆发被制度化为一种新的肯定性（positivity），它就会消失，或者更确切地说，在字面上不可见了。`
    },
    {
      id: "text-H1-subject-definition",
      content: `这一有关开放性（openness）的"不可能的"环节构成了主体性的环节："主体"是一个名称，指的是那个被召唤的、突然间负有责任的深不可测（unfathomable）的 X，它在这样一个有关不确定性（undecidability）的时刻被抛入一个责任的位置，被抛入这关于决定（decision）的紧急事态之中。这就是我们解读黑格尔的这一命题——"真理（True）不仅要被理解为实体，而且同样要被理解为主体"——不得不采取的方式：不仅要被理解为一个受某种隐藏的理性必然性支配的客观过程（即使这种必然性具有黑格尔式"理性的狡计"的），而且要被理解为一个被有关开放性／不确定性（undecidability）的环节所打断并审视（scan）的过程——主体的不可还原的偶然行为建立了一个新的必然性。`
    },
    {
      id: "text-H2-action-retroactive",
      content: `根据一个著名的意见（doxa），辩证法使我们能够穿透诸偶然性的表面戏剧，达至在主体背后"操纵着表演"的根本的（underlying）理性必然性。一个恰当的黑格尔式的辩证运动几乎是这一程序的完全颠倒：它驱散了对"客观历史进程"的迷信并让我们看到它的起源：历史上的必然性出现的方式——它是一种实证化（positivization）、主体在一个开放的、不确定的情势下的根本偶然决定的一个"凝结（coagulation）"。根据定义，"辩证的必然性"总是事后的（après coup）必然性：一个适当的辩证解读质疑对"实际上发生的事情"的自我证明，并将其与没有发生的事情对立起来——也就是说，它认为没有发生的事情（一系列错过的机会、一系列"替代性历史"）是"实际上发生的事情"的构成部分。

这个行动因而是"述行性"的，在超出了（exceeds）"言语行为"的意义上：其述行性是"回溯性的"：它重新定义了其诸预设的网络。行动的回溯述行性这一"溢出/过剩"也可以借助黑格尔关于法律与其逾越（transgression）、犯罪的辩证法得到阐释...`
    },
    {
      id: "text-H3-positing-presuppositions",
      content: `正是面对这样的背景，我们才必须理解黑格尔有关"设定预设（positing ofpresuppositions）"的论题：这种回溯性的设定恰恰是必然性从偶然性中出现的方式。主体"设定其预设"的环节，正是他作为主体被抹去的环节，他作为中介者消失的环节：当主体的决定行为（act of decision）变成它的反面时的那个结束的环节；建立一个新的象征网络，而历史借助这一网络再次获得了线性演进的自我证明。让我们回到十月革命：其"预设"在它的胜利和新政权的巩固之后、形势的开放性再次丧失之时才被"设定"——以"客观观察者"的身份叙述事件的线性发展（确定苏维埃政权如何在其最薄弱的环节打破帝国主义链条并从而开启世界历史的新纪元，等等）在这个时候才又一次得以可能。在此严格的意义上，主体是一个"消失的中介者"：它的行为通过变得不可见而成功——通过在一个新的象征网络中"实证化（positivizing）"自己，它将自己定位在此网络中并在其中将自己解释为历史进程的结果，从而将自己降为其自身行为所产生的整体中的一个单纯的环节。`
    },
    {
      id: "text-I-truth-political-intro",
      content: `**·为什么真理总是政治性（political）的？**

行动的概念直接相关于社会和政治（Social and Political）之间的关系——相关于"政治性（the Political）"和"政治（politics）"之间的区别，正如 Lefort和Laclau所阐述的那样...`
    },
    {
      id: "text-I1-politics-vs-thepolitical", 
      content: `"政治"是一个独立的社会综合体（separate socialcomplex）、一个与其他子系统（经济、文化形式）相互作用的、被肯定规定的（positively determined）社会关系的子系统，而"政治性"[le Politique]则是有关开放性的、不确定的环节（此时，社会的结构性原则、社会契约的基本形式被质疑）——简而言之，就是通过建立"新和谐"的行动来克服全球危机的环节。`
    },
    {
      id: "text-I2-thepolitical-explanation",
      content: `因此，"政治性"的维度得到了双重的刻画：它是社会整体的一个环节，是其子系统中的一个，并且也是整体之命运在其中被决定——新的契约在其中被设计并缔结——的地带。`
    },
    {
      id: "text-I3-origin-of-order-political",
      content: `在社会理论中，人们通常认为政治维度相对于社会（the Social）本身而言是次要的。在实证主义社会学中，政治是社会组织用以组织其自我调节的一个子系统；在经典马克思主义中，政治是社会阶级分化所导致的异化普遍性（alienatedUniversality）的独立领域（其基本含义是，无阶级社会将意味着作为一个独立领域的政治性（the Political）的终结）；甚至在一些"新社会运动"的意识形态中，政治性（the Political）被划定为国家权力的领域，公民社会必须组织其自卫调节机制反对它。针对这些概念，人们可以冒险提出这样的假设：社会的起源总是"政治性的（political）"——一个积极（positively）现存的社会体系只不过是一种形式，在这种形式中，一个彻底偶然之决定的否定性获得了（assumes）积极的（positive）、有规定的（determinate）实存。`
    },
    {
      id: "text-J-conclusion-subject-as-mediator",
      content: `现在我们可以回到臭名昭著的黑格尔三元组：主体是这个"消失的中介者"、这个第四环节，可以说，它颁布了自己的消失；它的消失正是衡量其"成功"的标准也是自我关联的否定性的虚空，一旦我们从其结果"回头"看这个过程，它就变得不可见了。`
    },
    {
      id: "text-K-truth-contingency-trauma",
      content: `对黑格尔三元组中这一溢出的第四环节的考察，使得我们能够在格雷马斯的"符号学矩阵"的背景下解读它：

必然性（necessity）和不可能性（impossibility）的对立本身溶解进入可能性（possibility）的领域（可以说，可能性是对必然性的"否定之否定"）——随之消失的是第四个术语，即绝不可能等同于可能（Possible）的偶然（theContingent）。在偶然性（contingency）中总存在某些"与实在界遭遇"的东西，某些前所未闻的实体的猛然出现，它违抗了人们对"可能"所持的既定场域的限度，而"可能"可以说是一种"温和的"、平和的偶然性，一种被拔掉了刺的偶然性。`
    },
    {
      id: "text-K1-analogy-greimas-lacan",
      content: `例如，在精神分析中，真理属于偶然性的秩序：我们在日常生活中过着无聊的生活，深陷于结构它的普遍的谎言（universal Lie）之中，而突然间，一些完全偶然的遭遇——朋友的一句闲话，我们目睹的一件事故——唤起了关于被压抑的旧创伤的记忆，打破了我们的自我欺骗。精神分析在这里是彻底反柏拉图的：普遍性是最卓越的虚假性（Falsity par excellence）的领域，而真理则是作为一种特殊的偶然遭遇出现的，这种遭遇使其"被压抑"的东西变得可见。在"可能性"中所失去的维度正是这种有关真理之出现的创伤性的、无保证的（unwarranted）特性：当一个真理变得"可能"时，它失去了"事件"的特性，它变成了一个单纯的有关事实的（factual）准确性，从而成为统治性的普遍谎言的组成部分。

现在我们可以看到，拉康的精神分析与罗蒂那种多元实用主义的"自由主义"有多远。拉康的最后一课不是真理（truths）的相对性和多元性，而是坚硬的、创伤性的事实，即在每一个具体的星丛中，真理（truth）必然会以某种偶然的细节出现。换句话说，尽管真理是依赖于语境的——尽管一般意义上的真理并不存在，有的总是某种情况的真理——但在每一个多元场域中都依然有一个阐明其真理并且本身不能被相对化的特殊的点；在这个确切的意义上，真理总是一。如果我们把"本体论"矩阵换成"义务论"矩阵，我们在这里的目标就会更清楚：

我们甚至缺乏一个合适的术语来形容这个X，来形容这"不是命令的（notprescribed）"、"容许的（facultative）"，但又不是简单的"允许的（permitted）"东西的奇怪状态——例如，在精神分析疗法中出现了一些迄今为止被禁止的知识，这些知识对禁令进行了嘲弄，暴露了其隐藏机制，但并没有因此而变成一种中性的"允许（permissiveness）"。两者之间的区别涉及到对普遍秩序的不同关系："允许（permissiveness）"是由它保证的（warranted），而这种保证在"你可以（may）……"的情况下是缺乏的，拉康称这种情况为scilicet：你可以知道（关于你的欲望的真相）——如果你为自己承担风险。这个scilicet 也许是批判性思维的最终追索。`
    }
  ];

  return (
    <div className="prose prose-sm max-w-none">
      {demoContentBlocks.map((block) => (
        <div 
          key={block.id}
          id={block.id}
          className="content-block mb-6 p-4"
          ref={(el) => {
            console.log('📍 [示例文档] 注册示例段落引用:', block.id, !!el);
            onContentBlockRef(el, block.id);
          }}
        >
          <ReactMarkdown
            components={{
              h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
              h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
              h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
              h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
              h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
              h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
              p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
              ul: ({node, ...props}) => <ul className="list-disc list-inside mb-3 space-y-1" {...props} />,
              ol: ({node, ...props}) => <ol className="list-decimal list-inside mb-3 space-y-1" {...props} />,
              li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
              blockquote: ({node, ...props}) => (
                <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
              ),
              code: ({node, inline, ...props}) => 
                inline 
                  ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                  : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
              pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
            }}
          >
            {block.content}
          </ReactMarkdown>
        </div>
      ))}
    </div>
  );
};

export { StructuredMarkdownRenderer, DemoModeRenderer };
</file>

<file path="frontend/src/App.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

/* 自定义样式 */
.App {
  text-align: left;
}

/* Markdown内容样式优化 */
.prose {
  max-width: none !important;
  font-size: 16px !important;
}

.prose h1 {
  @apply text-4xl font-bold text-gray-900 dark:text-gray-100 mb-4 pb-2 border-b border-gray-200 dark:border-gray-700;
}

.prose h2 {
  @apply text-3xl font-semibold text-gray-800 dark:text-gray-200 mb-3 mt-8;
}

.prose h3 {
  @apply text-2xl font-medium text-gray-700 dark:text-gray-300 mb-2 mt-6;
}

.prose p {
  @apply text-gray-600 dark:text-gray-300 mb-4 leading-relaxed;
  font-size: 16px !important;
  line-height: 1.7 !important;
}

.prose ul {
  @apply list-disc list-inside mb-4 space-y-1 text-gray-600 dark:text-gray-300;
  font-size: 16px !important;
}

.prose ol {
  @apply list-decimal list-inside mb-4 space-y-1 text-gray-600 dark:text-gray-300;
  font-size: 16px !important;
}

.prose blockquote {
  @apply border-l-4 border-blue-500 dark:border-blue-400 pl-4 italic text-gray-700 dark:text-gray-300 mb-4 bg-blue-50 dark:bg-blue-900/20 py-2;
  font-size: 16px !important;
}

.prose code {
  @apply bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-sm font-mono text-gray-800 dark:text-gray-200;
}

.prose pre {
  @apply bg-gray-100 dark:bg-gray-800 p-4 rounded-lg overflow-x-auto mb-4;
}

.prose pre code {
  @apply bg-transparent px-0 py-0;
}

/* Mermaid图表样式 */
.mermaid {
  @apply w-full h-auto;
}

/* 暗模式下的Mermaid图表样式 */
:root.dark .mermaid {
  filter: brightness(0.8) contrast(1.2);
}

/* Mermaid节点高亮样式 - 只高亮边框，不改变填充色 */
.mermaid svg g.mermaid-highlighted-node > rect,
.mermaid svg g.mermaid-highlighted-node > circle,
.mermaid svg g.mermaid-highlighted-node > ellipse,
.mermaid svg g.mermaid-highlighted-node > polygon,
.mermaid svg .mermaid-highlighted-node rect,
.mermaid svg .mermaid-highlighted-node circle,
.mermaid svg .mermaid-highlighted-node ellipse,
.mermaid svg .mermaid-highlighted-node polygon {
  stroke: #FB923C !important;
  stroke-width: 3px !important;
  filter: drop-shadow(0 0 8px rgba(251, 146, 60, 0.6)) drop-shadow(0 0 16px rgba(251, 146, 60, 0.3)) !important;
  /* 不改变填充色，保持原有颜色 */
}

/* 暗模式下的Mermaid高亮样式 */
:root.dark .mermaid svg g.mermaid-highlighted-node > rect,
:root.dark .mermaid svg g.mermaid-highlighted-node > circle,
:root.dark .mermaid svg g.mermaid-highlighted-node > ellipse,
:root.dark .mermaid svg g.mermaid-highlighted-node > polygon,
:root.dark .mermaid svg .mermaid-highlighted-node rect,
:root.dark .mermaid svg .mermaid-highlighted-node circle,
:root.dark .mermaid svg .mermaid-highlighted-node ellipse,
:root.dark .mermaid svg .mermaid-highlighted-node polygon {
  stroke: #FED7AA !important;
  stroke-width: 3px !important;
  filter: drop-shadow(0 0 8px rgba(253, 215, 170, 0.8)) drop-shadow(0 0 16px rgba(253, 215, 170, 0.4)) !important;
}

/* 确保连接线不受高亮影响 */
.mermaid svg g.mermaid-highlighted-node > path,
.mermaid svg g.mermaid-highlighted-node > line,
.mermaid svg .mermaid-highlighted-node path,
.mermaid svg .mermaid-highlighted-node line {
  stroke: #9ca3af !important;
  stroke-width: 1.5px !important;
  fill: none !important;
  filter: none !important;
}

/* 暗模式下的连接线 */
:root.dark .mermaid svg g.mermaid-highlighted-node > path,
:root.dark .mermaid svg g.mermaid-highlighted-node > line,
:root.dark .mermaid svg .mermaid-highlighted-node path,
:root.dark .mermaid svg .mermaid-highlighted-node line {
  stroke: #6b7280 !important;
}

/* React Flow节点高亮样式 */
.react-flow__node.highlighted-node,
.react-flow__node.mermaid-highlighted-node {
  border: 3px solid #FB923C !important;
  box-shadow: 0 0 15px rgba(251, 146, 60, 0.6), 0 0 30px rgba(251, 146, 60, 0.3) !important;
  z-index: 1000 !important;
  background-color: rgba(251, 146, 60, 0.05) !important;
  transition: all 0.2s ease-in-out !important;
}

/* 暗模式下的React Flow节点高亮 */
:root.dark .react-flow__node.highlighted-node,
:root.dark .react-flow__node.mermaid-highlighted-node {
  border: 3px solid #FED7AA !important;
  box-shadow: 0 0 15px rgba(253, 215, 170, 0.8), 0 0 30px rgba(253, 215, 170, 0.4) !important;
  background-color: rgba(253, 215, 170, 0.08) !important;
}

/* 活跃文本块样式 - 静止状态，无动画 */
.content-block {
  border-left: 4px solid transparent;
}

.content-block.active {
  border-left-color: #FB923C !important;
  /* 不设置任何其他样式，确保只有边框颜色改变 */
}

/* 暗模式下的活跃文本块 */
:root.dark .content-block.active {
  border-left-color: #FED7AA !important;
}

/* 确保文本内容不受Mermaid节点样式影响 */
.content-block,
.content-block *,
.prose,
.prose * {
  filter: none !important;
}

/* 强制确保活跃内容块内的所有元素都不受filter影响 */
.content-block.active,
.content-block.active *,
.content-block.active p,
.content-block.active h1,
.content-block.active h2,
.content-block.active h3,
.content-block.active h4,
.content-block.active h5,
.content-block.active h6,
.content-block.active span,
.content-block.active div {
  color: inherit !important;
  background-color: transparent !important;
  background: none !important;
  filter: none !important;
}

/* 防止选择状态影响显示 */
.content-block.active::selection,
.content-block.active *::selection {
  background-color: rgba(59, 130, 246, 0.2) !important;
  color: inherit !important;
}

/* 章节高亮样式 - 静止状态，无动画 */
.section-highlighted {
  border-left: 4px solid #FB923C;
  background-color: rgba(251, 146, 60, 0.05) !important;
  transform: translateX(2px);
}

/* 暗模式下的章节高亮 */
:root.dark .section-highlighted {
  border-left-color: #FED7AA;
  background-color: rgba(253, 215, 170, 0.1) !important;
}

/* 普通模式下的chunk-section高亮效果 */
.chunk-section.section-highlighted {
  border-left-color: #FB923C !important;
  background-color: rgba(251, 146, 60, 0.05) !important;
  transform: translateX(2px);
  box-shadow: 0 2px 8px rgba(251, 146, 60, 0.1);
}

:root.dark .chunk-section.section-highlighted {
  border-left-color: #FED7AA !important;
  background-color: rgba(253, 215, 170, 0.1) !important;
  box-shadow: 0 2px 8px rgba(253, 215, 170, 0.15);
}

/* chunk-section的基础样式 */
.chunk-section {
  padding: 12px 16px;
  border-radius: 6px;
  margin-bottom: 16px;
}

.chunk-section:hover {
  background-color: rgba(249, 250, 251, 0.5);
}

:root.dark .chunk-section:hover {
  background-color: rgba(31, 41, 55, 0.5);
}

/* 滚动条样式 */
::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}

::-webkit-scrollbar-track {
  @apply bg-gray-100 dark:bg-gray-800;
}

::-webkit-scrollbar-thumb {
  @apply bg-gray-300 dark:bg-gray-600 rounded;
}

::-webkit-scrollbar-thumb:hover {
  @apply bg-gray-400 dark:bg-gray-500;
}

/* 拖拽上传区域样式 */
.upload-area {
  transition: all 0.2s ease-in-out;
}

.upload-area:hover {
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
}

/* 暗模式下的拖拽上传区域 */
:root.dark .upload-area:hover {
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
}

/* 响应式调整 */
@media (max-width: 768px) {
  .prose {
    font-size: 15px !important;
  }
  
  .prose h1 {
    @apply text-3xl;
  }
  
  .prose h2 {
    @apply text-2xl;
  }
  
  .prose h3 {
    @apply text-xl;
  }
  
  .prose p {
    font-size: 15px !important;
  }
}

/* 加载动画 */
@keyframes pulse {
  0%, 100% {
    opacity: 1;
  }
  50% {
    opacity: 0.5;
  }
}

.pulse {
  animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite;
}

/* 渐入动画 */
@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.fade-in {
  animation: fadeIn 0.3s ease-out;
}

/* 工具提示样式 */
.tooltip {
  @apply invisible absolute z-10 py-2 px-3 text-sm font-medium text-white bg-gray-900 dark:bg-gray-700 rounded-lg shadow-sm opacity-0 transition-opacity duration-300;
}

.tooltip-trigger:hover .tooltip {
  @apply visible opacity-100;
}

/* 暗模式下的Toast通知样式 */
:root {
  --toast-bg: #363636;
  --toast-color: #fff;
}

:root.dark {
  --toast-bg: #374151;
  --toast-color: #f9fafb;
}

/* 移除不需要的样式 */
/* .content-block-highlighted 和其他背景高亮样式已删除 */

/* 
高亮样式说明：
- 亮模式：使用#FB923C (橙色-400)作为高亮色
- 暗模式：使用#FED7AA (橙色-200)作为高亮色，更适合暗色背景
- Mermaid节点：仅高亮边框和添加发光效果，不改变填充色
- 文本块：仅左侧边框高亮，不改变背景和文字颜色
*/

/* 语义块高亮样式 - 基于AI生成的语义分析 - 只有左边框，不改变格式 */
.semantic-block-highlighted {
  border-left: 4px solid #FB923C !important;
}

/* 暗模式下的语义块高亮 */
:root.dark .semantic-block-highlighted {
  border-left-color: #FED7AA !important;
}

/* 段落级语义高亮样式 - 固定边框，不移动文本 */
.semantic-paragraph-highlighted {
  border-left: 4px solid #FB923C !important;
  /* 确保原始布局不变，避免文本移动 */
  margin-left: 0 !important;
  padding-left: 16px; /* 增加内边距而不是外边距 */
  box-sizing: border-box;
}

/* 暗模式下的段落级高亮 */
:root.dark .semantic-paragraph-highlighted {
  border-left-color: #FED7AA !important;
}

/* 段落块的基础样式 - 移除hover效果 */
.paragraph-block, .content-block {
  /* 移除所有hover效果，确保没有交互式样式 */
  border-left: 4px solid transparent; /* 预留边框空间，避免高亮时布局变化 */
  padding-left: 16px;
  box-sizing: border-box;
  transition: border-left-color 0.2s ease; /* 只过渡边框颜色 */
}

/* 确保语义高亮样式优先级更高 */
.semantic-block-highlighted,
.semantic-paragraph-highlighted {
  position: relative;
  z-index: 10;
}

/* 语义块组合高亮 - 当多个段落属于同一语义块时 */
.semantic-group-highlighted {
  background: linear-gradient(to right, 
    rgba(59, 130, 246, 0.05), 
    rgba(59, 130, 246, 0.1), 
    rgba(59, 130, 246, 0.05)
  ) !important;
  border-left: 4px solid #3b82f6 !important;
  border-radius: 8px;
  padding: 12px 16px;
  margin: 8px 0;
  box-shadow: 0 4px 12px rgba(59, 130, 246, 0.1);
}

:root.dark .semantic-group-highlighted {
  background: linear-gradient(to right, 
    rgba(59, 130, 246, 0.08), 
    rgba(59, 130, 246, 0.15), 
    rgba(59, 130, 246, 0.08)
  ) !important;
  border-left-color: #60a5fa !important;
  box-shadow: 0 4px 12px rgba(96, 165, 250, 0.15);
}
</file>

<file path="frontend/src/components/FlowDiagram.js">
import React, { useEffect, useState, useCallback, useImperativeHandle, forwardRef, useMemo, useRef } from 'react';
import ReactFlow, {
  ReactFlowProvider,
  Background,
  Controls,
  MiniMap,
  useNodesState,
  useEdgesState,
  addEdge,
  ConnectionLineType,
  Panel,
  useReactFlow,
} from 'reactflow';
import 'reactflow/dist/style.css';

import { convertDataToReactFlow } from '../utils/dataConverter';
import { getLayoutedElements } from '../utils/layoutHelper';
import { updateNodeLabel, handleApiError } from '../utils/api';
import EditableNode from './EditableNode';

// 注册自定义节点类型
const nodeTypes = {
  editableNode: EditableNode,
};

/**
 * React Flow图表组件，兼容MermaidDiagram接口
 * @param {Object} props - 组件属性
 * @param {string} props.code - Mermaid代码字符串 (向后兼容)
 * @param {Object} props.apiData - 包含mermaid_string和node_mappings的数据
 * @param {string} props.highlightedNodeId - 需要高亮的节点ID
 * @param {Function} props.onNodeClick - 节点点击回调函数
 * @param {Function} props.onNodeLabelUpdate - 节点标签更新回调函数
 * @param {Function} props.onAddChildNode - 添加子节点回调函数
 * @param {Function} props.onAddSiblingNode - 添加同级节点回调函数
 * @param {Function} props.onDeleteNode - 删除节点回调函数
 * @param {Object} props.layoutOptions - 布局选项
 * @param {string} props.className - CSS类名
 */
const FlowDiagramInner = ({ 
  code, 
  apiData,
  highlightedNodeId,
  onNodeClick, 
  onNodeLabelUpdate,
  onAddChildNode,
  onAddSiblingNode,
  onDeleteNode,
  layoutOptions = {}, 
  className = '',
  onReactFlowInstanceChange
}) => {
  const [nodes, setNodes, onNodesChange] = useNodesState([]);
  const [edges, setEdges, onEdgesChange] = useEdgesState([]);
  const [isLoading, setIsLoading] = useState(false);
  const [documentId, setDocumentId] = useState(null);

  // 使用useRef来稳定化handleLabelChange函数，避免不必要的重新渲染
  const handleLabelChangeRef = useRef(null);

  // 标签更新的回调函数 - 使用useCallback但不包含在useEffect依赖中
  const handleLabelChange = useCallback(async (nodeId, newLabel) => {
    try {
      console.log('🔄 [FlowDiagram] 更新节点标签:', nodeId, '->', newLabel);
      
      // 更新本地状态
      setNodes((currentNodes) => 
        currentNodes.map(node => 
          node.id === nodeId 
            ? { ...node, data: { ...node.data, label: newLabel } }
            : node
        )
      );

      // 调用父组件的节点标签更新回调（优先级高）
      if (onNodeLabelUpdate) {
        console.log('🔄 [FlowDiagram] 调用父组件节点标签更新回调');
        onNodeLabelUpdate(nodeId, newLabel);
      }

      // 调用后端API持久化更改
      if (documentId) {
        try {
          await updateNodeLabel(documentId, nodeId, newLabel);
          console.log('📝 [FlowDiagram] 节点标签更新成功');
        } catch (apiError) {
          console.error('❌ [FlowDiagram] API调用失败:', apiError);
          // 可以选择显示用户友好的错误消息
          // alert(handleApiError(apiError));
        }
      }
    } catch (error) {
      console.error('❌ [FlowDiagram] 更新节点标签失败:', error);
      // 可以在这里添加错误提示
    }
  }, [documentId, setNodes, onNodeLabelUpdate]); // 🔑 添加onNodeLabelUpdate到依赖中

  // 将handleLabelChange存储到ref中，保持引用稳定
  useEffect(() => {
    handleLabelChangeRef.current = handleLabelChange;
  }, [handleLabelChange]);

  // 处理数据变化 - 移除handleLabelChange依赖，使用ref来避免重新渲染
  useEffect(() => {
    // 优先使用apiData，否则使用code进行向后兼容
    const dataToProcess = apiData || (code ? {
      mermaid_string: code,
      node_mappings: extractNodeMappingsFromMermaid(code)
    } : null);

    if (!dataToProcess) {
      setNodes([]);
      setEdges([]);
      return;
    }

    // 从 apiData 中提取 document_id（如果有的话）
    if (apiData && apiData.document_id) {
      setDocumentId(apiData.document_id);
    }

    setIsLoading(true);

    try {
      // 转换API数据为React Flow格式
      const { nodes: convertedNodes, edges: convertedEdges } = convertDataToReactFlow(dataToProcess);
      
      console.log('🔄 [FlowDiagram] 数据转换 - 节点:', convertedNodes.length, '边:', convertedEdges.length);
      
      if (convertedNodes.length === 0) {
        console.log('🔄 [FlowDiagram] 没有转换出节点，设置空数组');
        setNodes([]);
        setEdges([]);
        setIsLoading(false);
        return;
      }

      // 为节点添加 onLabelChange 回调并设置为可编辑类型
      // 使用ref中的函数避免重新创建
      const nodesWithCallback = convertedNodes.map(node => ({
        ...node,
        type: 'editableNode', // 设置为可编辑节点类型
        data: {
          ...node.data,
          onLabelChange: (...args) => handleLabelChangeRef.current?.(...args), // 使用ref中的函数
          onAddChildNode: onAddChildNode,
          onAddSiblingNode: onAddSiblingNode,
          onDeleteNode: onDeleteNode
        }
      }));

      // 应用自动布局
      const layoutOptionsToUse = {
        direction: layoutOptions.direction || 'TB',
        nodeSpacing: layoutOptions.nodeSpacing || 100,
        rankSpacing: layoutOptions.rankSpacing || 150,
        nodeWidth: layoutOptions.nodeWidth || 200,
        nodeHeight: layoutOptions.nodeHeight || 80,
        ...layoutOptions
      };
      
      const { nodes: layoutedNodes, edges: layoutedEdges } = getLayoutedElements(
        nodesWithCallback, 
        convertedEdges,
        layoutOptionsToUse
      );

      console.log('🔄 [FlowDiagram] 布局完成 - 节点数量:', layoutedNodes.length);
      console.log('🔄 [关键] 设置到状态的节点位置:', layoutedNodes.map(n => ({ id: n.id, position: n.position })));

      setNodes(layoutedNodes);
      setEdges(layoutedEdges);

    } catch (error) {
      console.error('处理图表数据时出错:', error);
      setNodes([]);
      setEdges([]);
    } finally {
      setIsLoading(false);
    }
  }, [code, apiData, layoutOptions]); // 移除handleLabelChange依赖

  // 非破坏性高亮实现 - 直接操作DOM而不修改节点对象
  const applyNodeHighlighting = useCallback((nodeIdToHighlight) => {
    console.log('🎯 [非破坏性高亮] 开始应用节点高亮:', nodeIdToHighlight);
    
    // 🔑 优化：使用更稳定的查找方式，避免在拖拽时失效
    const findNodeElement = (nodeId) => {
      // 策略1：直接通过data-id属性查找
      let nodeElement = document.querySelector(`[data-id="${nodeId}"]`);
      if (nodeElement) {
        console.log('🎯 [节点查找] 策略1成功 - data-id:', nodeId);
        return nodeElement;
      }
      
      // 策略2：查找React Flow节点容器
      nodeElement = document.querySelector(`.react-flow__node[data-id="${nodeId}"]`);
      if (nodeElement) {
        console.log('🎯 [节点查找] 策略2成功 - react-flow__node:', nodeId);
        return nodeElement;
      }
      
      // 策略3：遍历所有React Flow节点
      const allNodes = document.querySelectorAll('.react-flow__node');
      for (const node of allNodes) {
        const dataId = node.getAttribute('data-id');
        if (dataId === nodeId) {
          console.log('🎯 [节点查找] 策略3成功 - 遍历匹配:', nodeId);
          return node;
        }
        
        // 检查子元素
        const childMatch = node.querySelector(`[data-id="${nodeId}"]`);
        if (childMatch) {
          console.log('🎯 [节点查找] 策略3成功 - 子元素匹配:', nodeId);
          return node;
        }
      }
      
      console.warn('🎯 [节点查找] 所有策略都失败了:', nodeId);
      return null;
    };
    
    // 移除所有现有高亮
    const allNodes = document.querySelectorAll('.react-flow__node');
    allNodes.forEach(nodeElement => {
      nodeElement.classList.remove('highlighted-node');
    });
    console.log('🎯 [非破坏性高亮] 清除了所有现有高亮');
    
    // 如果有指定的节点ID，添加高亮
    if (nodeIdToHighlight) {
      const foundNode = findNodeElement(nodeIdToHighlight);
      
      if (foundNode) {
        foundNode.classList.add('highlighted-node');
        console.log('🎯 [非破坏性高亮] ✅ 成功高亮节点:', nodeIdToHighlight);
        
        // 🔑 延迟检查高亮是否还在，如果不在则重新应用
        setTimeout(() => {
          const stillHighlighted = foundNode.classList.contains('highlighted-node');
          if (!stillHighlighted) {
            console.log('🎯 [高亮恢复] 检测到高亮丢失，重新应用:', nodeIdToHighlight);
            foundNode.classList.add('highlighted-node');
          }
        }, 100);
        
        // 确保高亮的节点在视口中可见（可选）
        const nodeRect = foundNode.getBoundingClientRect();
        const viewportHeight = window.innerHeight;
        const viewportWidth = window.innerWidth;
        
        const isVisible = nodeRect.top >= 0 && 
                         nodeRect.left >= 0 && 
                         nodeRect.bottom <= viewportHeight && 
                         nodeRect.right <= viewportWidth;
        
        if (!isVisible) {
          console.log('🎯 [非破坏性高亮] 节点不在视口中，滚动到可见位置');
          foundNode.scrollIntoView({ 
            behavior: 'smooth', 
            block: 'center',
            inline: 'center'
          });
        }
      } else {
        console.warn('🎯 [非破坏性高亮] ❌ 未找到节点元素:', nodeIdToHighlight);
        
        // 输出调试信息
        const allNodes = document.querySelectorAll('.react-flow__node');
        const nodeIds = Array.from(allNodes).map(node => ({
          dataId: node.getAttribute('data-id'),
          id: node.id,
          className: node.className
        }));
        console.log('🎯 [调试] 页面中所有节点的信息:', nodeIds);
      }
    } else {
      console.log('🎯 [非破坏性高亮] 清除所有高亮（nodeIdToHighlight为空）');
    }
  }, []);

  // 监听高亮节点变化，使用非破坏性方式应用高亮
  useEffect(() => {
    if (nodes.length > 0) {
      // 延迟执行，确保DOM已经更新
      setTimeout(() => {
        applyNodeHighlighting(highlightedNodeId);
      }, 100);
    }
  }, [highlightedNodeId, nodes, applyNodeHighlighting]); // 🔑 修复：监听整个nodes数组而不只是length

  // 🔑 新增：处理ReactFlow节点变化事件，确保拖拽后重新应用高亮
  const handleNodesChange = useCallback((changes) => {
    console.log('🎯 [ReactFlow] 节点变化事件:', changes);
    
    // 调用原始的onNodesChange处理函数
    onNodesChange(changes);
    
    // 🔑 优化：只在特定变化类型且有高亮节点时才处理
    if (!highlightedNodeId) {
      console.log('🎯 [ReactFlow] 无高亮节点，跳过高亮处理');
      return;
    }
    
    // 检查是否有需要重新应用高亮的变化
    const needsHighlightReapply = changes.some(change => {
      const isRelevantChange = 
        change.type === 'position' ||     // 位置变化（拖拽）
        change.type === 'dimensions' ||   // 尺寸变化
        change.type === 'select' ||       // 选择状态变化
        change.type === 'replace';        // 节点替换
      
      // 如果是拖拽结束事件，也需要重新应用高亮
      const isDragEnd = change.type === 'position' && change.dragging === false;
      
      return isRelevantChange || isDragEnd;
    });
    
    if (needsHighlightReapply) {
      console.log('🎯 [ReactFlow] 检测到需要重新应用高亮的变化，节点:', highlightedNodeId);
      
      // 🔑 使用更短的延迟，提高响应速度
      setTimeout(() => {
        // 再次检查高亮节点ID是否仍然有效
        if (highlightedNodeId) {
          console.log('🎯 [ReactFlow] 执行延迟高亮重新应用:', highlightedNodeId);
          applyNodeHighlighting(highlightedNodeId);
        }
      }, 50); // 减少延迟，提高响应速度
    } else {
      console.log('🎯 [ReactFlow] 变化不需要重新应用高亮');
    }
  }, [onNodesChange, highlightedNodeId, applyNodeHighlighting]);

  // 🔑 新增：处理ReactFlow画布点击事件，防止高亮意外清除
  const handlePaneClick = useCallback((event) => {
    console.log('�� [ReactFlow] 画布点击事件，当前高亮节点:', highlightedNodeId);
    
    // 🔑 保持现有高亮状态，不执行任何清除操作
    // 如果需要清除高亮，应该通过外部控制highlightedNodeId的值
    // 这样可以确保高亮状态的管理是统一和可控的
    
    // 可选：在画布点击后验证高亮状态是否仍然正确
    if (highlightedNodeId) {
      setTimeout(() => {
        const highlightedElement = document.querySelector(`[data-id="${highlightedNodeId}"].highlighted-node`);
        if (!highlightedElement) {
          console.log('🎯 [ReactFlow] 画布点击后检测到高亮丢失，重新应用:', highlightedNodeId);
          applyNodeHighlighting(highlightedNodeId);
        } else {
          console.log('🎯 [ReactFlow] 画布点击后高亮状态正常');
        }
      }, 50);
    }
  }, [highlightedNodeId, applyNodeHighlighting]);

  // 从Mermaid代码中提取节点映射
  const extractNodeMappingsFromMermaid = (mermaidCode) => {
    const nodeMappings = {};
    
    if (!mermaidCode) return nodeMappings;

    // 匹配节点定义，如 A[文本], A(文本), A{文本}
    const nodeDefRegex = /([A-Za-z0-9_]+)[\[\(\{]([^\]\)\}]+)[\]\)\}]/g;
    let match;
    
    while ((match = nodeDefRegex.exec(mermaidCode)) !== null) {
      const [, nodeId, nodeText] = match;
      nodeMappings[nodeId] = {
        text_snippet: nodeText.trim(),
        paragraph_ids: []
      };
    }

    // 如果没有找到节点定义，从连接关系中提取节点ID
    if (Object.keys(nodeMappings).length === 0) {
      const connectionRegex = /([A-Za-z0-9_]+)\s*(-{1,2}>?|={1,2}>?)\s*([A-Za-z0-9_]+)/g;
      const nodeIds = new Set();
      
      while ((match = connectionRegex.exec(mermaidCode)) !== null) {
        const [, source, , target] = match;
        nodeIds.add(source);
        nodeIds.add(target);
      }
      
      // 为每个节点ID创建基本映射
      nodeIds.forEach(nodeId => {
        nodeMappings[nodeId] = {
          text_snippet: nodeId,
          paragraph_ids: []
        };
      });
    }

    return nodeMappings;
  };

  // 处理连接
  const onConnect = useCallback(
    (params) => setEdges((eds) => addEdge(params, eds)),
    [setEdges],
  );

  // 处理节点点击
  const onNodeClickHandler = useCallback((event, node) => {
    console.log('FlowDiagram节点点击:', node.id, node);
    if (onNodeClick) {
      // 调用与MermaidDiagram兼容的回调
      // 传递节点ID作为第一个参数，事件作为第二个参数
      onNodeClick(node.id, event);
    }
  }, [onNodeClick]);

  // 处理ReactFlow实例初始化
  const onInit = useCallback((reactFlowInstance) => {
    console.log('🔄 [FlowDiagram] ReactFlow实例初始化');
    if (onReactFlowInstanceChange) {
      onReactFlowInstanceChange(reactFlowInstance);
    }
    
    // 延迟适应视图，确保节点已经渲染
    setTimeout(() => {
      const allNodes = reactFlowInstance.getNodes();
      console.log('🔄 [关键] ReactFlow实例中的节点:', allNodes.map(n => ({ 
        id: n.id, 
        position: n.position,
        width: n.width,
        height: n.height
      })));
      
      if (allNodes.length > 0) {
        console.log('🔄 [FlowDiagram] 执行fitView');
        reactFlowInstance.fitView({ padding: 0.2, duration: 800 });
      }
    }, 500); // 增加延迟时间，确保布局完成
  }, [onReactFlowInstanceChange]);

  // 自定义节点样式 - 基础样式，高亮样式由CSS处理
  const nodeDefaults = {
    style: {
      background: '#ffffff',
      border: '2px solid #1a192b',
      borderRadius: '8px',
      fontSize: '12px',
      fontWeight: 500,
      padding: '10px',
      boxShadow: '0 2px 4px rgba(0,0,0,0.1)',
      minWidth: '150px',
      textAlign: 'center',
      width: 200,
      height: 80
    },
  };

  return (
    <div className={`w-full h-full ${className}`}>
      {isLoading ? (
        <div className="flex items-center justify-center h-full">
          <div className="text-center">
            <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
            <p className="text-sm text-gray-500">正在加载流程图...</p>
          </div>
        </div>
      ) : (
        <ReactFlow
          nodes={nodes}  // 直接使用原始节点，不再通过nodesWithHighlightClass处理
          edges={edges}
          onNodesChange={handleNodesChange}
          onEdgesChange={onEdgesChange}
          onConnect={onConnect}
          onNodeClick={onNodeClickHandler}
          onPaneClick={handlePaneClick}
          nodeTypes={nodeTypes}
          connectionLineType={ConnectionLineType.SmoothStep}
          fitView
          fitViewOptions={{
            padding: 0.1,
            includeHiddenNodes: false,
          }}
          onInit={(instance) => {
            console.log('🔄 [FlowDiagram] ReactFlow实例初始化完成');
            if (onReactFlowInstanceChange) {
              onReactFlowInstanceChange(instance);
            }
          }}
        >
          <Background variant="dots" gap={20} size={1} />
          <Controls />
          <MiniMap 
            nodeStrokeColor="#374151" 
            nodeColor="#f3f4f6" 
            nodeBorderRadius={8}
          />
        </ReactFlow>
      )}
    </div>
  );
};

const FlowDiagram = forwardRef(({ 
  code, 
  apiData, 
  highlightedNodeId, 
  onNodeClick, 
  onNodeLabelUpdate,
  onAddChildNode,
  onAddSiblingNode,
  onDeleteNode,
  layoutOptions = {}, 
  className = '' 
}, ref) => {
  const [reactFlowInstance, setReactFlowInstance] = useState(null);

  // 处理ReactFlow实例变化
  const handleReactFlowInstanceChange = useCallback((instance) => {
    setReactFlowInstance(instance);
  }, []);

  // 提供与MermaidDiagram兼容的ref方法
  useImperativeHandle(ref, () => ({
    // 兼容MermaidDiagram的ensureNodeVisible方法
    ensureNodeVisible: (nodeId) => {
      if (reactFlowInstance) {
        try {
          // 获取节点并聚焦到它
          const node = reactFlowInstance.getNode(nodeId);
          if (node) {
            // 使用更平滑的动画效果聚焦到节点
            reactFlowInstance.setCenter(
              node.position.x + (node.width || 200) / 2, 
              node.position.y + (node.height || 80) / 2, 
              { zoom: 1.2, duration: 800 }
            );
          }
        } catch (error) {
          console.warn('无法聚焦到节点:', nodeId, error);
        }
      }
    },
    
    // 提供获取React Flow实例的方法
    getReactFlowInstance: () => reactFlowInstance,
    
    // 重新布局方法
    fitView: () => {
      if (reactFlowInstance) {
        reactFlowInstance.fitView({ padding: 0.2, duration: 800 });
      }
    }
  }), [reactFlowInstance]);

  // 当高亮节点变化时，自动聚焦到该节点
  useEffect(() => {
    if (highlightedNodeId && reactFlowInstance) {
      // 延迟执行，确保节点已经更新
      setTimeout(() => {
        const node = reactFlowInstance.getNode(highlightedNodeId);
        if (node && node.position) {
          console.log('🎯 [自动聚焦] 聚焦到节点:', highlightedNodeId, '位置:', node.position);
          reactFlowInstance.setCenter(
            node.position.x + (node.width || 200) / 2, 
            node.position.y + (node.height || 80) / 2, 
            { zoom: 1.2, duration: 800 }
          );
        } else {
          console.warn('🎯 [自动聚焦] 未找到节点或节点位置无效:', highlightedNodeId, node);
        }
      }, 300); // 增加延迟时间，确保高亮样式更新完成
    }
  }, [highlightedNodeId, reactFlowInstance]);

      return (
      <ReactFlowProvider>
        <FlowDiagramInner 
          code={code}
          apiData={apiData}
          highlightedNodeId={highlightedNodeId}
          onNodeClick={onNodeClick}
          onNodeLabelUpdate={onNodeLabelUpdate}
          onAddChildNode={onAddChildNode}
          onAddSiblingNode={onAddSiblingNode}
          onDeleteNode={onDeleteNode}
          layoutOptions={layoutOptions}
          className={className}
          onReactFlowInstanceChange={handleReactFlowInstanceChange}
        />
      </ReactFlowProvider>
    );
});

FlowDiagram.displayName = 'FlowDiagram';

export default FlowDiagram;
</file>

<file path="frontend/src/components/MermaidDiagram.js">
import React, { useEffect, useRef, useState, useCallback, useImperativeHandle, forwardRef } from 'react';
import mermaid from 'mermaid';
import { AlertCircle, Copy, Check, ZoomIn, ZoomOut, RotateCcw, Move } from 'lucide-react';
import toast from 'react-hot-toast';

// 美化Mermaid节点的CSS样式 - 精确悬停版本
const mermaidStyles = `
  /* 基础节点样式 */
  .mermaid rect,
  .mermaid polygon,
  .mermaid circle,
  .mermaid ellipse {
    fill: #ffffff !important;
    stroke: rgba(0, 0, 0, 0.2) !important;
    stroke-width: 1px !important;
    filter: drop-shadow(2px 2px 6px rgba(0, 0, 0, 0.15)) !important;
    transition: all 0.3s ease !important;
  }

  /* 矩形圆角 */
  .mermaid rect {
    rx: 8 !important;
    ry: 8 !important;
  }

  /* 悬停效果 - 只作用于当前悬停的节点 */
  .mermaid g:hover > rect,
  .mermaid g:hover > polygon,
  .mermaid g:hover > circle,
  .mermaid g:hover > ellipse {
    fill: #ffffff !important;
    stroke: #3b82f6 !important;
    stroke-width: 2px !important;
    filter: drop-shadow(2px 2px 12px rgba(59, 130, 246, 0.3)) !important;
    cursor: pointer !important;
  }

  /* 确保悬停时不影响其他节点 */
  .mermaid g:not(:hover) rect,
  .mermaid g:not(:hover) polygon,
  .mermaid g:not(:hover) circle,
  .mermaid g:not(:hover) ellipse {
    fill: #ffffff !important;
    stroke: rgba(0, 0, 0, 0.2) !important;
    stroke-width: 1px !important;
    filter: drop-shadow(2px 2px 6px rgba(0, 0, 0, 0.15)) !important;
  }

  /* 确保文本可见 */
  .mermaid text {
    font-family: "Microsoft YaHei", Arial, sans-serif !important;
    font-weight: 500 !important;
    fill: #374151 !important;
    pointer-events: none !important;
  }

  /* 连接线样式 */
  .mermaid path {
    stroke: #9ca3af !important;
    stroke-width: 1.5px !important;
    fill: none !important;
  }

  .mermaid marker {
    fill: #9ca3af !important;
  }

  /* 节点点击样式 */
  .mermaid g {
    cursor: pointer !important;
  }

  /* 增强节点点击区域 */
  .mermaid g > rect,
  .mermaid g > polygon,
  .mermaid g > circle,
  .mermaid g > ellipse {
    cursor: pointer !important;
  }
`;

// 注入样式到页面
const injectStyles = () => {
  const styleId = 'mermaid-custom-styles';
  
  // 先移除之前的样式
  const existingStyle = document.getElementById(styleId);
  if (existingStyle) {
    existingStyle.remove();
    console.log('🗑️ 移除了之前的样式');
  }

  const styleSheet = document.createElement('style');
  styleSheet.id = styleId;
  styleSheet.type = 'text/css';
  styleSheet.textContent = mermaidStyles;
  document.head.appendChild(styleSheet);
  
  console.log('✨ Mermaid自定义样式已注入');
  console.log('📋 样式内容长度:', mermaidStyles.length);
  
  // 验证样式是否已添加
  setTimeout(() => {
    const appliedStyle = document.getElementById(styleId);
    if (appliedStyle) {
      console.log('✅ 样式确认已添加到DOM');
      console.log('📄 样式表内容预览:', appliedStyle.textContent.substring(0, 100) + '...');
      
      // 检查是否有Mermaid SVG元素存在
      const mermaidSvgs = document.querySelectorAll('.mermaid svg');
      console.log('🎨 找到', mermaidSvgs.length, '个Mermaid SVG元素');
      
      if (mermaidSvgs.length > 0) {
        const firstSvg = mermaidSvgs[0];
        const rects = firstSvg.querySelectorAll('g rect');
        console.log('📦 第一个SVG中有', rects.length, '个矩形节点');
        
        if (rects.length > 0) {
          const computedStyle = window.getComputedStyle(rects[0]);
          console.log('🎨 第一个矩形的计算样式:');
          console.log('  - fill:', computedStyle.fill);
          console.log('  - stroke:', computedStyle.stroke);
          console.log('  - filter:', computedStyle.filter);
        }
      }
    } else {
      console.error('❌ 样式添加失败');
    }
  }, 100);
};

const MermaidDiagram = forwardRef(({ code, onNodeClick }, ref) => {
  const [diagramId] = useState(() => `mermaid-${Math.random().toString(36).substr(2, 9)}`);
  const [error, setError] = useState(null);
  const [copied, setCopied] = useState(false);
  const [isRendering, setIsRendering] = useState(false);
  const [hasRendered, setHasRendered] = useState(false);
  const [scale, setScale] = useState(1);
  const [position, setPosition] = useState({ x: 0, y: 0 });
  const [isDragging, setIsDragging] = useState(false);
  const [dragStart, setDragStart] = useState({ x: 0, y: 0 });
  const [mermaidInitialized, setMermaidInitialized] = useState(false);
  const [domReady, setDomReady] = useState(false);
  const containerRef = useRef(null);
  const diagramRef = useRef(null);
  const copyTimeoutRef = useRef(null);
  const parentContainerRef = useRef(null);

  // 使用useRef来保存事件处理函数的引用
  const handleMouseMoveRef = useRef(null);
  const handleMouseUpRef = useRef(null);

  // 使用RAF优化的拖拽处理
  const dragAnimationFrame = useRef(null);
  const pendingPosition = useRef(null);
  
  // 防抖相关状态
  const isAnimating = useRef(false);
  const lastMoveTime = useRef(0);
  const lastMovedNode = useRef(null);
  const moveDebounceTimer = useRef(null);

  // 节点关系缓存
  const nodeRelationsRef = useRef(null);

  // 解析Mermaid代码构建节点关系的函数
  const parseMermaidCode = useCallback((mermaidCode) => {
    if (!mermaidCode) return { nodes: new Set(), edges: [], adjacencyList: new Map() };

    console.log('🔍 [代码解析] 开始解析Mermaid代码');
    console.log('🔍 [代码解析] 代码预览:', mermaidCode.substring(0, 200) + '...');

    const nodes = new Set();
    const edges = [];
    const adjacencyList = new Map(); // nodeId -> [childNodeIds]

    try {
      // 将代码按行分割并清理
      const lines = mermaidCode
        .split('\n')
        .map(line => line.trim())
        .filter(line => line && !line.startsWith('%%') && !line.startsWith('#'));

      // 匹配各种Mermaid语法的正则表达式
      const patterns = [
        // 基本箭头连接: A --> B, A->B
        /^([A-Za-z0-9_]+)\s*(-->|->)\s*([A-Za-z0-9_]+)/,
        // 带标签的箭头: A -->|label| B, A ->|label| B  
        /^([A-Za-z0-9_]+)\s*(-->|->)\s*\|[^|]*\|\s*([A-Za-z0-9_]+)/,
        // 实线连接: A --- B, A-B
        /^([A-Za-z0-9_]+)\s*(---|--|-)\s*([A-Za-z0-9_]+)/,
        // 带标签的实线: A ---|label| B
        /^([A-Za-z0-9_]+)\s*(---|--|-)\s*\|[^|]*\|\s*([A-Za-z0-9_]+)/,
        // 节点定义: A[label], A(label), A{label}
        /^([A-Za-z0-9_]+)[\[\(\{]([^\]\)\}]*)[\]\)\}]/,
        // 复杂箭头: A ==> B, A -.-> B
        /^([A-Za-z0-9_]+)\s*(==>|\.->|\.\.>)\s*([A-Za-z0-9_]+)/,
        // 带标签的复杂箭头: A ==>|label| B
        /^([A-Za-z0-9_]+)\s*(==>|\.->|\.\.>)\s*\|[^|]*\|\s*([A-Za-z0-9_]+)/,
        // 多连接模式: A --> B & C & D
        /^([A-Za-z0-9_]+)\s*(-->|->)\s*([A-Za-z0-9_]+(?:\s*&\s*[A-Za-z0-9_]+)*)/,
        // 从多个节点连接: A & B & C --> D
        /^([A-Za-z0-9_]+(?:\s*&\s*[A-Za-z0-9_]+)*)\s*(-->|->)\s*([A-Za-z0-9_]+)/
      ];

      for (const line of lines) {
        // 跳过图表类型定义行和子图定义
        if (line.includes('flowchart') || line.includes('graph') || line.includes('TD') || 
            line.includes('LR') || line.includes('TB') || line.includes('RL') ||
            line.includes('subgraph') || line === 'end') {
          continue;
        }

        // 尝试匹配各种模式
        let matched = false;
        
        for (const pattern of patterns) {
          const match = line.match(pattern);
          if (match) {
            matched = true;
            
            // 如果是连接关系（有箭头或连线）
            if (match[2] && (match[2].includes('>') || match[2].includes('-'))) {
              const fromPart = match[1];
              const toPart = match[3];
              
              // 处理多连接模式 (A --> B & C & D)
              if (toPart && toPart.includes('&')) {
                const toNodes = toPart.split('&').map(n => n.trim());
                toNodes.forEach(toNode => {
                  if (fromPart && toNode) {
                    nodes.add(fromPart);
                    nodes.add(toNode);
                    edges.push({ from: fromPart, to: toNode, type: match[2] });
                    
                    if (!adjacencyList.has(fromPart)) {
                      adjacencyList.set(fromPart, []);
                    }
                    if (!adjacencyList.get(fromPart).includes(toNode)) {
                      adjacencyList.get(fromPart).push(toNode);
                    }
                    
                    console.log('🔍 [代码解析] 发现边 (多连接):', fromPart, '->', toNode);
                  }
                });
              }
              // 处理从多个节点连接模式 (A & B & C --> D)
              else if (fromPart && fromPart.includes('&')) {
                const fromNodes = fromPart.split('&').map(n => n.trim());
                fromNodes.forEach(fromNode => {
                  if (fromNode && toPart) {
                    nodes.add(fromNode);
                    nodes.add(toPart);
                    edges.push({ from: fromNode, to: toPart, type: match[2] });
                    
                    if (!adjacencyList.has(fromNode)) {
                      adjacencyList.set(fromNode, []);
                    }
                    if (!adjacencyList.get(fromNode).includes(toPart)) {
                      adjacencyList.get(fromNode).push(toPart);
                    }
                    
                    console.log('🔍 [代码解析] 发现边 (多源连接):', fromNode, '->', toPart);
                  }
                });
              }
              // 普通单对单连接
              else if (fromPart && toPart) {
                nodes.add(fromPart);
                nodes.add(toPart);
                edges.push({ from: fromPart, to: toPart, type: match[2] });
                
                // 构建邻接表
                if (!adjacencyList.has(fromPart)) {
                  adjacencyList.set(fromPart, []);
                }
                if (!adjacencyList.get(fromPart).includes(toPart)) {
                  adjacencyList.get(fromPart).push(toPart);
                }
                
                console.log('🔍 [代码解析] 发现边:', fromPart, '->', toPart);
              }
            } else if (match[1]) {
              // 单纯的节点定义
              nodes.add(match[1]);
              console.log('🔍 [代码解析] 发现节点定义:', match[1]);
            }
            break;
          }
        }

        // 如果没有匹配到标准模式，尝试提取可能的节点ID
        if (!matched) {
          // 查找可能的节点ID (字母数字组合)
          const possibleNodes = line.match(/\b[A-Za-z][A-Za-z0-9_]*\b/g);
          if (possibleNodes && possibleNodes.length > 0) {
            // 过滤掉常见的关键词
            const keywords = ['flowchart', 'graph', 'TD', 'LR', 'TB', 'RL', 'subgraph', 'end', 'class', 'style'];
            possibleNodes.forEach(node => {
              if (!keywords.includes(node.toLowerCase()) && node.length <= 10) {
                nodes.add(node);
                console.log('🔍 [代码解析] 可能的节点:', node);
              }
            });
          }
        }
      }

      console.log('🔍 [代码解析] 解析完成');
      console.log('🔍 [代码解析] 发现节点:', Array.from(nodes));
      console.log('🔍 [代码解析] 发现边:', edges);
      console.log('🔍 [代码解析] 邻接表:', Object.fromEntries(adjacencyList));

      return { nodes, edges, adjacencyList };

    } catch (error) {
      console.error('🔍 [代码解析] 解析Mermaid代码时出错:', error);
      return { nodes: new Set(), edges: [], adjacencyList: new Map() };
    }
  }, []);

  // 获取节点关系数据
  const getNodeRelations = useCallback(() => {
    if (!nodeRelationsRef.current && code) {
      console.log('🔍 [节点关系] 构建节点关系缓存');
      nodeRelationsRef.current = parseMermaidCode(code);
    }
    return nodeRelationsRef.current || { nodes: new Set(), edges: [], adjacencyList: new Map() };
  }, [code, parseMermaidCode]);

  // 清理节点关系缓存当代码变化时
  useEffect(() => {
    nodeRelationsRef.current = null;
    console.log('🔍 [节点关系] 清理缓存，代码已变化');
  }, [code]);

  // 基于代码解析查找子节点
  const findChildNodes = useCallback((nodeId) => {
    const relations = getNodeRelations();
    const children = relations.adjacencyList.get(nodeId) || [];
    
    console.log('🔍 [子节点查找] 节点', nodeId, '的直接子节点:', children);
    return children;
  }, [getNodeRelations]);

  // 基于代码解析构建逻辑链条
  const findLogicalChain = useCallback((startNodeId) => {
    const relations = getNodeRelations();
    const visited = new Set();
    const chain = [];
    let currentNode = startNodeId;
    const maxNodes = 6; // 最多6个节点

    console.log('🔗 [逻辑链条] 开始构建链条，起始节点:', startNodeId);

    while (currentNode && !visited.has(currentNode) && chain.length < maxNodes) {
      visited.add(currentNode);
      chain.push(currentNode);
      
      console.log('🔗 [逻辑链条] 添加节点到链条:', currentNode);

      // 获取当前节点的子节点
      const children = relations.adjacencyList.get(currentNode) || [];
      
      if (children.length === 0) {
        // 没有子节点，链条结束
        console.log('🔗 [逻辑链条] 节点无子节点，链条结束');
        break;
      } else if (children.length === 1) {
        // 只有一个子节点，继续链条
        currentNode = children[0];
        console.log('🔗 [逻辑链条] 单子节点，继续链条:', currentNode);
      } else {
        // 多个子节点，根据策略决定是否继续
        console.log('🔗 [逻辑链条] 多子节点情况:', children);
        
        // 简单策略：多子节点时停止，因为这通常表示分支
        console.log('🔗 [逻辑链条] 遇到分支，停止链条构建');
        break;
      }

      // 安全检查：防止意外的无限循环
      if (chain.length >= maxNodes) {
        console.log('🔗 [逻辑链条] 达到最大节点数，停止构建');
        break;
      }
    }

    console.log('🔗 [逻辑链条] 最终链条:', chain);
    return chain;
  }, [getNodeRelations]);

  // 创建节点ID映射，将SVG中的节点ID映射到代码中的节点ID
  const createNodeIdMapping = useCallback(() => {
    if (!containerRef.current) return new Map();

    const mapping = new Map(); // SVG节点ID -> 代码节点ID
    const relations = getNodeRelations();
    const codeNodeIds = Array.from(relations.nodes);

    try {
      const svg = containerRef.current.querySelector('svg');
      if (!svg) return mapping;

      // 查找SVG中的所有节点元素
      const svgNodes = svg.querySelectorAll('g[class*="node"], g[data-id], g[id]');
      
      console.log('🔗 [节点映射] SVG节点数量:', svgNodes.length);
      console.log('🔗 [节点映射] 代码节点ID:', codeNodeIds);

      for (const svgNode of svgNodes) {
        // 获取SVG节点的各种可能ID
        const svgNodeId = svgNode.getAttribute('data-id') || 
                         svgNode.getAttribute('id') || 
                         svgNode.className.baseVal || '';

        // 尝试匹配代码中的节点ID
        let matchedCodeNodeId = null;

        // 1. 直接匹配
        if (codeNodeIds.includes(svgNodeId)) {
          matchedCodeNodeId = svgNodeId;
        } else {
          // 2. 从SVG节点ID中提取可能的代码节点ID
          const extractedIds = [];
          
          // 从类名中提取 (如: "node-A1" -> "A1")
          if (svgNodeId.includes('node')) {
            const match = svgNodeId.match(/node-?([A-Za-z0-9_]+)/);
            if (match) {
              extractedIds.push(match[1]);
            }
          }
          
          // 从ID中提取 (如: "flowchart-A1-123" -> "A1")
          const idMatches = svgNodeId.match(/[A-Za-z][A-Za-z0-9_]*/g);
          if (idMatches) {
            extractedIds.push(...idMatches);
          }

          // 尝试匹配提取的ID
          for (const extractedId of extractedIds) {
            if (codeNodeIds.includes(extractedId)) {
              matchedCodeNodeId = extractedId;
              break;
            }
          }

          // 3. 模糊匹配 (如果直接匹配失败)
          if (!matchedCodeNodeId) {
            for (const codeNodeId of codeNodeIds) {
              if (svgNodeId.includes(codeNodeId) || codeNodeId.includes(svgNodeId)) {
                matchedCodeNodeId = codeNodeId;
                break;
              }
            }
          }
        }

        if (matchedCodeNodeId) {
          mapping.set(svgNodeId, matchedCodeNodeId);
          console.log('🔗 [节点映射] 映射:', svgNodeId, '->', matchedCodeNodeId);
        } else {
          console.log('🔗 [节点映射] 未匹配:', svgNodeId);
        }
      }

      console.log('🔗 [节点映射] 完成，映射数量:', mapping.size);
      return mapping;

    } catch (error) {
      console.error('🔗 [节点映射] 创建节点映射时出错:', error);
      return new Map();
    }
  }, [getNodeRelations]);

  // 将SVG节点ID转换为代码节点ID
  const mapSvgNodeIdToCodeNodeId = useCallback((svgNodeId) => {
    const mapping = createNodeIdMapping();
    const codeNodeId = mapping.get(svgNodeId);
    
    if (codeNodeId) {
      console.log('🔗 [节点映射] SVG节点', svgNodeId, '映射到代码节点', codeNodeId);
      return codeNodeId;
    }

    // 如果没有找到映射，尝试直接返回可能的节点ID
    console.log('🔗 [节点映射] 未找到映射，尝试直接使用:', svgNodeId);
    return svgNodeId;
  }, [createNodeIdMapping]);

  // 调试函数：显示解析的节点关系
  const debugNodeRelations = useCallback(() => {
    const relations = getNodeRelations();
    console.log('🔍 [调试信息] ===== 节点关系调试 =====');
    console.log('🔍 [调试信息] 发现的节点:', Array.from(relations.nodes));
    console.log('🔍 [调试信息] 发现的边:', relations.edges);
    console.log('🔍 [调试信息] 邻接表:');
    
    relations.adjacencyList.forEach((children, parent) => {
      console.log(`🔍 [调试信息]   ${parent} -> [${children.join(', ')}]`);
    });
    
    console.log('🔍 [调试信息] ===========================');
    
    // 测试每个节点的逻辑链条
    relations.nodes.forEach(nodeId => {
      const chain = findLogicalChain(nodeId);
      console.log(`🔍 [调试信息] 节点 ${nodeId} 的逻辑链条:`, chain);
    });
  }, [getNodeRelations, findLogicalChain]);

  // 在代码变化时输出调试信息
  useEffect(() => {
    if (code && hasRendered) {
      // 延迟执行，确保DOM已经渲染完成
      setTimeout(() => {
        debugNodeRelations();
      }, 1000);
    }
  }, [code, hasRendered, debugNodeRelations]);

  // 安全的状态更新函数
  const safeSetState = useCallback((setter, value) => {
    try {
      setter(value);
    } catch (error) {
      console.warn('状态更新失败:', error);
    }
  }, []);

  // 安全的DOM操作函数
  const safeDOMOperation = useCallback((operation) => {
    if (containerRef.current) {
      try {
        return operation();
      } catch (error) {
        console.warn('DOM operation failed:', error);
        return false;
      }
    }
    return false;
  }, []);

  // 缩放控制函数
  const handleZoomIn = useCallback(() => {
    setScale(prev => Math.min(prev * 1.2, 3));
  }, []);

  const handleZoomOut = useCallback(() => {
    setScale(prev => Math.max(prev / 1.2, 0.3));
  }, []);

  const handleReset = useCallback(() => {
    setScale(1);
    setPosition({ x: 0, y: 0 });
  }, []);

  // 鼠标滚轮缩放
  const handleWheel = useCallback((e) => {
    e.preventDefault();
    const delta = e.deltaY > 0 ? 0.9 : 1.1;
    setScale(prev => Math.max(0.3, Math.min(3, prev * delta)));
  }, []);

  // 拖拽开始
  const handleMouseDown = useCallback((e) => {
    // 检查是否点击的是节点元素
    const target = e.target;
    const isNodeElement = target.closest('g[class*="node"], g[data-id], g[id]');
    
    // 如果点击的是节点，不启动拖拽
    if (isNodeElement) {
      console.log('🖱️ [拖拽处理] 点击的是节点，不启动拖拽');
      return;
    }

    if (e.button === 0) { // 左键
      e.preventDefault(); // 防止默认拖拽行为
      setIsDragging(true);
      setDragStart({
        x: e.clientX - position.x,
        y: e.clientY - position.y
      });
    }
  }, [position]);

  // 创建事件处理函数
  useEffect(() => {
    handleMouseMoveRef.current = (e) => {
      if (isDragging && e) {
        e.preventDefault && e.preventDefault();
        const newX = e.clientX - dragStart.x;
        const newY = e.clientY - dragStart.y;
        
        // 存储待更新的位置
        pendingPosition.current = { x: newX, y: newY };
        
        // 如果还没有安排更新，则安排一个
        if (!dragAnimationFrame.current) {
          dragAnimationFrame.current = requestAnimationFrame(() => {
            if (pendingPosition.current) {
              setPosition(pendingPosition.current);
              pendingPosition.current = null;
            }
            dragAnimationFrame.current = null;
          });
        }
      }
    };

    handleMouseUpRef.current = (e) => {
      e && e.preventDefault && e.preventDefault();
      setIsDragging(false);
      // 清理待处理的动画帧
      if (dragAnimationFrame.current) {
        cancelAnimationFrame(dragAnimationFrame.current);
        dragAnimationFrame.current = null;
      }
      // 如果有待处理的位置更新，立即应用
      if (pendingPosition.current) {
        setPosition(pendingPosition.current);
        pendingPosition.current = null;
      }
    };
  }, [isDragging, dragStart]);

  // 管理事件监听器
  useEffect(() => {
    // 使用局部变量存储事件处理函数的引用，避免闭包问题
    let localHandleMouseMove = null;
    let localHandleMouseUp = null;
    
    const handleMouseMove = (e) => {
      if (handleMouseMoveRef.current) {
        handleMouseMoveRef.current(e);
      }
    };

    const handleMouseUp = () => {
      if (handleMouseUpRef.current) {
        handleMouseUpRef.current();
      }
    };

    if (isDragging) {
      // 使用window.document确保获取全局document对象，并检查addEventListener方法是否存在
      const globalDocument = window.document;
      if (globalDocument && typeof globalDocument.addEventListener === 'function') {
        localHandleMouseMove = handleMouseMove;
        localHandleMouseUp = handleMouseUp;
        
        globalDocument.addEventListener('mousemove', localHandleMouseMove, { passive: false });
        globalDocument.addEventListener('mouseup', localHandleMouseUp, { passive: false });
      }
    }

    // 清理函数 - 添加多重安全检查
    return () => {
      try {
        // 使用window.document确保获取全局document对象
        const globalDocument = window.document;
        if (globalDocument && typeof globalDocument.removeEventListener === 'function') {
          if (localHandleMouseMove) {
            globalDocument.removeEventListener('mousemove', localHandleMouseMove);
          }
          if (localHandleMouseUp) {
            globalDocument.removeEventListener('mouseup', localHandleMouseUp);
          }
        }
      } catch (error) {
        // 静默处理清理错误，避免影响应用运行
        console.warn('清理事件监听器时出错:', error);
      }
    };
  }, [isDragging]);

  // 检查DOM环境是否完全可用
  const checkDOMEnvironment = useCallback(() => {
    try {
      // 基本DOM检查
      if (typeof window === 'undefined' || typeof document === 'undefined') {
        console.warn('DOM环境不可用');
        return false;
      }

      // 检查关键的DOM API
      const requiredAPIs = [
        'createElementNS',
        'createElement',
        'querySelector',
        'querySelectorAll'
      ];

      for (const api of requiredAPIs) {
        if (!document[api]) {
          console.warn(`DOM API ${api} 不可用`);
          return false;
        }
      }

      // 检查SVG支持
      try {
        const testSvg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
        if (!testSvg) {
          console.warn('SVG创建失败');
          return false;
        }
      } catch (e) {
        console.warn('SVG支持检查失败:', e);
        return false;
      }

      // 检查文档状态
      if (document.readyState === 'loading') {
        console.warn('文档仍在加载中');
        return false;
      }

      console.log('DOM环境检查通过');
      return true;
    } catch (error) {
      console.error('DOM环境检查异常:', error);
      return false;
    }
  }, []);

  // 初始化DOM检查
  useEffect(() => {
    const initDOM = () => {
      if (checkDOMEnvironment()) {
        setDomReady(true);
        // 注入自定义CSS样式
        try {
          injectStyles();
          console.log('Mermaid自定义样式已注入');
        } catch (error) {
          console.warn('注入自定义样式失败:', error);
        }
      } else {
        // 如果DOM还没准备好，稍后重试
        const retryTimeout = setTimeout(() => {
          if (checkDOMEnvironment()) {
            setDomReady(true);
            // 注入自定义CSS样式
            try {
              injectStyles();
              console.log('Mermaid自定义样式已注入');
            } catch (error) {
              console.warn('注入自定义样式失败:', error);
            }
          }
        }, 100);
        
        return () => clearTimeout(retryTimeout);
      }
    };

    // 立即检查
    initDOM();

    // 监听DOM加载完成事件
    const handleDOMContentLoaded = () => {
      setTimeout(() => {
        if (checkDOMEnvironment()) {
          setDomReady(true);
          // 注入自定义CSS样式
          try {
            injectStyles();
            console.log('Mermaid自定义样式已注入');
          } catch (error) {
            console.warn('注入自定义样式失败:', error);
          }
        }
      }, 50);
    };

    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', handleDOMContentLoaded);
      return () => document.removeEventListener('DOMContentLoaded', handleDOMContentLoaded);
    }
  }, [checkDOMEnvironment]);

  // 节点点击处理函数
  const handleNodeClick = useCallback((nodeId) => {
    console.log('🖱️ [节点点击] 节点被点击:', nodeId);
    
    if (onNodeClick && typeof onNodeClick === 'function') {
      console.log('🖱️ [节点点击] 调用回调函数');
      try {
        // 将SVG节点ID转换为代码节点ID
        const codeNodeId = mapSvgNodeIdToCodeNodeId(nodeId);
        console.log('🖱️ [节点点击] 映射后的代码节点ID:', codeNodeId);
        onNodeClick(codeNodeId);
      } catch (error) {
        console.error('🖱️ [节点点击] 回调函数执行出错:', error);
      }
    } else {
      console.log('🖱️ [节点点击] 未提供回调函数');
    }
  }, [onNodeClick, mapSvgNodeIdToCodeNodeId]);

  // 设置节点点击事件监听器
  const setupNodeClickListeners = useCallback(() => {
    if (!containerRef.current) {
      console.log('🖱️ [节点监听器] 容器不存在，跳过设置');
      return;
    }

    try {
      const svg = containerRef.current.querySelector('svg');
      if (!svg) {
        console.log('🖱️ [节点监听器] SVG元素不存在，跳过设置');
        return;
      }

      // 查找所有节点元素
      const nodeElements = svg.querySelectorAll('g[class*="node"], g[data-id], g[id]');
      console.log('🖱️ [节点监听器] 找到节点元素数量:', nodeElements.length);

      // 为每个节点添加点击监听器
      nodeElements.forEach((nodeElement, index) => {
        // 移除之前的监听器（如果存在）
        nodeElement.removeEventListener('click', nodeElement._nodeClickHandler);
        
        // 获取节点ID
        const nodeId = nodeElement.getAttribute('data-id') || 
                      nodeElement.getAttribute('id') || 
                      nodeElement.className.baseVal || 
                      `node-${index}`;

        // 创建点击处理函数
        const clickHandler = (e) => {
          e.preventDefault();
          e.stopPropagation();
          console.log('🖱️ [节点监听器] 节点点击事件触发:', nodeId);
          handleNodeClick(nodeId);
        };

        // 保存处理函数引用以便后续移除
        nodeElement._nodeClickHandler = clickHandler;
        
        // 添加点击监听器
        nodeElement.addEventListener('click', clickHandler, { passive: false });
        
        console.log('🖱️ [节点监听器] 为节点添加点击监听器:', nodeId);
      });

      console.log('🖱️ [节点监听器] 节点点击监听器设置完成');
    } catch (error) {
      console.error('🖱️ [节点监听器] 设置节点点击监听器时出错:', error);
    }
  }, [handleNodeClick]);

  // 清理节点点击事件监听器
  const cleanupNodeClickListeners = useCallback(() => {
    if (!containerRef.current) return;

    try {
      const svg = containerRef.current.querySelector('svg');
      if (!svg) return;

      const nodeElements = svg.querySelectorAll('g[class*="node"], g[data-id], g[id]');
      nodeElements.forEach(nodeElement => {
        if (nodeElement._nodeClickHandler) {
          nodeElement.removeEventListener('click', nodeElement._nodeClickHandler);
          delete nodeElement._nodeClickHandler;
        }
      });

      console.log('🖱️ [节点监听器] 节点点击监听器清理完成');
    } catch (error) {
      console.error('🖱️ [节点监听器] 清理节点点击监听器时出错:', error);
    }
  }, []);

  // 渲染图表
  const renderDiagram = useCallback(async () => {
    if (!code || isRendering || !domReady) {
      console.log('跳过渲染:', { hasCode: !!code, isRendering, domReady });
      return;
    }

    // 检查容器是否存在，如果不存在则延迟重试
    if (!containerRef.current) {
      console.log('容器未挂载，延迟重试...');
      setTimeout(() => {
        if (containerRef.current) {
          renderDiagram();
        }
      }, 100);
      return;
    }

    console.log('开始渲染Mermaid图表...');
    console.log('代码预览:', code.substring(0, 100) + (code.length > 100 ? '...' : ''));

    // 清理之前的点击监听器
    cleanupNodeClickListeners();

    safeSetState(setIsRendering, true);
    safeSetState(setError, null);
    safeSetState(setHasRendered, false);

    // 设置超时
    const timeoutId = setTimeout(() => {
      console.error('渲染超时，强制停止');
      safeSetState(setIsRendering, false);
      safeSetState(setError, '渲染超时，请重试');
    }, 15000); // 15秒超时

    try {
      // 再次确认DOM环境
      if (!checkDOMEnvironment()) {
        throw new Error('DOM环境检查失败');
      }

      // 初始化Mermaid配置（只初始化一次）
      if (!mermaidInitialized) {
        console.log('初始化Mermaid配置...');
        
        // 确保mermaid对象可用
        if (!mermaid || !mermaid.initialize) {
          throw new Error('Mermaid库未正确加载');
        }

        // 重置Mermaid状态
        try {
          mermaid.mermaidAPI.reset && mermaid.mermaidAPI.reset();
        } catch (resetError) {
          console.warn('Mermaid重置失败，继续初始化:', resetError);
        }

        // 配置Mermaid
        const config = {
          startOnLoad: false,
          theme: 'default',
          securityLevel: 'loose',
          fontFamily: '"Segoe UI", Tahoma, Geneva, Verdana, sans-serif',
          logLevel: 'error',
          flowchart: {
            useMaxWidth: false,
            htmlLabels: true,
            curve: 'basis'
          },
          mindmap: {
            useMaxWidth: false,
            padding: 20
          },
          // 添加更多配置以确保兼容性
          deterministicIds: false,
          suppressErrorRendering: false,
          // 确保正确的DOM访问
          htmlLabels: true,
          wrap: false
        };

        mermaid.initialize(config);
        
        // 验证初始化是否成功
        if (!mermaid.mermaidAPI) {
          throw new Error('Mermaid API初始化失败');
        }

        setMermaidInitialized(true);
        console.log('Mermaid初始化完成');
      }

      // 检查容器是否存在
      if (!containerRef.current) {
        throw new Error('图表容器不存在');
      }

      // 清空容器
      containerRef.current.innerHTML = '';
      console.log('容器已清空');

      // 检查语法（可选，如果失败就跳过）
      try {
        console.log('检查语法...');
        await mermaid.parse(code);
        console.log('语法检查通过');
      } catch (parseError) {
        console.warn('语法检查失败，尝试直接渲染:', parseError.message);
      }

      // 渲染图表
      console.log('开始渲染图表...');
      const renderResult = await mermaid.render(diagramId, code);
      console.log('渲染完成，结果:', renderResult ? '有数据' : '无数据');
      
      // 检查组件是否仍然存在
      if (!containerRef.current) {
        console.log('容器不存在，停止渲染');
        return;
      }

      if (renderResult && renderResult.svg) {
        console.log('处理SVG结果...');
        // 直接插入SVG，避免复杂的DOM操作
        containerRef.current.innerHTML = renderResult.svg;
        
        const svgElement = containerRef.current.querySelector('svg');
        if (svgElement) {
          console.log('SVG元素找到，设置基础样式...');
          // 只设置必要的SVG基础样式
          svgElement.style.maxWidth = 'none';
          svgElement.style.height = 'auto';
          svgElement.style.userSelect = 'none';
          svgElement.style.cursor = isDragging ? 'grabbing' : 'grab';
          svgElement.style.display = 'block';
          
          console.log('SVG基础样式已设置，其余样式由CSS控制');
          
          // 强制重新注入样式，确保样式应用到新渲染的SVG
          setTimeout(() => {
            console.log('🔄 强制重新注入样式');
            injectStyles();
          }, 50);
          
          // 设置节点点击监听器
          setTimeout(() => {
            console.log('🖱️ [渲染完成] 设置节点点击监听器');
            setupNodeClickListeners();
          }, 100);
          
          safeSetState(setHasRendered, true);
        } else {
          console.log('SVG元素未找到');
          throw new Error('SVG元素未找到');
        }
      } else {
        throw new Error('渲染结果为空');
      }

    } catch (error) {
      console.error('主渲染方法失败:', error);
      
      // 如果标准方法失败，尝试fallback方法
      if (containerRef.current && mermaidInitialized) {
        try {
          console.log('尝试fallback渲染方法...');
          
          // 检查mermaidAPI是否可用
          if (!mermaid.mermaidAPI || !mermaid.mermaidAPI.render) {
            throw new Error('MermaidAPI不可用');
          }
          
          // 使用回调方式渲染
          const fallbackPromise = new Promise((resolve, reject) => {
            const fallbackTimeout = setTimeout(() => {
              reject(new Error('Fallback渲染超时'));
            }, 10000);

            try {
              mermaid.mermaidAPI.render(
                diagramId + '_fallback',
                code,
                (svg) => {
                  clearTimeout(fallbackTimeout);
                  if (containerRef.current && svg) {
                    console.log('Fallback渲染成功');
                    containerRef.current.innerHTML = svg;
                    
                    const svgElement = containerRef.current.querySelector('svg');
                    if (svgElement) {
                      svgElement.style.maxWidth = 'none';
                      svgElement.style.height = 'auto';
                      svgElement.style.userSelect = 'none';
                      svgElement.style.cursor = isDragging ? 'grabbing' : 'grab';
                      
                      console.log('Fallback: SVG基础样式已设置');
                      
                      // 强制重新注入样式，确保样式应用到新渲染的SVG
                      setTimeout(() => {
                        console.log('🔄 Fallback: 强制重新注入样式');
                        injectStyles();
                      }, 50);

                      // Fallback渲染后也设置节点点击监听器
                      setTimeout(() => {
                        console.log('🖱️ [Fallback渲染完成] 设置节点点击监听器');
                        setupNodeClickListeners();
                      }, 100);
                    }
                    
                    safeSetState(setHasRendered, true);
                    resolve(svg);
                  } else {
                    reject(new Error('Fallback结果为空'));
                  }
                },
                containerRef.current
              );
            } catch (apiError) {
              clearTimeout(fallbackTimeout);
              reject(apiError);
            }
          });

          await fallbackPromise;
        } catch (fallbackError) {
          console.error('Fallback渲染也失败:', fallbackError);
          safeSetState(setError, fallbackError.message || error.message || '图表渲染失败');
        }
      } else {
        safeSetState(setError, error.message || '图表渲染失败');
      }
    } finally {
      clearTimeout(timeoutId);
      console.log('渲染流程结束');
      safeSetState(setIsRendering, false);
    }
  }, [code, isRendering, domReady, diagramId, safeSetState, isDragging, mermaidInitialized, checkDOMEnvironment, cleanupNodeClickListeners, setupNodeClickListeners]);

  // 监听代码变化重新渲染
  useEffect(() => {
    if (code && !isRendering && domReady) {
      // 延迟执行，确保DOM已准备好
      const timeoutId = setTimeout(() => {
        renderDiagram();
      }, 100); // 减少延迟时间，因为已经有了DOM准备检查
      
      return () => clearTimeout(timeoutId);
    }
  }, [code, domReady, renderDiagram]);

  // 复制代码功能
  const handleCopyCode = useCallback(async () => {
    try {
      await navigator.clipboard.writeText(code);
      safeSetState(setCopied, true);
      toast.success('Mermaid代码已复制到剪贴板');
      
      // 清理之前的timeout
      if (copyTimeoutRef.current) {
        clearTimeout(copyTimeoutRef.current);
      }
      
      // 设置新的timeout
      copyTimeoutRef.current = setTimeout(() => safeSetState(setCopied, false), 2000);
    } catch (error) {
      console.error('Failed to copy:', error);
      toast.error('复制失败，请手动复制');
    }
  }, [code, safeSetState]);

  // 组件卸载时清理定时器
  useEffect(() => {
    return () => {
      if (copyTimeoutRef.current) {
        clearTimeout(copyTimeoutRef.current);
      }
      // 清理防抖定时器
      if (moveDebounceTimer.current) {
        clearTimeout(moveDebounceTimer.current);
      }
      // 清理节点点击监听器
      cleanupNodeClickListeners();
      // 重置动画状态
      isAnimating.current = false;
    };
  }, [cleanupNodeClickListeners]);

  // 计算包含节点及其子节点的最优视图位置
  const calculateOptimalViewForNodes = useCallback((nodeIds) => {
    if (!containerRef.current || !parentContainerRef.current || nodeIds.length === 0) {
      return null;
    }

    try {
      const containerBounds = parentContainerRef.current.getBoundingClientRect();
      const nodes = [];

      // 收集所有节点的位置信息
      for (const nodeId of nodeIds) {
        const selectors = [
          `[data-id="${nodeId}"]`,
          `#${nodeId}`,
          `.node-${nodeId}`,
          `[id*="${nodeId}"]`,
          `[class*="${nodeId}"]`
        ];
        
        let targetNode = null;
        for (const selector of selectors) {
          const foundNodes = containerRef.current.querySelectorAll(selector);
          if (foundNodes.length > 0) {
            targetNode = foundNodes[0];
            break;
          }
        }

        if (targetNode) {
          const nodeBounds = targetNode.getBoundingClientRect();
          nodes.push({
            id: nodeId,
            bounds: nodeBounds,
            relativeLeft: nodeBounds.left - containerBounds.left,
            relativeRight: nodeBounds.right - containerBounds.left,
            relativeTop: nodeBounds.top - containerBounds.top,
            relativeBottom: nodeBounds.bottom - containerBounds.top
          });
        }
      }

      if (nodes.length === 0) {
        return null;
      }

      // 计算所有节点的边界框
      const minLeft = Math.min(...nodes.map(n => n.relativeLeft));
      const maxRight = Math.max(...nodes.map(n => n.relativeRight));
      const minTop = Math.min(...nodes.map(n => n.relativeTop));
      const maxBottom = Math.max(...nodes.map(n => n.relativeBottom));

      const groupWidth = maxRight - minLeft;
      const groupHeight = maxBottom - minTop;

      console.log('🎯 [节点组视图] 节点组边界:', { minLeft, maxRight, minTop, maxBottom });
      console.log('🎯 [节点组视图] 节点组尺寸:', { groupWidth, groupHeight });

      // 设置边距
      const margin = 60;
      const containerWidth = containerBounds.width;
      const containerHeight = containerBounds.height;

      // 检查是否已经完全可见
      const isGroupFullyVisible = (
        minLeft >= margin &&
        maxRight <= containerWidth - margin &&
        minTop >= margin &&
        maxBottom <= containerHeight - margin
      );

      if (isGroupFullyVisible) {
        console.log('🎯 [节点组视图] 节点组已完全可见');
        return null;
      }

      // 计算需要的移动距离
      let deltaX = 0;
      let deltaY = 0;

      // 水平方向调整
      if (minLeft < margin) {
        deltaX = margin - minLeft;
      } else if (maxRight > containerWidth - margin) {
        deltaX = (containerWidth - margin) - maxRight;
      }

      // 垂直方向调整
      if (minTop < margin) {
        deltaY = margin - minTop;
      } else if (maxBottom > containerHeight - margin) {
        deltaY = (containerHeight - margin) - maxBottom;
      }

      console.log('🎯 [节点组视图] 计算的移动距离:', { deltaX, deltaY });

      return { deltaX, deltaY };

    } catch (error) {
      console.error('🎯 [节点组视图] 计算最优视图时出错:', error);
      return null;
    }
  }, []);

  // 实际执行节点移动的函数
  const performNodeMove = useCallback((nodeId) => {
    if (!containerRef.current || !parentContainerRef.current) {
      return;
    }

    try {
      console.log('🎯 [节点可见性] 开始确保节点可见:', nodeId);
      
      // 将SVG节点ID转换为代码节点ID
      const codeNodeId = mapSvgNodeIdToCodeNodeId(nodeId);
      console.log('🎯 [节点可见性] 映射后的代码节点ID:', codeNodeId);
      
      // 基于代码解析查找子节点
      const childNodes = findChildNodes(codeNodeId);
      console.log('🎯 [节点可见性] 发现直接子节点:', childNodes);

      // 查找完整的逻辑链条
      const logicalChain = findLogicalChain(codeNodeId);
      console.log('🎯 [节点可见性] 完整逻辑链条:', logicalChain);

      // 确定需要确保可见的节点列表
      let nodesToShow = [codeNodeId];
      
      // 智能决策：包含逻辑链条中的节点
      if (childNodes.length > 0) {
        // 简化逻辑：只包含有限的链条节点
        if (logicalChain.length <= 4) { // 减少到最多4个节点
          nodesToShow = logicalChain;
          console.log('🎯 [节点可见性] 包含逻辑链条:', logicalChain.length, '个节点');
        } else {
          // 如果链条太长，只包含前3个节点
          nodesToShow = logicalChain.slice(0, 3);
          console.log('🎯 [节点可见性] 链条过长，只包含前3个节点');
        }
      } else {
        console.log('🎯 [节点可见性] 无子节点，只显示主节点');
      }

      // 创建节点映射以便在DOM中查找对应的SVG节点
      const nodeMapping = createNodeIdMapping();
      const reversedMapping = new Map(); // 代码节点ID -> SVG节点ID
      nodeMapping.forEach((codeId, svgId) => {
        reversedMapping.set(codeId, svgId);
      });

      // 将代码节点ID转换回SVG节点ID进行DOM操作
      const svgNodesToShow = nodesToShow.map(codeId => {
        const svgId = reversedMapping.get(codeId);
        if (svgId) {
          console.log('🎯 [节点可见性] 代码节点', codeId, '映射到SVG节点', svgId);
          return svgId;
        }
        // 如果没有找到映射，尝试直接使用代码节点ID
        console.log('🎯 [节点可见性] 未找到映射，直接使用代码节点ID:', codeId);
        return codeId;
      });

      console.log('🎯 [节点可见性] 最终需要显示的SVG节点:', svgNodesToShow);

      // 计算最优视图位置
      const optimalView = calculateOptimalViewForNodes(svgNodesToShow);
      
      if (!optimalView) {
        console.log('🎯 [节点可见性] 节点已在最优位置');
        return;
      }

      const { deltaX, deltaY } = optimalView;

      // 如果移动距离很小，就不移动了
      if (Math.abs(deltaX) < 5 && Math.abs(deltaY) < 5) {
        console.log('🎯 [节点可见性] 移动距离极小，无需调整');
        return;
      }

      // 计算目标位置
      const targetX = position.x + deltaX;
      const targetY = position.y + deltaY;

      console.log('🎯 [节点可见性] 计算结果 - 当前位置:', position);
      console.log('🎯 [节点可见性] 计算结果 - 目标位置:', { targetX, targetY });
      console.log('🎯 [节点可见性] 计算结果 - 移动距离:', { deltaX, deltaY });

      // 标记动画开始
      isAnimating.current = true;
      lastMoveTime.current = Date.now();
      lastMovedNode.current = nodeId;

      // 使用更平滑的动画
      const distance = Math.sqrt(deltaX * deltaX + deltaY * deltaY);
      const duration = Math.min(600, Math.max(300, distance * 1.5));
      const startTime = Date.now();
      const startPosition = { ...position };

      const animate = () => {
        const elapsed = Date.now() - startTime;
        const progress = Math.min(elapsed / duration, 1);
        
        // 使用更平滑的缓动函数 (ease-out-quart)
        const easeOutQuart = (t) => 1 - Math.pow(1 - t, 4);
        const easeProgress = easeOutQuart(progress);

        const currentX = startPosition.x + (targetX - startPosition.x) * easeProgress;
        const currentY = startPosition.y + (targetY - startPosition.y) * easeProgress;

        setPosition({ x: currentX, y: currentY });

        if (progress < 1) {
          requestAnimationFrame(animate);
        } else {
          console.log('🎯 [节点可见性] 动画完成，最终位置:', { x: currentX, y: currentY });
          isAnimating.current = false; // 标记动画结束
        }
      };

      requestAnimationFrame(animate);

    } catch (error) {
      console.error('🎯 [节点可见性] 确保节点可见时出错:', error);
      isAnimating.current = false; // 出错时也要重置动画状态
    }
  }, [scale, position, findChildNodes, findLogicalChain, calculateOptimalViewForNodes, mapSvgNodeIdToCodeNodeId, createNodeIdMapping]);

  // 确保节点完整显示在可视区域内的函数 - 带防抖
  const ensureNodeVisible = useCallback((nodeId) => {
    if (!containerRef.current || !parentContainerRef.current) {
      console.warn('🎯 [节点可见性] 容器引用不存在');
      return;
    }

    // 防抖检查
    const now = Date.now();
    const timeSinceLastMove = now - lastMoveTime.current;
    const minInterval = 200; // 最小移动间隔
    const isSameNode = lastMovedNode.current === nodeId;

    // 如果正在进行动画，跳过
    if (isAnimating.current) {
      console.log('🎯 [节点可见性] 动画进行中，跳过移动');
      return;
    }

    // 只对同一个节点进行严格的时间检查
    if (isSameNode && timeSinceLastMove < minInterval) {
      console.log('🎯 [节点可见性] 同一节点移动间隔太短，跳过移动');
      return;
    }

    // 清除之前的防抖定时器
    if (moveDebounceTimer.current) {
      clearTimeout(moveDebounceTimer.current);
    }

    // 如果是不同节点，立即执行；如果是同一节点，稍微延迟
    const debounceDelay = isSameNode ? 50 : 10;
    
    moveDebounceTimer.current = setTimeout(() => {
      performNodeMove(nodeId);
    }, debounceDelay);
  }, [performNodeMove]);

  // 暴露方法给父组件
  useImperativeHandle(ref, () => ({
    ensureNodeVisible,
    handleNodeClick,
    zoomIn: handleZoomIn,
    zoomOut: handleZoomOut,
    reset: handleReset
  }), [ensureNodeVisible, handleNodeClick, handleZoomIn, handleZoomOut, handleReset]);

  if (!code) {
    return (
      <div className="flex items-center justify-center h-full bg-gray-50">
        <div className="text-center text-gray-500">
          <AlertCircle className="h-8 w-8 mx-auto mb-2" />
          <p>暂无思维导图数据</p>
        </div>
      </div>
    );
  }

  // DOM环境未准备好时显示加载状态
  if (!domReady) {
    return (
      <div className="flex items-center justify-center h-full bg-gray-50">
        <div className="text-center">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
          <p className="text-sm text-gray-600">正在初始化渲染环境...</p>
        </div>
      </div>
    );
  }

  return (
    <div ref={parentContainerRef} className="relative h-full bg-gray-50 overflow-hidden">
      {/* 控制工具栏 */}
      <div className="absolute top-2 right-2 z-10 flex space-x-1 bg-white rounded-lg shadow-sm border p-1">
        <button
          onClick={handleZoomIn}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="放大"
        >
          <ZoomIn className="w-4 h-4 text-gray-600" />
        </button>
        <button
          onClick={handleZoomOut}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="缩小"
        >
          <ZoomOut className="w-4 h-4 text-gray-600" />
        </button>
        <button
          onClick={handleReset}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="重置视图"
        >
          <RotateCcw className="w-4 h-4 text-gray-600" />
        </button>
        <div className="w-px bg-gray-300 mx-1"></div>
        <button
          onClick={handleCopyCode}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="复制代码"
        >
          {copied ? (
            <Check className="w-4 h-4 text-green-600" />
          ) : (
            <Copy className="w-4 h-4 text-gray-600" />
          )}
        </button>
      </div>

      {/* 缩放比例显示 */}
      <div className="absolute bottom-2 right-2 z-10 bg-white rounded px-2 py-1 shadow-sm border text-xs text-gray-600">
        {Math.round(scale * 100)}%
      </div>

      {/* 拖拽提示 */}
      {!isDragging && hasRendered && (
        <div className="absolute bottom-2 left-2 z-10 bg-white rounded px-2 py-1 shadow-sm border text-xs text-gray-500 flex items-center">
          <Move className="w-3 h-3 mr-1" />
          拖拽移动 | 滚轮缩放
        </div>
      )}

      {/* 节点点击提示 */}
      {!isDragging && hasRendered && onNodeClick && (
        <div className="absolute bottom-8 left-2 z-10 bg-white rounded px-2 py-1 shadow-sm border text-xs text-gray-500 flex items-center">
          🖱️ 点击节点跳转到对应文本
        </div>
      )}

      {/* 图表容器 */}
      <div 
        className="w-full h-full flex items-center justify-center overflow-hidden"
        onWheel={handleWheel}
      >
        {isRendering && (
          <div className="absolute inset-0 flex items-center justify-center bg-gray-50 bg-opacity-75 z-20">
            <div className="text-center">
              <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
              <p className="text-sm text-gray-600">正在渲染图表...</p>
            </div>
          </div>
        )}

        {error ? (
          <div className="text-center text-red-600 p-4">
            <AlertCircle className="h-8 w-8 mx-auto mb-2" />
            <p className="font-medium mb-1">渲染失败</p>
            <p className="text-sm">{error}</p>
            <button
              onClick={renderDiagram}
              className="mt-2 px-3 py-1 bg-red-100 text-red-700 rounded text-sm hover:bg-red-200 transition-colors"
            >
              重试
            </button>
          </div>
        ) : (
          <div
            ref={containerRef}
            className="mermaid"
            style={{
              transform: `translate(${position.x}px, ${position.y}px) scale(${scale})`,
              transformOrigin: 'center center',
              cursor: isDragging ? 'grabbing' : 'grab',
              willChange: 'transform', // 提示浏览器优化transform性能
              transition: isDragging ? 'none' : 'transform 0.1s ease-out' // 拖拽时禁用transition，停止时启用
            }}
            onMouseDown={handleMouseDown}
          />
        )}
      </div>
    </div>
  );
});

MermaidDiagram.displayName = 'MermaidDiagram';

export default MermaidDiagram;
</file>

<file path="frontend/src/components/UploadPage.js">
import React, { useState } from 'react';
import { useDropzone } from 'react-dropzone';
import { useNavigate } from 'react-router-dom';
import toast from 'react-hot-toast';
import { Upload, FileText, AlertCircle, Eye } from 'lucide-react';
import axios from 'axios';

const UploadPage = () => {
  const [uploading, setUploading] = useState(false);
  const navigate = useNavigate();

  const onDrop = async (acceptedFiles) => {
    const file = acceptedFiles[0];
    
    if (!file) {
      toast.error('请选择一个文件');
      return;
    }

    // 验证文件类型
    if (!file.name.endsWith('.md') && !file.name.endsWith('.txt') && !file.name.endsWith('.pdf')) {
      toast.error('只支持 .md、.txt 和 .pdf 文件');
      return;
    }

    // 验证文件大小 (10MB限制)
    if (file.size > 10 * 1024 * 1024) {
      toast.error('文件大小不能超过 10MB');
      return;
    }

    setUploading(true);
    
    try {
      const formData = new FormData();
      formData.append('file', file);

      // 使用新的文档上传API
      const response = await axios.post('http://localhost:8000/api/upload-document', formData, {
        headers: {
          'Content-Type': 'multipart/form-data',
        },
      });

      if (response.data.success) {
        const documentId = response.data.document_id;
        toast.success('文件上传成功，将生成论证结构流程图...');
        
        // 直接跳转到查看页面，使用论证结构分析
        navigate(`/viewer/${documentId}`);
      } else {
        toast.error('上传失败，请重试');
      }
    } catch (error) {
      console.error('Upload error:', error);
      const errorMessage = error.response?.data?.detail || '上传失败，请检查网络连接';
      toast.error(errorMessage);
    } finally {
      setUploading(false);
    }
  };

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: {
      'text/markdown': ['.md'],
      'text/plain': ['.txt'],
      'application/pdf': ['.pdf'],
    },
    multiple: false,
  });

  return (
    <div className="min-h-screen flex items-center justify-center p-4">
      <div className="max-w-4xl w-full">
        <div className="text-center mb-8">
          <h2 className="text-3xl font-bold text-gray-900 dark:text-white mb-4">
            文档论证结构分析
          </h2>
          <p className="text-lg text-gray-600 dark:text-gray-300">
            上传文档文件生成论证结构流程图，或查看预设示例
          </p>
        </div>

        <div className="space-y-6">
          <div className="text-center">
            <h3 className="text-lg font-semibold text-gray-900 dark:text-white mb-4">选择使用方式</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300 mb-6">
              您可以上传自己的文件进行论证结构分析，或直接查看预设的流程图示例
            </p>
          </div>
          
          <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
            {/* 上传文件选项 */}
            <div
              {...getRootProps()}
              className={`
                border-2 border-dashed rounded-lg p-8 text-center cursor-pointer transition-all duration-200
                ${isDragActive 
                  ? 'border-blue-400 bg-blue-50 dark:bg-blue-900/20' 
                  : 'border-gray-300 dark:border-gray-600 hover:border-blue-400 dark:hover:border-blue-400 hover:bg-gray-50 dark:hover:bg-gray-800'
                }
                ${uploading ? 'pointer-events-none opacity-50' : ''}
              `}
            >
              <input {...getInputProps()} />
              
              <div className="flex flex-col items-center">
                {uploading ? (
                  <>
                    <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mb-3"></div>
                    <p className="text-sm font-medium text-gray-700 dark:text-gray-300">正在上传文件...</p>
                    <p className="text-xs text-gray-500 dark:text-gray-400 mt-1">
                      将分析文档的论证结构
                    </p>
                  </>
                ) : (
                  <>
                    <Upload className="h-8 w-8 text-blue-400 mb-3" />
                    <h4 className="font-medium text-gray-900 dark:text-white mb-2">上传您的文件</h4>
                    {isDragActive ? (
                      <p className="text-sm text-blue-600 dark:text-blue-400">
                        松开鼠标上传文件
                      </p>
                    ) : (
                      <>
                        <p className="text-sm text-gray-600 dark:text-gray-300 mb-1">
                          拖拽文件到此处，或点击选择
                        </p>
                        <p className="text-xs text-gray-500 dark:text-gray-400">
                          支持 .md、.txt、.pdf 文件
                        </p>
                      </>
                    )}
                  </>
                )}
              </div>
            </div>

            {/* 查看预设示例选项 */}
            <div className="border-2 border-dashed border-gray-300 dark:border-gray-600 rounded-lg p-8 text-center">
              <div className="flex flex-col items-center">
                <Eye className="h-8 w-8 text-orange-400 mb-3" />
                <h4 className="font-medium text-gray-900 dark:text-white mb-2">查看预设示例</h4>
                <p className="text-sm text-gray-600 dark:text-gray-300 mb-4">
                  无需上传文件，直接体验论证结构分析效果
                </p>
                <button
                  onClick={() => {
                    toast.success('进入示例模式，将显示预设的论证结构流程图...');
                    // 创建一个虚拟的文档ID用于演示
                    const demoDocId = 'demo-' + Date.now();
                    navigate(`/viewer/${demoDocId}`, { 
                      state: { selectedMode: 'demo' }
                    });
                  }}
                  className="inline-flex items-center px-4 py-2 bg-orange-600 hover:bg-orange-700 dark:bg-orange-500 dark:hover:bg-orange-600 text-white text-sm font-medium rounded-lg transition-colors"
                >
                  <Eye className="h-4 w-4 mr-2" />
                  查看示例
                </button>
              </div>
            </div>
          </div>
          
          <div className="text-center">
            <p className="text-xs text-gray-500 dark:text-gray-400">
              系统将自动为每个段落分配ID号，并生成论证结构的可视化流程图
            </p>
          </div>
        </div>

        <div className="mt-8 grid grid-cols-1 md:grid-cols-3 gap-6">
          <div className="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-sm border border-gray-200 dark:border-gray-700">
            <FileText className="h-8 w-8 text-green-600 dark:text-green-400 mb-3" />
            <h3 className="font-semibold text-gray-900 dark:text-white mb-2">即时阅读</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300">
              上传后立即显示文档内容，每个段落自动分配ID号
            </p>
          </div>
          
          <div className="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-sm border border-gray-200 dark:border-gray-700">
            <AlertCircle className="h-8 w-8 text-blue-600 dark:text-blue-400 mb-3" />
            <h3 className="font-semibold text-gray-900 dark:text-white mb-2">AI 论证分析</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300">
              智能提取文档的核心论证结构，生成可视化流程图
            </p>
          </div>

          <div className="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-sm border border-gray-200 dark:border-gray-700">
            <Eye className="h-8 w-8 text-orange-600 dark:text-orange-400 mb-3" />
            <h3 className="font-semibold text-gray-900 dark:text-white mb-2">示例体验</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300">
              无需上传文件即可体验完整的论证结构分析功能
            </p>
          </div>
        </div>

        <div className="mt-8 text-center">
          <p className="text-sm text-gray-500 dark:text-gray-400">
            系统将分析文档的论证结构，为每个段落分配ID，并生成包含节点映射的流程图
          </p>
        </div>
      </div>
    </div>
  );
};

export default UploadPage;
</file>

<file path="frontend/src/hooks/useScrollDetection.js">
import { useState, useEffect, useCallback, useRef } from 'react';

// 简易节流函数实现
const throttle = (func, limit) => {
  let inThrottle;
  return function() {
    const args = arguments;
    const context = this;
    if (!inThrottle) {
      func.apply(context, args);
      inThrottle = true;
      setTimeout(() => inThrottle = false, limit);
    }
  }
};

// 文本块到节点的映射关系 - 演示模式使用
const textToNodeMap = {
  "text-A-introduction": "A",
  "text-B-fourth-party": "B", 
  "text-C-vanishing-mediator-core": "C",
  "text-D-mechanism": "D",
  "text-D1D2D3-mechanism-stages": "D", // 包含了D1, D2, D3的逻辑
  "text-E-examples-intro": "E", // E的引言部分
  "text-E1-protestantism": "E1",
  "text-E2-jacobinism": "E2", 
  "text-E3-other-examples": "E3",
  "text-F-mediator-illusion": "F",
  "text-G-beautiful-soul-analogy": "G",
  "text-H-mediator-event-subject": "H",
  "text-H1-subject-definition": "H1",
  "text-H2-action-retroactive": "H2",
  "text-H3-positing-presuppositions": "H3",
  "text-I-truth-political-intro": "I",
  "text-I1-politics-vs-thepolitical": "I1",
  "text-I2-thepolitical-explanation": "I2",
  "text-I3-origin-of-order-political": "I3",
  "text-J-conclusion-subject-as-mediator": "J", // J的核心论点
  "text-J1-subject-fourth-element": "J1", // J1与J内容紧密，可共用或细分
  "text-K-truth-contingency-trauma": "K", // K的引言和核心思想
  "text-K1-analogy-greimas-lacan": "K1", // 包含两个矩阵类比和精神分析的阐述
  "text-K2-truth-revelation": "K2" // K2的核心思想在K1中关于精神分析的部分已阐明
};

// 节点到文本块的映射关系 - 用于节点点击跳转
const nodeToTextMap = {
  "A": "text-A-introduction",
  "B": "text-B-fourth-party", 
  "C": "text-C-vanishing-mediator-core",
  "D": "text-D-mechanism",
  "D1": "text-D1D2D3-mechanism-stages", // D1, D2, D3都映射到同一个文本块
  "D2": "text-D1D2D3-mechanism-stages",
  "D3": "text-D1D2D3-mechanism-stages",
  "E": "text-E-examples-intro",
  "E1": "text-E1-protestantism",
  "E2": "text-E2-jacobinism", 
  "E3": "text-E3-other-examples",
  "F": "text-F-mediator-illusion",
  "G": "text-G-beautiful-soul-analogy",
  "H": "text-H-mediator-event-subject",
  "H1": "text-H1-subject-definition",
  "H2": "text-H2-action-retroactive",
  "H3": "text-H3-positing-presuppositions",
  "I": "text-I-truth-political-intro",
  "I1": "text-I1-politics-vs-thepolitical",
  "I2": "text-I2-thepolitical-explanation",
  "I3": "text-I3-origin-of-order-political",
  "J": "text-J-conclusion-subject-as-mediator",
  "J1": "text-J1-subject-fourth-element",
  "K": "text-K-truth-contingency-trauma",
  "K1": "text-K1-analogy-greimas-lacan",
  "K2": "text-K2-truth-revelation"
};

export const useScrollDetection = (containerRef, documentId, currentMindmapMode, mermaidDiagramRef) => {
  // 当前活动段落和节点ID的状态
  const [activeContentBlockId, setActiveContentBlockId] = useState(null);
  const [activeChunkId, setActiveChunkId] = useState(null);

  // 存储段落映射关系
  const [dynamicTextToNodeMap, setDynamicTextToNodeMap] = useState({});
  const [dynamicNodeToTextMap, setDynamicNodeToTextMap] = useState({});

  // 存储静态映射关系
  const [textToNodeMap, setTextToNodeMap] = useState({});
  const [nodeToTextMap, setNodeToTextMap] = useState({});

  // 存储内容块/段落的引用
  const contentBlockRefs = useRef(new Map());
  const sectionRefs = useRef(new Map());

  // 存储内容块
  const [contentChunks, setContentChunks] = useState([]);

  // 存储之前的活动节点，用于优化高亮性能
  const [previousActiveNode, setPreviousActiveNode] = useState(null);
  
  // 静态映射关系 - 用于示例文档的固定映射
  const staticNodeToTextMap = {
    'A': 'text-block-0',
    'B': 'text-block-1',
    'C': 'text-block-2',
    'D': 'text-block-3',
    'E': 'text-block-4',
    'F': 'text-block-5',
    'G': 'text-block-6',
    'H': 'text-block-7',
    'I': 'text-block-8',
    'J': 'text-block-9',
  };

  // 初始化静态映射
  useEffect(() => {
    // 创建反向映射 textToNodeMap
    const reverseMapping = {};
    Object.keys(staticNodeToTextMap).forEach(nodeId => {
      const textId = staticNodeToTextMap[nodeId];
      reverseMapping[textId] = nodeId;
    });
    
    setTextToNodeMap(reverseMapping);
    setNodeToTextMap(staticNodeToTextMap);
    
    console.log('📊 [静态映射] 初始化完成');
    console.log('📊 [静态映射] 节点到文本映射:', staticNodeToTextMap);
    console.log('📊 [静态映射] 文本到节点映射:', reverseMapping);
  }, []);

  // onSectionRef 回调实现 - 仅用于目录导航，不影响段落高亮
  const handleSectionRef = useCallback((element, chunkId) => {
    if (element) {
      sectionRefs.current.set(chunkId, element);
      console.log('📍 [章节引用] 设置章节引用 (仅用于目录):', chunkId, '总数:', sectionRefs.current.size);
    } else {
      sectionRefs.current.delete(chunkId);
      console.log('📍 [章节引用] 移除章节引用:', chunkId, '剩余:', sectionRefs.current.size);
    }
  }, []);

  // 内容块引用回调 - 用于段落级检测和高亮
  const handleContentBlockRef = useCallback((element, blockId) => {
    if (element) {
      contentBlockRefs.current.set(blockId, element);
      console.log('📍 [段落引用] 设置段落引用:', blockId, '总数:', contentBlockRefs.current.size);
      
      // 设置初始检测，确保页面加载后立即检测当前阅读的段落
      setTimeout(() => {
        if (contentBlockRefs.current.size > 0) {
          console.log('📍 [段落引用] 触发段落检测，因为有新段落被注册');
          // 手动触发一次滚动检测事件
          const event = new Event('scroll');
          const scrollContainer = containerRef.current?.querySelector('.overflow-y-auto');
          if (scrollContainer) {
            scrollContainer.dispatchEvent(event);
          } else {
            window.dispatchEvent(event);
          }
        }
      }, 200);
    } else {
      contentBlockRefs.current.delete(blockId);
      console.log('📍 [段落引用] 移除段落引用:', blockId, '剩余:', contentBlockRefs.current.size);
    }
  }, []);

  // 高亮Mermaid节点
  const highlightMermaidNode = useCallback((nodeId) => {
    // 确保在浏览器环境中且document可用
    if (typeof window === 'undefined' || !window.document || typeof window.document.querySelectorAll !== 'function') {
      console.warn('🎯 [节点高亮] DOM环境不可用，跳过节点高亮');
      return;
    }

    try {
      console.log('🎯 [节点高亮] 开始高亮节点:', nodeId);
      
      // 定义高亮应用函数
      const applyHighlighting = () => {
        // 🔑 关键修复：只有当需要切换到不同节点时，才移除之前的高亮
        if (previousActiveNode && previousActiveNode !== nodeId) {
          console.log('🎯 [节点高亮] 移除之前的高亮（不同节点）:', previousActiveNode, '→', nodeId);
          const prevSelectors = [
            `[data-id="${previousActiveNode}"]`,
            `#${previousActiveNode}`,
            `.node-${previousActiveNode}`,
            `[id*="${previousActiveNode}"]`
          ];
          
          let foundPrev = false;
          prevSelectors.forEach(selector => {
            const prevNodes = window.document.querySelectorAll(selector);
            if (prevNodes.length > 0) {
              foundPrev = true;
              console.log('🎯 [节点高亮] 找到之前的节点:', selector, prevNodes.length);
            }
            prevNodes.forEach(node => {
              if (node && node.classList) {
                node.classList.remove('mermaid-highlighted-node');
              }
            });
          });
          
          if (!foundPrev) {
            console.warn('🎯 [节点高亮] 未找到之前的节点:', previousActiveNode);
          }
        } else if (previousActiveNode === nodeId) {
          console.log('🎯 [节点高亮] 点击同一节点，保持现有高亮:', nodeId);
        }

        // 添加新的高亮（即使是同一节点也确保高亮存在）
        if (nodeId) {
          console.log('🎯 [节点高亮] 确保节点高亮:', nodeId);
          
          // 使用精简的选择器列表，避免重复操作
          const selectors = [
            `[data-id="${nodeId}"]`,
            `#${nodeId}`,
            `[id*="${nodeId}"]`,
            `g[data-id="${nodeId}"]`
          ];
          
          console.log('🎯 [节点搜索] 尝试的选择器列表:', selectors);
          
          let foundCurrent = false;
          let foundElements = [];
          
          selectors.forEach((selector, index) => {
            try {
              const currentNodes = window.document.querySelectorAll(selector);
              if (currentNodes.length > 0) {
                foundCurrent = true;
                foundElements.push(...currentNodes);
                console.log(`🎯 [节点高亮] 选择器 ${index + 1} 成功匹配: ${selector} (找到 ${currentNodes.length} 个元素)`);
                currentNodes.forEach((node, nodeIndex) => {
                  if (node && node.classList) {
                    // 确保添加高亮class
                    if (!node.classList.contains('mermaid-highlighted-node')) {
                      node.classList.add('mermaid-highlighted-node');
                      console.log(`🎯 [节点高亮] 新增高亮节点 ${nodeIndex + 1}:`, {
                        tagName: node.tagName,
                        id: node.id,
                        dataId: node.getAttribute('data-id'),
                        className: node.className,
                        selector: selector
                      });
                    } else {
                      console.log(`🎯 [节点高亮] 节点已高亮 ${nodeIndex + 1}:`, selector);
                    }
                  }
                });
              } else {
                console.log(`🎯 [节点搜索] 选择器 ${index + 1} 无匹配: ${selector}`);
              }
            } catch (error) {
              console.warn(`🎯 [节点搜索] 选择器 ${index + 1} 执行出错: ${selector}`, error);
            }
          });
          
          if (!foundCurrent) {
            console.warn('🎯 [节点高亮] 所有选择器都未找到节点:', nodeId);
            
            // 输出详细的调试信息
            console.log('🔍 [调试分析] 开始分析页面中的所有可能节点...');
            
            // 查找所有Mermaid相关元素
            const allMermaidElements = window.document.querySelectorAll('[class*="node"], [data-id], [id], g, .mermaid *');
            console.log('🔍 [调试分析] 页面中所有可能的Mermaid元素数量:', allMermaidElements.length);
            
            // 筛选出可能与目标节点相关的元素
            const relevantElements = Array.from(allMermaidElements).filter(el => {
              const id = el.id || '';
              const dataId = el.getAttribute('data-id') || '';
              const className = el.className || '';
              
              return id.includes(nodeId) || 
                     dataId.includes(nodeId) || 
                     className.includes(nodeId) ||
                     // 检查是否包含节点ID的任何部分
                     (nodeId.length > 1 && (id.includes(nodeId.substring(0, nodeId.length-1)) || 
                                           dataId.includes(nodeId.substring(0, nodeId.length-1))));
            });
            
            console.log(`🔍 [调试分析] 与节点 "${nodeId}" 相关的元素 (${relevantElements.length} 个):`, 
              relevantElements.map(el => ({
                tagName: el.tagName,
                id: el.id,
                dataId: el.getAttribute('data-id'),
                className: el.className.substring(0, 100),
                textContent: el.textContent?.substring(0, 50)
              }))
            );
            
            // 特别检查是否有类似的节点ID
            const allDataIds = Array.from(allMermaidElements)
              .map(el => el.getAttribute('data-id'))
              .filter(Boolean);
            const allIds = Array.from(allMermaidElements)
              .map(el => el.id)
              .filter(Boolean);
            
            console.log('🔍 [调试分析] 所有data-id值:', [...new Set(allDataIds)]);
            console.log('🔍 [调试分析] 所有id值:', [...new Set(allIds)]);
            
            // 查找最相似的ID
            const similarIds = [...new Set([...allDataIds, ...allIds])].filter(id => 
              id.toLowerCase().includes(nodeId.toLowerCase()) ||
              nodeId.toLowerCase().includes(id.toLowerCase())
            );
            console.log(`🔍 [调试分析] 与 "${nodeId}" 相似的ID:`, similarIds);
            
          } else {
            console.log(`🎯 [节点高亮] 成功找到并确保高亮 ${foundElements.length} 个元素`);
          }
          
          console.log('🎯 [节点高亮] 高亮节点完成:', nodeId);
          
          // 自动确保高亮的节点可见
          if (foundCurrent && mermaidDiagramRef && mermaidDiagramRef.current) {
            console.log('🎯 [节点可见性] 尝试确保节点可见:', nodeId);
            // 延迟一点时间确保高亮样式已应用
            setTimeout(() => {
              try {
                mermaidDiagramRef.current.ensureNodeVisible(nodeId);
                console.log('🎯 [节点可见性] 成功调用ensureNodeVisible方法');
              } catch (error) {
                console.error('🎯 [节点可见性] 调用ensureNodeVisible失败:', error);
              }
            }, 100); // 减少延迟时间，让响应更快
          } else {
            console.warn('🎯 [节点可见性] 无法确保节点可见，原因:', {
              foundCurrent,
              hasMermaidRef: !!mermaidDiagramRef,
              hasCurrentRef: !!(mermaidDiagramRef && mermaidDiagramRef.current)
            });
          }
        }
      };

      // 立即尝试应用高亮
      applyHighlighting();

      // 🔑 减少重试次数到1次，避免过度操作
      const retryTimeouts = [100];
      retryTimeouts.forEach(delay => {
        setTimeout(() => {
          console.log(`🎯 [节点高亮] 延迟${delay}ms重试高亮:`, nodeId);
          applyHighlighting();
        }, delay);
      });

      // 设置MutationObserver监听Mermaid图表变化
      if (nodeId) {
        const mermaidContainer = window.document.querySelector('.mermaid, [data-processed-by-mermaid]');
        if (mermaidContainer) {
          console.log('🎯 [节点高亮] 设置MutationObserver监听图表变化');
          
          // 清除之前的观察者
          if (window.mermaidMutationObserver) {
            window.mermaidMutationObserver.disconnect();
          }
          
          window.mermaidMutationObserver = new MutationObserver((mutations) => {
            let shouldReapply = false;
            mutations.forEach(mutation => {
              if (mutation.type === 'childList' || mutation.type === 'attributes') {
                shouldReapply = true;
              }
            });
            
            if (shouldReapply) {
              console.log('🎯 [节点高亮] 检测到Mermaid图表变化，重新应用高亮');
              setTimeout(() => {
                applyHighlighting();
              }, 100);
            }
          });
          
          window.mermaidMutationObserver.observe(mermaidContainer, {
            childList: true,
            subtree: true,
            attributes: true,
            attributeFilter: ['class', 'style']
          });
        }
      }
      
      setPreviousActiveNode(nodeId);
    } catch (error) {
      console.error('🎯 [节点高亮] 高亮节点时出错:', error);
    }
  }, [previousActiveNode, mermaidDiagramRef]);

  // 高亮段落内容块
  const highlightParagraph = useCallback((blockId) => {
    // 确保在浏览器环境中且document可用
    if (typeof window === 'undefined' || !window.document || typeof window.document.querySelectorAll !== 'function') {
      console.warn('🎯 [段落高亮] DOM环境不可用，跳过段落高亮');
      return;
    }

    try {
      console.log('🎯 [段落高亮] 开始高亮段落:', blockId);
      
      // 🔑 关键修复：智能高亮逻辑
      const allElements = window.document.querySelectorAll('.paragraph-block, .content-block, [id^="para-"], [data-para-id], [id^="text-"], [id^="chunk-"]');
      console.log('🎯 [段落高亮] 找到所有段落元素数量:', allElements.length);
      
      // 首先检查目标段落的当前状态
      const targetElement = contentBlockRefs.current.get(blockId);
      const targetCurrentlyHighlighted = targetElement?.classList?.contains('semantic-paragraph-highlighted');
      console.log('🎯 [段落高亮] 目标段落当前高亮状态:', blockId, '→', targetCurrentlyHighlighted);
      
      // 移除其他段落的高亮，但保护目标段落
      let removedCount = 0;
      allElements.forEach(element => {
        const elementId = element.id || element.getAttribute('data-para-id');
        if (element && element.classList && elementId !== blockId) {
          if (element.classList.contains('semantic-paragraph-highlighted')) {
            element.classList.remove('semantic-paragraph-highlighted');
            removedCount++;
            console.log('🎯 [段落高亮] 移除其他段落的高亮:', elementId);
          }
        }
      });
      console.log('🎯 [段落高亮] 总共移除了', removedCount, '个段落的高亮');

      // 确保目标段落被高亮
      if (blockId && targetElement) {
        if (!targetElement.classList.contains('semantic-paragraph-highlighted')) {
          targetElement.classList.add('semantic-paragraph-highlighted');
          console.log('🎯 [段落高亮] ✅ 成功高亮目标段落:', blockId);
        } else {
          console.log('🎯 [段落高亮] ✅ 目标段落已高亮，状态保持:', blockId);
        }
        
        // 验证高亮状态
        const finalHighlighted = targetElement.classList.contains('semantic-paragraph-highlighted');
        console.log('🎯 [段落高亮] 最终验证 - 目标段落高亮状态:', blockId, '→', finalHighlighted);
        
        // 确保段落可见（滚动到视图中）
        const rect = targetElement.getBoundingClientRect();
        const viewportHeight = window.innerHeight;
        const isVisible = rect.top >= 0 && rect.bottom <= viewportHeight;
        
        if (!isVisible) {
          console.log('🎯 [段落高亮] 段落不完全可见，滚动到视图中');
          targetElement.scrollIntoView({ 
            behavior: 'smooth', 
            block: 'center' 
          });
        }
      } else {
        console.warn('🎯 [段落高亮] ❌ 未找到目标段落元素:', blockId);
        console.warn('🎯 [段落高亮] contentBlockRefs中的所有键:', Array.from(contentBlockRefs.current.keys()));
      }
    } catch (error) {
      console.error('🎯 [段落高亮] 高亮段落时出错:', error);
    }
  }, []);

  // 段落检测函数 - 专门用于检测当前阅读的段落
  const determineActiveParagraph = useCallback(() => {
    const viewportHeight = window.innerHeight;
    const anchorY = viewportHeight * 0.4; // 视口顶部向下40%作为阅读锚点

    console.log('📖 [段落检测] 开始检测当前阅读段落，锚点Y:', anchorY, '段落数量:', contentBlockRefs.current.size);
    console.log('📖 [段落检测] 当前状态 - 动态映射数量:', Object.keys(dynamicTextToNodeMap).length);
    console.log('📖 [段落检测] 当前状态 - 静态映射数量:', Object.keys(textToNodeMap).length);

    let currentActiveParagraphId = null;
    let bestDistance = Infinity;

    // 遍历所有段落块，找到最接近阅读锚点的段落
    contentBlockRefs.current.forEach((element, blockId) => {
      const rect = element.getBoundingClientRect();
      
      // 计算段落中心点到阅读锚点的距离
      const paragraphCenter = rect.top + rect.height / 2;
      const distance = Math.abs(paragraphCenter - anchorY);
      
      console.log(`📖 [段落检测] 段落 ${blockId}: top=${rect.top.toFixed(1)}, center=${paragraphCenter.toFixed(1)}, bottom=${rect.bottom.toFixed(1)}, distance=${distance.toFixed(1)}`);
      
      // 确保段落在视口中且距离阅读锚点最近
      if (rect.top < viewportHeight && rect.bottom > 0 && distance < bestDistance) {
        currentActiveParagraphId = blockId;
        bestDistance = distance;
        console.log(`📖 [段落检测] 段落 ${blockId} 成为最佳候选，距离=${distance.toFixed(1)}`);
      }
    });

    console.log(`📖 [段落检测] 最终确定活动段落: ${currentActiveParagraphId}`);

    // 更新活动段落状态
    setActiveContentBlockId(prevId => {
      // 🔑 关键修复：无论段落是否变更，都要确保高亮状态正确
      if (currentActiveParagraphId) {
        if (prevId !== currentActiveParagraphId) {
          console.log("📖 [段落检测] 活动段落变更:", prevId, "→", currentActiveParagraphId);
        } else {
          console.log("📖 [段落检测] 段落未变更，但确保高亮状态正确:", currentActiveParagraphId);
        }
        
        // 🔑 无论段落是否变更，都要确保高亮正确
        highlightParagraph(currentActiveParagraphId);
        
        // 优先使用动态映射，只有在动态映射为空时才使用静态映射
        const hasDynamicMapping = Object.keys(dynamicTextToNodeMap).length > 0;
        const currentTextToNodeMap = hasDynamicMapping ? dynamicTextToNodeMap : textToNodeMap;
        const nodeId = currentTextToNodeMap[currentActiveParagraphId];
        
        console.log('🔍 [节点映射检查] 段落ID:', currentActiveParagraphId);
        console.log('🔍 [节点映射检查] 动态映射数量:', Object.keys(dynamicTextToNodeMap).length);
        console.log('🔍 [节点映射检查] 静态映射数量:', Object.keys(textToNodeMap).length);
        console.log('🔍 [节点映射检查] 使用映射类型:', hasDynamicMapping ? '动态映射' : '静态映射');
        console.log('🔍 [节点映射检查] 映射表前5个键:', Object.keys(currentTextToNodeMap).slice(0, 5));
        console.log('🔍 [节点映射检查] 找到节点ID:', nodeId);
        
        if (nodeId) {
          console.log('📖 [段落检测] ✅ 找到对应节点，开始高亮:', nodeId);
          highlightMermaidNode(nodeId);
        } else {
          console.warn('📖 [段落检测] ❌ 未找到段落对应的节点映射:', currentActiveParagraphId);
          
          // 详细调试信息
          if (hasDynamicMapping) {
            console.log('🔍 [调试] 动态映射详情:', dynamicTextToNodeMap);
            // 检查是否存在类似的键
            const similarKeys = Object.keys(dynamicTextToNodeMap).filter(key => 
              key.includes(currentActiveParagraphId.replace('para-', '')) || 
              currentActiveParagraphId.includes(key.replace('para-', ''))
            );
            console.log('🔍 [调试] 相似的键:', similarKeys);
          } else {
            console.log('🔍 [调试] 静态映射详情:', Object.keys(textToNodeMap));
          }
          
          // 如果是上传模式且没有找到映射，这是一个问题
          if (currentActiveParagraphId.startsWith('para-') && !hasDynamicMapping) {
            console.error('❌ [严重错误] 上传文档使用了静态映射！动态映射应该已经创建');
          }
        }
      }
      
      return currentActiveParagraphId;
    });
  }, [highlightParagraph, highlightMermaidNode, textToNodeMap, dynamicTextToNodeMap]);

  // 等待Mermaid图表渲染完成的检查函数 - 移到顶层作用域
  const waitForMermaidRender = useCallback(() => {
    return new Promise((resolve) => {
      const checkMermaid = () => {
        const mermaidElement = window.document?.querySelector('.mermaid, [data-processed-by-mermaid]');
        const svgElement = mermaidElement?.querySelector('svg');
        
        if (svgElement && svgElement.children.length > 0) {
          console.log('🎨 [Mermaid检查] Mermaid图表已渲染完成');
          resolve(true);
        } else {
          console.log('🎨 [Mermaid检查] 等待Mermaid图表渲染...');
          setTimeout(checkMermaid, 200);
        }
      };
      
      checkMermaid();
      
      // 超时保护
      setTimeout(() => {
        console.log('🎨 [Mermaid检查] Mermaid图表渲染检查超时，继续执行');
        resolve(false);
      }, 5000);
    });
  }, []);

  // 初始化检测函数 - 统一使用内容块检测
  const initializeDetection = useCallback(async () => {
    console.log('🎨 [统一模式] 开始初始化内容块检测，文档ID:', documentId);
    
    // 立即启动段落检测，不等待思维导图渲染
    setTimeout(() => {
      console.log('🎨 [统一模式] 执行初始内容块检测');
      determineActiveParagraph();
    }, 300);
    
    // 如果存在思维导图，额外等待渲染完成后再次检测
    const mermaidElement = window.document?.querySelector('.mermaid, [data-processed-by-mermaid]');
    if (mermaidElement) {
      console.log('🎨 [思维导图检测] 发现思维导图，等待渲染完成');
      await waitForMermaidRender();
      setTimeout(() => {
        console.log('🎨 [思维导图检测] 思维导图渲染完成，重新执行段落检测');
        determineActiveParagraph();
      }, 100);
    }
  }, [documentId, waitForMermaidRender, determineActiveParagraph]);

  // 段落级滚动检测逻辑 - 使用稳定的引用避免重复执行
  useEffect(() => {
    console.log('🔧 [段落滚动检测] useEffect触发，文档ID:', documentId);
    console.log('🔧 [段落滚动检测] 当前动态映射数量:', Object.keys(dynamicTextToNodeMap).length);
    console.log('🔧 [段落滚动检测] 当前静态映射数量:', Object.keys(textToNodeMap).length);
    
    // 🔑 简化滚动处理：只调用统一的检测函数，避免重复逻辑
    const throttledHandler = throttle(() => {
      if (contentBlockRefs.current.size > 0) {
        console.log('📜 [滚动事件] 触发段落检测，当前段落数量:', contentBlockRefs.current.size);
        // 直接调用统一的段落检测函数，避免重复实现
        determineActiveParagraph();
      } else {
        console.log('📜 [滚动事件] 没有可用的段落进行检测');
      }
    }, 200);

    // 查找滚动容器
    let scrollContainer = null;
    
    const findScrollContainer = () => {
      if (containerRef.current) {
        const selectors = [
          '.overflow-y-auto',
          '[style*="overflow-y: auto"]',
          '[style*="overflow: auto"]',
          '.h-full.overflow-hidden.flex.flex-col > div:last-child',
        ];
        
        for (const selector of selectors) {
          scrollContainer = containerRef.current.querySelector(selector);
          if (scrollContainer) {
            console.log('📜 [滚动检测] 找到滚动容器，选择器:', selector);
            return scrollContainer;
          }
        }
        
        // 通过样式检测
        const allElements = containerRef.current.querySelectorAll('*');
        for (const el of allElements) {
          const style = window.getComputedStyle(el);
          if (style.overflowY === 'auto' || style.overflowY === 'scroll' || 
              style.overflow === 'auto' || style.overflow === 'scroll') {
            scrollContainer = el;
            console.log('📜 [滚动检测] 通过样式检测找到滚动容器:', el.className);
            return scrollContainer;
          }
        }
      }
      return null;
    };

    // 延迟设置监听器，确保DOM已经渲染
    const setupScrollListener = () => {
      scrollContainer = findScrollContainer();
      
      if (scrollContainer) {
        console.log('📜 [滚动检测] 添加滚动监听到容器');
        scrollContainer.addEventListener('scroll', throttledHandler, { passive: true });
      } else {
        console.log('📜 [滚动检测] 未找到滚动容器，使用window滚动监听');
        window.addEventListener('scroll', throttledHandler, { passive: true });
      }
      
      window.addEventListener('resize', throttledHandler, { passive: true });
    };

    // 延迟设置，确保DOM完全加载
    const timer = setTimeout(setupScrollListener, 300);

    return () => {
      console.log('🔧 [段落滚动检测] 清理事件监听器');
      clearTimeout(timer);
      if (scrollContainer) {
        scrollContainer.removeEventListener('scroll', throttledHandler);
      } else {
        window.removeEventListener('scroll', throttledHandler);
      }
      window.removeEventListener('resize', throttledHandler);
    };
  }, [documentId, determineActiveParagraph]); // 只依赖determineActiveParagraph函数

  // 统一的初始化检测 - 在内容加载完成后启动
  useEffect(() => {
    console.log('🎨 [统一初始化] 启动初始化检测');
    
    // 启动初始化检测
    const timer = setTimeout(() => {
      initializeDetection();
    }, 200); // 稍微延迟确保DOM已准备好
    
    return () => {
      clearTimeout(timer);
    };
  }, [initializeDetection]); // 只依赖初始化函数

  // 组件卸载时清理MutationObserver
  useEffect(() => {
    return () => {
      if (typeof window !== 'undefined' && window.mermaidMutationObserver) {
        console.log('🧹 [清理] 断开MutationObserver连接');
        window.mermaidMutationObserver.disconnect();
        window.mermaidMutationObserver = null;
      }
    };
  }, []);

  // 监听动态映射状态变化，确保状态更新后重新检测
  useEffect(() => {
    const dynamicMappingCount = Object.keys(dynamicTextToNodeMap).length;
    console.log('🔄 [映射状态监听] 动态映射状态变化，数量:', dynamicMappingCount);
    
    if (dynamicMappingCount > 0) {
      console.log('🔄 [映射状态监听] 检测到动态映射已创建，触发段落重新检测');
      
      // 延迟一点时间确保状态完全更新
      const timer = setTimeout(() => {
        console.log('🔄 [映射状态监听] 执行延迟段落检测');
        determineActiveParagraph();
      }, 100);
      
      return () => clearTimeout(timer);
    }
  }, [dynamicTextToNodeMap, determineActiveParagraph]);

  const scrollToSection = (item) => {
    const element = sectionRefs.current.get(item.id);
    if (element) {
      element.scrollIntoView({ 
        behavior: 'smooth', 
        block: 'start' 
      });
    }
  };

  // 根据节点ID滚动到对应的语义块（支持多段落高亮）
  const scrollToContentBlock = useCallback((nodeId) => {
    console.log('📜 [语义块滚动] 开始查找节点对应的语义块:', nodeId);
    console.log('📜 [语义块滚动] 当前文档ID:', documentId);
    
    // 优先使用动态映射，回退到静态映射
    const currentNodeToTextMap = Object.keys(dynamicNodeToTextMap).length > 0 ? dynamicNodeToTextMap : nodeToTextMap;
    const textBlockIds = currentNodeToTextMap[nodeId];
    
    console.log('📜 [语义块滚动] 使用映射类型:', Object.keys(dynamicNodeToTextMap).length > 0 ? 'semantic' : 'static');
    console.log('📜 [语义块滚动] 动态映射数量:', Object.keys(dynamicNodeToTextMap).length);
    console.log('📜 [语义块滚动] 找到的文本块:', textBlockIds);
    
    if (!textBlockIds) {
      console.warn('📜 [语义块滚动] 未找到节点对应的文本块映射:', nodeId);
      console.log('📜 [语义块滚动] 可用的节点映射:', Object.keys(currentNodeToTextMap));
      console.log('📜 [语义块滚动] 动态映射详情:', dynamicNodeToTextMap);
      console.log('📜 [语义块滚动] 静态映射详情:', nodeToTextMap);
      return;
    }

    // 处理语义映射（数组）或静态映射（字符串）
    const targetIds = Array.isArray(textBlockIds) ? textBlockIds : [textBlockIds];
    console.log('📜 [语义块滚动] 目标段落/块ID列表:', targetIds);

    // 查找并高亮所有相关的段落
    const foundElements = [];
    let primaryElement = null;

    targetIds.forEach(blockId => {
      // 首先尝试查找段落元素（para-X格式）
      let element = null;
      
      if (blockId.startsWith('para-')) {
        // 查找段落元素
        element = window.document?.getElementById(blockId) || 
                 window.document?.querySelector(`[data-para-id="${blockId}"]`) ||
                 contentBlockRefs.current.get(blockId);
        
        if (element) {
          console.log('📜 [语义块滚动] 找到段落元素:', blockId);
          foundElements.push({ element, id: blockId, type: 'paragraph' });
          
          // 将第一个找到的段落作为主要滚动目标
          if (!primaryElement) {
            primaryElement = element;
          }
        } else {
          console.warn('📜 [语义块滚动] 未找到段落元素:', blockId);
        }
      } else {
        // 查找内容块元素（chunk-X格式）
        element = contentBlockRefs.current.get(blockId);
        
        if (element) {
          console.log('📜 [语义块滚动] 找到内容块元素:', blockId);
          foundElements.push({ element, id: blockId, type: 'block' });
          
          if (!primaryElement) {
            primaryElement = element;
          }
        } else {
          console.warn('📜 [语义块滚动] 未找到内容块元素:', blockId);
        }
      }
    });

    if (foundElements.length > 0) {
      console.log('📜 [语义块滚动] 找到', foundElements.length, '个相关元素');
      
      // 滚动到主要元素 - 将目标段落放在视口40%位置
      if (primaryElement) {
        console.log('📜 [语义块滚动] 滚动到主要元素（40%位置）');
        
        // 查找滚动容器（优先查找.overflow-y-auto容器）
        let scrollContainer = null;
        if (containerRef.current) {
          const selectors = [
            '.overflow-y-auto',
            '[style*="overflow-y: auto"]',
            '[style*="overflow: auto"]'
          ];
          
          for (const selector of selectors) {
            scrollContainer = containerRef.current.querySelector(selector);
            if (scrollContainer) {
              console.log('📜 [滚动容器] 找到滚动容器，选择器:', selector);
              break;
            }
          }
        }
        
        if (scrollContainer) {
          // 使用容器滚动
          const containerRect = scrollContainer.getBoundingClientRect();
          const elementRect = primaryElement.getBoundingClientRect();
          
          // 计算元素相对于滚动容器的位置
          const elementRelativeTop = elementRect.top - containerRect.top + scrollContainer.scrollTop;
          
          // 计算容器高度的40%
          const containerHeight = scrollContainer.clientHeight;
          const targetOffset = containerHeight * 0.35;
          
          // 计算滚动位置：元素顶部 - 容器40%位置
          const scrollTo = elementRelativeTop - targetOffset;
          
          console.log('📜 [滚动计算] 使用容器滚动 - 元素相对位置:', elementRelativeTop, '容器40%偏移:', targetOffset, '目标滚动位置:', scrollTo);
          
          // 平滑滚动到目标位置
          scrollContainer.scrollTo({
            top: Math.max(0, scrollTo), // 确保不滚动到负数位置
            behavior: 'smooth'
          });
          
          // 🔑 滚动完成后立即触发段落检测，确保高亮快速响应
          setTimeout(() => {
            console.log('📜 [滚动完成] 触发段落检测以更新高亮');
            determineActiveParagraph();
          }, 200); // 减少延迟，提高响应速度
          
        } else {
          // 回退到窗口滚动
          console.log('📜 [滚动计算] 未找到容器，使用window滚动');
          const elementRect = primaryElement.getBoundingClientRect();
          const elementTop = elementRect.top + window.pageYOffset;
          
          // 计算视口高度的40%
          const viewportHeight = window.innerHeight;
          const targetOffset = viewportHeight * 0.35;
          
          // 计算滚动位置：元素顶部 - 视口40%位置
          const scrollTo = elementTop - targetOffset;
          
          console.log('📜 [滚动计算] 元素顶部位置:', elementTop, '视口40%偏移:', targetOffset, '目标滚动位置:', scrollTo);
          
          // 平滑滚动到目标位置
          window.scrollTo({
            top: Math.max(0, scrollTo), // 确保不滚动到负数位置
            behavior: 'smooth'
          });
          
          // 🔑 滚动完成后立即触发段落检测，确保高亮快速响应
          setTimeout(() => {
            console.log('📜 [滚动完成] 触发段落检测以更新高亮');
            determineActiveParagraph();
          }, 200); // 减少延迟，提高响应速度
        }
      }
      
      console.log('📜 [语义块滚动] 滚动完成，段落检测将自动处理高亮');
    } else {
      console.warn('📜 [语义块滚动] 未找到任何目标元素');
      console.log('📜 [语义块滚动] 可用的内容块:', Array.from(contentBlockRefs.current.keys()));
      
      // 输出DOM中所有可能的段落元素进行调试
      const allParaElements = window.document?.querySelectorAll('[id^="para-"], [data-para-id]');
      if (allParaElements && allParaElements.length > 0) {
        console.log('📜 [语义块滚动] DOM中的段落元素:', Array.from(allParaElements).map(el => el.id || el.getAttribute('data-para-id')));
      }
    }
  }, [documentId, dynamicNodeToTextMap, containerRef, determineActiveParagraph]);

  // 调试辅助函数
  const debugScrollDetection = useCallback(() => {
    console.log('🔍 [调试信息] 滚动检测状态:');
    console.log('  - 当前活动段落:', activeContentBlockId);
    console.log('  - 当前活动章节:', activeChunkId);
    console.log('  - 段落引用数量:', contentBlockRefs.current.size);
    console.log('  - 章节引用数量:', sectionRefs.current.size);
    console.log('  - 动态映射数量:', Object.keys(dynamicTextToNodeMap).length);
    console.log('  - 静态映射数量:', Object.keys(textToNodeMap).length);
    console.log('  - 文档ID:', documentId);
    console.log('  - 思维导图模式:', currentMindmapMode);
    console.log('  - 所有段落ID:', Array.from(contentBlockRefs.current.keys()));
  }, [activeContentBlockId, activeChunkId, dynamicTextToNodeMap, textToNodeMap, documentId, currentMindmapMode]);

  // 将调试函数暴露到全局window对象
  useEffect(() => {
    if (typeof window !== 'undefined') {
      window.debugScrollDetection = debugScrollDetection;
      console.log('🔧 [调试工具] debugScrollDetection函数已挂载到window对象，可在控制台中调用 window.debugScrollDetection() 查看详细信息');
    }
    
    return () => {
      if (typeof window !== 'undefined') {
        delete window.debugScrollDetection;
      }
    };
  }, [debugScrollDetection]);

  // 动态映射更新函数
  const updateDynamicMapping = useCallback((textToNodeMapping, nodeToTextMapping) => {
    console.log('📊 [动态映射] 更新动态映射关系');
    console.log('📊 [动态映射] 文本到节点映射项数:', Object.keys(textToNodeMapping).length);
    console.log('📊 [动态映射] 节点到文本映射项数:', Object.keys(nodeToTextMapping).length);
    
    setDynamicTextToNodeMap(textToNodeMapping);
    setDynamicNodeToTextMap(nodeToTextMapping);
    
    console.log('📊 [动态映射] 示例映射条目:');
    Object.keys(textToNodeMapping).slice(0, 3).forEach(textId => {
      console.log(`  ${textId} -> ${textToNodeMapping[textId]}`);
    });
  }, []);

  return {
    activeChunkId,
    activeContentBlockId,
    contentChunks,
    handleSectionRef,
    handleContentBlockRef,
    scrollToSection,
    scrollToContentBlock,
    highlightParagraph,
    highlightMermaidNode,
    updateDynamicMapping, // 暴露动态映射函数
    dynamicMapping: { textToNodeMap: dynamicTextToNodeMap, nodeToTextMap: dynamicNodeToTextMap }, // 暴露动态映射关系
    nodeToTextMap, // 暴露静态映射关系供外部使用
    textToNodeMap,  // 暴露静态映射关系供外部使用
    debugScrollDetection, // 暴露调试函数
    setActiveContentBlockId, // 🔑 暴露状态设置函数供外部直接调用
  };
};
</file>

<file path="web_backend.py">
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import asyncio
import os
import hashlib
import tempfile
import re
from datetime import datetime
from pathlib import Path
import logging
import base64
from typing import List, Dict, Any
import json

# 导入现有的思维导图生成器
from mindmap_generator import MindMapGenerator, MinimalDatabaseStub, get_logger, generate_mermaid_html, DocumentOptimizer

# 导入文档解析器
from document_parser import DocumentParser

# 导入MinerU相关模块
from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.config.enums import SupportedPdfParseMethod

app = FastAPI(title="Argument Structure Analyzer API", version="1.0.0")

# 配置CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # React开发服务器
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 配置日志
logger = get_logger()

# 创建上传目录
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# 创建PDF处理目录
PDF_OUTPUT_DIR = Path("pdf_outputs")
PDF_OUTPUT_DIR.mkdir(exist_ok=True)

# 存储文档状态的内存数据库
document_status = {}



# 存储文档结构的内存数据库
document_structures = {}

class ArgumentStructureAnalyzer:
    """论证结构分析器"""
    
    def __init__(self):
        self.generator = MindMapGenerator()
        self.document_parser = DocumentParser()
        # 添加DocumentOptimizer实例用于AI调用
        self.optimizer = DocumentOptimizer()
    
    def add_paragraph_ids(self, text: str) -> str:
        """为文本的每个段落添加ID号"""
        try:
            # 按段落分割文本
            paragraphs = text.split('\n\n')
            processed_paragraphs = []
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():  # 只处理非空段落
                    # 为每个段落添加ID标记
                    para_id = f"para-{i+1}"
                    processed_paragraph = f"[{para_id}] {paragraph.strip()}"
                    processed_paragraphs.append(processed_paragraph)
                else:
                    processed_paragraphs.append(paragraph)
            
            return '\n\n'.join(processed_paragraphs)
            
        except Exception as e:
            print(f"❌ [段落ID添加错误] {str(e)}")
            return text
    
    def split_text_into_chunks(self, text: str, document_id: str) -> List[Dict[str, Any]]:
        """将文档按Markdown标题层级分块并分配唯一标识符"""
        try:
            # 使用新的文档解析器
            chunks = self.document_parser.parse_to_chunks(text, document_id)
            
            # 同时保存文档结构用于目录生成
            root = self.document_parser.parse_document(text, document_id)
            toc = self.document_parser.generate_toc(root)
            
            document_structures[document_id] = {
                'structure': root.to_dict(),
                'toc': toc,
                'chunks': chunks
            }
            
            print(f"📄 [文本分块] 文档 {document_id} 分为 {len(chunks)} 个结构化块")
            for i, chunk in enumerate(chunks[:3]):  # 显示前3个块的信息
                print(f"   块 {i}: {chunk.get('title', '无标题')} (级别 {chunk.get('level', 0)})")
            
            return chunks
            
        except Exception as e:
            print(f"❌ [分块错误] {str(e)}")
            return []
    
    async def generate_argument_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """使用AI分析文档的论证结构"""
        try:
            # 构建基于段落的论证结构分析prompt
            prompt = f"""我希望你扮演一个专业的学术分析师，你的任务是阅读我提供的、已经按段落标记好ID的文本，并基于现有的段落划分来分析其论证结构。

请按照以下步骤进行分析：

第一步：段落角色识别
- 基于现有的段落划分（[para-X]标记），分析每个段落在论证中的角色
- 不要重新划分段落，而是基于现有段落来理解论证逻辑
- 识别每个段落是引言、论点、证据、反驳、结论等哪种类型

第二步：构建论证结构流程图
- 基于段落的论证角色，构建逻辑流程图
- 将具有相同或相关论证功能的段落组合成逻辑节点
- 用箭头表示论证的逻辑流向和依赖关系

你的输出必须是一个单一的、完整的 JSON 对象，不要在 JSON 代码块前后添加任何额外的解释性文字。

这个 JSON 对象必须包含三个顶级键："mermaid_string"、"node_mappings" 和 "edges"。

mermaid_string:
- 值为符合 Mermaid.js 语法的流程图（graph TD）
- 图中的每个节点代表一组相关的段落（基于论证功能）
- 节点 ID 使用简短的字母或字母数字组合（如：A, B, C1, D2）
- 节点标签应该简洁概括该组段落的核心论证功能（不超过20字）
- 使用箭头 --> 表示论证的逻辑流向和依赖关系
- 可以使用不同的节点形状来区分不同类型的论证功能：
  - [方括号] 用于主要论点
  - (圆括号) 用于支撑证据
  - {{花括号}} 用于逻辑转折或关键判断

node_mappings:
- 值为 JSON 对象，键为 Mermaid 图中的节点 ID
- 每个节点对应的值包含：
  - "text_snippet": 该节点包含段落的核心内容总结（30-80字）
  - "paragraph_ids": 构成该节点的段落ID数组（如 ["para-2", "para-3"]）
  - "semantic_role": 该节点在论证中的角色（如 "引言"、"核心论点"、"支撑证据"、"反驳"、"结论" 等）

edges:
- 值为对象数组，每个对象代表一条边
- 每个对象必须包含两个键：
  - "source": 边的起始节点ID
  - "target": 边的目标节点ID
- 这些边必须与 mermaid_string 中的连接关系一致

关键要求：
1. 所有节点 ID 必须在 mermaid_string 中存在
2. paragraph_ids 必须严格使用原文的段落标记 [para-X]，不可修改
3. 原文的每个段落都应该被分配给至少一个节点
4. 节点的划分应该基于段落的论证功能，相关功能的段落可以组合在一个节点中
5. 流程图应该清晰展现论证的逻辑推理路径
6. 保持段落的完整性，不要拆分或重组段落内容
7. edges 数组中的每条边必须与 mermaid_string 中的连接关系完全一致

现在，请分析以下带有段落ID的文本：

{text_with_ids}"""
            
            # 使用DocumentOptimizer的generate_completion方法
            response = await self.optimizer.generate_completion(
                prompt, 
                max_tokens=2000,
                task="分析论证结构"
            )
            
            if not response:
                print(f"❌ [API调用失败] 未收到AI响应")
                return {"success": False, "error": "API调用失败，未收到AI响应"}
            
            # 保存API原始响应到文件
            try:
                from datetime import datetime
                import os
                
                # 创建api_responses文件夹（如果不存在）
                api_responses_dir = "api_responses"
                os.makedirs(api_responses_dir, exist_ok=True)
                
                # 生成文件名：时间戳_论证结构分析
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                response_filename = f"{timestamp}_argument_structure_analysis.txt"
                response_filepath = os.path.join(api_responses_dir, response_filename)
                
                # 保存原始响应和相关信息
                with open(response_filepath, 'w', encoding='utf-8') as f:
                    f.write("=== API调用信息 ===\n")
                    f.write(f"时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"任务: 论证结构分析\n")
                    f.write(f"最大tokens: 2000\n")
                    f.write(f"响应长度: {len(response)} 字符\n")
                    f.write(f"文本长度: {len(text_with_ids)} 字符\n")
                    f.write("\n=== 发送的Prompt ===\n")
                    f.write(prompt)
                    f.write("\n\n=== AI原始响应 ===\n")
                    f.write(response)
                    f.write(f"\n\n=== 响应结束 ===\n")
                
                print(f"💾 [API响应保存] 已保存到: {response_filepath}")
                
            except Exception as save_error:
                print(f"⚠️ [响应保存失败] {str(save_error)}")
            
            # 解析JSON响应
            try:
                # 详细记录原始响应
                print(f"🔍 [原始AI响应] 长度: {len(response)} 字符")
                print(f"🔍 [原始响应前200字符]: {response[:200]}")
                
                # 更彻底的响应清理
                clean_response = response.strip()
                
                # 移除可能的代码块标记
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                elif clean_response.startswith('```'):
                    clean_response = clean_response[3:]
                    
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                
                clean_response = clean_response.strip()
                
                # 移除可能的说明文字，只保留JSON部分
                json_start = clean_response.find('{')
                json_end = clean_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    clean_response = clean_response[json_start:json_end+1]
                    print(f"🔧 [提取JSON] 提取到JSON部分，长度: {len(clean_response)}")
                else:
                    print(f"⚠️ [JSON提取失败] 无法找到有效的JSON结构")
                
                print(f"🔍 [清理后响应前200字符]: {clean_response[:200]}")
                
                structure_data = json.loads(clean_response)
                
                # 验证必要的键
                if 'mermaid_string' not in structure_data or 'node_mappings' not in structure_data:
                    print(f"❌ [数据结构错误] 响应键: {list(structure_data.keys())}")
                    return {"success": False, "error": "AI响应格式不正确：缺少必要的键"}
                
                # 验证节点映射的结构
                node_mappings = structure_data['node_mappings']
                valid_mappings = {}
                
                for node_id, mapping in node_mappings.items():
                    if isinstance(mapping, dict):
                        # 确保必要字段存在，如果缺少semantic_role就添加默认值
                        valid_mapping = {
                            "text_snippet": mapping.get("text_snippet", "语义块内容"),
                            "paragraph_ids": mapping.get("paragraph_ids", []),
                            "semantic_role": mapping.get("semantic_role", "论证要素")
                        }
                        valid_mappings[node_id] = valid_mapping
                    else:
                        print(f"⚠️ [映射格式错误] 节点 {node_id} 的映射不是字典格式")
                
                structure_data['node_mappings'] = valid_mappings
                
                # 检查是否包含edges字段，如果没有则尝试从mermaid_string中提取
                if 'edges' not in structure_data:
                    print("⚠️ [数据结构警告] 响应中没有edges字段，将从mermaid_string中提取")
                    # 从mermaid_string中提取边关系
                    edges = []
                    mermaid_string = structure_data['mermaid_string']
                    # 匹配形如 "A --> B" 的边定义
                    edge_pattern = r'([A-Za-z0-9_]+)\s*-->\s*([A-Za-z0-9_]+)'
                    for match in re.finditer(edge_pattern, mermaid_string):
                        source, target = match.groups()
                        edges.append({"source": source, "target": target})
                    structure_data['edges'] = edges
                    print(f"🔧 [自动提取] 从mermaid_string中提取了 {len(edges)} 条边")
                
                print(f"✅ [论证结构分析] 成功生成包含 {len(structure_data['node_mappings'])} 个节点的流程图")
                
                # 返回成功结果
                return {
                    "success": True,
                    "mermaid_code": structure_data['mermaid_string'],
                    "node_mappings": structure_data['node_mappings'],
                    "edges": structure_data['edges']
                }
                
            except json.JSONDecodeError as parse_error:
                print(f"❌ [JSON解析错误] {str(parse_error)}")
                print(f"❌ [完整原始响应]: {response}")
                print(f"❌ [清理后响应]: {clean_response}")
                return {"success": False, "error": f"JSON解析失败: {str(parse_error)}"}
                
        except Exception as e:
            print(f"❌ [论证结构分析错误] {str(e)}")
            # 提供降级策略 - 生成基本的论证结构
            try:
                fallback_structure = self.generate_fallback_structure(text_with_ids)
                print(f"🔄 [降级策略] 使用基本论证结构，包含 {len(fallback_structure['node_mappings'])} 个节点")
                return fallback_structure
            except Exception as fallback_error:
                print(f"❌ [降级策略失败] {str(fallback_error)}")
                return {"success": False, "error": f"AI分析失败且降级策略也失败: {str(e)}"}

    def generate_fallback_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """生成基本的论证结构作为降级策略"""
        import re
        
        # 提取所有段落ID
        para_ids = re.findall(r'\[para-(\d+)\]', text_with_ids)
        
        if not para_ids:
            # 如果没有找到段落ID，创建一个基本结构
            return {
                "success": True,
                "mermaid_code": "graph TD\n    A[文档分析] --> B[主要内容]\n    B --> C[总结]",
                "node_mappings": {
                    "A": {
                        "text_snippet": "文档开始",
                        "paragraph_ids": ["para-1"],
                        "semantic_role": "引言"
                    },
                    "B": {
                        "text_snippet": "主要内容",
                        "paragraph_ids": ["para-2"],
                        "semantic_role": "核心论点"
                    },
                    "C": {
                        "text_snippet": "文档结论",
                        "paragraph_ids": ["para-3"],
                        "semantic_role": "结论"
                    }
                }
            }
        
        # 基于段落数量生成结构
        total_paras = len(para_ids)
        
        if total_paras <= 3:
            # 简单线性结构
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[引言] --> B[主体]\n"
            mermaid_code += "    B --> C[结论]"
            
            node_mappings = {
                "A": {
                    "text_snippet": "文档引言部分",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "引言"
                },
                "B": {
                    "text_snippet": "文档主体内容",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:-1]] if total_paras > 2 else [f"para-{para_ids[1]}"] if total_paras > 1 else [],
                    "semantic_role": "核心论点"
                },
                "C": {
                    "text_snippet": "文档结论",
                    "paragraph_ids": [f"para-{para_ids[-1]}"] if total_paras > 1 else [],
                    "semantic_role": "结论"
                }
            }
        else:
            # 复杂结构：引言 -> 多个论点 -> 结论
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[引言] --> B[论点1]\n"
            mermaid_code += "    A --> C[论点2]\n"
            if total_paras > 5:
                mermaid_code += "    A --> D[论点3]\n"
                mermaid_code += "    B --> E[结论]\n"
                mermaid_code += "    C --> E\n"
                mermaid_code += "    D --> E"
            else:
                mermaid_code += "    B --> D[结论]\n"
                mermaid_code += "    C --> D"
            
            # 将段落分配给不同节点
            para_per_section = max(1, total_paras // 4)
            
            node_mappings = {
                "A": {
                    "text_snippet": "文档引言",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "引言"
                },
                "B": {
                    "text_snippet": "第一个论点",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:1+para_per_section]],
                    "semantic_role": "核心论点"
                },
                "C": {
                    "text_snippet": "第二个论点", 
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+para_per_section:1+2*para_per_section]],
                    "semantic_role": "支撑证据"
                }
            }
            
            if total_paras > 5:
                node_mappings["D"] = {
                    "text_snippet": "第三个论点",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:-1]],
                    "semantic_role": "补充论证"
                }
                node_mappings["E"] = {
                    "text_snippet": "文档结论",
                    "paragraph_ids": [f"para-{para_ids[-1]}"],
                    "semantic_role": "结论"
                }
            else:
                node_mappings["D"] = {
                    "text_snippet": "文档结论",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:]],
                    "semantic_role": "结论"
                }
        
        return {
            "success": True,
            "mermaid_code": mermaid_code,
            "node_mappings": node_mappings
        }

# 创建全局分析器实例
argument_analyzer = ArgumentStructureAnalyzer()

async def process_pdf_to_markdown(pdf_file_path: str, document_id: str) -> str:
    """
    使用MinerU处理PDF文件，转换为Markdown格式
    
    Args:
        pdf_file_path: PDF文件路径
        document_id: 文档ID
        
    Returns:
        转换后的Markdown内容
    """
    try:
        print(f"\n📄 [MinerU-PDF处理] 开始处理PDF文件")
        print(f"    📁 文件路径: {pdf_file_path}")
        print(f"    🆔 文档ID: {document_id}")
        print("=" * 60)
        
        # 创建输出目录
        output_dir = PDF_OUTPUT_DIR / document_id
        image_dir = output_dir / "images"
        os.makedirs(image_dir, exist_ok=True)
        
        print(f"📁 [MinerU-目录] 创建输出目录: {output_dir}")
        print(f"🖼️  [MinerU-图片] 图片目录: {image_dir}")
        
        # 创建数据读写器
        print("🔧 [MinerU-初始化] 创建数据读写器...")
        reader = FileBasedDataReader("")
        image_writer = FileBasedDataWriter(str(image_dir))
        md_writer = FileBasedDataWriter(str(output_dir))
        
        # 读取PDF文件
        print("📖 [MinerU-读取] 正在读取PDF文件...")
        pdf_bytes = reader.read(pdf_file_path)
        print(f"📊 [MinerU-数据] PDF文件大小: {len(pdf_bytes)} 字节")
        
        # 创建数据集实例
        print("🏗️  [MinerU-数据集] 创建PymuDocDataset实例...")
        ds = PymuDocDataset(pdf_bytes)
        
        # 分类处理模式
        print("🔍 [MinerU-检测] 检测PDF处理模式...")
        pdf_mode = ds.classify()
        
        # 进行推理
        if pdf_mode == SupportedPdfParseMethod.OCR:
            print(f"🔤 [MinerU-OCR模式] 检测到需要OCR处理，开始文字识别...")
            print("    📸 正在提取图片中的文字...")
            print("    🧠 调用OCR引擎进行文字识别...")
            infer_result = ds.apply(doc_analyze, ocr=True)
            
            print("⚡ [MinerU-管道] 使用OCR模式管道处理...")
            pipe_result = infer_result.pipe_ocr_mode(image_writer)
            print("✅ [MinerU-OCR] OCR处理完成")
        else:
            print(f"📝 [MinerU-文本模式] 检测到可直接提取文本，开始文本处理...")
            print("    📄 正在提取PDF中的文本内容...")
            print("    🔧 分析文档结构和版面...")
            infer_result = ds.apply(doc_analyze, ocr=False)
            
            print("⚡ [MinerU-管道] 使用文本模式管道处理...")
            pipe_result = infer_result.pipe_txt_mode(image_writer)
            print("✅ [MinerU-文本] 文本提取完成")
        
        print("📋 [MinerU-转换] 正在生成Markdown格式...")
        # 获取Markdown内容
        markdown_content = pipe_result.get_markdown("images")
        
        # 保存Markdown文件
        md_file_path = output_dir / f"{document_id}.md"
        print(f"💾 [MinerU-保存] 保存Markdown文件: {md_file_path}")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # 统计信息
        lines_count = len(markdown_content.split('\n'))
        words_count = len(markdown_content.split())
        
        print("=" * 60)
        print("✅ [MinerU-完成] PDF转换成功完成！")
        print(f"    📊 生成内容统计:")
        print(f"       • Markdown总长度: {len(markdown_content):,} 字符")
        print(f"       • 总行数: {lines_count:,} 行")
        print(f"       • 单词数: {words_count:,} 个")
        print(f"    📁 输出文件: {md_file_path}")
        print("=" * 60)
        
        return markdown_content
        
    except Exception as e:
        print("=" * 60)
        print(f"❌ [MinerU-错误] PDF处理失败！")
        print(f"    🚨 错误信息: {str(e)}")
        print(f"    📄 文件路径: {pdf_file_path}")
        print(f"    🆔 文档ID: {document_id}")
        print("=" * 60)
        logger.error(f"MinerU PDF processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"PDF处理失败: {str(e)}")

@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """上传文档，支持PDF、MD和TXT文件"""
    
    # 验证文件类型
    allowed_extensions = ['.md', '.txt', '.pdf']
    file_extension = Path(file.filename).suffix.lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(status_code=400, detail="只支持 .md、.txt 和 .pdf 文件")
    
    try:
        # 读取文件内容
        content = await file.read()
        
        # 生成唯一的文档ID
        content_hash = hashlib.md5(content).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = Path(file.filename).stem
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        
        print(f"\n📤 [文件上传] {file.filename}")
        print(f"🆔 [文档ID] {document_id}")
        print(f"📊 [文件大小] {len(content)} 字节")
        print(f"📋 [文件类型] {file_extension}")
        
        # 保存原始文件
        original_file_path = UPLOAD_DIR / f"{document_id}{file_extension}"
        with open(original_file_path, 'wb') as f:
            f.write(content)
        
        # 根据文件类型处理内容
        if file_extension == '.pdf':
            # 处理PDF文件
            print(f"🔄 [PDF处理] 开始转换PDF为Markdown...")
            markdown_content = await process_pdf_to_markdown(str(original_file_path), document_id)
            text_content = markdown_content
            
            # 将原始PDF文件编码为base64用于前端显示
            pdf_base64 = base64.b64encode(content).decode('utf-8')
            
        else:
            # 处理文本文件
            text_content = content.decode('utf-8')
            pdf_base64 = None
        
        # 存储到内存数据库
        MinimalDatabaseStub.store_text(text_content)
        
        # 立即为文档内容添加段落ID，无需等待生成论证结构
        print("📝 [处理段落] 为上传的文档添加段落ID标记...")
        content_with_ids = argument_analyzer.add_paragraph_ids(text_content)
        print(f"📝 [段落处理完成] 已为文档添加段落ID，内容长度: {len(content_with_ids)} 字符")
        
        # 初始化文档状态
        document_status[document_id] = {
            "status": "uploaded",
            "content": text_content,
            "filename": file.filename,
            "file_type": file_extension,
            "original_file_path": str(original_file_path),
            "pdf_base64": pdf_base64,  # 仅PDF文件有此字段
            "status_demo": "not_started",
            "mermaid_code_demo": None,
            "node_mappings_demo": {},
            "error_demo": None,
            "content_with_ids": content_with_ids  # 立即设置带段落ID的内容
        }
        
        print(f"✅ [上传成功] 文档已保存并准备生成思维导图")
        print("=" * 60)
        
        logger.info(f"Document uploaded: {document_id}")
        
        # 返回文档信息
        response_data = {
            "success": True,
            "document_id": document_id,
            "filename": file.filename,
            "content": text_content,
            "file_type": file_extension,
            "status": "uploaded",
            "message": "文档上传成功"
        }
        
        # 如果是PDF文件，返回base64编码的原始PDF
        if file_extension == '.pdf':
            response_data["pdf_base64"] = pdf_base64
            response_data["message"] = "PDF文件上传成功，已转换为Markdown"
        
        return JSONResponse(response_data)
        
    except UnicodeDecodeError:
        print(f"❌ [编码错误] 文件: {file.filename}")
        raise HTTPException(status_code=400, detail="文件编码错误，请确保文件是UTF-8编码")
    except Exception as e:
        print(f"❌ [上传失败] 文件: {file.filename}, 错误: {str(e)}")
        logger.error(f"处理文件时出错: {str(e)}")
        raise HTTPException(status_code=500, detail=f"处理文件时出错: {str(e)}")

@app.get("/api/document-pdf/{document_id}")
async def get_document_pdf(document_id: str):
    """获取原始PDF文件"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="文档不存在")
    
    doc_info = document_status[document_id]
    
    if doc_info.get("file_type") != ".pdf":
        raise HTTPException(status_code=400, detail="该文档不是PDF文件")
    
    original_file_path = doc_info.get("original_file_path")
    if not original_file_path or not os.path.exists(original_file_path):
        raise HTTPException(status_code=404, detail="原始PDF文件不存在")
    
    return FileResponse(
        path=original_file_path,
        media_type='application/pdf',
        filename=doc_info["filename"]
    )

@app.post("/api/generate-argument-structure/{document_id}")
async def generate_argument_structure(document_id: str):
    """为指定文档生成论证结构流程图"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="文档不存在")
    
    doc_info = document_status[document_id]
    
    # 检查状态
    if doc_info.get("status_demo") == "generating":
        print(f"⏳ [状态查询] 文档 {document_id} 论证结构正在分析中...")
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "论证结构正在分析中..."
        })
    
    if doc_info.get("status_demo") == "completed" and doc_info.get("mermaid_code_demo"):
        print(f"✅ [状态查询] 文档 {document_id} 论证结构已分析完成")
        return JSONResponse({
            "success": True,
            "status": "completed",
            "mermaid_code": doc_info["mermaid_code_demo"],
            "node_mappings": doc_info.get("node_mappings_demo", {}),
            "message": "论证结构已生成"
        })
    
    try:
        print(f"🔄 [开始分析] 为文档 {document_id} 启动论证结构分析任务")
        
        # 更新状态为分析中
        doc_info["status_demo"] = "generating"
        
        # 异步生成论证结构
        asyncio.create_task(generate_argument_structure_async(document_id, doc_info["content"]))
        
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "开始分析论证结构..."
        })
        
    except Exception as e:
        print(f"❌ [启动失败] 文档 {document_id} 论证结构分析启动失败: {str(e)}")
        logger.error(f"生成论证结构时出错: {str(e)}")
        doc_info["status_demo"] = "error"
        doc_info["error_demo"] = str(e)
        raise HTTPException(status_code=500, detail=f"生成论证结构时出错: {str(e)}")

async def generate_argument_structure_async(document_id: str, content: str):
    """异步生成论证结构"""
    try:
        print(f"🔄 [异步任务] 开始为文档 {document_id} 生成论证结构")
        argument_analyzer = ArgumentStructureAnalyzer()
        
        # 为文本添加段落ID
        text_with_ids = argument_analyzer.add_paragraph_ids(content)
        
        # 生成论证结构
        result = await argument_analyzer.generate_argument_structure(text_with_ids)
        
        if result["success"]:
            # 更新文档状态
            document_status[document_id]["status_demo"] = "completed"
            document_status[document_id]["mermaid_code_demo"] = result["mermaid_code"]
            document_status[document_id]["node_mappings_demo"] = result["node_mappings"]
            document_status[document_id]["edges_demo"] = result["edges"]  # 保存edges数据
            document_status[document_id]["content_with_ids"] = text_with_ids  # 保存带ID的内容
            
            print(f"✅ [分析完成] 文档 {document_id} 论证结构分析成功")
            print(f"📊 [生成结果] 包含 {len(result['node_mappings'])} 个论证节点和 {len(result['edges'])} 条边")
        else:
            # 分析失败
            document_status[document_id]["status_demo"] = "error"
            document_status[document_id]["error_demo"] = result["error"]
            print(f"❌ [分析失败] 文档 {document_id}: {result['error']}")
            
    except Exception as e:
        print(f"❌ [异步分析错误] 文档 {document_id}: {str(e)}")
        logger.error(f"异步生成论证结构时出错: {str(e)}")
        document_status[document_id]["status_demo"] = "error"
        document_status[document_id]["error_demo"] = str(e)

@app.get("/api/document-status/{document_id}")
async def get_document_status(document_id: str):
    """获取文档状态和论证结构分析进度"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="文档不存在")
    
    doc_info = document_status[document_id]
    
    response_data = {
        "success": True,
        "document_id": document_id,
        "filename": doc_info.get("filename"),
        "content": doc_info.get("content"),
        "file_type": doc_info.get("file_type", ".md"),
        
        # 论证结构分析状态
        "status_demo": doc_info.get("status_demo", "not_started"),
        "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
        "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
        "edges_demo": doc_info.get("edges_demo", []),
        "error_demo": doc_info.get("error_demo"),
        "content_with_ids": doc_info.get("content_with_ids"),
    }
    
    # 如果是PDF文件，添加PDF相关信息
    if doc_info.get("file_type") == ".pdf":
        response_data["pdf_base64"] = doc_info.get("pdf_base64")
        response_data["original_file_path"] = doc_info.get("original_file_path")
    
    return JSONResponse(response_data)

@app.get("/api/document/{document_id}")
async def get_document(document_id: str):
    """获取文档内容和论证结构"""
    
    try:
        # 如果文件在内存状态中存在，直接返回
        if document_id in document_status:
            doc_info = document_status[document_id]
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": doc_info["content"],
                "filename": doc_info.get("filename", ""),
                "file_type": doc_info.get("file_type", ".md"),
                "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
                "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
                "edges_demo": doc_info.get("edges_demo", []),
                "status_demo": doc_info.get("status_demo", "not_started"),
                "error_demo": doc_info.get("error_demo"),
                "content_with_ids": doc_info.get("content_with_ids"),
                "pdf_base64": doc_info.get("pdf_base64")
            })
        else:
            # 尝试查找文件
            file_path = UPLOAD_DIR / f"{document_id}.md"
            
            if not file_path.exists():
                raise HTTPException(status_code=404, detail="文档不存在")
            
            # 读取文件内容
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": content,
                "filename": f"{document_id}.md",
                "file_type": ".md",
                "mermaid_code_demo": None,
                "node_mappings_demo": {},
                "edges_demo": [],
                "status_demo": "not_started",
                "error_demo": None,
                "content_with_ids": None
            })
        
    except Exception as e:
        logger.error(f"获取文档时出错: {str(e)}")
        raise HTTPException(status_code=500, detail=f"获取文档时出错: {str(e)}")

@app.get("/api/health")
async def health_check():
    """健康检查接口"""
    return {"status": "healthy", "message": "Argument Structure Analyzer API is running"}

@app.get("/")
async def root():
    return {"message": "Argument Structure Analyzer API is running"}

# 文档结构相关API端点（保留用于目录生成等）

# 文档结构和目录相关API端点

@app.get("/api/document-structure/{document_id}")
async def get_document_structure(document_id: str):
    """获取文档的层级结构"""
    try:
        if document_id not in document_structures:
            # 如果结构不存在，尝试从文档内容生成
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    chunks = parser.parse_to_chunks(content, document_id)
                    
                    # 保存结构
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': chunks
                    }
                    
                    print(f"📄 [自动生成] 为文档 {document_id} 生成了结构和 {len(chunks)} 个chunks")
                    
                    return {
                        "success": True,
                        "structure": root.to_dict(),
                        "toc": toc,
                        "chunks": chunks,
                        "chunks_count": len(chunks)
                    }
            
            return {
                "success": False,
                "message": "文档结构尚未生成，且无法自动生成",
                "structure": None,
                "toc": [],
                "chunks": [],
                "chunks_count": 0
            }
        
        structure_data = document_structures[document_id]
        chunks = structure_data.get('chunks', [])
        
        print(f"📄 [API] 返回文档结构，chunks数量: {len(chunks)}")
        
        return {
            "success": True,
            "structure": structure_data['structure'],
            "toc": structure_data['toc'], 
            "chunks": chunks,  # 返回实际的chunks数据
            "chunks_count": len(chunks)
        }
        
    except Exception as e:
        logger.error(f"Get document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"获取文档结构失败: {str(e)}")

@app.get("/api/document-toc/{document_id}")
async def get_document_toc(document_id: str):
    """获取文档目录"""
    try:
        if document_id not in document_structures:
            # 如果结构不存在，尝试从文档内容生成
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    
                    # 保存结构
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': parser.parse_to_chunks(content, document_id)
                    }
                    
                    return {
                        "success": True,
                        "toc": toc
                    }
            
            return {
                "success": False,
                "message": "文档目录尚未生成",
                "toc": []
            }
        
        structure_data = document_structures[document_id]
        return {
            "success": True,
            "toc": structure_data['toc']
        }
        
    except Exception as e:
        logger.error(f"Get document TOC error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"获取文档目录失败: {str(e)}")

@app.post("/api/generate-document-structure/{document_id}")
async def generate_document_structure(document_id: str):
    """生成或重新生成文档结构"""
    try:
        if document_id not in document_status:
            raise HTTPException(status_code=404, detail="文档不存在")
        
        content = document_status[document_id].get('content')
        if not content:
            raise HTTPException(status_code=400, detail="文档内容为空")
        
        # 使用文档解析器生成结构
        parser = DocumentParser()
        root = parser.parse_document(content, document_id)
        toc = parser.generate_toc(root)
        chunks = parser.parse_to_chunks(content, document_id)
        
        # 保存结构
        document_structures[document_id] = {
            'structure': root.to_dict(),
            'toc': toc,
            'chunks': chunks
        }
        
        print(f"📄 [文档结构] 为文档 {document_id} 生成了 {len(toc)} 个目录项，{len(chunks)} 个内容块")
        
        return {
            "success": True,
            "message": "文档结构生成成功",
            "toc_items": len(toc),
            "chunks_count": len(chunks)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generate document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"生成文档结构失败: {str(e)}")

@app.post("/api/document/{document_id}/remap")
async def update_node_mappings(document_id: str, request_data: dict):
    """更新文档的节点映射关系"""
    try:
        print(f"📍 [API] 收到节点映射更新请求 - 文档ID: {document_id}")
        print(f"📍 [API] 新的节点映射: {request_data}")
        
        # 验证请求数据
        if 'node_mappings' not in request_data:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "缺少 node_mappings 参数"}
            )
        
        new_node_mappings = request_data['node_mappings']
        
        # 检查文档是否存在
        if document_id not in document_status:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": f"文档 {document_id} 不存在"}
            )
        
        # 更新文档状态中的节点映射
        document_status[document_id]['node_mappings_demo'] = new_node_mappings
        
        print(f"📍 [API] ✅ 成功更新文档 {document_id} 的节点映射")
        print(f"📍 [API] 更新后的映射键数量: {len(new_node_mappings)}")
        
        # 可选：保存到持久化存储（这里可以添加数据库保存逻辑）
        # TODO: 添加数据库持久化逻辑
        
        return JSONResponse(content={
            "success": True,
            "message": "节点映射更新成功",
            "document_id": document_id,
            "updated_mappings_count": len(new_node_mappings)
        })
        
    except Exception as e:
        print(f"❌ [API错误] 更新节点映射失败: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"更新节点映射失败: {str(e)}"}
        )

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 80)
    print("🎯 智能思维导图生成器 - 后端API服务")
    print("=" * 80)
    print("📍 服务地址: http://localhost:8000")
    print("📚 API文档: http://localhost:8000/docs")
    print("🔧 服务模式: 开发模式 (支持热重载)")
    print("=" * 80)
    print("📋 控制台日志说明:")
    print("   📤 [文件上传] - 文件上传相关信息")
    print("   🔄 [开始生成] - 思维导图生成任务启动")
    print("   🚀 [开始生成] - AI处理开始")
    print("   🤖 [AI处理] - 调用思维导图生成器")
    print("   ✅ [生成完成] - 思维导图生成成功")
    print("   ❌ [生成失败] - 生成过程出现错误")
    print("   ⏳ [状态查询] - 客户端查询生成状态")
    print("=" * 80)
    print("🎯 新功能: 支持两种生成模式")
    print("   📊 标准详细模式: 3-5分钟，详细分析，高质量结果")
    print("   ⚡ 快速简化模式: 1-2分钟，基础结构，快速预览")
    print("   📋 API端点: /api/generate-mindmap/{id} 和 /api/generate-mindmap-simple/{id}")
    print("=" * 80)
    print("🚀 启动服务中...")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
</file>

<file path="frontend/src/components/ViewerPageRefactored.js">
import React, { useState, useRef, useCallback, useEffect, useMemo } from 'react';
import { useNavigate, useLocation, useParams } from 'react-router-dom';
import { ArrowLeft, Download, Eye, EyeOff, FileText, File, Bot } from 'lucide-react';
import axios from 'axios';
import { toast, ToastContainer } from 'react-toastify';
import 'react-toastify/dist/ReactToastify.css';
import FlowDiagram from './FlowDiagram';
import ThemeToggle from './ThemeToggle';

// 导入自定义hooks
import { useDocumentViewer } from '../hooks/useDocumentViewer';
import { useMindmapGeneration } from '../hooks/useMindmapGeneration';
import { usePanelResize } from '../hooks/usePanelResize';

import { useScrollDetection } from '../hooks/useScrollDetection';

// 导入UI组件
import TableOfContents from './TableOfContents';
import PDFViewer from './PDFViewer';

import { StructuredMarkdownRenderer, DemoModeRenderer } from './DocumentRenderer';

const ViewerPageRefactored = () => {
  const navigate = useNavigate();
  const location = useLocation();
  const containerRef = useRef(null);
  const mermaidDiagramRef = useRef(null);
  
  const [showToc, setShowToc] = useState(false);

  // 添加contentChunks ref
  const contentChunks = useRef([]);

  // 使用文档查看器 hook
  const {
    documentId,
    document,
    setDocument,
    loading,
    error: documentError,
    viewMode,
    setViewMode,
    isPdfFile,
    toc,
    expandedTocItems,
    toggleTocItem,
    loadDocument,
    loadDocumentStructure
  } = useDocumentViewer();

  // 使用思维导图生成 hook
  const {
    demoMindmapStatus,
    startMindmapGeneration,
    handleDownloadMarkdown,
    handleDownloadMermaid,
    handleOpenMermaidEditor,
    MindmapStatusDisplay
  } = useMindmapGeneration(documentId, document, setDocument);

  // 使用面板拖拽 hook
  const {
    tocPanelWidth,
    leftPanelWidth,
    isDragging,
    handleMouseDown
  } = usePanelResize();

  // 使用滚动检测 hook
  const {
    activeChunkId,
    activeContentBlockId, // 添加段落级状态
    contentChunks: scrollChunks,
    handleSectionRef,
    handleContentBlockRef,
    scrollToSection,
    scrollToContentBlock,
    highlightParagraph,
    highlightMermaidNode,
    updateDynamicMapping,
    dynamicMapping,
    textToNodeMap, // 添加静态映射关系
    setActiveContentBlockId // 🔑 添加状态设置函数
  } = useScrollDetection(
    containerRef,
    documentId,
    'argument', // 论证结构分析模式
    mermaidDiagramRef
  );

  // 计算当前需要高亮的节点ID
  const highlightedNodeId = useMemo(() => {
    if (!activeContentBlockId) {
      return null;
    }

    // 优先使用动态映射，如果没有则使用静态映射
    const hasDynamicMapping = Object.keys(dynamicMapping.textToNodeMap).length > 0;
    const currentMapping = hasDynamicMapping ? dynamicMapping.textToNodeMap : textToNodeMap;
    
    const mappedNodeId = currentMapping[activeContentBlockId];
    
    console.log('🎯 [高亮计算] 活跃段落:', activeContentBlockId);
    console.log('🎯 [高亮计算] 使用映射类型:', hasDynamicMapping ? '动态' : '静态');
    console.log('🎯 [高亮计算] 映射结果:', mappedNodeId);
    
    return mappedNodeId || null;
  }, [activeContentBlockId, dynamicMapping.textToNodeMap, textToNodeMap]);

  // 处理节点点击事件
  const handleNodeClick = useCallback((nodeId) => {
    console.log('🖱️ [父组件] 接收到节点点击事件:', nodeId);
    
    // 🔑 方案1：点击只负责导航，不负责高亮
    // 高亮由滚动检测系统统一管理，确保状态一致
    console.log('🖱️ [点击导航] 滚动到对应文本块，高亮由滚动检测自动处理');
    
    // 滚动到对应文本块，滚动完成后滚动检测会自动处理高亮
    scrollToContentBlock(nodeId);
  }, [scrollToContentBlock]);

  // 🔑 新增：处理节点标签更新的回调函数
  const handleNodeLabelUpdate = useCallback((nodeId, newLabel) => {
    console.log('📝 [节点标签更新] 同步更新document状态:', nodeId, '->', newLabel);
    
    // 同步更新document.node_mappings_demo中的对应节点标签
    setDocument(prevDoc => {
      if (!prevDoc || !prevDoc.node_mappings_demo) {
        console.warn('📝 [节点标签更新] document或node_mappings_demo不存在，跳过更新');
        return prevDoc;
      }
      
      const newNodeMappings = { ...prevDoc.node_mappings_demo };
      if (newNodeMappings[nodeId]) {
        newNodeMappings[nodeId] = { 
          ...newNodeMappings[nodeId], 
          text_snippet: newLabel 
        };
        console.log('📝 [节点标签更新] ✅ document状态已同步更新');
      } else {
        console.warn('📝 [节点标签更新] 节点ID在node_mappings中不存在:', nodeId);
      }
      
      return { 
        ...prevDoc, 
        node_mappings_demo: newNodeMappings 
      };
    });
  }, [setDocument]);

  // 创建动态映射的辅助函数
  const createDynamicMapping = useCallback((chunks, mermaidCode, nodeMapping) => {
    console.log('🔗 [映射创建] 开始创建动态映射');
    console.log('🔗 [映射创建] chunks数量:', chunks?.length);
    console.log('🔗 [映射创建] mermaidCode长度:', mermaidCode?.length);
    console.log('🔗 [映射创建] nodeMapping类型:', typeof nodeMapping);
    
    if (!mermaidCode || !nodeMapping) {
      console.warn('🔗 [映射创建] 缺少必要参数，跳过映射创建');
      return;
    }
    
    const newTextToNodeMap = {};
    const newNodeToTextMap = {};
    
    if (nodeMapping && typeof nodeMapping === 'object') {
      console.log('🔗 [映射创建] 基于AI语义块创建段落级映射');
      console.log('🔗 [映射创建] nodeMapping键数量:', Object.keys(nodeMapping).length);
      
      // 为每个AI语义块创建映射
      Object.entries(nodeMapping).forEach(([nodeId, nodeInfo]) => {
        console.log(`🔗 [映射创建] 处理节点 ${nodeId}:`, nodeInfo);
        
        if (nodeInfo && nodeInfo.paragraph_ids && Array.isArray(nodeInfo.paragraph_ids)) {
          console.log(`🔗 [映射创建] 节点 ${nodeId} 包含段落:`, nodeInfo.paragraph_ids);
          
          // 为每个段落创建到节点的映射
          nodeInfo.paragraph_ids.forEach(paraId => {
            if (paraId && typeof paraId === 'string') {
              // 统一段落ID格式
              const paragraphId = paraId.startsWith('para-') ? paraId : `para-${paraId}`;
              
              // 段落到节点的映射（多对一：多个段落可能对应同一个节点）
              newTextToNodeMap[paragraphId] = nodeId;
              
              console.log(`📍 [映射创建] ${paragraphId} -> 节点 ${nodeId}`);
            } else {
              console.warn(`📍 [映射创建] 无效的段落ID:`, paraId);
            }
          });
          
          // 节点到段落组的映射（一对多：一个节点对应多个段落）
          newNodeToTextMap[nodeId] = nodeInfo.paragraph_ids.map(paraId => 
            paraId.startsWith('para-') ? paraId : `para-${paraId}`
          );
          
          console.log(`🔗 [映射创建] 节点 ${nodeId} -> 段落组 [${newNodeToTextMap[nodeId].join(', ')}]`);
        } else {
          console.warn(`🔗 [映射创建] 节点 ${nodeId} 缺少有效的段落ID数组:`, nodeInfo);
        }
      });
      
      console.log('🔗 [映射创建] 映射创建完成');
      console.log('🔗 [映射创建] 段落到节点映射数量:', Object.keys(newTextToNodeMap).length);
      console.log('🔗 [映射创建] 节点到段落映射数量:', Object.keys(newNodeToTextMap).length);
      
      // 调用updateDynamicMapping来更新状态
      updateDynamicMapping(newTextToNodeMap, newNodeToTextMap);
    } else {
      console.warn('🔗 [映射创建] nodeMapping无效，跳过映射创建');
    }
  }, [updateDynamicMapping]);

  // 文档查看区域切换按钮
  const ViewModeToggle = () => {
    if (!isPdfFile) return null;

    return (
      <div className="flex bg-gray-100 dark:bg-gray-700 p-0.5 rounded mb-2">
        <button
          onClick={() => setViewMode('markdown')}
          className={`flex-1 flex items-center justify-center px-2 py-1 rounded text-xs font-medium transition-all ${
            viewMode === 'markdown'
              ? 'bg-white dark:bg-gray-600 text-blue-600 dark:text-blue-400 shadow-sm'
              : 'text-gray-600 dark:text-gray-400 hover:text-gray-800 dark:hover:text-gray-200'
          }`}
        >
          <FileText className="h-3 w-3 mr-1" />
          转换后的Markdown
        </button>
        <button
          onClick={() => setViewMode('pdf')}
          className={`flex-1 flex items-center justify-center px-2 py-1 rounded text-xs font-medium transition-all ${
            viewMode === 'pdf'
              ? 'bg-white dark:bg-gray-600 text-red-600 dark:text-red-400 shadow-sm'
              : 'text-gray-600 dark:text-gray-400 hover:text-gray-800 dark:hover:text-gray-200'
          }`}
        >
          <File className="h-3 w-3 mr-1" />
          原始PDF文件
        </button>
      </div>
    );
  };

  // 跟踪chunks加载状态
  const [chunksLoaded, setChunksLoaded] = useState(false);

  // 当documentId改变时，重置chunks加载状态
  useEffect(() => {
    setChunksLoaded(false);
    contentChunks.current = []; // 也清空之前的chunks
  }, [documentId]);

  // 在文档加载完成后，加载文档结构和chunks
  useEffect(() => {
    // 只对真实上传的文档（非示例模式）加载结构，且只加载一次
    if (document && !documentId.startsWith('demo-') && document.content && !chunksLoaded) {
      const loadChunks = async () => {
        console.log('📄 [文档加载] 开始加载文档结构和chunks');
        const chunks = await loadDocumentStructure();
        if (chunks && chunks.length > 0) {
          contentChunks.current = chunks;
          setChunksLoaded(true); // 设置chunks加载完成标志
          console.log('📄 [文档加载] 成功设置chunks到contentChunks.current，数量:', chunks.length);
        } else {
          console.log('📄 [文档加载] 没有获取到chunks数据');
        }
      };
      
      loadChunks();
    }
  }, [document, documentId, loadDocumentStructure, chunksLoaded]);

  // 🔑 新增：防止动态映射重复执行的标志
  const mappingInitialized = useRef(false);

  // 在文档、chunks和思维导图都加载完成后，创建动态映射
  useEffect(() => {
    // 🔑 只有在所有条件满足，并且映射尚未初始化时，才执行
    if (!documentId.startsWith('demo-') && document && document.content && chunksLoaded && !mappingInitialized.current) {
      const mermaidCode = document.mermaid_code_demo;
      const nodeMapping = document.node_mappings_demo;
      
      console.log('🔗 [主组件动态映射] useEffect触发条件检查:');
      console.log('🔗 [主组件动态映射] documentId是否非demo:', !documentId.startsWith('demo-'));
      console.log('🔗 [主组件动态映射] document存在:', !!document);
      console.log('🔗 [主组件动态映射] document.content存在:', !!document?.content);
      console.log('🔗 [主组件动态映射] chunksLoaded:', chunksLoaded);
      console.log('🔗 [主组件动态映射] mappingInitialized.current:', mappingInitialized.current);
      console.log('🔗 [主组件动态映射] contentChunks.current数量:', contentChunks.current?.length || 0);
      console.log('🔗 [主组件动态映射] mermaidCode存在:', !!mermaidCode);
      console.log('🔗 [主组件动态映射] mermaidCode长度:', mermaidCode?.length || 0);
      console.log('🔗 [主组件动态映射] nodeMapping存在:', !!nodeMapping);
      console.log('🔗 [主组件动态映射] nodeMapping类型:', typeof nodeMapping);
      console.log('🔗 [主组件动态映射] nodeMapping内容:', nodeMapping);
      console.log('🔗 [主组件动态映射] nodeMapping键数量:', nodeMapping ? Object.keys(nodeMapping).length : 0);
      
      if (mermaidCode && contentChunks.current.length > 0) {
        console.log('🔗 [主组件] 🚀 正在进行首次动态映射创建...');
        console.log('🔗 [主组件] 参数检查 - chunks数量:', contentChunks.current.length);
        console.log('🔗 [主组件] 参数检查 - mermaidCode前100字符:', mermaidCode.substring(0, 100));
        console.log('🔗 [主组件] 参数检查 - nodeMapping详情:', JSON.stringify(nodeMapping, null, 2));
        
        // 调用更新动态映射函数
        console.log('🔗 [主组件] 📞 正在调用createDynamicMapping...');
        createDynamicMapping(contentChunks.current, mermaidCode, nodeMapping);
        console.log('🔗 [主组件] ✅ createDynamicMapping调用完成');
        
        // 🔑 关键：标记为已初始化，防止重复执行
        mappingInitialized.current = true;
        console.log('🔗 [主组件] 🔒 映射已标记为初始化完成，防止重复执行');
      } else {
        console.log('🔗 [主组件] ❌ 动态映射创建条件不满足:');
        if (!mermaidCode) {
          console.log('🔗 [主组件] - 缺少mermaidCode，等待思维导图生成完成...');
        }
        if (contentChunks.current.length === 0) {
          console.log('🔗 [主组件] - 缺少contentChunks，chunks数量:', contentChunks.current.length);
        }
      }
    } else {
      console.log('🔗 [主组件动态映射] useEffect触发条件不满足:');
      console.log('🔗 [主组件动态映射] - documentId:', documentId);
      console.log('🔗 [主组件动态映射] - 是否demo模式:', documentId.startsWith('demo-'));
      console.log('🔗 [主组件动态映射] - document存在:', !!document);
      console.log('🔗 [主组件动态映射] - chunksLoaded:', chunksLoaded);
      console.log('🔗 [主组件动态映射] - mappingInitialized.current:', mappingInitialized.current);
    }
  }, [document, chunksLoaded, createDynamicMapping, documentId]);

  // 调试文档状态
  useEffect(() => {
    if (document) {
      console.log('📄 [文档调试] 文档加载完成，基本信息:');
      console.log('📄 [文档调试] - documentId:', documentId);
      console.log('📄 [文档调试] - 是否demo模式:', documentId.startsWith('demo-'));
      console.log('📄 [文档调试] - document.content存在:', !!document.content);
      console.log('📄 [文档调试] - document.content长度:', document.content?.length || 0);
      console.log('📄 [文档调试] - document.mermaid_code_demo存在:', !!document.mermaid_code_demo);
      console.log('📄 [文档调试] - document.mermaid_code_demo长度:', document.mermaid_code_demo?.length || 0);
      console.log('📄 [文档调试] - document.node_mappings_demo存在:', !!document.node_mappings_demo);
      console.log('📄 [文档调试] - document.node_mappings_demo类型:', typeof document.node_mappings_demo);
      if (document.node_mappings_demo) {
        console.log('📄 [文档调试] - node_mappings_demo键数量:', Object.keys(document.node_mappings_demo).length);
        console.log('📄 [文档调试] - node_mappings_demo样本键:', Object.keys(document.node_mappings_demo).slice(0, 3));
      }
      console.log('📄 [文档调试] - 完整document对象:', document);
      
      // 暴露全局调试函数
      if (typeof window !== 'undefined') {
        window.debugDocument = () => {
          console.log('=== 📄 文档调试信息 ===');
          console.log('文档ID:', documentId);
          console.log('文档对象:', document);
          console.log('chunks加载状态:', chunksLoaded);
          console.log('chunks数据:', contentChunks.current);
          console.log('思维导图代码:', document?.mermaid_code_demo?.substring(0, 200) + '...');
          console.log('节点映射:', document?.node_mappings_demo);
          console.log('=== 📄 调试信息结束 ===');
          return {
            documentId,
            document,
            chunksLoaded,
            chunks: contentChunks.current,
            mermaidCode: document?.mermaid_code_demo,
            nodeMapping: document?.node_mappings_demo
          };
        };
        console.log('🔧 [全局调试] debugDocument函数已挂载，可在控制台调用 window.debugDocument()');
      }
    }
  }, [document, documentId, chunksLoaded]);

  // 🔑 新增：添加子节点的回调函数
  const handleAddChildNode = useCallback(async (parentNodeId) => {
    try {
      console.log('🆕 [父组件] 添加子节点:', parentNodeId);
      
      // 生成新节点ID和边ID（使用时间戳确保唯一性）
      const newNodeId = `node_${Date.now()}`;
      const newEdgeId = `edge_${parentNodeId}_${newNodeId}`;
      const newNodeLabel = '新节点';
      
      // 更新document状态
      setDocument(prevDoc => {
        if (!prevDoc) {
          console.warn('🆕 [父组件] document不存在，无法添加子节点');
          return prevDoc;
        }
        
        // 创建新的node_mappings
        const newNodeMappings = {
          ...prevDoc.node_mappings_demo,
          [newNodeId]: {
            text_snippet: newNodeLabel,
            paragraph_ids: []
          }
        };
        
        // 创建新的edges（如果存在edges数组）
        const newEdges = prevDoc.edges ? [
          ...prevDoc.edges,
          {
            id: newEdgeId,
            source: parentNodeId,
            target: newNodeId,
            type: 'smoothstep'
          }
        ] : [];
        
        // 更新mermaid代码（添加新的节点和连接）
        let updatedMermaidCode = prevDoc.mermaid_code_demo || '';
        if (updatedMermaidCode) {
          updatedMermaidCode += `\n    ${parentNodeId} --> ${newNodeId}[${newNodeLabel}]`;
        }
        
        console.log('🆕 [父组件] 子节点添加完成，新节点ID:', newNodeId);
        
        return {
          ...prevDoc,
          node_mappings_demo: newNodeMappings,
          edges: newEdges,
          mermaid_code_demo: updatedMermaidCode
        };
      });
      
      // 如果不是示例模式，调用后端API
      if (!documentId.startsWith('demo-')) {
        try {
          // 这里可以添加后端API调用
          console.log('🆕 [父组件] 后端API调用暂未实现');
        } catch (apiError) {
          console.error('❌ [父组件] 添加子节点API调用失败:', apiError);
        }
      }
    } catch (error) {
      console.error('❌ [父组件] 添加子节点失败:', error);
    }
  }, [documentId, setDocument]);
  
  // 🔑 新增：添加同级节点的回调函数
  const handleAddSiblingNode = useCallback(async (siblingNodeId) => {
    try {
      console.log('🆕 [父组件] 添加同级节点:', siblingNodeId);
      
      // 从当前document的edges中找到同级节点的父节点
      const parentEdge = document?.edges?.find(edge => edge.target === siblingNodeId);
      if (!parentEdge && document?.mermaid_code_demo) {
        // 如果没有edges数组，尝试从mermaid代码中解析
        const mermaidLines = document.mermaid_code_demo.split('\n');
        const parentLine = mermaidLines.find(line => line.includes(`--> ${siblingNodeId}`));
        if (parentLine) {
          const match = parentLine.match(/(\w+)\s*-->\s*\w+/);
          if (match) {
            const parentNodeId = match[1];
            await addSiblingWithParent(siblingNodeId, parentNodeId);
            return;
          }
        }
        console.warn('❌ [父组件] 无法找到同级节点的父节点');
        return;
      }
      
      const parentNodeId = parentEdge?.source;
      if (!parentNodeId) {
        console.warn('❌ [父组件] 无法确定父节点ID');
        return;
      }
      
      await addSiblingWithParent(siblingNodeId, parentNodeId);
      
    } catch (error) {
      console.error('❌ [父组件] 添加同级节点失败:', error);
    }
  }, [document]);
  
  // 添加同级节点的辅助函数
  const addSiblingWithParent = useCallback(async (siblingNodeId, parentNodeId) => {
    const newNodeId = `node_${Date.now()}`;
    const newEdgeId = `edge_${parentNodeId}_${newNodeId}`;
    const newNodeLabel = '新节点';
    
    // 更新document状态
    setDocument(prevDoc => {
      if (!prevDoc) {
        console.warn('🆕 [父组件] document不存在，无法添加同级节点');
        return prevDoc;
      }
      
      // 创建新的node_mappings
      const newNodeMappings = {
        ...prevDoc.node_mappings_demo,
        [newNodeId]: {
          text_snippet: newNodeLabel,
          paragraph_ids: []
        }
      };
      
      // 创建新的edges（如果存在edges数组）
      const newEdges = prevDoc.edges ? [
        ...prevDoc.edges,
        {
          id: newEdgeId,
          source: parentNodeId,
          target: newNodeId,
          type: 'smoothstep'
        }
      ] : [];
      
      // 更新mermaid代码（添加新的节点和连接）
      let updatedMermaidCode = prevDoc.mermaid_code_demo || '';
      if (updatedMermaidCode) {
        updatedMermaidCode += `\n    ${parentNodeId} --> ${newNodeId}[${newNodeLabel}]`;
      }
      
      console.log('🆕 [父组件] 同级节点添加完成，新节点ID:', newNodeId);
      
      return {
        ...prevDoc,
        node_mappings_demo: newNodeMappings,
        edges: newEdges,
        mermaid_code_demo: updatedMermaidCode
      };
    });
  }, [setDocument]);
  
  // 🔑 新增：删除节点的回调函数
  const handleDeleteNode = useCallback(async (nodeIdToDelete) => {
    try {
      console.log('🗑️ [父组件] 删除节点:', nodeIdToDelete);
      
      // 更新document状态
      setDocument(prevDoc => {
        if (!prevDoc) {
          console.warn('🗑️ [父组件] document不存在，无法删除节点');
          return prevDoc;
        }
        
        // 移除节点映射
        const newNodeMappings = { ...prevDoc.node_mappings_demo };
        delete newNodeMappings[nodeIdToDelete];
        
        // 移除相关的edges（如果存在edges数组）
        const newEdges = prevDoc.edges ? 
          prevDoc.edges.filter(edge => 
            edge.source !== nodeIdToDelete && edge.target !== nodeIdToDelete
          ) : [];
        
        // 更新mermaid代码（移除相关的节点和连接）
        let updatedMermaidCode = prevDoc.mermaid_code_demo || '';
        if (updatedMermaidCode) {
          const lines = updatedMermaidCode.split('\n');
          const filteredLines = lines.filter(line => 
            !line.includes(nodeIdToDelete) && 
            !line.includes(`--> ${nodeIdToDelete}`) &&
            !line.includes(`${nodeIdToDelete} -->`)
          );
          updatedMermaidCode = filteredLines.join('\n');
        }
        
        console.log('🗑️ [父组件] 节点删除完成');
        
        return {
          ...prevDoc,
          node_mappings_demo: newNodeMappings,
          edges: newEdges,
          mermaid_code_demo: updatedMermaidCode
        };
      });
      
      // 如果不是示例模式，调用后端API
      if (!documentId.startsWith('demo-')) {
        try {
          // 这里可以添加后端API调用
          console.log('🗑️ [父组件] 后端API调用暂未实现');
        } catch (apiError) {
          console.error('❌ [父组件] 删除节点API调用失败:', apiError);
        }
      }
    } catch (error) {
      console.error('❌ [父组件] 删除节点失败:', error);
    }
  }, [documentId, setDocument]);

  // 处理 node_mappings 更新的函数
  const handleNodeMappingUpdate = useCallback(async (newNodeMappings) => {
    try {
      console.log('📍 [节点映射更新] 开始更新 node_mappings:', newNodeMappings);
      
      // 更新前端状态
      setDocument(prev => ({
        ...prev,
        node_mappings_demo: newNodeMappings
      }));
      
      console.log('📍 [节点映射更新] 前端状态已更新');
      
      // 如果不是示例模式，调用后端API进行持久化
      if (!documentId.startsWith('demo-')) {
        console.log('📍 [节点映射更新] 开始调用后端API保存映射');
        
        const response = await axios.post(`http://localhost:8000/api/document/${documentId}/remap`, {
          node_mappings: newNodeMappings
        });
        
        if (response.data.success) {
          console.log('📍 [节点映射更新] ✅ 后端保存成功');
          toast.success('拖拽排序已保存');
        } else {
          console.error('📍 [节点映射更新] ❌ 后端保存失败:', response.data.message);
          toast.error('保存失败: ' + response.data.message);
        }
      } else {
        console.log('📍 [节点映射更新] 示例模式，跳过后端保存');
      }
      
      // 更新动态映射以反映新的节点关系
      if (contentChunks.current.length > 0 && document && document.mermaid_code_demo) {
        console.log('📍 [节点映射更新] 重新生成动态映射');
        createDynamicMapping(contentChunks.current, document.mermaid_code_demo, newNodeMappings);
      }
      
    } catch (error) {
      console.error('📍 [节点映射更新] 错误:', error);
      const errorMessage = error.response?.data?.detail || '保存节点映射失败';
      toast.error(errorMessage);
    }
  }, [documentId, setDocument, createDynamicMapping, document]);

  // 处理拖拽排序后的回调函数
  const handleOrderChange = useCallback(async (newItems) => {
    try {
      console.log('📍 [排序更新] 开始处理拖拽排序结果');
      console.log('📍 [排序更新] 新项目顺序数组长度:', newItems?.length || 0);
      console.log('📍 [排序更新] 新项目顺序:', newItems);
      
      // 健壮性检查
      if (!newItems || newItems.length === 0) {
        console.warn('📍 [排序更新] ⚠️ 新项目数组为空，跳过处理');
        return;
      }
      
      // 健壮性检查：确保 document 对象存在
      const docObj = document;
      if (!docObj) {
        console.warn('📍 [排序更新] ⚠️ document 对象不存在，跳过处理');
        return;
      }
      
      // 重新计算 node_mappings - 使用 SortableContentRenderer 中的重构版本逻辑
      const recalculateNodeMappings = (sortedItems) => {
        console.log('📍 [排序更新-重新计算] 开始重新计算 node_mappings');
        console.log('📍 [排序更新-重新计算] 输入参数:', { 
          sortedItemsLength: sortedItems?.length || 0, 
          nodeMapping: !!docObj.node_mappings_demo
        });
        
        // 健壮性检查：如果输入的 items 数组为空，返回空的 node_mappings 对象
        if (!sortedItems || sortedItems.length === 0) {
          console.log('📍 [排序更新-重新计算] ⚠️ 输入项目为空，返回空映射');
          return {};
        }
        
        if (!docObj.node_mappings_demo) {
          console.log('📍 [排序更新-重新计算] ⚠️ 缺少节点映射，跳过重新计算');
          return {};
        }
        
        const newNodeMappings = {};
        let currentNodeId = null;
        
        // 获取第一个节点ID作为默认值，处理段落出现在所有分割线之前的边界情况
        const firstNodeId = Object.keys(docObj.node_mappings_demo)[0];
        console.log('📍 [排序更新-重新计算] 默认第一个节点ID:', firstNodeId);
        
        // 遍历排序后的项目列表
        sortedItems.forEach((item, index) => {
          if (item.type === 'divider') {
            // 遇到分割线，设置当前节点ID
            currentNodeId = item.nodeId;
            console.log(`📍 [排序更新-重新计算] 位置 ${index}: 进入节点 ${currentNodeId}`);
          } else if (item.type === 'paragraph') {
            // 遇到段落，将其分配给当前节点
            // 如果还没有遇到分割线，使用第一个节点作为默认值
            const targetNodeId = currentNodeId || firstNodeId;
            
            if (targetNodeId) {
              // 确保 newNodeMappings[targetNodeId] 已经存在并且是一个包含 paragraph_ids 数组的对象
              if (!newNodeMappings[targetNodeId]) {
                // 从原始 nodeMapping 中复制节点信息
                newNodeMappings[targetNodeId] = {
                  ...docObj.node_mappings_demo[targetNodeId],
                  paragraph_ids: []
                };
                console.log(`📍 [排序更新-重新计算] 初始化节点 ${targetNodeId} 的映射`);
              }
              
              // 将段落ID添加到当前节点
              newNodeMappings[targetNodeId].paragraph_ids.push(item.paragraphId);
              console.log(`📍 [排序更新-重新计算] 位置 ${index}: 段落 ${item.paragraphId} 分配给节点 ${targetNodeId}`);
            } else {
              console.warn(`📍 [排序更新-重新计算] 警告: 段落 ${item.paragraphId} 在位置 ${index} 没有对应的节点`);
            }
          }
        });
        
        console.log('📍 [排序更新-重新计算] 新的 node_mappings:', newNodeMappings);
        return newNodeMappings;
      };
      
      // 重新计算节点映射
      const newNodeMappings = recalculateNodeMappings(newItems);
      
      if (Object.keys(newNodeMappings).length === 0) {
        console.warn('📍 [排序更新] ⚠️ 重新计算结果为空，跳过后续处理');
        return;
      }
      
      console.log('📍 [排序更新] 开始更新前端状态');
      
      // 更新前端状态
      setDocument(prev => {
        if (!prev) {
          console.warn('📍 [排序更新] ⚠️ 前一个文档状态不存在，无法更新');
          return prev;
        }
        
        const updatedDocument = {
          ...prev,
          node_mappings_demo: newNodeMappings
        };
        console.log('📍 [排序更新] 前端状态已更新');
        
        // 立即重新生成动态映射
        if (contentChunks.current.length > 0 && prev.mermaid_code_demo) {
          console.log('📍 [排序更新] 重新生成动态映射');
          createDynamicMapping(contentChunks.current, prev.mermaid_code_demo, newNodeMappings);
        }
        
        return updatedDocument;
      });
      
      console.log('📍 [排序更新] 开始调用后端API保存映射');
      
      // 如果不是示例模式，调用后端API进行持久化
      if (!documentId.startsWith('demo-')) {
        console.log('📍 [排序更新] 调用后端API保存节点映射');
        
        const response = await axios.post(`http://localhost:8000/api/document/${documentId}/remap`, {
          node_mappings: newNodeMappings
        });
        
        if (response.data.success) {
          console.log('📍 [排序更新] ✅ 后端保存成功');
          toast.success('拖拽排序已保存');
        } else {
          console.error('📍 [排序更新] ❌ 后端保存失败:', response.data.message);
          toast.error('保存失败: ' + response.data.message);
        }
      } else {
        console.log('📍 [排序更新] 示例模式，跳过后端保存');
        toast.success('拖拽排序已更新（示例模式）');
      }
      
    } catch (error) {
      console.error('📍 [排序更新] 错误:', error);
      const errorMessage = error.response?.data?.detail || '处理拖拽排序失败';
      toast.error(errorMessage);
    }
  }, [documentId, document, setDocument, createDynamicMapping]);

  // 加载状态
  if (loading) {
    return (
      <div className="min-h-screen flex items-center justify-center bg-gray-50 dark:bg-gray-900">
        <div className="text-center">
          <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mx-auto mb-4"></div>
          <p className="text-lg text-gray-700 dark:text-gray-300">正在加载文档...</p>
        </div>
      </div>
    );
  }

  // 错误状态
  if (documentError) {
    return (
      <div className="min-h-screen flex items-center justify-center bg-gray-50 dark:bg-gray-900">
        <div className="text-center max-w-md">
          <div className="bg-red-50 dark:bg-red-900/20 border border-red-200 dark:border-red-700 rounded-lg p-6">
            <h2 className="text-xl font-semibold text-red-800 dark:text-red-200 mb-2">加载失败</h2>
            <p className="text-red-600 dark:text-red-400 mb-4">{documentError}</p>
            <div className="space-x-3">
              <button
                onClick={loadDocument}
                className="inline-flex items-center px-4 py-2 bg-red-600 dark:bg-red-500 text-white rounded-md hover:bg-red-700 dark:hover:bg-red-600 transition-colors"
              >
                重试
              </button>
              <button
                onClick={() => navigate('/')}
                className="inline-flex items-center px-4 py-2 bg-gray-600 dark:bg-gray-500 text-white rounded-md hover:bg-gray-700 dark:hover:bg-gray-600 transition-colors"
              >
                <ArrowLeft className="w-4 h-4 mr-2" />
                返回
              </button>
            </div>
          </div>
        </div>
      </div>
    );
  }

  // 文档不存在
  if (!document) {
    return (
      <div className="min-h-screen flex items-center justify-center bg-gray-50 dark:bg-gray-900">
        <div className="text-center">
          <p className="text-lg text-gray-700 dark:text-gray-300">文档不存在</p>
          <button
            onClick={() => navigate('/')}
            className="mt-4 inline-flex items-center px-4 py-2 bg-blue-600 dark:bg-blue-500 text-white rounded-md hover:bg-blue-700 dark:hover:bg-blue-600 transition-colors"
          >
            <ArrowLeft className="w-4 h-4 mr-2" />
            返回首页
          </button>
        </div>
      </div>
    );
  }

  return (
    <div className="h-screen bg-gray-50 dark:bg-gray-900 overflow-hidden flex flex-col">
      {/* 三列分割容器 */}
      <div ref={containerRef} className="flex flex-1 h-full">
        
        {/* 左侧目录栏 */}
        {showToc && (
          <div 
            className="bg-white dark:bg-gray-800 border-r border-gray-200 dark:border-gray-700 shadow-sm overflow-hidden flex flex-col"
            style={{ width: `${tocPanelWidth}%` }}
          >
            <div className="px-3 py-2 border-b border-gray-200 dark:border-gray-700 bg-gray-50 dark:bg-gray-700 flex-shrink-0">
              <div className="flex items-center justify-between">
                <h2 className="text-sm font-semibold text-gray-900 dark:text-white flex items-center">
                  <FileText className="w-3 h-3 mr-1" />
                  文档目录
                </h2>
                <button
                  onClick={() => setShowToc(false)}
                  className="text-gray-400 dark:text-gray-500 hover:text-gray-600 dark:hover:text-gray-300 transition-colors"
                >
                  <EyeOff className="w-3 h-3" />
                </button>
              </div>
            </div>
            <div className="flex-1 overflow-y-auto">
              <TableOfContents 
                toc={toc}
                expandedItems={expandedTocItems}
                activeItem={activeChunkId}
                onToggle={toggleTocItem}
                onItemClick={scrollToSection}
              />
            </div>
          </div>
        )}
        
        {/* 目录分隔线 */}
        {showToc && (
          <div
            className="w-1 bg-gray-300 dark:bg-gray-600 hover:bg-blue-500 dark:hover:bg-blue-600 cursor-col-resize flex-shrink-0 transition-colors"
            onMouseDown={(e) => handleMouseDown(e, 'toc-divider')}
          >
            <div className="w-full h-full flex items-center justify-center">
              <div className="w-0.5 h-8 bg-white dark:bg-gray-700 opacity-50 rounded"></div>
            </div>
          </div>
        )}

        {/* 中间文档阅读器 */}
        <div 
          className="bg-white dark:bg-gray-800 border-r border-gray-200 dark:border-gray-700 shadow-sm overflow-hidden flex flex-col"
          style={{ width: `${showToc ? leftPanelWidth : leftPanelWidth + tocPanelWidth}%` }}
        >
          <div className="px-3 py-2 border-b border-gray-200 dark:border-gray-700 bg-gray-50 dark:bg-gray-700 flex-shrink-0">
            <div className="flex items-center justify-between">
              <div className="flex items-center space-x-2">
                <button
                  onClick={() => navigate('/')}
                  className="inline-flex items-center px-2 py-1 text-xs text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white transition-colors"
                >
                  <ArrowLeft className="w-3 h-3 mr-1" />
                  返回
                </button>
                {!showToc && (
                  <button
                    onClick={() => setShowToc(true)}
                    className="inline-flex items-center px-2 py-1 text-xs text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white transition-colors"
                  >
                    <Eye className="w-3 h-3 mr-1" />
                    目录
                  </button>
                )}
                <h2 className="text-sm font-semibold text-gray-900 dark:text-white">
                  文档内容
                  {isPdfFile && (
                    <span className="ml-2 text-xs text-gray-500 dark:text-gray-400">
                      ({viewMode === 'pdf' ? '原始PDF' : '转换后的Markdown'})
                    </span>
                  )}
                </h2>
              </div>
              <div className="flex items-center space-x-2">
                <ThemeToggle className="scale-75" />
                {/* 调试按钮 - 只在非示例模式下显示 */}
                {!documentId.startsWith('demo-') && (
                  <button
                    onClick={() => {
                      console.log('=== 调试信息 ===');
                      console.log('文档ID:', documentId);
                      console.log('当前活跃章节ID:', activeChunkId);
                      console.log('当前活跃段落ID:', activeContentBlockId);
                      console.log('chunks数量:', contentChunks.current?.length || 0);
                      console.log('chunks列表:', contentChunks.current?.map(c => c.chunk_id) || []);
                      console.log('动态映射:', dynamicMapping);
                      console.log('思维导图代码长度:', document?.mermaid_code_demo?.length || 0);
                      console.log('节点映射:', document?.node_mappings_demo);
                      console.log('原始内容长度:', document?.content?.length || 0);
                      console.log('带段落ID内容长度:', document?.content_with_ids?.length || 0);
                      console.log('带段落ID内容前100字符:', document?.content_with_ids?.substring(0, 100) || '无');
                      
                      // 检查页面中的段落元素
                      const allParagraphs = document.querySelectorAll('[id^="para-"], [data-para-id]');
                      console.log('页面中的段落数量:', allParagraphs.length);
                      console.log('段落ID列表:', Array.from(allParagraphs).map(el => el.id || el.getAttribute('data-para-id')));
                      
                      // 显示localStorage中的调试数据
                      const debugData = {
                        textToNodeMap: JSON.parse(localStorage.getItem('debug_semanticTextToNodeMap') || '{}'),
                        nodeToTextMap: JSON.parse(localStorage.getItem('debug_semanticNodeToTextMap') || '{}'),
                        aiNodeMapping: JSON.parse(localStorage.getItem('debug_aiNodeMapping') || '{}')
                      };
                      console.log('localStorage调试数据:', debugData);
                      
                      alert(`调试信息已输出到控制台\n当前活跃章节: ${activeChunkId || '无'}\n当前活跃段落: ${activeContentBlockId || '无'}\n段落数量: ${allParagraphs.length}`);
                    }}
                    className="inline-flex items-center px-2 py-1 text-xs bg-purple-600 dark:bg-purple-500 text-white rounded hover:bg-purple-700 dark:hover:bg-purple-600 transition-colors"
                  >
                    🐛 调试
                  </button>
                )}
                <button
                  onClick={handleDownloadMarkdown}
                  className="inline-flex items-center px-2 py-1 text-xs bg-green-600 dark:bg-green-500 text-white rounded hover:bg-green-700 dark:hover:bg-green-600 transition-colors"
                >
                  <Download className="w-3 h-3 mr-1" />
                  下载MD
                </button>
              </div>
            </div>
            <ViewModeToggle />
          </div>
          <div className={`flex-1 ${viewMode === 'pdf' && isPdfFile ? 'overflow-hidden' : 'overflow-y-auto p-4'}`}>
            {(() => {
              // PDF文件模式
              if (viewMode === 'pdf' && isPdfFile) {
                return <PDFViewer pdfBase64={document.pdf_base64} />;
              }
              
              // 纯示例模式（demo-开头且没有真实内容）
              if (documentId.startsWith('demo-') && !document.content) {
                console.log('📄 [渲染判断] 纯示例模式');
                return (
                  <DemoModeRenderer 
                    content={null}
                    onContentBlockRef={handleContentBlockRef}
                    nodeMapping={document.node_mappings_demo}
                    onNodeMappingUpdate={handleNodeMappingUpdate}
                    onOrderChange={handleOrderChange}
                  />
                );
              }
              
              // 上传文件模式 - 等待chunks加载
              if (!documentId.startsWith('demo-') && !chunksLoaded) {
                console.log('📄 [渲染判断] 上传文件模式 - 等待chunks加载');
                return (
                  <div className="flex items-center justify-center h-full">
                    <div className="text-center">
                      <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
                      <p className="text-sm text-gray-500 dark:text-gray-400">正在加载文档结构...</p>
                    </div>
                  </div>
                );
              }
              
              // 上传文件模式 - chunks已加载 或 带内容的示例模式
              console.log('📄 [渲染判断] 渲染真实文档内容', {
                documentId, 
                chunksLoaded, 
                chunksCount: contentChunks.current.length,
                hasContent: !!document.content,
                hasContentWithIds: !!document.content_with_ids
              });
              
              // 优先使用带段落ID的内容，如果不存在则使用原始内容
              const contentToRender = document.content_with_ids || document.content;
              console.log('📄 [内容选择] 使用内容类型:', document.content_with_ids ? '带段落ID的内容' : '原始内容');
              
              return (
                <DemoModeRenderer 
                  content={contentToRender}
                  onContentBlockRef={handleContentBlockRef}
                  isRealDocument={!documentId.startsWith('demo-')}
                  chunks={contentChunks.current}
                  nodeMapping={document.node_mappings_demo}
                  onNodeMappingUpdate={handleNodeMappingUpdate}
                  onOrderChange={handleOrderChange}
                />
              );
            })()}
          </div>
        </div>

        {/* 主分隔线 */}
        <div
          className="w-1 bg-gray-300 dark:bg-gray-600 hover:bg-blue-500 dark:hover:bg-blue-600 cursor-col-resize flex-shrink-0 transition-colors"
          onMouseDown={(e) => handleMouseDown(e, 'main-divider')}
        >
          <div className="w-full h-full flex items-center justify-center">
            <div className="w-0.5 h-8 bg-white dark:bg-gray-700 opacity-50 rounded"></div>
          </div>
        </div>

        {/* 右侧论证结构流程图 */}
        <div 
          className="bg-white dark:bg-gray-800 overflow-hidden flex flex-col"
          style={{ width: `${100 - (showToc ? tocPanelWidth : 0) - leftPanelWidth}%` }}
        >
          {/* 论证结构流程图区域 */}
          <div className="h-full flex flex-col">
            <div className="px-4 py-3 border-b border-gray-200 dark:border-gray-700 bg-gray-50 dark:bg-gray-700 flex-shrink-0">
              <div className="flex items-center justify-between">
                <h2 className="text-base font-semibold text-gray-900 dark:text-white">论证结构流程图</h2>
                <div className="flex items-center space-x-2">
                  <MindmapStatusDisplay />
                  {document.mermaid_code_demo && (
                    <button
                      onClick={() => handleDownloadMermaid('demo')}
                      className="inline-flex items-center px-2 py-1 text-xs bg-blue-600 dark:bg-blue-500 text-white rounded hover:bg-blue-700 dark:hover:bg-blue-600 transition-colors"
                    >
                      <Download className="w-3 h-3 mr-1" />
                      下载流程图
                    </button>
                  )}
                </div>
              </div>
              <div className="flex items-center justify-between mt-2">
                <div className="flex items-center space-x-1 text-xs text-gray-500 dark:text-gray-400">
                  <span>分析文档的核心论证结构和逻辑流向</span>
                </div>
              </div>
            </div>
            <div className="flex-1 overflow-hidden">
              {/* 流程图内容区域 */}
              {(demoMindmapStatus === 'completed' && document.mermaid_code_demo) ? (
                <div className="h-full overflow-hidden">
                  <FlowDiagram 
                    ref={mermaidDiagramRef}
                    apiData={{
                      mermaid_string: document.mermaid_code_demo,
                      node_mappings: document.node_mappings_demo || {},
                      document_id: documentId
                    }}
                    highlightedNodeId={highlightedNodeId}
                    onNodeClick={handleNodeClick}
                    onNodeLabelUpdate={handleNodeLabelUpdate}
                    onAddChildNode={handleAddChildNode}
                    onAddSiblingNode={handleAddSiblingNode}
                    onDeleteNode={handleDeleteNode}
                  />
                </div>
              ) : (
                <div className="flex items-center justify-center h-full">
                  <div className="text-center max-w-md px-4">
                    <div className="bg-gray-50 dark:bg-gray-700 border border-gray-200 dark:border-gray-600 rounded-lg p-4">
                      <h3 className="text-sm font-semibold text-gray-800 dark:text-gray-200 mb-3">生成论证结构流程图</h3>
                      
                      <button
                        onClick={() => startMindmapGeneration('demo')}
                        className="flex items-center justify-center px-4 py-3 bg-blue-600 dark:bg-blue-500 text-white rounded-lg hover:bg-blue-700 dark:hover:bg-blue-600 transition-colors w-full"
                        disabled={demoMindmapStatus === 'generating'}
                      >
                        {demoMindmapStatus === 'generating' ? (
                          <>
                            <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-white mr-2"></div>
                            <span>分析中...</span>
                          </>
                        ) : (
                          <>
                            <Eye className="w-4 h-4 mr-2" />
                            <span>开始分析</span>
                          </>
                        )}
                      </button>
                      
                      <p className="text-xs text-gray-500 dark:text-gray-400 mt-2">
                        将分析文档的核心论点和论证逻辑
                      </p>
                    </div>
                  </div>
                </div>
              )}
            </div>
          </div>
        </div>
      </div>
      <ToastContainer />
    </div>
  );
};

export default ViewerPageRefactored;
</file>

</files>
