This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/rules/senior-engineer-task-execution-rule.mdc
.env.example
.gitignore
.python-version
æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨æŠ€æœ¯å®ç°è¯¦è§£.md
AIè¾…åŠ©é˜…è¯»å™¨-å®Œæ•´å­¦ä¹ æ–‡æ¡£.md
api_responses/.gitignore
api_responses/README.md
Condaç¯å¢ƒä½¿ç”¨æŒ‡å—.md
document_parser.py
download_models.py
frontend/package.json
frontend/postcss.config.js
frontend/public/index.html
frontend/public/manifest.json
frontend/src/App.css
frontend/src/App.js
frontend/src/components/DocumentRenderer.css
frontend/src/components/DocumentRenderer.js
frontend/src/components/EditableNode.css
frontend/src/components/EditableNode.js
frontend/src/components/FlowDiagram.js
frontend/src/components/LogicalDivider.js
frontend/src/components/MermaidDiagram.js
frontend/src/components/PDFViewer.js
frontend/src/components/SortableDivider.js
frontend/src/components/SortableParagraph.js
frontend/src/components/TableOfContents.js
frontend/src/components/ThemeToggle.js
frontend/src/components/UploadPage.js
frontend/src/components/ViewerPageRefactored.js
frontend/src/contexts/ThemeContext.js
frontend/src/hooks/useDocumentViewer.js
frontend/src/hooks/useMindmapGeneration.js
frontend/src/hooks/usePanelResize.js
frontend/src/hooks/useScrollDetection.js
frontend/src/index.css
frontend/src/index.js
frontend/src/utils/api.js
frontend/src/utils/dataConverter.js
frontend/src/utils/flowDiagramExample.js
frontend/src/utils/layoutHelper.js
frontend/tailwind.config.js
markdown.md
mindmap_generator.py
package.json
README.md
requirements-web.txt
sample_input_document_as_markdown__durnovo_memo.md
sample_input_document_as_markdown__small.md
screenshots/mindmap-architecture.svg
start_conda_web_app.py
web_backend.py
WEBåº”ç”¨ä½¿ç”¨è¯´æ˜.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨æŠ€æœ¯å®ç°è¯¦è§£.md">
# æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨æŠ€æœ¯å®ç°è¯¦è§£

## ğŸ—ï¸ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†ææ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆç»“æ„åŒ–çš„æ€ç»´å¯¼å›¾ã€‚ç³»ç»Ÿé‡‡ç”¨ç°ä»£å¼‚æ­¥æ¶æ„ï¼Œæ”¯æŒå¤šç§APIæä¾›å•†ï¼Œå…·æœ‰å¼ºå¤§çš„å†…å®¹åˆ†æã€å»é‡å’Œå¯è§†åŒ–èƒ½åŠ›ã€‚

## ğŸ“š ç›®å½•

1. [æ ¸å¿ƒæ¶æ„è®¾è®¡](#æ ¸å¿ƒæ¶æ„è®¾è®¡)
2. [é…ç½®ç³»ç»Ÿ](#é…ç½®ç³»ç»Ÿ)
3. [å¤šAPIæä¾›å•†æ”¯æŒ](#å¤šapiæä¾›å•†æ”¯æŒ)
4. [æ–‡æ¡£ç±»å‹æ£€æµ‹](#æ–‡æ¡£ç±»å‹æ£€æµ‹)
5. [å†…å®¹æå–å¼•æ“](#å†…å®¹æå–å¼•æ“)
6. [æ™ºèƒ½å»é‡ç®—æ³•](#æ™ºèƒ½å»é‡ç®—æ³•)
7. [æˆæœ¬æ§åˆ¶ç³»ç»Ÿ](#æˆæœ¬æ§åˆ¶ç³»ç»Ÿ)
8. [è¾“å‡ºæ ¼å¼ç”Ÿæˆ](#è¾“å‡ºæ ¼å¼ç”Ÿæˆ)
9. [é”™è¯¯å¤„ç†æœºåˆ¶](#é”™è¯¯å¤„ç†æœºåˆ¶)
10. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)

---

## ğŸ¯ æ ¸å¿ƒæ¶æ„è®¾è®¡

### ä¸»è¦ç»„ä»¶ç»“æ„

```python
# æ ¸å¿ƒç±»ç»§æ‰¿å…³ç³»å’ŒèŒè´£åˆ†å·¥
class Config:
    """ç»Ÿä¸€é…ç½®ç®¡ç†ä¸­å¿ƒ"""
    
class TokenUsageTracker:
    """æˆæœ¬å’Œä½¿ç”¨æƒ…å†µè¿½è¸ª"""
    
class DocumentOptimizer:
    """æ–‡æ¡£ä¼˜åŒ–å’ŒAPIè°ƒç”¨ç®¡ç†"""
    
class MindMapGenerator:
    """æ ¸å¿ƒæ€ç»´å¯¼å›¾ç”Ÿæˆé€»è¾‘"""
    
class MinimalDatabaseStub:
    """è½»é‡çº§æ•°æ®å­˜å‚¨æ¥å£"""
```

### æ•°æ®æµæ¶æ„

```
æ–‡æ¡£è¾“å…¥ â†’ ç±»å‹æ£€æµ‹ â†’ ä¸»é¢˜æå– â†’ å­ä¸»é¢˜æå– â†’ è¯¦ç»†ä¿¡æ¯æå– â†’ å»é‡éªŒè¯ â†’ æ ¼å¼ç”Ÿæˆ â†’ è¾“å‡º
    â†“         â†“         â†“          â†“            â†“          â†“        â†“
  ç¼“å­˜å±‚   æç¤ºå·¥ç¨‹   å¹¶å‘å¤„ç†    æ™ºèƒ½åˆ†å—     ç›¸ä¼¼åº¦æ£€æµ‹   éªŒè¯å±‚   å¤šæ ¼å¼
```

---

## âš™ï¸ é…ç½®ç³»ç»Ÿ

### ç¯å¢ƒå˜é‡ç®¡ç†

```python
from decouple import Config as DecoupleConfig, RepositoryEnv

# é…ç½®æ–‡ä»¶åŠ è½½æœºåˆ¶
config = DecoupleConfig(RepositoryEnv('.env'))

class Config:
    # APIé…ç½® - æ”¯æŒå¤šæä¾›å•†
    OPENAI_API_KEY = config.get("OPENAI_API_KEY")
    OPENAI_BASE_URL = config.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
    ANTHROPIC_API_KEY = config.get('ANTHROPIC_API_KEY')
    DEEPSEEK_API_KEY = config.get('DEEPSEEK_API_KEY')
    GEMINI_API_KEY = config.get('GEMINI_API_KEY')
    API_PROVIDER = config.get('API_PROVIDER')  # åŠ¨æ€é€‰æ‹©æä¾›å•†
    
    # æ¨¡å‹é…ç½® - é’ˆå¯¹ä¸åŒæä¾›å•†ä¼˜åŒ–
    CLAUDE_MODEL_STRING = "claude-3-5-haiku-latest"
    OPENAI_COMPLETION_MODEL = "gpt-4o-mini-2024-07-18"
    DEEPSEEK_COMPLETION_MODEL = "deepseek-chat"
    GEMINI_MODEL_STRING = "gemini-2.0-flash-lite"
    
    # æˆæœ¬æ§åˆ¶ - ç²¾ç¡®åˆ°å¾®ç¾å…ƒ
    OPENAI_INPUT_TOKEN_PRICE = 0.15/1000000
    OPENAI_OUTPUT_TOKEN_PRICE = 0.60/1000000
    # ... å…¶ä»–æä¾›å•†ä»·æ ¼é…ç½®
```

### åŠ¨æ€é…ç½®ç³»ç»Ÿ

```python
def __init__(self):
    self.config = {
        'max_summary_length': 2500,
        'max_tokens': 3000,
        'max_topics': 6,        # æœ€å¤§ä¸»é¢˜æ•°
        'max_subtopics': 4,     # æ¯ä¸ªä¸»é¢˜æœ€å¤§å­ä¸»é¢˜æ•°
        'max_details': 8,       # æ¯ä¸ªå­ä¸»é¢˜æœ€å¤§è¯¦ç»†ä¿¡æ¯æ•°
        'similarity_threshold': {
            'topic': 75,        # ä¸»é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
            'subtopic': 70,     # å­ä¸»é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
            'detail': 65        # è¯¦ç»†ä¿¡æ¯ç›¸ä¼¼åº¦é˜ˆå€¼
        },
        'reality_check': {
            'batch_size': 8,    # å¹¶è¡ŒéªŒè¯æ‰¹æ¬¡å¤§å°
            'min_verified_topics': 4,
            'min_verified_ratio': 0.6
        }
    }
```

---

## ğŸŒ å¤šAPIæä¾›å•†æ”¯æŒ

### ç»Ÿä¸€å®¢æˆ·ç«¯ç®¡ç†

```python
class DocumentOptimizer:
    def __init__(self):
        # åˆå§‹åŒ–å¤šä¸ªAPIå®¢æˆ·ç«¯
        self.openai_client = AsyncOpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL  # æ”¯æŒç¡…åŸºæµåŠ¨ç­‰å…¼å®¹æœåŠ¡
        )
        self.anthropic_client = AsyncAnthropic(api_key=Config.ANTHROPIC_API_KEY)
        self.deepseek_client = AsyncOpenAI(
            api_key=Config.DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        self.gemini_client = genai.configure(api_key=Config.GEMINI_API_KEY)
```

### æ™ºèƒ½APIè·¯ç”±

```python
async def generate_completion(self, prompt: str, max_tokens: int = 5000, 
                            request_id: str = None, task: Optional[str] = None):
    """ç»Ÿä¸€çš„APIè°ƒç”¨æ¥å£ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨è·¯ç”±åˆ°ç›¸åº”æä¾›å•†"""
    
    if Config.API_PROVIDER == "CLAUDE":
        kwargs = {
            "model": Config.CLAUDE_MODEL_STRING,
            "max_tokens": min(max_tokens, Config.CLAUDE_MAX_TOKENS),
            "messages": [{"role": "user", "content": prompt}]
        }
        response = await self.anthropic_client.messages.create(**kwargs)
        return response.content[0].text
        
    elif Config.API_PROVIDER == "DEEPSEEK":
        kwargs = {
            "model": Config.DEEPSEEK_COMPLETION_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": min(max_tokens, Config.DEEPSEEK_MAX_TOKENS)
        }
        
        # æ”¯æŒä¸åŒDeepSeekæ¨¡å‹
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            response = await self.deepseek_client.chat.completions.create(**kwargs)
        else:  # reasoneræ¨¡å‹
            response = await self.deepseek_client.chat.completions.create(**kwargs)
            
        return response.choices[0].message.content
        
    # ... å…¶ä»–æä¾›å•†å®ç°
```

---

## ğŸ” æ–‡æ¡£ç±»å‹æ£€æµ‹

### æ”¯æŒçš„æ–‡æ¡£ç±»å‹æšä¸¾

```python
class DocumentType(Enum):
    """æ”¯æŒçš„æ–‡æ¡£ç±»å‹ï¼Œæ¯ç§ç±»å‹å¯¹åº”ä¸åŒçš„åˆ†æç­–ç•¥"""
    TECHNICAL = auto()      # æŠ€æœ¯æ–‡æ¡£
    SCIENTIFIC = auto()     # ç§‘å­¦ç ”ç©¶
    NARRATIVE = auto()      # å™è¿°æ–‡æ¡£
    BUSINESS = auto()       # å•†ä¸šæ–‡æ¡£
    ACADEMIC = auto()       # å­¦æœ¯è®ºæ–‡
    LEGAL = auto()          # æ³•å¾‹æ–‡æ¡£
    MEDICAL = auto()        # åŒ»å­¦æ–‡æ¡£
    INSTRUCTIONAL = auto()  # æŒ‡å¯¼æ‰‹å†Œ
    ANALYTICAL = auto()     # åˆ†ææŠ¥å‘Š
    PROCEDURAL = auto()     # æµç¨‹æ–‡æ¡£
    GENERAL = auto()        # é€šç”¨æ–‡æ¡£
```

### ç±»å‹ç‰¹å®šçš„æç¤ºå·¥ç¨‹

```python
def _initialize_prompts(self) -> None:
    """ä¸ºæ¯ç§æ–‡æ¡£ç±»å‹åˆå§‹åŒ–ä¸“é—¨çš„æç¤ºæ¨¡æ¿"""
    self.type_specific_prompts = {
        DocumentType.TECHNICAL: {
            'topics': """åˆ†æè¿™ä¸ªæŠ€æœ¯æ–‡æ¡£ï¼Œå…³æ³¨æ ¸å¿ƒç³»ç»Ÿç»„ä»¶å’Œå…³ç³»ã€‚
            
é¦–å…ˆï¼Œè¯†åˆ«æ„æˆå®Œæ•´ç‹¬ç«‹åŠŸèƒ½å•å…ƒçš„ä¸»è¦æ¶æ„æˆ–æŠ€æœ¯ç»„ä»¶ã€‚
æ¯ä¸ªç»„ä»¶åº”è¯¥ï¼š
- æ˜¯ç‹¬ç‰¹çš„æŠ€æœ¯ç³»ç»Ÿã€æ¨¡å—æˆ–æµç¨‹
- è¶³å¤Ÿç‹¬ç«‹ï¼Œå¯ä»¥å•ç‹¬ç†è§£
- å¯¹æ•´ä½“ç³»ç»ŸåŠŸèƒ½è‡³å…³é‡è¦
- ä¸è‡³å°‘ä¸€ä¸ªå…¶ä»–ç»„ä»¶ç›¸è¿

é¿å…ä»¥ä¸‹ä¸»é¢˜ï¼š
- è¿‡äºç»†ç²’åº¦ï¼ˆå®ç°ç»†èŠ‚ï¼‰
- è¿‡äºå®½æ³›ï¼ˆæ•´ä¸ªç³»ç»Ÿç±»åˆ«ï¼‰
- æ²¡æœ‰ç³»ç»Ÿå½±å“çš„å­¤ç«‹åŠŸèƒ½
- çº¯æ–‡æ¡£å…ƒç´ 

æ€è€ƒï¼š
1. æ ¸å¿ƒæ„å»ºå—æ˜¯ä»€ä¹ˆï¼Ÿ
2. è¿™äº›éƒ¨åˆ†å¦‚ä½•ç»„åˆï¼Ÿ
3. ç»„ä»¶ä¹‹é—´å­˜åœ¨ä»€ä¹ˆä¾èµ–å…³ç³»ï¼Ÿ
4. å…³é”®çš„æŠ€æœ¯è¾¹ç•Œæ˜¯ä»€ä¹ˆï¼Ÿ

æ ¼å¼ï¼šè¿”å›è¡¨ç¤ºæœ€é«˜çº§æŠ€æœ¯æ„å»ºå—çš„ç»„ä»¶åç§°çš„JSONæ•°ç»„ã€‚""",

            'subtopics': """å¯¹äºæŠ€æœ¯ç»„ä»¶'{topic}'ï¼Œè¯†åˆ«å…¶å…³é”®å­ç»„ä»¶å’Œæ¥å£ã€‚

æ¯ä¸ªå­ä¸»é¢˜åº”è¯¥ï¼š
- ä»£è¡¨æ­¤ç»„ä»¶çš„å…³é”®æ–¹é¢
- å…·æœ‰æ˜ç¡®çš„æŠ€æœ¯èŒè´£
- ä¸ç³»ç»Ÿå…¶ä»–éƒ¨åˆ†æ¥å£
- å¯¹ç»„ä»¶çš„æ ¸å¿ƒç›®çš„æœ‰è´¡çŒ®

è€ƒè™‘ï¼š
1. æ­¤ç»„ä»¶æš´éœ²ä»€ä¹ˆæ¥å£ï¼Ÿ
2. å®ƒçš„å†…éƒ¨å­ç³»ç»Ÿæ˜¯ä»€ä¹ˆï¼Ÿ
3. å®ƒå¦‚ä½•å¤„ç†æ•°æ®æˆ–å¤„ç†è¯·æ±‚ï¼Ÿ
4. å®ƒä¸ºå…¶ä»–ç»„ä»¶æä¾›ä»€ä¹ˆæœåŠ¡ï¼Ÿ
5. å®ƒå®ç°ä»€ä¹ˆæŠ€æœ¯æ ‡å‡†æˆ–åè®®ï¼Ÿ

æ ¼å¼ï¼šè¿”å›æ„æˆæ­¤ç»„ä»¶æ¶æ„çš„æŠ€æœ¯å­ä¸»é¢˜åç§°çš„JSONæ•°ç»„ã€‚""",

            'details': """å¯¹äºæŠ€æœ¯å­ä¸»é¢˜'{subtopic}'ï¼Œè¯†åˆ«å…·ä½“çš„å®ç°æ–¹é¢å’Œè¦æ±‚ã€‚

å…³æ³¨ï¼š
1. å…³é”®ç®—æ³•æˆ–æ–¹æ³•
2. æ•°æ®ç»“æ„å’Œæ ¼å¼
3. åè®®è§„èŒƒ
4. æ€§èƒ½ç‰¹å¾
5. é”™è¯¯å¤„ç†æ–¹æ³•
6. å®‰å…¨è€ƒè™‘
7. ä¾èµ–å…³ç³»å’Œè¦æ±‚

åŒ…æ‹¬å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼š
- å®ç°ç‰¹å®šçš„
- å¯æµ‹é‡æˆ–å¯æµ‹è¯•çš„
- å¯¹ç†è§£è‡³å…³é‡è¦çš„
- ä¸é›†æˆç›¸å…³çš„

æ ¼å¼ï¼šè¿”å›æŠ€æœ¯è§„èŒƒå’Œå®ç°ç»†èŠ‚çš„JSONæ•°ç»„ã€‚"""
        },
        
        DocumentType.SCIENTIFIC: {
            'topics': """åˆ†æè¿™ä¸ªç§‘å­¦æ–‡æ¡£ï¼Œå…³æ³¨ä¸»è¦ç ”ç©¶ç»„ä»¶å’Œæ–¹æ³•æ¡†æ¶ã€‚

è¯†åˆ«ä¸»è¦ç§‘å­¦ä¸»é¢˜ï¼š
- ä»£è¡¨å®Œæ•´çš„å®éªŒæˆ–ç†è®ºå•å…ƒ
- éµå¾ªç§‘å­¦æ–¹æ³•åŸåˆ™
- æ”¯æŒç ”ç©¶ç›®æ ‡
- åŸºäºæ—¢å®šçš„ç§‘å­¦æ¦‚å¿µ

è€ƒè™‘ï¼š
1. ä¸»è¦ç ”ç©¶é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. ä½¿ç”¨äº†ä»€ä¹ˆæ–¹æ³•è®ºæ–¹æ³•ï¼Ÿ
3. åº”ç”¨äº†ä»€ä¹ˆç†è®ºæ¡†æ¶ï¼Ÿ
4. å®æ–½äº†ä»€ä¹ˆå®éªŒè®¾è®¡ï¼Ÿ
5. ä¸åŒç ”ç©¶ç»„ä»¶å¦‚ä½•äº¤äº’ï¼Ÿ

æ ¼å¼ï¼šè¿”å›ä¸»è¦ç§‘å­¦ä¸»é¢˜æˆ–ç ”ç©¶ç»„ä»¶çš„JSONæ•°ç»„ã€‚""",
            # ... ç§‘å­¦æ–‡æ¡£çš„å…¶ä»–æç¤º
        },
        # ... å…¶ä»–æ–‡æ¡£ç±»å‹çš„æç¤º
    }
```

### æ™ºèƒ½ç±»å‹æ£€æµ‹ç®—æ³•

```python
async def detect_document_type(self, content: str, request_id: str) -> DocumentType:
    """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ–‡æ¡£ç±»å‹æ£€æµ‹"""
    
    detection_prompt = """ä½ æ­£åœ¨åˆ†æä¸€ä¸ªæ–‡æ¡£ä»¥ç¡®å®šå…¶ä¸»è¦ç±»å‹å’Œç»“æ„ã€‚
    
å„æ–‡æ¡£ç±»å‹çš„å…³é”®ç‰¹å¾ï¼š
- TECHNICAL: åŒ…å«ç³»ç»Ÿè§„èŒƒã€APIæ–‡æ¡£æˆ–å®ç°ç»†èŠ‚
- SCIENTIFIC: åŒ…å«ç ”ç©¶æ–¹æ³•ã€å®éªŒæ•°æ®æˆ–ç§‘å­¦åˆ†æ
- BUSINESS: åŒ…å«å•†ä¸šç­–ç•¥ã€å¸‚åœºåˆ†ææˆ–ç»„ç»‡è§„åˆ’
- ACADEMIC: åŒ…å«å­¦æœ¯ç†è®ºã€æ–‡çŒ®ç»¼è¿°æˆ–ç ”ç©¶è®ºæ–‡
- LEGAL: åŒ…å«æ³•å¾‹æ¡æ–‡ã€åˆåŒæˆ–æ³•è§„
- MEDICAL: åŒ…å«åŒ»å­¦è¯Šæ–­ã€æ²»ç–—æ–¹æ¡ˆæˆ–å¥åº·ä¿¡æ¯
- NARRATIVE: åŒ…å«æ•…äº‹æƒ…èŠ‚ã€äººç‰©å‘å±•æˆ–å™è¿°ç»“æ„
- INSTRUCTIONAL: åŒ…å«æ“ä½œæ­¥éª¤ã€å­¦ä¹ ææ–™æˆ–æŒ‡å¯¼æ‰‹å†Œ
- ANALYTICAL: åŒ…å«æ•°æ®åˆ†æã€ç»Ÿè®¡ç ”ç©¶æˆ–è¯„ä¼°æŠ¥å‘Š
- PROCEDURAL: åŒ…å«å·¥ä½œæµç¨‹ã€æ“ä½œè§„ç¨‹æˆ–æ ‡å‡†ç¨‹åº
- GENERAL: é€šç”¨å†…å®¹ï¼Œä¸ç¬¦åˆä»¥ä¸Šç‰¹å®šç±»åˆ«

ä»…è¿”å›æœ€åŒ¹é…çš„æ–‡æ¡£ç±»å‹åç§°ï¼ˆå¦‚ï¼šTECHNICALï¼‰ã€‚"""
    
    try:
        response = await self.optimizer.generate_completion(
            detection_prompt + f"\n\næ–‡æ¡£å†…å®¹é¢„è§ˆï¼š\n{content[:2000]}...", 
            max_tokens=50, 
            request_id=request_id,
            task="detecting_document_type"
        )
        
        # è§£æå“åº”å¹¶éªŒè¯ç±»å‹
        if response:
            detected_type = response.strip().upper()
            try:
                return DocumentType.from_str(detected_type)
            except ValueError:
                logger.warning(f"Unknown document type detected: {detected_type}")
                return DocumentType.GENERAL
        else:
            return DocumentType.GENERAL
            
    except Exception as e:
        logger.error(f"Error detecting document type: {str(e)}")
        return DocumentType.GENERAL
```

---

## ğŸ§  å†…å®¹æå–å¼•æ“

### åˆ†å±‚æå–ç­–ç•¥

```python
async def generate_mindmap(self, document_content: str, request_id: str) -> str:
    """ä¸‰å±‚åˆ†çº§æå–ï¼šä¸»é¢˜ â†’ å­ä¸»é¢˜ â†’ è¯¦ç»†ä¿¡æ¯"""
    
    # ç¬¬ä¸€å±‚ï¼šä¸»é¢˜æå–
    main_topics = await self._extract_main_topics(
        document_content, type_prompts['topics'], request_id
    )
    
    # ç¬¬äºŒå±‚ï¼šå­ä¸»é¢˜æå–
    for topic in main_topics:
        subtopics = await self._extract_subtopics(
            topic, document_content, type_prompts['subtopics'], request_id
        )
        topic['subtopics'] = subtopics
        
        # ç¬¬ä¸‰å±‚ï¼šè¯¦ç»†ä¿¡æ¯æå–
        for subtopic in subtopics:
            details = await self._extract_details(
                subtopic, document_content, details_prompt_template, request_id
            )
            subtopic['details'] = details
```

### æ™ºèƒ½åˆ†å—å¤„ç†

```python
async def _extract_main_topics(self, content: str, topics_prompt: str, request_id: str):
    """ä¸»é¢˜æå–çš„åˆ†å—å¹¶è¡Œå¤„ç†"""
    
    # è®¡ç®—æœ€ä¼˜åˆ†å—å¤§å°
    chunk_size = min(len(content) // 3, 15000)  # åŠ¨æ€åˆ†å—
    chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
    
    async def process_chunk(chunk: str, chunk_idx: int) -> List[Dict[str, Any]]:
        """å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡æ¡£å—"""
        chunk_prompt = f"{topics_prompt}\n\nåˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼š\n{chunk}"
        
        try:
            response = await self._retry_generate_completion(
                chunk_prompt, 3000, request_id, f"extracting_main_topics_chunk_{chunk_idx}"
            )
            
            if response:
                # è§£æå’ŒéªŒè¯å“åº”
                parsed_topics = self._parse_llm_response(response, "array")
                validated_topics = []
                
                for topic_data in parsed_topics:
                    if self._validate_topic(topic_data):
                        validated_topics.append({
                            'name': str(topic_data.get('name', '')).strip(),
                            'importance': str(topic_data.get('importance', 'medium')).lower(),
                            'chunk_source': chunk_idx
                        })
                
                return validated_topics
            
        except Exception as e:
            logger.warning(f"Error processing chunk {chunk_idx}: {str(e)}")
            
        return []
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰å—
    chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
    chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)
    
    # åˆå¹¶ç»“æœå¹¶å»é‡
    all_topics = []
    for result in chunk_results:
        if isinstance(result, list):
            all_topics.extend(result)
    
    return await self._consolidate_topics(all_topics, request_id)
```

### é«˜çº§JSONè§£æä¸æ¢å¤

```python
def _clean_json_response(self, response: str) -> str:
    """é«˜çº§JSONå“åº”æ¸…ç†ï¼Œæ”¯æŒå¤šç§å¼‚å¸¸æƒ…å†µæ¢å¤"""
    
    if not response or not isinstance(response, str):
        return "[]"
    
    def find_json_structure(text: str) -> Optional[str]:
        """æ™ºèƒ½æŸ¥æ‰¾JSONç»“æ„"""
        # æŸ¥æ‰¾æ•°ç»„æ¨¡å¼
        array_match = re.search(r'\[[\s\S]*?\](?=\s*$|\s*[,}\]])', text)
        if array_match:
            return array_match.group(0)
        
        # æŸ¥æ‰¾å¯¹è±¡æ¨¡å¼
        object_match = re.search(r'\{[\s\S]*?\}(?=\s*$|\s*[,\]])', text)
        if object_match:
            return object_match.group(0)
        
        return None
    
    def clean_characters(text: str) -> str:
        """æ¸…ç†æ§åˆ¶å­—ç¬¦ï¼Œä¿ç•™æœ‰æ•ˆç©ºæ ¼"""
        text = self.control_chars_regex.sub('', text)
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)  # é›¶å®½å­—ç¬¦
        return text
    
    def fix_json_syntax(text: str) -> str:
        """ä¿®å¤JSONè¯­æ³•é”™è¯¯"""
        # ä¿®å¤å°¾éšé€—å·
        text = re.sub(r',(\s*})', r'\1', text)
        text = re.sub(r',(\s*\])', r'\1', text)
        
        # ä¿®å¤å¤šé‡é€—å·
        text = re.sub(r',+', ',', text)
        
        # ä¿®å¤æœªè½¬ä¹‰çš„å¼•å·
        text = self.unescaped_quotes_regex.sub(r'\\"', text)
        
        # ä¿®å¤ç™¾åˆ†æ¯”æ ¼å¼
        text = self.percentage_regex1.sub(r'\1%', text)
        text = self.percentage_regex2.sub('%', text)
        
        return text
    
    # é€æ­¥æ¸…ç†å’Œæ¢å¤
    response = clean_characters(response)
    json_content = find_json_structure(response)
    
    if json_content:
        json_content = fix_json_syntax(json_content)
        
        try:
            # éªŒè¯JSONæœ‰æ•ˆæ€§
            json.loads(json_content)
            return json_content
        except json.JSONDecodeError:
            pass
    
    # æœ€åçš„æ¢å¤å°è¯•
    return self._attempt_json_recovery(response)
```

---

## ğŸ”„ æ™ºèƒ½å»é‡ç®—æ³•

### å¤šå±‚æ¬¡ç›¸ä¼¼åº¦æ£€æµ‹

```python
async def _batch_redundancy_check(self, items, content_type='topic', 
                                context_prefix='', batch_size=10):
    """æ‰¹é‡å†—ä½™æ£€æŸ¥ï¼Œæ”¯æŒå¹¶å‘ç›¸ä¼¼åº¦æ£€æµ‹"""
    
    if len(items) <= 1:
        return items
    
    # ç»„åˆæ‰€æœ‰é¡¹ç›®å¯¹è¿›è¡Œæ¯”è¾ƒ
    pairs_to_check = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            pairs_to_check.append((i, j))
    
    # åˆ†æ‰¹å¤„ç†ä»¥é¿å…APIé™åˆ¶
    similar_pairs = set()
    
    for batch_start in range(0, len(pairs_to_check), batch_size):
        batch = pairs_to_check[batch_start:batch_start + batch_size]
        
        async def check_pair(i, j):
            """æ£€æŸ¥ä¸¤ä¸ªé¡¹ç›®çš„ç›¸ä¼¼æ€§"""
            item1_name = items[i]['name']
            item2_name = items[j]['name']
            
            # é¦–å…ˆä½¿ç”¨å¿«é€Ÿå­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            quick_similarity = fuzz.ratio(item1_name, item2_name)
            threshold = self.config['similarity_threshold'][content_type]
            
            if quick_similarity >= threshold:
                # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦ç›¸ä¼¼åº¦æ£€æŸ¥
                context1 = f"{context_prefix}: {item1_name}" if context_prefix else item1_name
                context2 = f"{context_prefix}: {item2_name}" if context_prefix else item2_name
                
                is_similar = await self.check_similarity_llm(
                    item1_name, item2_name, context1, context2
                )
                
                if is_similar:
                    return (i, j)
            
            return None
        
        # å¹¶è¡Œæ£€æŸ¥è¿™æ‰¹å¯¹
        batch_tasks = [check_pair(i, j) for i, j in batch]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        for result in batch_results:
            if result and not isinstance(result, Exception):
                similar_pairs.add(result)
    
    # æ„å»ºå»é‡åçš„é¡¹ç›®åˆ—è¡¨
    items_to_remove = set()
    for i, j in similar_pairs:
        # ä¿ç•™é‡è¦æ€§æ›´é«˜çš„é¡¹ç›®
        importance_i = self._get_importance_value(items[i]['importance'])
        importance_j = self._get_importance_value(items[j]['importance'])
        
        if importance_i >= importance_j:
            items_to_remove.add(j)
        else:
            items_to_remove.add(i)
    
    # è¿”å›å»é‡åçš„é¡¹ç›®
    deduplicated_items = [
        items[i] for i in range(len(items)) 
        if i not in items_to_remove
    ]
    
    logger.info(f"Removed {len(items_to_remove)} redundant {content_type}s "
               f"from {len(items)} total items")
    
    return deduplicated_items
```

### LLMé©±åŠ¨çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹

```python
async def check_similarity_llm(self, text1: str, text2: str, 
                             context1: str, context2: str) -> bool:
    """ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹"""
    
    similarity_prompt = f"""æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ¦‚å¿µæ˜¯å¦è¡¨è¾¾ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„æƒ³æ³•ï¼š

æ¦‚å¿µ1: "{text1}"
ä¸Šä¸‹æ–‡1: {context1}

æ¦‚å¿µ2: "{text2}"  
ä¸Šä¸‹æ–‡2: {context2}

è¯„ä¼°æ ‡å‡†ï¼š
1. æ ¸å¿ƒå«ä¹‰æ˜¯å¦ç›¸åŒï¼Ÿ
2. æ‰€æŒ‡å‘çš„å¯¹è±¡/æ¦‚å¿µæ˜¯å¦ä¸€è‡´ï¼Ÿ
3. åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­æ˜¯å¦é‡å¤ï¼Ÿ
4. æ˜¯å¦å¯ä»¥åˆå¹¶è€Œä¸æŸå¤±é‡è¦ä¿¡æ¯ï¼Ÿ

å¦‚æœä¸¤ä¸ªæ¦‚å¿µé«˜åº¦ç›¸ä¼¼æˆ–é‡å¤ï¼Œå›ç­”"æ˜¯"ã€‚
å¦‚æœå®ƒä»¬æ˜¯ä¸åŒçš„æ¦‚å¿µï¼Œå³ä½¿ç›¸å…³ï¼Œä¹Ÿå›ç­”"å¦"ã€‚

å›ç­”ï¼š"""

    try:
        response = await self.optimizer.generate_completion(
            similarity_prompt, 
            max_tokens=50,
            task="checking_content_similarity"
        )
        
        if response:
            return response.strip().lower() in ['æ˜¯', 'yes', 'true', 'ç›¸ä¼¼', 'similar']
        
    except Exception as e:
        logger.warning(f"Error in LLM similarity check: {str(e)}")
    
    return False
```

---

## ğŸ’° æˆæœ¬æ§åˆ¶ç³»ç»Ÿ

### è¯¦ç»†çš„ä½¿ç”¨è¿½è¸ª

```python
class TokenUsageTracker:
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost = 0
        self.call_counts = {}
        
        # ä»»åŠ¡åˆ†ç±»ç”¨äºæ›´å¥½çš„æŠ¥å‘Š
        self.task_categories = {
            'topics': ['extracting_main_topics', 'consolidating_topics', 'detecting_document_type'],
            'subtopics': ['extracting_subtopics', 'consolidate_subtopics'],
            'details': ['extracting_details', 'consolidate_details'],
            'similarity': ['checking_content_similarity'],
            'verification': ['verifying_against_source'],
            'emoji': ['selecting_emoji']
        }
        
        # æŒ‰ç±»åˆ«åˆå§‹åŒ–è®¡æ•°å™¨
        self.call_counts_by_category = {category: 0 for category in self.task_categories}
        self.token_counts_by_category = {
            category: {'input': 0, 'output': 0} 
            for category in self.task_categories
        }
        self.cost_by_category = {category: 0 for category in self.task_categories}
```

### åŠ¨æ€æˆæœ¬è®¡ç®—

```python
def update(self, input_tokens: int, output_tokens: int, task: str):
    """æ ¹æ®ä¸åŒAPIæä¾›å•†è®¡ç®—ç²¾ç¡®æˆæœ¬"""
    
    task_cost = 0
    if Config.API_PROVIDER == "CLAUDE":
        task_cost = (
            input_tokens * Config.ANTHROPIC_INPUT_TOKEN_PRICE +
            output_tokens * Config.ANTHROPIC_OUTPUT_TOKEN_PRICE
        )
    elif Config.API_PROVIDER == "DEEPSEEK":
        # DeepSeekä¸åŒæ¨¡å‹æœ‰ä¸åŒå®šä»·
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            task_cost = (
                input_tokens * Config.DEEPSEEK_CHAT_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_CHAT_OUTPUT_PRICE
            )
        else:  # reasoneræ¨¡å‹
            task_cost = (
                input_tokens * Config.DEEPSEEK_REASONER_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_REASONER_OUTPUT_PRICE
            )
    elif Config.API_PROVIDER == "GEMINI":
        task_cost = (
            input_tokens * Config.GEMINI_INPUT_TOKEN_PRICE +
            output_tokens * Config.GEMINI_OUTPUT_TOKEN_PRICE
        )
    else:  # OPENAI
        task_cost = (
            input_tokens * Config.OPENAI_INPUT_TOKEN_PRICE +
            output_tokens * Config.OPENAI_OUTPUT_TOKEN_PRICE
        )
    
    self.total_cost += task_cost
    # ... æ›´æ–°å…¶ä»–ç»Ÿè®¡ä¿¡æ¯
```

### æ™ºèƒ½è°ƒç”¨é™åˆ¶

```python
# ç”Ÿæˆè¿‡ç¨‹ä¸­çš„LLMè°ƒç”¨é™åˆ¶
max_llm_calls = {
    'topics': 20,      # ä¸»é¢˜æå–è°ƒç”¨é™åˆ¶
    'subtopics': 30,   # å­ä¸»é¢˜æå–è°ƒç”¨é™åˆ¶
    'details': 40      # è¯¦ç»†ä¿¡æ¯æå–è°ƒç”¨é™åˆ¶
}

# åŠ¨æ€å­—æ•°é™åˆ¶
doc_words = len(document_content.split())
word_limit = min(doc_words * 0.9, 8000)  # æœ€å¤šå¤„ç†8000å­—

# å®æ—¶ç›‘æ§å’Œæ—©åœæœºåˆ¶
if self._llm_calls['topics'] >= max_llm_calls['topics']:
    logger.info("è¾¾åˆ°ä¸»é¢˜æå–LLMè°ƒç”¨é™åˆ¶")
    break

if current_word_count > word_limit * 0.95:
    logger.info(f"æ¥è¿‘å­—æ•°é™åˆ¶ {current_word_count}/{word_limit:.0f} å­—")
    break
```

---

## ğŸ¨ è¾“å‡ºæ ¼å¼ç”Ÿæˆ

### Mermaidè¯­æ³•ç”Ÿæˆ

```python
def _generate_mermaid_mindmap(self, concepts: Dict[str, Any]) -> str:
    """ç”Ÿæˆå®Œæ•´çš„Mermaidæ€ç»´å¯¼å›¾è¯­æ³•"""
    
    mindmap_lines = ["mindmap"]
    
    # æ ¹èŠ‚ç‚¹ï¼ˆæ–‡æ¡£emojiï¼‰
    self._add_node_to_mindmap({'name': ''}, mindmap_lines, indent_level=1)
    
    # æ·»åŠ æ‰€æœ‰ä¸»é¢˜åˆ°æ ¹èŠ‚ç‚¹ä¸‹
    for topic in concepts.get('central_theme', {}).get('subtopics', []):
        self._add_node_to_mindmap(topic, mindmap_lines, indent_level=2)
    
    return "\n".join(mindmap_lines)

def _add_node_to_mindmap(self, node: Dict[str, Any], mindmap_lines: List[str], 
                        indent_level: int) -> None:
    """é€’å½’æ·»åŠ èŠ‚ç‚¹åˆ°æ€ç»´å¯¼å›¾"""
    
    # æ ¹æ®å±‚çº§ç¡®å®šèŠ‚ç‚¹å½¢çŠ¶
    if indent_level == 1:  # æ ¹èŠ‚ç‚¹
        shape = NodeShape.ROOT
        node_line = f"    {shape.apply('ğŸ“„')}"
    elif indent_level == 2:  # ä¸»é¢˜
        shape = NodeShape.TOPIC
        emoji = node.get('emoji', '')
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{emoji} {node['name']}"
        node_line = f"        {shape.apply(text)}"
    elif indent_level == 3:  # å­ä¸»é¢˜
        shape = NodeShape.SUBTOPIC
        emoji = node.get('emoji', '')
        text = f"{emoji} {node['name']}"
        node_line = f"            {shape.apply(text)}"
    else:  # è¯¦ç»†ä¿¡æ¯
        shape = NodeShape.DETAIL
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{importance} {node['name']}"
        node_line = f"                {shape.apply(text)}"
    
    mindmap_lines.append(node_line)
    
    # é€’å½’å¤„ç†å­èŠ‚ç‚¹
    for subtopic in node.get('subtopics', []):
        self._add_node_to_mindmap(subtopic, mindmap_lines, indent_level + 1)
    
    for detail in node.get('details', []):
        self._add_node_to_mindmap(detail, mindmap_lines, indent_level + 1)
```

### HTMLå¯è§†åŒ–ç”Ÿæˆ

```python
def generate_mermaid_html(mermaid_code):
    """ç”Ÿæˆäº¤äº’å¼HTMLæ€ç»´å¯¼å›¾"""
    
    # åˆ›å»ºMermaid Live Editorçš„ç¼–è¾‘é“¾æ¥
    data = {
        "code": mermaid_code,
        "mermaid": {"theme": "default"}
    }
    json_string = json.dumps(data)
    compressed_data = zlib.compress(json_string.encode('utf-8'), level=9)
    base64_string = base64.urlsafe_b64encode(compressed_data).decode('utf-8').rstrip('=')
    edit_url = f'https://mermaid.live/edit#pako:{base64_string}'
    
    # ç”Ÿæˆå®Œæ•´çš„HTMLæ¨¡æ¿
    html_template = f'''<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Mermaid Mindmap</title>
  <!-- Tailwind CSS for styling -->
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <!-- Mermaid JS for rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.0/dist/mermaid.min.js"></script>
  <style>
    body {{ margin: 0; padding: 0; }}
    #mermaid {{
      width: 100%;
      height: calc(100vh - 64px);
      overflow: auto;
    }}
  </style>
</head>
<body class="bg-gray-100">
  <div class="flex items-center justify-between p-4 bg-white shadow">
    <h1 class="text-xl font-bold">Mermaid Mindmap</h1>
    <a href="{edit_url}" target="_blank" 
       class="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600">
       Edit in Mermaid Live Editor
    </a>
  </div>
  <div id="mermaid" class="p-4">
    <pre class="mermaid">
{mermaid_code}
    </pre>
  </div>
  <script>
    mermaid.initialize({{
      startOnLoad: true,
      securityLevel: 'loose',
      theme: 'default',
      mindmap: {{ useMaxWidth: true }},
      themeConfig: {{ controlBar: true }}
    }});
  </script>
</body>
</html>'''
    return html_template
```

### Markdownå¤§çº²è½¬æ¢

```python
def _convert_mindmap_to_markdown(self, mermaid_syntax: str) -> str:
    """å°†Mermaidè¯­æ³•è½¬æ¢ä¸ºMarkdownå¤§çº²"""
    
    markdown_lines = []
    lines = mermaid_syntax.split('\n')[1:]  # è·³è¿‡'mindmap'å¤´éƒ¨
    
    for line in lines:
        if not line.strip():
            continue
        
        # è®¡ç®—ç¼©è¿›çº§åˆ«
        indent_level = len(re.match(r'^\s*', line).group()) // 4
        content = line.strip()
        
        if indent_level == 1 and '((ğŸ“„))' in content:
            continue  # è·³è¿‡æ–‡æ¡£emojièŠ‚ç‚¹
        elif indent_level == 2:  # ä¸»é¢˜ -> H1
            node_text = re.search(r'\(\((.*?)\)\)', content)
            if node_text:
                markdown_lines.append(f"# {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 3:  # å­ä¸»é¢˜ -> H2
            node_text = re.search(r'\((.*?)\)', content)
            if node_text:
                markdown_lines.append(f"## {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 4:  # è¯¦ç»†ä¿¡æ¯ -> åˆ—è¡¨é¡¹
            node_text = re.search(r'\[(.*?)\]', content)
            if node_text:
                markdown_lines.append(node_text.group(1).strip())
                markdown_lines.append("")
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    markdown_text = "\n".join(markdown_lines)
    markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
    
    return markdown_text.strip()
```

---

## âš¡ é”™è¯¯å¤„ç†æœºåˆ¶

### æŒ‡æ•°é€€é¿é‡è¯•

```python
async def _retry_with_exponential_backoff(self, func, *args, **kwargs):
    """å¸¦æŠ–åŠ¨çš„æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶"""
    
    retries = 0
    max_retries = self.retry_config['max_retries']
    base_delay = self.retry_config['base_delay']
    max_delay = self.retry_config['max_delay']
    
    while retries < max_retries:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            retries += 1
            if retries >= max_retries:
                raise
            
            # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
            delay = min(base_delay * (2 ** (retries - 1)), max_delay)
            actual_delay = random.uniform(0, delay)
            
            logger.warning(f"å°è¯• {retries}/{max_retries} å¤±è´¥: {str(e)}. "
                         f"{actual_delay:.2f}ç§’åé‡è¯•")
            
            await asyncio.sleep(actual_delay)
```

### ä¼˜é›…é™çº§ç­–ç•¥

```python
try:
    # å°è¯•å®Œæ•´å¤„ç†
    main_topics = await self._extract_main_topics(...)
except Exception as e:
    logger.warning(f"å®Œæ•´ä¸»é¢˜æå–å¤±è´¥ï¼Œå°è¯•ç®€åŒ–å¤„ç†: {str(e)}")
    
    # é™çº§åˆ°æ›´ç®€å•çš„æ–¹æ³•
    main_topics = await self._extract_simple_topics(...)
    
    if not main_topics:
        # æœ€åçš„å…œåº•æ–¹æ¡ˆ
        main_topics = self._generate_default_topics(document_content)
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

```python
def __init__(self):
    # å¤šå±‚ç¼“å­˜
    self._content_cache = {}        # å†…å®¹ç¼“å­˜
    self._emoji_cache = {}          # emojiç¼“å­˜
    self._similarity_cache = {}     # ç›¸ä¼¼åº¦è®¡ç®—ç¼“å­˜
    
def _load_emoji_cache(self):
    """ä»ç£ç›˜åŠ è½½emojiç¼“å­˜"""
    try:
        if os.path.exists(self._emoji_file):
            with open(self._emoji_file, 'r', encoding='utf-8') as f:
                loaded_cache = json.load(f)
                # å°†å­—ç¬¦ä¸²é”®è½¬æ¢å›å…ƒç»„
                self._emoji_cache = {tuple(eval(k)): v for k, v in loaded_cache.items()}
                logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(self._emoji_cache)} ä¸ªemojiæ˜ å°„")
    except Exception as e:
        logger.warning(f"åŠ è½½emojiç¼“å­˜å¤±è´¥: {str(e)}")
        self._emoji_cache = {}
```

### å¹¶å‘å¤„ç†ä¼˜åŒ–

```python
# å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£å—
chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)

# å¹¶è¡Œç›¸ä¼¼åº¦æ£€æŸ¥
batch_tasks = [check_pair(i, j) for i, j in batch]
batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

# å¹¶è¡ŒéªŒè¯
verification_tasks = [verify_node_in_chunk(node, chunk) for chunk in doc_chunks]
verification_results = await asyncio.gather(*verification_tasks, return_exceptions=True)
```

### å†…å­˜ä¼˜åŒ–

```python
# æµå¼å¤„ç†å¤§æ–‡æ¡£
async def process_large_document(self, filepath: str):
    """æµå¼å¤„ç†å¤§æ–‡æ¡£ï¼Œé¿å…å†…å­˜æº¢å‡º"""
    
    async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
        chunk_size = 8192
        buffer = ""
        
        async for line in f:
            buffer += line
            if len(buffer) >= chunk_size:
                # å¤„ç†å½“å‰å—
                await self.process_chunk(buffer)
                buffer = ""
        
        # å¤„ç†å‰©ä½™å†…å®¹
        if buffer:
            await self.process_chunk(buffer)
```

---

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
# åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
generator = MindMapGenerator()

# ç”Ÿæˆæ€ç»´å¯¼å›¾
mindmap = await generator.generate_mindmap(document_content, request_id)

# ç”ŸæˆHTMLæ–‡ä»¶
html_output = generate_mermaid_html(mindmap)

# ä¿å­˜æ–‡ä»¶
with open('mindmap.html', 'w', encoding='utf-8') as f:
    f.write(html_output)
```

### é«˜çº§é…ç½®

```python
# è‡ªå®šä¹‰é…ç½®
generator = MindMapGenerator()
generator.config.update({
    'max_topics': 8,
    'similarity_threshold': {
        'topic': 80,
        'subtopic': 75,
        'detail': 70
    }
})

# å¤„ç†ç‰¹å®šæ–‡æ¡£ç±»å‹
doc_type = DocumentType.TECHNICAL
mindmap = await generator.generate_mindmap(
    document_content, 
    request_id,
    document_type=doc_type
)
```

---

## ğŸ¯ æ€»ç»“

è¿™ä¸ªæ€ç»´å¯¼å›¾ç”Ÿæˆå™¨å±•ç°äº†ç°ä»£AIåº”ç”¨å¼€å‘çš„æœ€ä½³å®è·µï¼š

1. **æ¨¡å—åŒ–æ¶æ„**: æ¸…æ™°çš„ç»„ä»¶åˆ†ç¦»å’ŒèŒè´£åˆ’åˆ†
2. **æ™ºèƒ½å¤„ç†**: åŸºäºLLMçš„æ–‡æ¡£ç†è§£å’Œå†…å®¹æå–
3. **æ€§èƒ½ä¼˜åŒ–**: å¹¶å‘å¤„ç†ã€æ™ºèƒ½ç¼“å­˜å’Œå†…å­˜ç®¡ç†
4. **é”™è¯¯æ¢å¤**: å¤šå±‚æ¬¡çš„é”™è¯¯å¤„ç†å’Œä¼˜é›…é™çº§
5. **æˆæœ¬æ§åˆ¶**: ç²¾ç¡®çš„ä½¿ç”¨è¿½è¸ªå’Œèµ„æºç®¡ç†
6. **å¤šæ ¼å¼è¾“å‡º**: æ”¯æŒHTMLã€Markdownå’ŒMermaidæ ¼å¼
7. **å¯æ‰©å±•æ€§**: æ”¯æŒå¤šç§APIæä¾›å•†å’Œæ–‡æ¡£ç±»å‹

è¯¥é¡¹ç›®ä¸ä»…æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå·¥å…·ï¼Œæ›´æ˜¯å­¦ä¹ ç°ä»£Pythonå¼‚æ­¥ç¼–ç¨‹ã€AIé›†æˆå’Œç³»ç»Ÿè®¾è®¡çš„ä¼˜ç§€æ¡ˆä¾‹ã€‚
</file>

<file path="AIè¾…åŠ©é˜…è¯»å™¨-å®Œæ•´å­¦ä¹ æ–‡æ¡£.md">
# ğŸ“š AIè¾…åŠ©é˜…è¯»å™¨ä»£ç åº“å®Œæ•´å­¦ä¹ æ–‡æ¡£

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ª**AIè¾…åŠ©é˜…è¯»å™¨**é¡¹ç›®çš„å®Œæ•´å­¦ä¹ æ–‡æ¡£ï¼Œé‡‡ç”¨**å‰åç«¯åˆ†ç¦»æ¶æ„**ï¼ŒåŸºäº**Python FastAPI + React**æŠ€æœ¯æ ˆæ„å»ºã€‚é¡¹ç›®ä¸»è¦åŠŸèƒ½æ˜¯å°†æ–‡æ¡£å†…å®¹è½¬æ¢ä¸ºäº¤äº’å¼æ€ç»´å¯¼å›¾ï¼Œå®ç°æ™ºèƒ½é˜…è¯»å’Œå¯è§†åŒ–åˆ†æã€‚

---

## ğŸ—ï¸ ç¬¬ä¸€éƒ¨åˆ†ï¼šé¡¹ç›®ç»“æ„è¯¦è§£

### ğŸ“‹ æ•´ä½“æ¶æ„æ¦‚è§ˆ

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- ğŸ¤– **å¤šAIæ¨¡å‹æ”¯æŒ**ï¼šDeepSeekã€OpenAI GPTã€Claudeã€Geminiç­‰
- ğŸ“„ **å¤šæ ¼å¼æ”¯æŒ**ï¼šMarkdown (.md)ã€æ–‡æœ¬ (.txt) æ–‡ä»¶
- ğŸ¨ **äº¤äº’å¼å¯è§†åŒ–**ï¼šåŸºäºReactFlowçš„é«˜è´¨é‡æ€ç»´å¯¼å›¾
- ğŸ”„ **å®æ—¶åŒæ­¥**ï¼šæ–‡æ¡£é˜…è¯»ä¸æ€ç»´å¯¼å›¾è”åŠ¨é«˜äº®
- ğŸ’» **ç°ä»£åŒ–ç•Œé¢**ï¼šReact + Tailwind CSSå“åº”å¼è®¾è®¡

### ğŸ“ è¯¦ç»†ç›®å½•ç»“æ„

```
mindmap-generator-main/
â”œâ”€â”€ ğŸ“„ README.md                     # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ ğŸ“„ requirements-web.txt          # Pythonä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ ğŸ“„ .env.example                  # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ ğŸ“„ start_conda_web_app.py        # ğŸš€ ä¸€é”®å¯åŠ¨è„šæœ¬
â”œâ”€â”€ ğŸ“„ web_backend.py                # ğŸŒ FastAPIåç«¯æœåŠ¡å™¨
â”œâ”€â”€ ğŸ“„ mindmap_generator.py          # ğŸ§  AIæ€ç»´å¯¼å›¾ç”Ÿæˆæ ¸å¿ƒ
â”œâ”€â”€ ğŸ“„ document_parser.py            # ğŸ“„ æ–‡æ¡£è§£æå™¨
â”œâ”€â”€ ğŸ“„ mindmap_test.py               # ğŸ§ª æµ‹è¯•æ–‡ä»¶
â””â”€â”€ ğŸ“ frontend/                     # âš›ï¸ Reactå‰ç«¯åº”ç”¨
    â”œâ”€â”€ ğŸ“„ package.json              # Node.jsé¡¹ç›®é…ç½®
    â”œâ”€â”€ ğŸ“„ tailwind.config.js        # Tailwind CSSé…ç½®
    â”œâ”€â”€ ğŸ“ public/                   # é™æ€èµ„æº
    â”‚   â”œâ”€â”€ ğŸ“„ index.html           # HTMLå…¥å£
    â”‚   â””â”€â”€ ğŸ“„ favicon.ico          # å›¾æ ‡
    â””â”€â”€ ğŸ“ src/                      # æºä»£ç ç›®å½•
        â”œâ”€â”€ ğŸ“„ index.js             # Reactåº”ç”¨å…¥å£
        â”œâ”€â”€ ğŸ“„ App.js               # ä¸»åº”ç”¨ç»„ä»¶
        â”œâ”€â”€ ğŸ“„ index.css            # å…¨å±€æ ·å¼
        â”œâ”€â”€ ğŸ“ components/          # ç»„ä»¶ç›®å½•
        â”‚   â”œâ”€â”€ ğŸ“„ UploadPage.js    # æ–‡æ¡£ä¸Šä¼ é¡µé¢
        â”‚   â”œâ”€â”€ ğŸ“„ ViewerPageRefactored.js  # ä¸»æŸ¥çœ‹å™¨é¡µé¢
        â”‚   â”œâ”€â”€ ğŸ“„ FlowDiagram.js   # ReactFlowæ€ç»´å¯¼å›¾
        â”‚   â””â”€â”€ ğŸ“„ EditableNode.js  # å¯ç¼–è¾‘èŠ‚ç‚¹ç»„ä»¶
        â”œâ”€â”€ ğŸ“ hooks/               # è‡ªå®šä¹‰Hook
        â”‚   â”œâ”€â”€ ğŸ“„ useScrollDetection.js    # æ»šåŠ¨æ£€æµ‹å’Œè”åŠ¨
        â”‚   â”œâ”€â”€ ğŸ“„ useMindmapGeneration.js  # æ€ç»´å¯¼å›¾ç”Ÿæˆ
        â”‚   â””â”€â”€ ğŸ“„ useDocumentViewer.js     # æ–‡æ¡£æŸ¥çœ‹å™¨
        â”œâ”€â”€ ğŸ“ utils/               # å·¥å…·å‡½æ•°
        â”‚   â”œâ”€â”€ ğŸ“„ api.js           # APIå®¢æˆ·ç«¯
        â”‚   â””â”€â”€ ğŸ“„ dataConverter.js # æ•°æ®è½¬æ¢å™¨
        â””â”€â”€ ğŸ“ contexts/            # Reactä¸Šä¸‹æ–‡
            â””â”€â”€ ğŸ“„ AuthContext.js   # è®¤è¯ä¸Šä¸‹æ–‡
```

---

## ğŸš€ ç¬¬äºŒéƒ¨åˆ†ï¼šç¼–è¯‘æ–¹å¼è¯¦è§£

### ğŸ”§ ç¯å¢ƒè¦æ±‚

- **Python**: 3.8+ (æ¨è3.9+)
- **Node.js**: 16+ (æ¨è18+)
- **æ“ä½œç³»ç»Ÿ**: Windows 10/11, macOS 10.15+, Linux (Ubuntu 18.04+)

### ğŸ¯ ä¸€é”®ç¼–è¯‘å¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd mindmap-generator-main

# 2. ğŸ”¥ ä¸€é”®ç¼–è¯‘å’Œå¯åŠ¨
python start_conda_web_app.py
```

**è‡ªåŠ¨åŒ–æµç¨‹**ï¼š
1. âœ… æ£€æŸ¥Pythonç¯å¢ƒå’ŒCondaç¯å¢ƒ
2. âœ… å®‰è£…åç«¯Pythonä¾èµ– (`requirements-web.txt`)
3. âœ… æ£€æŸ¥Node.jsç¯å¢ƒï¼Œè‡ªåŠ¨å®‰è£…npmä¾èµ–
4. âœ… å¯åŠ¨åç«¯FastAPIæœåŠ¡ï¼ˆ8000ç«¯å£ï¼‰
5. âœ… ç¼–è¯‘å¹¶å¯åŠ¨å‰ç«¯ReactæœåŠ¡ï¼ˆ3000ç«¯å£ï¼‰
6. âœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ (`http://localhost:3000`)

### ğŸ”— æ‰‹åŠ¨åˆ†æ­¥ç¼–è¯‘

#### åç«¯ç¼–è¯‘
```bash
# 1. å®‰è£…Pythonä¾èµ–
pip install -r requirements-web.txt

# 2. å¯åŠ¨åç«¯æœåŠ¡
python web_backend.py
```

#### å‰ç«¯ç¼–è¯‘
```bash
# 1. è¿›å…¥å‰ç«¯ç›®å½•
cd frontend

# 2. å®‰è£…ä¾èµ–
npm install

# 3. å¼€å‘æ¨¡å¼å¯åŠ¨
npm start

# 4. ç”Ÿäº§æ„å»º
npm run build
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå…¥å£è¯¦è§£

### ğŸš€ å¯åŠ¨å…¥å£ï¼š`start_conda_web_app.py`

```python
def main():
    """åº”ç”¨å¯åŠ¨çš„çœŸæ­£å…¥å£"""
    # ğŸ” 1. ç¯å¢ƒæ£€æŸ¥
    check_conda_env()
    install_requirements()
    
    # ğŸ”„ 2. å¯åŠ¨æœåŠ¡
    backend_process = start_backend()    # 8000ç«¯å£
    frontend_process = start_frontend()  # 3000ç«¯å£
    
    # ğŸŒ 3. è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    open_browser("http://localhost:3000")
```

### ğŸŒ åç«¯å…¥å£ï¼š`web_backend.py`

```python
# FastAPIåº”ç”¨å®ä¾‹
app = FastAPI(title="AIè¾…åŠ©é˜…è¯»å™¨ API", version="1.0.0")

# ğŸ”‘ æ ¸å¿ƒAPIè·¯ç”±
@app.post("/api/upload-document")         # æ–‡æ¡£ä¸Šä¼ 
@app.post("/api/generate-mindmap")        # æ€ç»´å¯¼å›¾ç”Ÿæˆ
@app.get("/api/document/{document_id}")   # æ–‡æ¡£è·å–
@app.get("/api/mindmap/{document_id}")    # æ€ç»´å¯¼å›¾è·å–

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### âš›ï¸ å‰ç«¯å…¥å£ï¼š`index.js` â†’ `App.js`

```javascript
// index.js - Reactåº”ç”¨å…¥å£
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(<App />);

// App.js - ä¸»åº”ç”¨ç»„ä»¶
function App() {
  return (
    <Router>
      <Routes>
        <Route path="/" element={<UploadPage />} />
        <Route path="/viewer" element={<ViewerPageRefactored />} />
      </Routes>
    </Router>
  );
}
```

---

## ğŸ§  ç¬¬å››éƒ¨åˆ†ï¼šæ ¸å¿ƒé€»è¾‘è¯¦è§£

### ğŸ—ºï¸ AIæ€ç»´å¯¼å›¾ç”Ÿæˆå¼•æ“

#### æ ¸å¿ƒç±»ï¼š`MindMapGenerator`

```python
class MindMapGenerator:
    """æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - ç³»ç»Ÿçš„AIæ ¸å¿ƒ"""
    
    async def generate_mindmap(self, document_content: str, request_id: str) -> str:
        """ğŸ”‘ æ ¸å¿ƒç”Ÿæˆæµç¨‹"""
        # 1ï¸âƒ£ æ–‡æ¡£ç±»å‹æ£€æµ‹
        doc_type = await self.detect_document_type(document_content, request_id)
        
        # 2ï¸âƒ£ æå–ä¸»é¢˜ (æ”¯æŒå¤§æ–‡æ¡£åˆ†å—å¤„ç†)
        topics = await self._extract_main_topics(document_content, topics_prompt, request_id)
        
        # 3ï¸âƒ£ æ‰¹é‡ç›¸ä¼¼æ€§æ£€æŸ¥ (AIå»é‡)
        filtered_topics = await self._batch_redundancy_check(topics, 'topic')
        
        # 4ï¸âƒ£ é€’å½’ç”Ÿæˆå­ä¸»é¢˜å’Œç»†èŠ‚
        for topic in filtered_topics:
            subtopics = await self._extract_subtopics(topic, content, subtopics_prompt, request_id)
            for subtopic in subtopics:
                details = await self._extract_details(subtopic, content, details_prompt, request_id)
        
        # 5ï¸âƒ£ æœ€ç»ˆè¿‡æ»¤å’ŒéªŒè¯
        final_mindmap = await self.final_pass_filter_for_duplicative_content(mindmap_data)
        
        # 6ï¸âƒ£ è½¬æ¢ä¸ºMermaidè¯­æ³•
        return self._generate_mermaid_mindmap(final_mindmap)
```

### ğŸ”„ å‰ç«¯çŠ¶æ€ç®¡ç†æ ¸å¿ƒ

#### 1. **æ»šåŠ¨è”åŠ¨ç³»ç»Ÿï¼š`useScrollDetection.js`**

```javascript
const useScrollDetection = (containerRef, documentId, currentMindmapMode, mermaidDiagramRef) => {
  // ğŸ”‘ åŒå‘è”åŠ¨çŠ¶æ€
  const [activeContentBlockId, setActiveContentBlockId] = useState(null);
  const [highlightedNodeId, setHighlightedNodeId] = useState(null);
  
  // ğŸ”‘ æ­£å‘è”åŠ¨ï¼šæ–‡æ¡£æ»šåŠ¨ â†’ æ€ç»´å¯¼å›¾é«˜äº®
  const detectActiveSection = useCallback(() => {
    const sections = document.querySelectorAll('[data-block-id]');
    // å¤æ‚çš„å¯è§†åŒºåŸŸæ£€æµ‹ç®—æ³•
    // æ›´æ–°activeContentBlockId
    // è§¦å‘æ€ç»´å¯¼å›¾èŠ‚ç‚¹é«˜äº®
  }, []);
  
  // ğŸ”‘ åå‘è”åŠ¨ï¼šæ€ç»´å¯¼å›¾ç‚¹å‡» â†’ æ–‡æ¡£æ»šåŠ¨
  const scrollToSection = (item) => {
    const targetElement = document.querySelector(`[data-block-id="${item.id}"]`);
    if (targetElement) {
      targetElement.scrollIntoView({ behavior: 'smooth' });
    }
  };
};
```

#### 2. **ReactFlowé›†æˆï¼š`FlowDiagram.js`**

```javascript
const FlowDiagram = ({ mindmapData, highlightedNodeId, onNodeClick }) => {
  // ğŸ”‘ èŠ‚ç‚¹å˜åŒ–å¤„ç† (è§£å†³é«˜äº®æ¶ˆå¤±é—®é¢˜)
  const handleNodesChange = useCallback((changes) => {
    onNodesChange(changes);
    
    // æ£€æµ‹æ‹–æ‹½ç­‰å˜åŒ–ï¼Œé‡æ–°åº”ç”¨é«˜äº®
    const needsHighlightReapply = changes.some(change => 
      change.type === 'position' || change.type === 'dimensions'
    );
    
    if (needsHighlightReapply && highlightedNodeId) {
      setTimeout(() => {
        applyNodeHighlighting(highlightedNodeId);
      }, 150);
    }
  }, []);
  
  // ğŸ”‘ éç ´åæ€§é«˜äº®å®ç°
  const applyNodeHighlighting = useCallback((nodeIdToHighlight) => {
    // å¤šç­–ç•¥èŠ‚ç‚¹æŸ¥æ‰¾
    // ç›´æ¥DOMæ“ä½œæ·»åŠ CSSç±»
    // é¿å…ReactçŠ¶æ€å†²çª
  }, []);
};
```

---

## â±ï¸ ç¬¬äº”éƒ¨åˆ†ï¼šæ—¶åºå›¾è¯¦è§£

### ğŸ”„ 1. æ–‡æ¡£ä¸Šä¼ å’Œåˆå§‹åŒ–æµç¨‹

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ ç”¨æˆ·
    participant UP as ğŸ“¤ UploadPage
    participant BE as ğŸŒ Backend API
    participant DP as ğŸ“„ DocumentParser
    participant VP as ğŸ“– ViewerPage

    U->>UP: 1. é€‰æ‹©æ–‡ä»¶ä¸Šä¼ 
    UP->>UP: 2. éªŒè¯æ–‡ä»¶ç±»å‹(.md/.txt)
    UP->>BE: 3. POST /api/upload-document
    BE->>BE: 4. ç”Ÿæˆdocument_id
    BE->>DP: 5. è§£ææ–‡æ¡£å†…å®¹
    DP-->>BE: 6. è¿”å›è§£æç»“æœ
    BE-->>UP: 7. è¿”å›document_id
    UP->>VP: 8. è·³è½¬åˆ°æŸ¥çœ‹å™¨é¡µé¢
    VP->>BE: 9. GET /api/document/{id}
    BE-->>VP: 10. è¿”å›æ–‡æ¡£å†…å®¹
    VP->>VP: 11. æ¸²æŸ“æ–‡æ¡£å†…å®¹
```

### ğŸ§  2. AIæ€ç»´å¯¼å›¾ç”Ÿæˆæµç¨‹

```mermaid
sequenceDiagram
    participant VP as ğŸ“– ViewerPage
    participant BE as ğŸŒ Backend API
    participant MG as ğŸ§  MindMapGenerator
    participant AI as ğŸ¤– AIæ¨¡å‹

    VP->>BE: 1. POST /api/generate-mindmap
    BE->>MG: 2. è°ƒç”¨ç”Ÿæˆå™¨
    MG->>AI: 3. æ£€æµ‹æ–‡æ¡£ç±»å‹
    AI-->>MG: 4. è¿”å›æ–‡æ¡£ç±»å‹
    MG->>AI: 5. æå–ä¸»é¢˜
    AI-->>MG: 6. è¿”å›ä¸»é¢˜åˆ—è¡¨
    MG->>AI: 7. æå–å­ä¸»é¢˜
    AI-->>MG: 8. è¿”å›å­ä¸»é¢˜
    MG->>AI: 9. æå–ç»†èŠ‚
    AI-->>MG: 10. è¿”å›ç»†èŠ‚
    MG->>MG: 11. å»é‡å’ŒéªŒè¯
    MG->>MG: 12. è½¬æ¢ä¸ºMermaidè¯­æ³•
    MG-->>BE: 13. è¿”å›æ€ç»´å¯¼å›¾
    BE-->>VP: 14. è¿”å›ç”Ÿæˆç»“æœ
    VP->>VP: 15. æ¸²æŸ“æ€ç»´å¯¼å›¾
```

### ğŸ”„ 3. æ»šåŠ¨è”åŠ¨é«˜äº®æµç¨‹

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ ç”¨æˆ·
    participant VP as ğŸ“– ViewerPage
    participant SD as ğŸ”„ ScrollDetection
    participant FD as ğŸ¨ FlowDiagram

    U->>VP: 1. æ»šåŠ¨æ–‡æ¡£
    VP->>SD: 2. è§¦å‘æ»šåŠ¨äº‹ä»¶
    SD->>SD: 3. æ£€æµ‹å½“å‰å¯è§åŒºåŸŸ
    SD->>SD: 4. è®¡ç®—activeContentBlockId
    SD->>FD: 5. æ›´æ–°highlightedNodeId
    FD->>FD: 6. æŸ¥æ‰¾å¯¹åº”èŠ‚ç‚¹
    FD->>FD: 7. åº”ç”¨é«˜äº®æ ·å¼
    
    Note over U,FD: åå‘è”åŠ¨
    U->>FD: 8. ç‚¹å‡»æ€ç»´å¯¼å›¾èŠ‚ç‚¹
    FD->>SD: 9. è§¦å‘scrollToSection
    SD->>VP: 10. æ»šåŠ¨åˆ°å¯¹åº”å†…å®¹
    VP->>VP: 11. é«˜äº®å¯¹åº”æ®µè½
```

---

## ğŸ”§ ç¬¬å…­éƒ¨åˆ†ï¼šå„ä¸ªæ­¥éª¤å…³é”®å®ç°å‡½æ•°

### ğŸ§  AIæ€ç»´å¯¼å›¾ç”Ÿæˆæ ¸å¿ƒå‡½æ•°

#### 1. **ä¸»ç”Ÿæˆå‡½æ•°ï¼š`generate_mindmap()`**
```python
async def generate_mindmap(self, document_content: str, request_id: str) -> str:
    """ğŸ”‘ æ ¸å¿ƒç”Ÿæˆæµç¨‹ - å®Œæ•´ç‰ˆ"""
    # æ–‡æ¡£ç±»å‹æ£€æµ‹
    doc_type = await self.detect_document_type(document_content, request_id)
    
    # æå–ä¸»é¢˜
    topics = await self._extract_main_topics(document_content, topics_prompt, request_id)
    
    # æ‰¹é‡ç›¸ä¼¼æ€§æ£€æŸ¥
    filtered_topics = await self._batch_redundancy_check(topics, 'topic')
    
    # é€’å½’ç”Ÿæˆå­ä¸»é¢˜å’Œç»†èŠ‚
    for topic in filtered_topics:
        subtopics = await self._extract_subtopics(topic, content, subtopics_prompt, request_id)
        
    # æœ€ç»ˆè¿‡æ»¤å’ŒéªŒè¯
    final_mindmap = await self.final_pass_filter_for_duplicative_content(mindmap_data)
    
    # è½¬æ¢ä¸ºMermaidè¯­æ³•
    return self._generate_mermaid_mindmap(final_mindmap)
```

#### 2. **AIæ¨¡å‹ç»Ÿä¸€æ¥å£ï¼š`DocumentOptimizer.generate_completion()`**
```python
async def generate_completion(self, prompt: str, max_tokens: int = 5000, request_id: str = None, task: str = None) -> str:
    """ç»Ÿä¸€çš„AIæ¨¡å‹è°ƒç”¨æ¥å£"""
    if Config.API_PROVIDER == "CLAUDE":
        # Claude APIè°ƒç”¨
        async with self.anthropic_client.messages.stream(...) as stream:
            message = await stream.get_final_message()
            return message.content[0].text
    elif Config.API_PROVIDER == "OPENAI":
        # OpenAI APIè°ƒç”¨
        response = await self.openai_client.chat.completions.create(...)
        return response.choices[0].message.content
    elif Config.API_PROVIDER == "DEEPSEEK":
        # DeepSeek APIè°ƒç”¨
        response = await self.deepseek_client.chat.completions.create(...)
        return response.choices[0].message.content
    # ... å…¶ä»–æ¨¡å‹
```

### ğŸ”„ å‰ç«¯æ ¸å¿ƒå‡½æ•°

#### 1. **æ»šåŠ¨è”åŠ¨æ ¸å¿ƒï¼š`useScrollDetection()`**
```javascript
const useScrollDetection = (containerRef, documentId, currentMindmapMode, mermaidDiagramRef) => {
  // ğŸ”‘ æ»šåŠ¨æ£€æµ‹å’ŒèŠ‚ç‚¹é«˜äº®
  const highlightMermaidNode = useCallback((nodeId) => {
    // å¤æ‚çš„èŠ‚ç‚¹æŸ¥æ‰¾é€»è¾‘
    const selectors = [
      `[data-id="${nodeId}"]`,
      `#${nodeId}`,
      `[id*="${nodeId}"]`,
      `g[data-id="${nodeId}"]`
    ];
    
    // å¤šç­–ç•¥æŸ¥æ‰¾å’Œé«˜äº®
    selectors.forEach(selector => {
      const nodes = document.querySelectorAll(selector);
      nodes.forEach(node => {
        if (!node.classList.contains('mermaid-highlighted-node')) {
          node.classList.add('mermaid-highlighted-node');
        }
      });
    });
  }, []);
  
  // ğŸ”‘ åå‘è”åŠ¨ï¼šæ€ç»´å¯¼å›¾åˆ°æ–‡æ¡£
  const scrollToSection = (item) => {
    const targetElement = document.querySelector(`[data-block-id="${item.id}"]`);
    if (targetElement) {
      targetElement.scrollIntoView({ behavior: 'smooth', block: 'center' });
    }
  };
};
```

#### 2. **ReactFlowé›†æˆæ ¸å¿ƒï¼š`FlowDiagram.js`**
```javascript
// ğŸ”‘ èŠ‚ç‚¹å˜åŒ–å¤„ç† (è§£å†³é«˜äº®æ¶ˆå¤±é—®é¢˜)
const handleNodesChange = useCallback((changes) => {
  onNodesChange(changes);
  
  // æ£€æµ‹éœ€è¦é‡æ–°åº”ç”¨é«˜äº®çš„å˜åŒ–
  const needsHighlightReapply = changes.some(change => 
    change.type === 'position' || 
    change.type === 'dimensions' ||
    change.type === 'select'
  );
  
  if (needsHighlightReapply && highlightedNodeId) {
    setTimeout(() => {
      applyNodeHighlighting(highlightedNodeId);
    }, 150);
  }
}, []);

// ğŸ”‘ éç ´åæ€§é«˜äº®å®ç°
const applyNodeHighlighting = useCallback((nodeIdToHighlight) => {
  // å¤šç­–ç•¥èŠ‚ç‚¹æŸ¥æ‰¾
  const strategies = [
    () => document.querySelector(`[data-id="${nodeIdToHighlight}"]`),
    () => document.querySelector(`#${nodeIdToHighlight}`),
    () => document.querySelector(`.react-flow__node[data-id="${nodeIdToHighlight}"]`)
  ];
  
  let foundElement = null;
  for (const strategy of strategies) {
    foundElement = strategy();
    if (foundElement) break;
  }
  
  if (foundElement) {
    foundElement.classList.add('highlighted-node');
  }
}, []);
```

### ğŸŒ APIé€šä¿¡æ ¸å¿ƒå‡½æ•°

#### 1. **æ–‡æ¡£å¤„ç†APIï¼š`web_backend.py`**
```python
@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """æ–‡æ¡£ä¸Šä¼ å’Œå¤„ç†å…¥å£"""
    # æ–‡ä»¶éªŒè¯
    if not file.filename.endswith(('.md', '.txt')):
        raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
    
    # ç”Ÿæˆæ–‡æ¡£ID
    document_id = hashlib.md5(f"{file.filename}{time.time()}".encode()).hexdigest()
    
    # è§£ææ–‡æ¡£å†…å®¹
    content = await file.read()
    parsed_content = DocumentParser.parse(content.decode('utf-8'))
    
    # å­˜å‚¨å’Œè¿”å›
    store_document(document_id, parsed_content)
    return {"document_id": document_id, "status": "success"}

@app.post("/api/generate-mindmap")
async def generate_mindmap_endpoint(request: GenerateMindmapRequest):
    """æ€ç»´å¯¼å›¾ç”ŸæˆAPI"""
    generator = MindMapGenerator()
    
    # è·å–æ–‡æ¡£å†…å®¹
    document = await get_document_by_id(request.document_id)
    
    # ç”Ÿæˆæ€ç»´å¯¼å›¾
    mindmap_result = await generator.generate_mindmap(
        document['content'], 
        request.request_id
    )
    
    return {"mindmap": mindmap_result, "status": "completed"}
```

#### 2. **å‰ç«¯APIå®¢æˆ·ç«¯ï¼š`api.js`**
```javascript
// æ–‡æ¡£ä¸Šä¼ 
export const uploadDocument = async (file, onProgress) => {
  const formData = new FormData();
  formData.append('file', file);
  
  return await fetch('/api/upload-document', {
    method: 'POST',
    body: formData
  });
};

// æ€ç»´å¯¼å›¾ç”Ÿæˆ
export const generateMindmap = async (documentId, options = {}) => {
  const response = await fetch('/api/generate-mindmap', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      document_id: documentId,
      request_id: generateRequestId(),
      ...options
    })
  });
  
  return await response.json();
};

// è½®è¯¢çŠ¶æ€
export const pollMindmapStatus = async (requestId) => {
  const response = await fetch(`/api/mindmap-status/${requestId}`);
  return await response.json();
};
```

---

## ğŸ¯ æ€»ç»“

è¿™ä¸ªAIè¾…åŠ©é˜…è¯»å™¨æ˜¯ä¸€ä¸ª**å¤æ‚è€Œç²¾å¯†çš„ç³»ç»Ÿ**ï¼Œæ¶‰åŠï¼š

### ğŸ”‘ æ ¸å¿ƒæŠ€æœ¯æ ˆ
- **åç«¯**: Python + FastAPI + å¤šAIæ¨¡å‹é›†æˆ
- **å‰ç«¯**: React + ReactFlow + Tailwind CSS
- **é€šä¿¡**: RESTful API + WebSocket (å¯é€‰)
- **éƒ¨ç½²**: ä¸€é”®å¯åŠ¨è„šæœ¬ + Dockeræ”¯æŒ

### ğŸ§  æ ¸å¿ƒåˆ›æ–°ç‚¹
1. **AIé©±åŠ¨çš„æ™ºèƒ½åˆ†æ**: å¤šæ¨¡å‹æ”¯æŒï¼Œæ™ºèƒ½æ–‡æ¡£ç±»å‹æ£€æµ‹
2. **åŒå‘è”åŠ¨æœºåˆ¶**: æ–‡æ¡£æ»šåŠ¨ä¸æ€ç»´å¯¼å›¾é«˜äº®å®Œç¾åŒæ­¥
3. **éç ´åæ€§é«˜äº®**: è§£å†³Reactç»„ä»¶çŠ¶æ€ç®¡ç†å¤æ‚æ€§
4. **åˆ†å—å¤„ç†ç®—æ³•**: æ”¯æŒå¤§æ–‡æ¡£çš„é«˜æ•ˆå¤„ç†
5. **æˆæœ¬è¿½è¸ªç³»ç»Ÿ**: å®Œæ•´çš„AIè°ƒç”¨æˆæœ¬ç®¡ç†

### ğŸ”„ ç³»ç»Ÿç‰¹ç‚¹
- **é«˜åº¦æ¨¡å—åŒ–**: æ¯ä¸ªåŠŸèƒ½æ¨¡å—èŒè´£æ˜ç¡®
- **é”™è¯¯å¤„ç†å®Œå–„**: å¤šå±‚æ¬¡å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–**: å¼‚æ­¥å¤„ç†ã€æ‰¹é‡æ“ä½œã€æ™ºèƒ½ç¼“å­˜
- **ç”¨æˆ·ä½“éªŒ**: å®æ—¶åé¦ˆã€è¿›åº¦æç¤ºã€å“åº”å¼è®¾è®¡

è¿™ä¸ªç³»ç»Ÿå±•ç¤ºäº†**ç°ä»£AIåº”ç”¨å¼€å‘çš„æœ€ä½³å®è·µ**ï¼Œæ˜¯å­¦ä¹ AIé›†æˆã€å‰ç«¯çŠ¶æ€ç®¡ç†ã€åç«¯APIè®¾è®¡çš„ä¼˜ç§€æ¡ˆä¾‹ã€‚

---

ğŸ“ **å­¦ä¹ å»ºè®®**ï¼š
1. å…ˆç†è§£æ•´ä½“æ¶æ„å’Œæ•°æ®æµ
2. é‡ç‚¹å…³æ³¨AIç”Ÿæˆé€»è¾‘å’Œå‰ç«¯è”åŠ¨æœºåˆ¶
3. å®è·µæ—¶å¯ä»¥ä»ç®€å•åŠŸèƒ½å¼€å§‹ï¼Œé€æ­¥æ‰©å±•
4. æ³¨æ„å¼‚æ­¥å¤„ç†å’Œé”™è¯¯å¤„ç†çš„å®ç°ç»†èŠ‚

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´ï¼š2024å¹´12æœˆ*
*ç‰ˆæœ¬ï¼šv1.0*
</file>

<file path="Condaç¯å¢ƒä½¿ç”¨æŒ‡å—.md">
# Condaç¯å¢ƒä½¿ç”¨æŒ‡å—

## å‰è¨€

æœ¬æŒ‡å—ä¸“é—¨é’ˆå¯¹åœ¨condaç¯å¢ƒï¼ˆå¦‚MinerUï¼‰ä¸­è¿è¡Œæ€ç»´å¯¼å›¾ç”Ÿæˆå™¨çš„ç”¨æˆ·ã€‚è§£å†³äº†å¸¸è§çš„ä¾èµ–é—®é¢˜å’ŒGoogle Generative AIé›†æˆé—®é¢˜ã€‚

## ğŸ¯ åœ¨Condaç¯å¢ƒä¸­è¿è¡ŒWebåº”ç”¨

### 1. ç¡®ä¿æ‚¨å·²ç»æ¿€æ´»äº†py312ç¯å¢ƒ

```bash
# æ¿€æ´»æ‚¨çš„condaç¯å¢ƒ
conda activate py312

# ç¡®è®¤Pythonç‰ˆæœ¬
python --version
```

### 2. ä½¿ç”¨ä¸“ç”¨çš„Condaå¯åŠ¨è„šæœ¬

```bash
# ä½¿ç”¨ä¸“é—¨ä¸ºcondaç¯å¢ƒä¼˜åŒ–çš„å¯åŠ¨è„šæœ¬
python start_conda_web_app.py
```

## ğŸ”§ ç¯å¢ƒé…ç½®

### æ–¹å¼1: ä½¿ç”¨Conda + Pipæ··åˆå®‰è£…ï¼ˆæ¨èï¼‰

```bash
# é¦–å…ˆç”¨condaå®‰è£…åŸºç¡€åŒ…
conda install -y numpy scikit-learn requests

# ç„¶åç”¨pipå®‰è£…Webç›¸å…³åŒ…
pip install fastapi uvicorn python-multipart aiofiles

# æœ€åè¿è¡Œå¯åŠ¨è„šæœ¬
python start_conda_web_app.py
```

### æ–¹å¼2: çº¯Pipå®‰è£…

```bash
# ç›´æ¥ç”¨pipå®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements-web.txt

# è¿è¡Œå¯åŠ¨è„šæœ¬
python start_conda_web_app.py
```

## ğŸš€ å¿«é€Ÿå¯åŠ¨

å¦‚æœæ‚¨çš„condaç¯å¢ƒå·²ç»é…ç½®å¥½ï¼Œåªéœ€è¦ä¸€æ¡å‘½ä»¤ï¼š

```bash
python start_conda_web_app.py
```

å¯åŠ¨è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- âœ… æ£€æµ‹condaç¯å¢ƒ
- âœ… æ£€æŸ¥Pythonç‰ˆæœ¬å…¼å®¹æ€§
- âœ… å®‰è£…ç¼ºå¤±çš„ä¾èµ–
- âœ… å¯åŠ¨å‰åç«¯æœåŠ¡
- âœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

- **Python**: 3.8+ (æ‚¨çš„py312å®Œå…¨ç¬¦åˆ)
- **Node.js**: 16+ (ç”¨äºReactå‰ç«¯)
- **Conda**: ä»»æ„ç‰ˆæœ¬
- **æ“ä½œç³»ç»Ÿ**: Windows/Linux/macOS

## ğŸ› ï¸ ä¾èµ–è§£å†³

å¦‚æœé‡åˆ°ä¾èµ–å†²çªï¼š

```bash
# æ›´æ–°conda
conda update conda

# æ›´æ–°pip
python -m pip install --upgrade pip

# æ¸…ç†å¹¶é‡æ–°å®‰è£…
pip install --force-reinstall -r requirements-web.txt
```

## ğŸŒ è®¿é—®åœ°å€

å¯åŠ¨æˆåŠŸåè®¿é—®ï¼š
- **å‰ç«¯**: http://localhost:3000
- **åç«¯API**: http://localhost:8000
- **APIæ–‡æ¡£**: http://localhost:8000/docs

## âš¡ æ€§èƒ½ä¼˜åŒ–

åœ¨condaç¯å¢ƒä¸­çš„ä¼˜åŒ–å»ºè®®ï¼š

1. **ä½¿ç”¨conda-forgeé¢‘é“**:
   ```bash
   conda config --add channels conda-forge
   ```

2. **åˆ›å»ºä¸“ç”¨ç¯å¢ƒ**ï¼ˆå¯é€‰ï¼‰:
   ```bash
   conda create -n mindmap python=3.12
   conda activate mindmap
   ```

3. **é¢„å®‰è£…ç§‘å­¦è®¡ç®—åŒ…**:
   ```bash
   conda install numpy scikit-learn
   ```

## ğŸš¨ å¸¸è§é—®é¢˜

### Q: æç¤ºæƒé™é”™è¯¯
A: åœ¨condaç¯å¢ƒä¸­ä½¿ç”¨ `--user` æ ‡å¿—ï¼š
```bash
pip install --user -r requirements-web.txt
```

### Q: Node.jsæœªæ‰¾åˆ°
A: å®‰è£…Node.js:
```bash
# ä½¿ç”¨condaå®‰è£…
conda install nodejs npm

# æˆ–ä»å®˜ç½‘ä¸‹è½½: https://nodejs.org/
```

### Q: ç«¯å£è¢«å ç”¨
A: ä¿®æ”¹ç«¯å£æˆ–ç»“æŸå ç”¨è¿›ç¨‹ï¼š
```bash
# ä½¿ç”¨ä¸åŒç«¯å£
uvicorn web_backend:app --port 8001
```

ç°åœ¨æ‚¨å¯ä»¥åœ¨py312 condaç¯å¢ƒä¸­è¿è¡Œï¼š
```bash
python start_conda_web_app.py
```

## ç¯å¢ƒè®¾ç½®

### 1. æ¿€æ´»Condaç¯å¢ƒ
```bash
conda activate MinerU
```

### 2. å®‰è£…ä¾èµ–åŒ…

**é‡è¦**ï¼šç¡®ä¿å®‰è£…æ­£ç¡®çš„Google Generative AIåŒ…ï¼š

```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install -r requirements.txt

# æˆ–è€…å•ç‹¬å®‰è£…å·²ä¿®å¤çš„ä¾èµ–
pip install google-generativeai>=0.3.0
pip install openai anthropic aiofiles termcolor fuzzywuzzy
```

### 3. ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```env
# é€‰æ‹©APIæä¾›å•†ï¼ˆå¿…éœ€ï¼‰
API_PROVIDER=DEEPSEEK  # æˆ– OPENAI, CLAUDE, GEMINI

# DeepSeeké…ç½®ï¼ˆæ¨èï¼‰
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# å…¶ä»–APIé…ç½®ï¼ˆå¯é€‰ï¼‰
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
```

## é—®é¢˜è§£å†³

### Google Generative AI é”™è¯¯

**é”™è¯¯ä¿¡æ¯**ï¼š`module 'google.generativeai' has no attribute 'Client'`

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„åŒ…ï¼š
   ```bash
   pip uninstall google-genai  # å¸è½½æ—§åŒ…
   pip install google-generativeai>=0.3.0  # å®‰è£…æ­£ç¡®çš„åŒ…
   ```

2. é‡å¯Pythonç¯å¢ƒï¼š
   ```bash
   conda deactivate
   conda activate MinerU
   ```

### è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
python mindmap_generator.py

# æµ‹è¯•Webåº”ç”¨
python start_conda_web_app.py
```

## æ¨èé…ç½®

å¯¹äºåœ¨MinerUç¯å¢ƒä¸­ä½¿ç”¨DeepSeekçš„ç”¨æˆ·ï¼š

```env
API_PROVIDER=DEEPSEEK
DEEPSEEK_API_KEY=your_key_here
```

DeepSeekæä¾›ï¼š
- æˆæœ¬æ•ˆç›Šé«˜
- å“åº”é€Ÿåº¦å¿«  
- æ”¯æŒä¸­æ–‡å¤„ç†
- ä¸MinerUç¯å¢ƒå…¼å®¹æ€§å¥½

## æ•…éšœæ’é™¤

### ä¾èµ–å†²çª
```bash
# æ¸…ç†pipç¼“å­˜
pip cache purge

# é‡æ–°å®‰è£…ä¾èµ–
pip install --force-reinstall -r requirements.txt
```

### æƒé™é—®é¢˜
```bash
# ä½¿ç”¨ç”¨æˆ·çº§å®‰è£…
pip install --user google-generativeai>=0.3.0
```

### ç½‘ç»œé—®é¢˜
```bash
# ä½¿ç”¨å›½å†…é•œåƒæº
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple google-generativeai>=0.3.0
```

## éªŒè¯å®‰è£…

è¿è¡Œä»¥ä¸‹Pythonä»£ç éªŒè¯å®‰è£…ï¼š

```python
# éªŒè¯å¯¼å…¥
try:
    import google.generativeai as genai
    print("âœ… Google Generative AI å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")

# éªŒè¯å…¶ä»–ä¾èµ–
import openai, anthropic, aiofiles
print("âœ… å…¶ä»–ä¾èµ–å¯¼å…¥æˆåŠŸ")
```

## æ”¯æŒçš„APIæä¾›å•†

| æä¾›å•† | ç¯å¢ƒå˜é‡ | æ¨èç¨‹åº¦ | å¤‡æ³¨ |
|-------|---------|---------|------|
| DeepSeek | DEEPSEEK_API_KEY | â­â­â­â­â­ | æ¨èï¼Œæˆæœ¬ä½ï¼Œä¸­æ–‡å‹å¥½ |
| OpenAI | OPENAI_API_KEY | â­â­â­â­ | è´¨é‡é«˜ï¼Œæˆæœ¬è¾ƒé«˜ |
| Claude | ANTHROPIC_API_KEY | â­â­â­â­ | è´¨é‡ä¼˜ç§€ï¼Œæˆæœ¬é€‚ä¸­ |
| Gemini | GEMINI_API_KEY | â­â­â­ | å…è´¹é¢åº¦ï¼Œéœ€è¦æ­£ç¡®é…ç½® |
</file>

<file path="WEBåº”ç”¨ä½¿ç”¨è¯´æ˜.md">
# æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - Webåº”ç”¨ç‰ˆ

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

åŸºäºæ‚¨ç°æœ‰çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨é¡¹ç›®ï¼Œæˆ‘å·²ç»ä¸ºæ‚¨åˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„Webåº”ç”¨ã€‚è¯¥åº”ç”¨åŒ…å«ï¼š

- **Reactå‰ç«¯**: ç°ä»£åŒ–çš„ç”¨æˆ·ç•Œé¢ï¼Œæ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œå¯è§†åŒ–
- **FastAPIåç«¯**: é«˜æ€§èƒ½çš„Python APIæœåŠ¡
- **å“åº”å¼å¸ƒå±€**: å·¦ä¾§MDé˜…è¯»å™¨(2/3) + å³ä¾§æ€ç»´å¯¼å›¾(1/3)
- **æ™ºèƒ½AIåˆ†æ**: åŸºäºæ‚¨ç°æœ‰çš„LLMé©±åŠ¨çš„å†…å®¹åˆ†æå¼•æ“

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
ğŸ“ æ‚¨çš„é¡¹ç›®æ ¹ç›®å½•/
â”œâ”€â”€ ğŸ“„ mindmap_generator.py      # åŸæœ‰çš„æ ¸å¿ƒç”Ÿæˆå™¨
â”œâ”€â”€ ğŸ“„ web_backend.py            # FastAPIåç«¯æœåŠ¡
â”œâ”€â”€ ğŸ“„ start_web_app.py          # ä¸€é”®å¯åŠ¨è„šæœ¬
â”œâ”€â”€ ğŸ“„ requirements-web.txt      # Webç‰ˆPythonä¾èµ–
â”œâ”€â”€ ğŸ“„ .env                      # APIé…ç½®ï¼ˆéœ€è¦æ‚¨é…ç½®ï¼‰
â”œâ”€â”€ ğŸ“ frontend/                 # Reactå‰ç«¯åº”ç”¨
â”‚   â”œâ”€â”€ ğŸ“„ package.json         # å‰ç«¯ä¾èµ–é…ç½®
â”‚   â”œâ”€â”€ ğŸ“„ tailwind.config.js   # æ ·å¼é…ç½®
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ App.js           # ä¸»åº”ç”¨ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ App.css          # ä¸»æ ·å¼æ–‡ä»¶
â”‚   â”‚   â””â”€â”€ ğŸ“ components/
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ UploadPage.js    # æ–‡ä»¶ä¸Šä¼ é¡µé¢
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ ViewerPage.js    # æ–‡æ¡£æŸ¥çœ‹é¡µé¢
â”‚   â”‚       â””â”€â”€ ğŸ“„ MermaidDiagram.js # æ€ç»´å¯¼å›¾ç»„ä»¶
â”‚   â””â”€â”€ ğŸ“ public/
â”‚       â””â”€â”€ ğŸ“„ index.html       # HTMLæ¨¡æ¿
â””â”€â”€ ğŸ“ uploads/                  # ä¸Šä¼ æ–‡ä»¶å­˜å‚¨ç›®å½•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šä¸€é”®å¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# ç›´æ¥è¿è¡Œå¯åŠ¨è„šæœ¬
python start_web_app.py
```

å¯åŠ¨è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- âœ… æ£€æŸ¥å¹¶å®‰è£…Pythonä¾èµ–
- âœ… æ£€æŸ¥Node.jsç¯å¢ƒ
- âœ… å®‰è£…å‰ç«¯ä¾èµ–
- âœ… å¯åŠ¨åç«¯å’Œå‰ç«¯æœåŠ¡
- âœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨

### æ–¹å¼äºŒï¼šæ‰‹åŠ¨å¯åŠ¨

```bash
# 1. å®‰è£…Pythonä¾èµ–
pip install -r requirements-web.txt

# 2. è¿›å…¥å‰ç«¯ç›®å½•å¹¶å®‰è£…ä¾èµ–
cd frontend
npm install

# 3. å¯åŠ¨åç«¯ï¼ˆæ–°ç»ˆç«¯ï¼‰
python -m uvicorn web_backend:app --host 0.0.0.0 --port 8000 --reload

# 4. å¯åŠ¨å‰ç«¯ï¼ˆæ–°ç»ˆç«¯ï¼‰
cd frontend
npm start
```

## ğŸ”§ ç¯å¢ƒé…ç½®

ç¡®ä¿æ‚¨çš„ `.env` æ–‡ä»¶åŒ…å«æ­£ç¡®çš„APIé…ç½®ï¼š

```env
# æ¨èé…ç½®ï¼šç¡…åŸºæµåŠ¨
OPENAI_API_KEY="æ‚¨çš„ç¡…åŸºæµåŠ¨APIå¯†é’¥"
OPENAI_BASE_URL="https://api.siliconflow.cn/v1"
API_PROVIDER="OPENAI"

# å…¶ä»–APIï¼ˆå¯é€‰ï¼‰
DEEPSEEK_API_KEY=""
ANTHROPIC_API_KEY=""
GEMINI_API_KEY=""
```

## ğŸŒ è®¿é—®åœ°å€

å¯åŠ¨æˆåŠŸåï¼Œæ‚¨å¯ä»¥è®¿é—®ï¼š

- **å‰ç«¯åº”ç”¨**: http://localhost:3000
- **åç«¯API**: http://localhost:8000
- **APIæ–‡æ¡£**: http://localhost:8000/docs

## ğŸ“± åŠŸèƒ½ç‰¹æ€§

### ğŸ¨ ç”¨æˆ·ç•Œé¢

1. **ä¸Šä¼ é¡µé¢**
   - æ”¯æŒæ‹–æ‹½ä¸Šä¼  `.md` å’Œ `.txt` æ–‡ä»¶
   - æ–‡ä»¶å¤§å°é™åˆ¶ï¼š10MB
   - å®æ—¶ä¸Šä¼ è¿›åº¦æ˜¾ç¤º
   - ä¼˜é›…çš„é”™è¯¯å¤„ç†

2. **æŸ¥çœ‹é¡µé¢**
   - **å·¦ä¾§ (2/3å®½åº¦)**: Markdowné˜…è¯»å™¨
     - è¯­æ³•é«˜äº®
     - å“åº”å¼æ’ç‰ˆ
     - è‡ªå®šä¹‰æ ·å¼ä¼˜åŒ–
   - **å³ä¾§ (1/3å®½åº¦)**: Mermaidæ€ç»´å¯¼å›¾
     - å®æ—¶æ¸²æŸ“
     - äº¤äº’å¼å›¾è¡¨
     - ä»£ç å¤åˆ¶åŠŸèƒ½

### ğŸ”§ å·¥å…·åŠŸèƒ½

- **ä¸‹è½½åŠŸèƒ½**: 
  - ä¸‹è½½åŸå§‹Markdownæ–‡ä»¶
  - ä¸‹è½½Mermaidå›¾è¡¨ä»£ç 
- **åœ¨çº¿ç¼–è¾‘**: ç›´æ¥è·³è½¬åˆ°Mermaid Live Editor
- **å“åº”å¼è®¾è®¡**: é€‚é…å„ç§å±å¹•å°ºå¯¸

## ğŸ¯ ä½¿ç”¨æµç¨‹

### 1. è®¿é—®åº”ç”¨
æ‰“å¼€æµè§ˆå™¨è®¿é—® `http://localhost:3000`

### 2. ä¸Šä¼ æ–‡ä»¶
- æ‹–æ‹½æ–‡ä»¶åˆ°ä¸Šä¼ åŒºåŸŸï¼Œæˆ–ç‚¹å‡»é€‰æ‹©æ–‡ä»¶
- æ”¯æŒçš„æ ¼å¼ï¼š`.md`ã€`.txt`
- ç³»ç»Ÿä¼šè‡ªåŠ¨éªŒè¯æ–‡ä»¶ç±»å‹å’Œå¤§å°

### 3. æŸ¥çœ‹ç»“æœ
- ä¸Šä¼ æˆåŠŸåè‡ªåŠ¨è·³è½¬åˆ°æŸ¥çœ‹é¡µé¢
- å·¦ä¾§é˜…è¯»Markdownå†…å®¹
- å³ä¾§æŸ¥çœ‹AIç”Ÿæˆçš„æ€ç»´å¯¼å›¾

### 4. å¯¼å‡ºå’Œåˆ†äº«
- ä½¿ç”¨å·¥å…·æ ä¸‹è½½æ–‡ä»¶æˆ–å›¾è¡¨ä»£ç 
- ç‚¹å‡»"ç¼–è¾‘å›¾è¡¨"åœ¨çº¿ç¼–è¾‘Mermaidä»£ç 

## ğŸ” æŠ€æœ¯æ¶æ„

### å‰ç«¯æŠ€æœ¯æ ˆ
- **React 18**: ç°ä»£åŒ–Reactæ¡†æ¶
- **React Router**: å•é¡µåº”ç”¨è·¯ç”±
- **Tailwind CSS**: å®ç”¨ä¼˜å…ˆçš„CSSæ¡†æ¶
- **Mermaid**: å›¾è¡¨æ¸²æŸ“å¼•æ“
- **React Markdown**: Markdownæ¸²æŸ“ç»„ä»¶
- **React Dropzone**: æ–‡ä»¶æ‹–æ‹½ä¸Šä¼ 
- **Axios**: HTTPå®¢æˆ·ç«¯
- **React Hot Toast**: æ¶ˆæ¯æç¤º

### åç«¯æŠ€æœ¯æ ˆ
- **FastAPI**: é«˜æ€§èƒ½å¼‚æ­¥Webæ¡†æ¶
- **Uvicorn**: ASGIæœåŠ¡å™¨
- **æ‚¨çš„ç°æœ‰å¼•æ“**: 
  - MindMapGenerator
  - å¤šAPIæä¾›å•†æ”¯æŒ
  - æ™ºèƒ½å†…å®¹åˆ†æ

## ğŸ› ï¸ å¼€å‘å’Œè‡ªå®šä¹‰

### ä¿®æ”¹æ ·å¼
ç¼–è¾‘ `frontend/src/App.css` æˆ–ä½¿ç”¨Tailwindå·¥å…·ç±»

### æ·»åŠ æ–°åŠŸèƒ½
- å‰ç«¯ï¼šåœ¨ `frontend/src/components/` æ·»åŠ æ–°ç»„ä»¶
- åç«¯ï¼šåœ¨ `web_backend.py` æ·»åŠ æ–°APIç«¯ç‚¹

### é…ç½®è°ƒæ•´
- ä¿®æ”¹ `frontend/package.json` è°ƒæ•´å‰ç«¯é…ç½®
- ä¿®æ”¹ `web_backend.py` è°ƒæ•´APIé…ç½®

## ğŸ”’ å®‰å…¨è€ƒè™‘

- **æ–‡ä»¶éªŒè¯**: ä¸¥æ ¼çš„æ–‡ä»¶ç±»å‹å’Œå¤§å°æ£€æŸ¥
- **APIå®‰å…¨**: CORSé…ç½®å’Œè¯·æ±‚éªŒè¯
- **é”™è¯¯å¤„ç†**: ä¼˜é›…çš„é”™è¯¯è¾¹ç•Œå’Œç”¨æˆ·åé¦ˆ
- **æ•°æ®æ¸…ç†**: è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç«¯å£å ç”¨**
   ```bash
   # æ›´æ”¹ç«¯å£
   uvicorn web_backend:app --port 8001
   ```

2. **APIå¯†é’¥é—®é¢˜**
   - æ£€æŸ¥ `.env` æ–‡ä»¶é…ç½®
   - ç¡®ä¿APIå¯†é’¥æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿä½™é¢

3. **ä¾èµ–å®‰è£…å¤±è´¥**
   ```bash
   # æ¸…ç†å¹¶é‡æ–°å®‰è£…
   pip install --force-reinstall -r requirements-web.txt
   ```

4. **å‰ç«¯æ„å»ºå¤±è´¥**
   ```bash
   cd frontend
   rm -rf node_modules package-lock.json
   npm install
   ```

### æ—¥å¿—æŸ¥çœ‹

- **åç«¯æ—¥å¿—**: åœ¨è¿è¡Œuvicornçš„ç»ˆç«¯æŸ¥çœ‹
- **å‰ç«¯æ—¥å¿—**: åœ¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„ConsoleæŸ¥çœ‹

## ğŸ“ æ”¯æŒå’Œåé¦ˆ

å¦‚æœæ‚¨é‡åˆ°ä»»ä½•é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·ï¼š

1. æ£€æŸ¥ä¸Šè¿°æ•…éšœæ’é™¤æŒ‡å—
2. æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—
3. ç¡®è®¤APIé…ç½®æ˜¯å¦æ­£ç¡®
4. éªŒè¯ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸

## ğŸ‰ æ€»ç»“

è¿™ä¸ªWebåº”ç”¨å®Œç¾ç»“åˆäº†æ‚¨ç°æœ‰çš„AIæ€ç»´å¯¼å›¾ç”Ÿæˆèƒ½åŠ›å’Œç°ä»£åŒ–çš„Webç•Œé¢ã€‚ç”¨æˆ·å¯ä»¥ï¼š

- ğŸ“¤ **è½»æ¾ä¸Šä¼ **æ–‡æ¡£
- ğŸ‘€ **å¹¶æ’æŸ¥çœ‹**åŸæ–‡å’Œæ€ç»´å¯¼å›¾  
- ğŸ’¾ **ä¾¿æ·å¯¼å‡º**ç»“æœ
- ğŸ¨ **äº«å—ç°ä»£åŒ–**çš„ç”¨æˆ·ä½“éªŒ

ç°åœ¨æ‚¨å¯ä»¥è¿è¡Œ `python start_web_app.py` æ¥å¯åŠ¨è¿™ä¸ªå®Œæ•´çš„Webåº”ç”¨äº†ï¼
</file>

<file path=".cursor/rules/senior-engineer-task-execution-rule.mdc">
---
description: 
globs: 
alwaysApply: true
---
Rule:
You are a senior engineer with deep experience building production-grade AI agents, automations, and workflow systems. Every task you execute must follow this procedure without exception:
	
1.Clarify Scope First
â€¢Before writing any code, map out exactly how you will approach the task.
â€¢Confirm your interpretation of the objective.
â€¢Write a clear plan showing what functions, modules, or components will be touched and why.
â€¢Do not begin implementation until this is done and reasoned through.
	
2.Locate Exact Code Insertion Point
â€¢Identify the precise file(s) and line(s) where the change will live.
â€¢Never make sweeping edits across unrelated files.
â€¢If multiple files are needed, justify each inclusion explicitly.
â€¢Do not create new abstractions or refactor unless the task explicitly says so.
	
3.Minimal, Contained Changes
â€¢Only write code directly required to satisfy the task.
â€¢Avoid adding logging, comments, tests, TODOs, cleanup, or error handling unless directly necessary.
â€¢No speculative changes or â€œwhile weâ€™re hereâ€ edits.
â€¢All logic should be isolated to not break existing flows.
	
4.Double Check Everything
â€¢Review for correctness, scope adherence, and side effects.
â€¢Ensure your code is aligned with the existing codebase patterns and avoids regressions.
â€¢Explicitly verify whether anything downstream will be impacted.
	
5.Deliver Clearly
â€¢Summarize what was changed and why.
â€¢List every file modified and what was done in each.
â€¢If there are any assumptions or risks, flag them for review.
	

Reminder: You are not a co-pilot, assistant, or brainstorm partner. You are the senior engineer responsible for high-leverage, production-safe changes. Do not improvise. Do not over-engineer. Do not deviate
</file>

<file path=".env.example">
# æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - ç¯å¢ƒå˜é‡é…ç½®

# =============================================================================
# AI æä¾›å•†é€‰æ‹©
# =============================================================================
# é€‰æ‹©ä½ è¦ä½¿ç”¨çš„AIæä¾›å•† (DEEPSEEK, OPENAI, CLAUDE, GEMINI)
API_PROVIDER=DEEPSEEK

# =============================================================================
# DeepSeek API é…ç½®ï¼ˆæ¨èï¼šæˆæœ¬ä½å»‰ï¼Œä¸­æ–‡æ”¯æŒå¥½ï¼‰
# =============================================================================
# è·å–APIå¯†é’¥ï¼šhttps://platform.deepseek.com/
DEEPSEEK_API_KEY=your_deepseek_api_key_here
# å¯é€‰ï¼šé€‰æ‹©æ¨¡å‹ (deepseek-chat æˆ– deepseek-reasoner)
DEEPSEEK_COMPLETION_MODEL=deepseek-chat

# =============================================================================
# OpenAI API é…ç½®ï¼ˆæ¨èä½¿ç”¨ç¡…åŸºæµåŠ¨ç­‰ä»£ç†æœåŠ¡ï¼‰
# =============================================================================
# å®˜æ–¹OpenAIæˆ–ç¬¬ä¸‰æ–¹ä»£ç†æœåŠ¡
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# æ¨èï¼šä½¿ç”¨ç¡…åŸºæµåŠ¨ä»£ç†ï¼ˆä¾¿å®œä¸”ç¨³å®šï¼‰
# OPENAI_API_KEY=your_siliconflow_api_key
# OPENAI_BASE_URL=https://api.siliconflow.cn/v1

# å¯é€‰ï¼šæŒ‡å®šæ¨¡å‹
OPENAI_COMPLETION_MODEL=gpt-4o-mini-2024-07-18

# =============================================================================
# Claude API é…ç½®
# =============================================================================
# è·å–APIå¯†é’¥ï¼šhttps://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# =============================================================================
# Gemini API é…ç½®
# =============================================================================
# è·å–APIå¯†é’¥ï¼šhttps://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

#ignore .bin model files and .sqlite files
*.bin
*.sqlite
*.sqlite-shm
*.sqlite-wal
*.sqlite-journal
*.gguf
folder_of_source_documents__original_format
folder_of_source_documents__converted_to_plaintext
*.zip
venv/
.env
emoji_cache.json
.env

# ç¯å¢ƒå˜é‡æ–‡ä»¶
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# ä¾èµ–ç›®å½•
node_modules/
venv/
__pycache__/
*.pyc
*.pyo
*.pyd

# ç¼“å­˜æ–‡ä»¶
.cache/
*.log
*.tmp
*.temp

# IDEå’Œç¼–è¾‘å™¨æ–‡ä»¶
.vscode/
.idea/
*.swp
*.swo
*~

# æ“ä½œç³»ç»Ÿæ–‡ä»¶
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# ä¸Šä¼ æ–‡ä»¶ç›®å½•
uploads/
mindmap_outputs/

# æ„å»ºè¾“å‡º
build/
dist/
*.egg-info/

# Pythonç‰¹å®š
*.egg
*.egg-info/
.tox/
.pytest_cache/
.coverage
htmlcov/

# Reactç‰¹å®š
frontend/build/
frontend/.env.local
frontend/.env.development.local
frontend/.env.test.local
frontend/.env.production.local

# å…¶ä»–ä¸´æ—¶æ–‡ä»¶
*.bak
*.orig
test_*.py
debug_*.py
*_test.html

# Conda/è™šæ‹Ÿç¯å¢ƒ
conda-meta/
envs/

# æ¨¡å‹ç¼“å­˜
emoji_cache.json

# æµ‹è¯•æ–‡ä»¶
test_mindmap.mmd
</file>

<file path=".python-version">
3.12
</file>

<file path="api_responses/.gitignore">
# æ’é™¤æ‰€æœ‰APIå“åº”æ–‡ä»¶
*.txt
*.json
*.log

# ä½†ä¿ç•™è¿™ä¸ªREADMEæ–‡ä»¶æ¥è¯´æ˜æ–‡ä»¶å¤¹çš„ç”¨é€”
!README.md
!.gitignore
</file>

<file path="api_responses/README.md">
# APIå“åº”æ—¥å¿—æ–‡ä»¶å¤¹

æœ¬æ–‡ä»¶å¤¹ç”¨äºä¿å­˜è®ºè¯ç»“æ„åˆ†æå™¨è°ƒç”¨AI APIæ—¶çš„åŸå§‹å“åº”æ•°æ®ï¼Œä¾¿äºåˆ†æå’Œè°ƒè¯•ã€‚

## æ–‡ä»¶å‘½åè§„åˆ™

æ–‡ä»¶æŒ‰ä»¥ä¸‹æ ¼å¼å‘½åï¼š
```
YYYYMMDD_HHMMSS_argument_structure_analysis.txt
```

ä¾‹å¦‚ï¼š`20250702_221909_argument_structure_analysis.txt`

## æ–‡ä»¶å†…å®¹

æ¯ä¸ªå“åº”æ–‡ä»¶åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **APIè°ƒç”¨ä¿¡æ¯**
   - è°ƒç”¨æ—¶é—´
   - ä»»åŠ¡ç±»å‹ï¼ˆè®ºè¯ç»“æ„åˆ†æï¼‰
   - æœ€å¤§tokensè®¾ç½®
   - å“åº”å’Œè¾“å…¥æ–‡æœ¬é•¿åº¦

2. **å‘é€çš„Prompt**
   - å®Œæ•´çš„AIåˆ†ææŒ‡ä»¤
   - åŒ…å«ç”¨æˆ·è¦æ±‚å’Œæ ¼å¼è§„èŒƒ

3. **AIåŸå§‹å“åº”**
   - æœªç»å¤„ç†çš„AIè¿”å›å†…å®¹
   - ç”¨äºåˆ†æAIç†è§£å’Œæ‰§è¡Œæƒ…å†µ

## ç”¨é€”

- **è°ƒè¯•åˆ†æ**ï¼šå½“AIå“åº”å¼‚å¸¸æ—¶ï¼Œå¯æŸ¥çœ‹åŸå§‹å†…å®¹è¿›è¡Œé—®é¢˜è¯Šæ–­
- **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆ†æä¸åŒpromptçš„æ•ˆæœï¼Œä¼˜åŒ–æŒ‡ä»¤è´¨é‡
- **ç»“æœéªŒè¯**ï¼šç¡®ä¿AIæŒ‰é¢„æœŸæ ¼å¼è¿”å›ç»“æ„åŒ–æ•°æ®
- **æ¨¡å‹æ¯”è¾ƒ**ï¼šæ¯”è¾ƒä¸åŒæ¨¡å‹æˆ–å‚æ•°è®¾ç½®çš„å“åº”è´¨é‡

## æ³¨æ„äº‹é¡¹

- è¿™äº›æ–‡ä»¶åŒ…å«å®Œæ•´çš„æ–‡æ¡£å†…å®¹å’ŒAIå“åº”ï¼Œå¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯
- æ–‡ä»¶å·²è¢«`.gitignore`æ’é™¤ï¼Œä¸ä¼šè¢«æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶
- å»ºè®®å®šæœŸæ¸…ç†æ—§æ–‡ä»¶ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´
- æ–‡ä»¶ä»¥UTF-8ç¼–ç ä¿å­˜ï¼Œæ”¯æŒä¸­æ–‡å†…å®¹
</file>

<file path="document_parser.py">
"""
æ–‡æ¡£ç»“æ„åˆ†æå™¨ - åŸºäºMarkdownæ ‡é¢˜æ„å»ºå±‚çº§ç»“æ„
å®ç°ç”¨æˆ·æä¾›çš„ç®—æ³•ï¼šæŒ‰Markdownæ ‡é¢˜å±‚çº§åˆ’åˆ†æ–‡æ¡£
"""

import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import json


@dataclass
class HeadingInfo:
    """æ ‡é¢˜ä¿¡æ¯æ•°æ®ç±»"""
    level: int
    title: str
    raw_heading: str
    start_char: int
    end_char: int


class DocumentNode:
    """æ–‡æ¡£èŠ‚ç‚¹ç±» - è¡¨ç¤ºæ–‡æ¡£çš„å±‚çº§ç»“æ„"""
    
    def __init__(self, node_id: str, level: int = 0, title: Optional[str] = None, 
                 raw_heading: Optional[str] = None):
        self.id = node_id
        self.level = level
        self.title = title
        self.raw_heading = raw_heading
        
        # æ ‡é¢˜ä½ç½®
        self.heading_start_char: Optional[int] = None
        self.heading_end_char: Optional[int] = None
        
        # å†…å®¹ä½ç½®ï¼ˆç›´æ¥éš¶å±äºè¯¥æ ‡é¢˜çš„å†…å®¹ï¼Œä¸å«å­æ ‡é¢˜ï¼‰
        self.content_start_char: Optional[int] = None
        self.content_end_char: Optional[int] = None
        self.content: str = ""
        
        # å®Œæ•´èŒƒå›´ä½ç½®ï¼ˆåŒ…å«æ‰€æœ‰å­å­™èŠ‚ç‚¹ï¼‰
        self.span_start_char: Optional[int] = None
        self.span_end_char: Optional[int] = None
        
        # æ ‘ç»“æ„
        self.children: List['DocumentNode'] = []
        self.parent: Optional['DocumentNode'] = None

    def add_child(self, child: 'DocumentNode'):
        """æ·»åŠ å­èŠ‚ç‚¹"""
        child.parent = self
        self.children.append(child)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "id": self.id,
            "level": self.level,
            "title": self.title,
            "raw_heading": self.raw_heading,
            "heading_start_char": self.heading_start_char,
            "heading_end_char": self.heading_end_char,
            "content_start_char": self.content_start_char,
            "content_end_char": self.content_end_char,
            "content": self.content,
            "span_start_char": self.span_start_char,
            "span_end_char": self.span_end_char,
            "children": [child.to_dict() for child in self.children]
        }


class DocumentParser:
    """æ–‡æ¡£è§£æå™¨ - æŒ‰ç…§ç”¨æˆ·æä¾›çš„ç®—æ³•å®ç°"""
    
    def __init__(self):
        # åŒ¹é…Markdownæ ‡é¢˜çš„æ­£åˆ™è¡¨è¾¾å¼
        self.heading_pattern = re.compile(r'^ *(#+)\s+(.*)\s*$', re.MULTILINE)
    
    def parse_document(self, markdown_text: str, document_id: str = "doc") -> DocumentNode:
        """
        è§£æMarkdownæ–‡æ¡£ï¼Œæ„å»ºå±‚çº§ç»“æ„
        
        Args:
            markdown_text: Markdownæ–‡æœ¬å†…å®¹
            document_id: æ–‡æ¡£IDï¼Œç”¨äºç”ŸæˆèŠ‚ç‚¹ID
            
        Returns:
            DocumentNode: æ ¹èŠ‚ç‚¹
        """
        # ç¬¬ä¸€æ­¥ï¼šæ‰«ææ ‡é¢˜
        headings_list = self._extract_headings(markdown_text)
        
        # ç¬¬äºŒæ­¥ï¼šæ„å»ºæ ‘ç»“æ„
        root = self._build_tree_structure(headings_list, markdown_text, document_id)
        
        # ç¬¬ä¸‰æ­¥ï¼šåå¤„ç† - ä¿®æ­£èŒƒå›´å’Œå†…å®¹
        self._post_process_tree(root, markdown_text)
        
        return root
    
    def _extract_headings(self, markdown_text: str) -> List[HeadingInfo]:
        """
        ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰æ ‡é¢˜ä¿¡æ¯
        """
        headings_list = []
        
        for match in self.heading_pattern.finditer(markdown_text):
            level = len(match.group(1))  # è®¡ç®—#å·çš„æ•°é‡
            title = match.group(2).strip()  # æå–æ ‡é¢˜æ–‡æœ¬
            raw_heading = match.group(0).strip()  # å®Œæ•´çš„æ ‡é¢˜è¡Œ
            start_char = match.start()
            end_char = match.end()
            
            heading_info = HeadingInfo(
                level=level,
                title=title,
                raw_heading=raw_heading,
                start_char=start_char,
                end_char=end_char
            )
            headings_list.append(heading_info)
        
        return headings_list
    
    def _build_tree_structure(self, headings_list: List[HeadingInfo], 
                            markdown_text: str, document_id: str) -> DocumentNode:
        """
        ç¬¬äºŒæ­¥ï¼šæ„å»ºå±‚çº§ç»“æ„
        """
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = DocumentNode(node_id=f"{document_id}_root", level=0, title="æ–‡æ¡£æ ¹")
        root.span_start_char = 0
        root.content_start_char = 0
        
        # ç”¨æ ˆæ¥è¿½è¸ªå½“å‰çš„çˆ¶èŠ‚ç‚¹
        stack = [root]
        
        # å¤„ç†å‰è¨€ï¼ˆç¬¬ä¸€ä¸ªæ ‡é¢˜å‰çš„å†…å®¹ï¼‰
        if headings_list and headings_list[0].start_char > 0:
            preface_node = DocumentNode(
                node_id=f"{document_id}_preface",
                level=1,
                title="å¼•è¨€"
            )
            preface_node.content_start_char = 0
            preface_node.content_end_char = headings_list[0].start_char
            preface_node.content = markdown_text[0:headings_list[0].start_char].strip()
            preface_node.span_start_char = 0
            preface_node.span_end_char = headings_list[0].start_char
            
            root.add_child(preface_node)
        elif not headings_list:
            # æ•´ä¸ªæ–‡æ¡£æ— æ ‡é¢˜
            root.content = markdown_text
            root.content_start_char = 0
            root.content_end_char = len(markdown_text)
            root.span_start_char = 0
            root.span_end_char = len(markdown_text)
            return root
        
        # éå†æ ‡é¢˜åˆ—è¡¨æ„å»ºæ ‘
        for i, heading_info in enumerate(headings_list):
            # ç¡®å®šçˆ¶èŠ‚ç‚¹
            current_parent = stack[-1]
            
            # æ ¹æ®çº§åˆ«å…³ç³»è°ƒæ•´æ ˆ
            if heading_info.level > current_parent.level:
                # å­èŠ‚ç‚¹ï¼Œcurrent_parentå°±æ˜¯çˆ¶èŠ‚ç‚¹
                pass
            elif heading_info.level == current_parent.level:
                # å…„å¼ŸèŠ‚ç‚¹ï¼Œå¼¹å‡ºå½“å‰çˆ¶èŠ‚ç‚¹
                stack.pop()
                current_parent = stack[-1]
            else:
                # éœ€è¦å‘ä¸Šå›æº¯
                while stack and stack[-1].level >= heading_info.level:
                    stack.pop()
                current_parent = stack[-1]
            
            # åˆ›å»ºæ–°èŠ‚ç‚¹
            node_id = f"{document_id}_sec_{len(current_parent.children) + 1}"
            if current_parent.id != f"{document_id}_root":
                node_id = f"{current_parent.id}_{len(current_parent.children) + 1}"
            
            new_node = DocumentNode(
                node_id=node_id,
                level=heading_info.level,
                title=heading_info.title,
                raw_heading=heading_info.raw_heading
            )
            
            # è®¾ç½®æ ‡é¢˜ä½ç½®
            new_node.heading_start_char = heading_info.start_char
            new_node.heading_end_char = heading_info.end_char
            
            # è®¾ç½®å†…å®¹å’ŒèŒƒå›´çš„åˆå§‹ä½ç½®
            new_node.content_start_char = heading_info.end_char
            new_node.span_start_char = heading_info.start_char
            
            # ç¡®å®šç»“æŸä½ç½®
            if i == len(headings_list) - 1:
                # æœ€åä¸€ä¸ªæ ‡é¢˜
                next_boundary_char = len(markdown_text)
            else:
                next_boundary_char = headings_list[i + 1].start_char
            
            new_node.content_end_char = next_boundary_char
            new_node.span_end_char = next_boundary_char
            
            # æå–å†…å®¹
            new_node.content = markdown_text[new_node.content_start_char:new_node.content_end_char].strip()
            
            # æ·»åŠ åˆ°çˆ¶èŠ‚ç‚¹
            current_parent.add_child(new_node)
            
            # æ›´æ–°æ ˆ
            stack.append(new_node)
        
        return root
    
    def _post_process_tree(self, root: DocumentNode, markdown_text: str):
        """
        ç¬¬ä¸‰æ­¥ï¼šåå¤„ç† - ä¿®æ­£èŒƒå›´ä¸å†…å®¹
        """
        # ä¿®æ­£span_end_charï¼ˆååºéå†ï¼‰
        self._fix_span_ranges(root)
        
        # ä¿®æ­£contentå’Œcontent_end_charï¼ˆå‰åºéå†ï¼‰
        self._fix_content_ranges(root, markdown_text)
        
        # ç¡®ä¿æ ¹èŠ‚ç‚¹çš„èŒƒå›´æ˜¯æ•´ä¸ªæ–‡æ¡£
        root.span_end_char = len(markdown_text)
    
    def _fix_span_ranges(self, node: DocumentNode):
        """ååºéå†ä¿®æ­£span_end_char"""
        # å…ˆå¤„ç†æ‰€æœ‰å­èŠ‚ç‚¹
        for child in node.children:
            self._fix_span_ranges(child)
        
        # å¦‚æœæœ‰å­èŠ‚ç‚¹ï¼Œspan_end_charåº”è¯¥ç­‰äºæœ€åä¸€ä¸ªå­èŠ‚ç‚¹çš„span_end_char
        if node.children:
            node.span_end_char = node.children[-1].span_end_char
    
    def _fix_content_ranges(self, node: DocumentNode, markdown_text: str):
        """å‰åºéå†ä¿®æ­£contentå’Œcontent_end_char"""
        # å¦‚æœæœ‰å­èŠ‚ç‚¹ï¼Œcontent_end_charåº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªå­èŠ‚ç‚¹æ ‡é¢˜å¼€å§‹ä¹‹å‰
        if node.children:
            first_child = node.children[0]
            if first_child.heading_start_char is not None:
                node.content_end_char = min(
                    node.content_end_char or len(markdown_text),
                    first_child.heading_start_char
                )
        
        # é‡æ–°æå–content
        if node.content_start_char is not None and node.content_end_char is not None:
            node.content = markdown_text[node.content_start_char:node.content_end_char].strip()
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.children:
            self._fix_content_ranges(child, markdown_text)
    
    def parse_to_chunks(self, markdown_text: str, document_id: str = "doc") -> List[Dict[str, Any]]:
        """
        è§£ææ–‡æ¡£å¹¶è¿”å›åˆ†å—åˆ—è¡¨ï¼Œç”¨äºAIé—®é¢˜ç”Ÿæˆ
        
        Returns:
            List[Dict]: åŒ…å«åˆ†å—ä¿¡æ¯çš„åˆ—è¡¨
        """
        root = self.parse_document(markdown_text, document_id)
        chunks = []
        
        def collect_chunks(node: DocumentNode, chunk_index_ref: List[int]):
            """é€’å½’æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹ä½œä¸ºåˆ†å—"""
            if node.content and node.content.strip():
                chunk = {
                    'chunk_id': node.id,
                    'paragraph_index': chunk_index_ref[0],
                    'content': node.content,
                    'start_char': node.content_start_char,
                    'end_char': node.content_end_char,
                    'document_id': document_id,
                    'level': node.level,
                    'title': node.title,
                    'heading': node.raw_heading
                }
                chunks.append(chunk)
                chunk_index_ref[0] += 1
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            for child in node.children:
                collect_chunks(child, chunk_index_ref)
        
        chunk_index_ref = [0]
        collect_chunks(root, chunk_index_ref)
        
        return chunks
    
    def generate_toc(self, root: DocumentNode) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆç›®å½•ç»“æ„ï¼Œç”¨äºå‰ç«¯æ˜¾ç¤º
        
        Returns:
            List[Dict]: ç›®å½•é¡¹åˆ—è¡¨
        """
        toc = []
        
        def build_toc(node: DocumentNode, depth: int = 0):
            """é€’å½’æ„å»ºç›®å½•"""
            if node.title and node.level > 0:  # è·³è¿‡æ ¹èŠ‚ç‚¹
                toc_item = {
                    'id': node.id,
                    'title': node.title,
                    'level': node.level,
                    'depth': depth,
                    'span_start_char': node.span_start_char,
                    'span_end_char': node.span_end_char,
                    'has_children': len(node.children) > 0,
                    'children': []
                }
                
                # é€’å½’æ·»åŠ å­èŠ‚ç‚¹
                for child in node.children:
                    child_toc = build_toc(child, depth + 1)
                    if child_toc:
                        toc_item['children'].extend(child_toc if isinstance(child_toc, list) else [child_toc])
                
                return [toc_item]
            else:
                # å¯¹äºæ ¹èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›å…¶å­èŠ‚ç‚¹çš„ç›®å½•
                result = []
                for child in node.children:
                    child_toc = build_toc(child, depth)
                    if child_toc:
                        result.extend(child_toc if isinstance(child_toc, list) else [child_toc])
                return result
        
        return build_toc(root)


# æµ‹è¯•å‡½æ•°
def test_document_parser():
    """æµ‹è¯•æ–‡æ¡£è§£æå™¨"""
    markdown_sample = """è¿™æ˜¯æ–‡æ¡£çš„å‰è¨€éƒ¨åˆ†ï¼Œæ²¡æœ‰æ ‡é¢˜ã€‚

# ç¬¬ä¸€ç« ï¼šä»‹ç»

è¿™æ˜¯ç¬¬ä¸€ç« çš„ä¸»è¦å†…å®¹ã€‚å®ƒåŒ…å«äº†é‡è¦çš„ä»‹ç»ä¿¡æ¯ã€‚

## 1.1 èƒŒæ™¯

è¿™æ˜¯1.1å°èŠ‚çš„å†…å®¹ï¼Œè®²è¿°äº†é¡¹ç›®çš„èƒŒæ™¯ã€‚

## 1.2 ç›®æ ‡

è¿™æ˜¯1.2å°èŠ‚çš„å†…å®¹ï¼Œæè¿°äº†é¡¹ç›®ç›®æ ‡ã€‚

### 1.2.1 ä¸»è¦ç›®æ ‡

è¿™æ˜¯ä¸»è¦ç›®æ ‡çš„è¯¦ç»†è¯´æ˜ã€‚

### 1.2.2 æ¬¡è¦ç›®æ ‡

è¿™æ˜¯æ¬¡è¦ç›®æ ‡çš„è¯¦ç»†è¯´æ˜ã€‚

# ç¬¬äºŒç« ï¼šæ–¹æ³•

è¿™æ˜¯ç¬¬äºŒç« çš„å†…å®¹ï¼Œä»‹ç»äº†ä½¿ç”¨çš„æ–¹æ³•ã€‚

## 2.1 æ•°æ®æ”¶é›†

æ•°æ®æ”¶é›†çš„æ–¹æ³•å’Œè¿‡ç¨‹ã€‚

## 2.2 æ•°æ®åˆ†æ

æ•°æ®åˆ†æçš„å…·ä½“æ­¥éª¤ã€‚
"""
    
    parser = DocumentParser()
    root = parser.parse_document(markdown_sample, "test_doc")
    
    print("=== æ–‡æ¡£ç»“æ„ ===")
    print(json.dumps(root.to_dict(), ensure_ascii=False, indent=2))
    
    print("\n=== åˆ†å—ä¿¡æ¯ ===")
    chunks = parser.parse_to_chunks(markdown_sample, "test_doc")
    for chunk in chunks:
        print(f"å— {chunk['paragraph_index']}: {chunk['title']} (çº§åˆ« {chunk['level']})")
        print(f"  å†…å®¹: {chunk['content'][:50]}...")
        print()
    
    print("\n=== ç›®å½•ç»“æ„ ===")
    toc = parser.generate_toc(root)
    print(json.dumps(toc, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    test_document_parser()
</file>

<file path="download_models.py">
import json
import shutil
import os

import requests
from modelscope import snapshot_download


def download_json(url):
    # ä¸‹è½½JSONæ–‡ä»¶
    response = requests.get(url)
    response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    return response.json()


def download_and_modify_json(url, local_filename, modifications):
    if os.path.exists(local_filename):
        data = json.load(open(local_filename))
        config_version = data.get('config_version', '0.0.0')
        if config_version < '1.2.0':
            data = download_json(url)
    else:
        data = download_json(url)

    # ä¿®æ”¹å†…å®¹
    for key, value in modifications.items():
        data[key] = value

    # ä¿å­˜ä¿®æ”¹åçš„å†…å®¹
    with open(local_filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


if __name__ == '__main__':
    mineru_patterns = [
        # "models/Layout/LayoutLMv3/*",
        "models/Layout/YOLO/*",
        "models/MFD/YOLO/*",
        "models/MFR/unimernet_hf_small_2503/*",
        "models/OCR/paddleocr_torch/*",
        # "models/TabRec/TableMaster/*",
        # "models/TabRec/StructEqTable/*",
    ]
    model_dir = snapshot_download('opendatalab/PDF-Extract-Kit-1.0', allow_patterns=mineru_patterns)
    layoutreader_model_dir = snapshot_download('ppaanngggg/layoutreader')
    model_dir = model_dir + '/models'
    print(f'model_dir is: {model_dir}')
    print(f'layoutreader_model_dir is: {layoutreader_model_dir}')

    # paddleocr_model_dir = model_dir + '/OCR/paddleocr'
    # user_paddleocr_dir = os.path.expanduser('~/.paddleocr')
    # if os.path.exists(user_paddleocr_dir):
    #     shutil.rmtree(user_paddleocr_dir)
    # shutil.copytree(paddleocr_model_dir, user_paddleocr_dir)

    json_url = 'https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/magic-pdf.template.json'
    config_file_name = 'magic-pdf.json'
    home_dir = os.path.expanduser('~')
    config_file = os.path.join(home_dir, config_file_name)

    json_mods = {
        'models-dir': model_dir,
        'layoutreader-model-dir': layoutreader_model_dir,
    }

    download_and_modify_json(json_url, config_file, json_mods)
    print(f'The configuration file has been configured successfully, the path is: {config_file}')
</file>

<file path="frontend/postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}
</file>

<file path="frontend/public/index.html">
<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - åŸºäºAIçš„æ–‡æ¡£åˆ†æå’Œæ€ç»´å¯¼å›¾ç”Ÿæˆå·¥å…·" />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <title>æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
  </body>
</html>
</file>

<file path="frontend/public/manifest.json">
{
  "short_name": "æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨",
  "name": "æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨",
  "icons": [
    {
      "src": "favicon.ico",
      "sizes": "64x64 32x32 24x24 16x16",
      "type": "image/x-icon"
    }
  ],
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#000000",
  "background_color": "#ffffff"
}
</file>

<file path="frontend/src/components/DocumentRenderer.css">
/* æ‹–æ‹½æ’åºç›¸å…³æ ·å¼ */
.sortable-content {
  position: relative;
}

/* æ®µè½é«˜äº®æ ·å¼ */
.semantic-paragraph-highlighted {
  background-color: rgba(59, 130, 246, 0.1) !important;
  border-left: 4px solid #3b82f6 !important;
  transform: translateX(2px);
  transition: all 0.2s ease-in-out;
}

/* æ·±è‰²æ¨¡å¼ä¸‹çš„æ®µè½é«˜äº® */
.dark .semantic-paragraph-highlighted {
  background-color: rgba(59, 130, 246, 0.15) !important;
  border-left-color: #60a5fa !important;
}

/* æ‹–æ‹½çŠ¶æ€æ ·å¼ */
.sortable-content [data-dnd-dragging="true"] {
  opacity: 0.7;
}

/* æ‹–æ‹½ä¸­çš„æ®µè½ */
.sortable-content .paragraph-block {
  transition: all 0.2s ease-in-out;
}

/* åˆ†å‰²çº¿æ‹–æ‹½æ ·å¼ */
.sortable-divider {
  position: relative;
}

/* åˆ†å‰²çº¿æ‚¬åœæ—¶ä¸åšä»»ä½•å˜åŒ– */
.sortable-divider:hover {
  /* å®Œå…¨å»æ‰æ‚¬åœæ•ˆæœ */
}

/* æ‹–æ‹½é¢„è§ˆæ ·å¼ */
.sortable-content [data-dnd-over="true"] {
  border: 2px dashed #3b82f6;
  border-radius: 8px;
  background-color: rgba(59, 130, 246, 0.05);
}

.dark .sortable-content [data-dnd-over="true"] {
  border-color: #60a5fa;
  background-color: rgba(59, 130, 246, 0.1);
}

/* æ‹–æ‹½æ‰‹æŸ„åŠ¨ç”» */
.sortable-content .group .absolute {
  transition: all 0.2s ease-in-out;
}

/* æ‹–æ‹½æ—¶çš„é˜´å½±æ•ˆæœ */
.sortable-content [data-dnd-dragging="true"] {
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  z-index: 1000;
}

/* å“åº”å¼æ‹–æ‹½æ‰‹æŸ„ */
@media (max-width: 768px) {
  .sortable-content {
    touch-action: none;
  }
}
</file>

<file path="frontend/src/components/PDFViewer.js">
import React from 'react';
import { File } from 'lucide-react';

const PDFViewer = ({ pdfBase64 }) => {
  if (!pdfBase64) {
    return (
      <div className="flex items-center justify-center h-96 bg-gray-100 rounded-lg">
        <div className="text-center">
          <File className="h-12 w-12 text-gray-400 mx-auto mb-2" />
          <p className="text-gray-500">PDFæ–‡ä»¶ä¸å¯ç”¨</p>
        </div>
      </div>
    );
  }

  return (
    <div className="w-full h-full bg-white">
      <embed
        src={`data:application/pdf;base64,${pdfBase64}`}
        type="application/pdf"
        width="100%"
        height="100%"
        className="border-0 rounded-none block"
        style={{ 
          minHeight: '100%',
          margin: 0,
          padding: 0,
          display: 'block'
        }}
      />
    </div>
  );
};

export default PDFViewer;
</file>

<file path="frontend/src/components/SortableDivider.js">
import React from 'react';
import { useSortable } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import LogicalDivider from './LogicalDivider';

const SortableDivider = ({ id, nodeInfo, className = '' }) => {
  const {
    attributes,
    listeners,
    setNodeRef,
    transform,
    transition,
    isDragging
  } = useSortable({ id });

  const style = {
    transform: CSS.Translate.toString(transform),
    transition,
    opacity: isDragging ? 0.5 : 1,
    zIndex: isDragging ? 100 : 'auto',
  };

  return (
    <div
      ref={setNodeRef}
      style={style}
      className={`sortable-divider ${isDragging ? 'shadow-lg' : ''} ${className}`}
    >
      <LogicalDivider 
        nodeInfo={nodeInfo}
        // å°†æ‹–æ‹½å±æ€§ä¼ é€’ç»™ LogicalDivider çš„æ‹–æ‹½æ‰‹æŸ„
        dragHandleProps={{
          ...attributes,
          ...listeners,
        }}
      />
    </div>
  );
};

export default SortableDivider;
</file>

<file path="frontend/src/components/SortableParagraph.js">
import React from 'react';
import { useSortable } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';

const SortableParagraph = ({ id, children, className = '' }) => {
  const {
    attributes,
    listeners,
    setNodeRef,
    transform,
    transition,
    isDragging
  } = useSortable({ id });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition,
    opacity: isDragging ? 0.5 : 1,
  };

  return (
    <div
      ref={setNodeRef}
      style={style}
      className={`sortable-paragraph ${className}`}
    >
      {children}
    </div>
  );
};

export default SortableParagraph;
</file>

<file path="frontend/src/components/TableOfContents.js">
import React from 'react';
import { FileText } from 'lucide-react';

const TableOfContents = ({ toc, expandedItems, activeItem, onToggle, onItemClick }) => {
  const renderTocItem = (item, depth = 0) => {
    const hasChildren = item.children && item.children.length > 0;
    const isExpanded = expandedItems.has(item.id);
    const isActive = activeItem === item.id;
    
    return (
      <div key={item.id} className={`${depth > 0 ? 'ml-3' : ''}`}>
        <div 
          className={`flex items-center py-1 px-2 text-xs rounded transition-colors cursor-pointer ${
            isActive 
              ? 'bg-blue-100 text-blue-800 border-l-2 border-blue-500' 
              : 'text-gray-700 hover:bg-gray-100'
          }`}
          onClick={() => onItemClick(item)}
        >
          <div className="flex items-center flex-1 min-w-0">
            {hasChildren && (
              <button
                onClick={(e) => {
                  e.stopPropagation();
                  onToggle(item.id);
                }}
                className="mr-1 p-0.5 hover:bg-gray-200 rounded"
              >
                {isExpanded ? 'â–¼' : 'â–¶'}
              </button>
            )}
            <span 
              className={`truncate ${item.level === 1 ? 'font-semibold' : 'font-normal'}`}
              title={item.title}
            >
              {item.title}
            </span>
          </div>
          <span className="text-xs text-gray-400 ml-1">
            H{item.level}
          </span>
        </div>
        
        {hasChildren && isExpanded && (
          <div className="mt-1">
            {item.children.map(child => renderTocItem(child, depth + 1))}
          </div>
        )}
      </div>
    );
  };

  if (!toc || toc.length === 0) {
    return (
      <div className="p-4 text-center text-gray-500 text-xs">
        <FileText className="w-8 h-8 mx-auto mb-2 text-gray-300" />
        <p>æ–‡æ¡£æ²¡æœ‰æ ‡é¢˜ç»“æ„</p>
        <p>æˆ–æ­£åœ¨åˆ†æä¸­...</p>
      </div>
    );
  }

  return (
    <div className="p-2 space-y-1">
      {toc.map(item => renderTocItem(item))}
    </div>
  );
};

export default TableOfContents;
</file>

<file path="frontend/src/components/ThemeToggle.js">
import React from 'react';
import { useTheme } from '../contexts/ThemeContext';

const ThemeToggle = ({ className = '' }) => {
  const { isDarkMode, toggleTheme } = useTheme();

  return (
    <button
      onClick={toggleTheme}
      className={`
        relative inline-flex items-center justify-center w-10 h-10 
        rounded-full transition-colors duration-200 
        bg-gray-100 hover:bg-gray-200 
        dark:bg-gray-800 dark:hover:bg-gray-700
        focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2
        dark:focus:ring-offset-gray-900
        ${className}
      `}
      title={isDarkMode ? 'åˆ‡æ¢åˆ°äº®è‰²æ¨¡å¼' : 'åˆ‡æ¢åˆ°æš—è‰²æ¨¡å¼'}
      aria-label={isDarkMode ? 'åˆ‡æ¢åˆ°äº®è‰²æ¨¡å¼' : 'åˆ‡æ¢åˆ°æš—è‰²æ¨¡å¼'}
    >
      {isDarkMode ? (
        // å¤ªé˜³å›¾æ ‡ (äº®è‰²æ¨¡å¼)
        <svg
          className="w-5 h-5 text-yellow-500"
          fill="currentColor"
          viewBox="0 0 24 24"
        >
          <path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0zM18.894 6.166a.75.75 0 00-1.06-1.06l-1.591 1.59a.75.75 0 101.06 1.061l1.591-1.59zM21.75 12a.75.75 0 01-.75.75h-2.25a.75.75 0 010-1.5H21a.75.75 0 01.75.75zM17.834 18.894a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 10-1.061 1.06l1.59 1.591zM12 18a.75.75 0 01.75.75V21a.75.75 0 01-1.5 0v-2.25A.75.75 0 0112 18zM7.758 17.303a.75.75 0 00-1.061-1.06l-1.591 1.59a.75.75 0 001.06 1.061l1.591-1.59zM6 12a.75.75 0 01-.75.75H3a.75.75 0 010-1.5h2.25A.75.75 0 016 12zM6.697 7.757a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 00-1.061 1.06l1.59 1.591z" />
        </svg>
      ) : (
        // æœˆäº®å›¾æ ‡ (æš—è‰²æ¨¡å¼)
        <svg
          className="w-5 h-5 text-gray-600"
          fill="currentColor"
          viewBox="0 0 24 24"
        >
          <path fillRule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clipRule="evenodd" />
        </svg>
      )}
    </button>
  );
};

export default ThemeToggle;
</file>

<file path="frontend/src/contexts/ThemeContext.js">
import React, { createContext, useContext, useState, useEffect } from 'react';

const ThemeContext = createContext();

export const useTheme = () => {
  const context = useContext(ThemeContext);
  if (!context) {
    throw new Error('useTheme must be used within a ThemeProvider');
  }
  return context;
};

export const ThemeProvider = ({ children }) => {
  const [isDarkMode, setIsDarkMode] = useState(() => {
    // æ£€æŸ¥localStorageä¸­çš„ä¸»é¢˜è®¾ç½®
    const savedTheme = localStorage.getItem('theme');
    if (savedTheme) {
      return savedTheme === 'dark';
    }
    // å¦‚æœæ²¡æœ‰ä¿å­˜çš„è®¾ç½®ï¼Œæ£€æŸ¥ç³»ç»Ÿåå¥½
    return window.matchMedia('(prefers-color-scheme: dark)').matches;
  });

  useEffect(() => {
    // æ›´æ–°documentçš„classä»¥åº”ç”¨æš—æ¨¡å¼
    if (isDarkMode) {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
    
    // å°†è®¾ç½®ä¿å­˜åˆ°localStorage
    localStorage.setItem('theme', isDarkMode ? 'dark' : 'light');
  }, [isDarkMode]);

  const toggleTheme = () => {
    setIsDarkMode(!isDarkMode);
  };

  const value = {
    isDarkMode,
    toggleTheme,
    theme: isDarkMode ? 'dark' : 'light'
  };

  return (
    <ThemeContext.Provider value={value}>
      {children}
    </ThemeContext.Provider>
  );
};
</file>

<file path="frontend/src/hooks/usePanelResize.js">
import { useState, useEffect, useCallback, useRef } from 'react';

export const usePanelResize = () => {
  // åˆ†å‰²é¢æ¿ç›¸å…³çŠ¶æ€ - è°ƒæ•´ä¸ºä¸‰åˆ—å¸ƒå±€
  const [tocPanelWidth, setTocPanelWidth] = useState(20); // ç›®å½•æ å®½åº¦
  const [leftPanelWidth, setLeftPanelWidth] = useState(45); // æ–‡æ¡£æŸ¥çœ‹å™¨å®½åº¦  
  const [isDragging, setIsDragging] = useState(false);
  const [dragTarget, setDragTarget] = useState(null); // 'toc-divider' æˆ– 'main-divider'
  const [showToc, setShowToc] = useState(false);
  const containerRef = useRef(null);

  // ä½¿ç”¨useRefæ¥ä¿å­˜äº‹ä»¶å¤„ç†å‡½æ•°çš„å¼•ç”¨
  const handleMouseMoveRef = useRef(null);
  const handleMouseUpRef = useRef(null);

  // æ‹–æ‹½å¤„ç†å‡½æ•°
  const handleMouseDown = useCallback((e, target) => {
    setIsDragging(true);
    setDragTarget(target);
    e.preventDefault();
  }, []);

  // åˆ›å»ºäº‹ä»¶å¤„ç†å‡½æ•°
  useEffect(() => {
    handleMouseMoveRef.current = (e) => {
      if (!isDragging || !containerRef.current || !dragTarget) return;
      
      const container = containerRef.current;
      const containerRect = container.getBoundingClientRect();
      const containerWidth = containerRect.width;
      const mouseX = e.clientX - containerRect.left;
      
      if (dragTarget === 'toc-divider') {
        // è°ƒæ•´ç›®å½•æ å®½åº¦
        const newTocWidth = (mouseX / containerWidth) * 100;
        const minTocWidth = 15;
        const maxTocWidth = 30;
        
        if (newTocWidth >= minTocWidth && newTocWidth <= maxTocWidth) {
          setTocPanelWidth(newTocWidth);
        }
      } else if (dragTarget === 'main-divider') {
        // è°ƒæ•´ä¸»å†…å®¹åŒºåŸŸå®½åº¦
        const currentTocWidth = showToc ? tocPanelWidth : 0;
        const availableWidth = 100 - currentTocWidth;
        const newLeftWidth = ((mouseX - (currentTocWidth * containerWidth / 100)) / containerWidth) * 100;
        const minLeftWidth = 25;
        const maxLeftWidth = availableWidth - 25; // ä¸ºå³ä¾§ç•™å‡ºè‡³å°‘25%
        
        if (newLeftWidth >= minLeftWidth && newLeftWidth <= maxLeftWidth) {
          setLeftPanelWidth(newLeftWidth);
        }
      }
    };

    handleMouseUpRef.current = () => {
      setIsDragging(false);
      setDragTarget(null);
    };
  }, [isDragging, dragTarget, tocPanelWidth, showToc]);

  // ç®¡ç†äº‹ä»¶ç›‘å¬å™¨
  useEffect(() => {
    // ä½¿ç”¨å±€éƒ¨å˜é‡å­˜å‚¨äº‹ä»¶å¤„ç†å‡½æ•°çš„å¼•ç”¨ï¼Œé¿å…é—­åŒ…é—®é¢˜
    let localHandleMouseMove = null;
    let localHandleMouseUp = null;
    
    const handleMouseMove = (e) => {
      if (handleMouseMoveRef.current) {
        handleMouseMoveRef.current(e);
      }
    };

    const handleMouseUp = () => {
      if (handleMouseUpRef.current) {
        handleMouseUpRef.current();
      }
    };

    if (isDragging) {
      // ä½¿ç”¨window.documentç¡®ä¿è·å–å…¨å±€documentå¯¹è±¡ï¼Œå¹¶æ£€æŸ¥addEventListeneræ–¹æ³•æ˜¯å¦å­˜åœ¨
      const globalDocument = window.document;
      if (globalDocument && typeof globalDocument.addEventListener === 'function') {
        localHandleMouseMove = handleMouseMove;
        localHandleMouseUp = handleMouseUp;
        
        globalDocument.addEventListener('mousemove', localHandleMouseMove, { passive: false });
        globalDocument.addEventListener('mouseup', localHandleMouseUp, { passive: false });
        
        if (globalDocument.body) {
          globalDocument.body.style.cursor = 'col-resize';
          globalDocument.body.style.userSelect = 'none';
        }
      }
    }

    // æ¸…ç†å‡½æ•° - æ·»åŠ å¤šé‡å®‰å…¨æ£€æŸ¥
    return () => {
      try {
        // ä½¿ç”¨window.documentç¡®ä¿è·å–å…¨å±€documentå¯¹è±¡
        const globalDocument = window.document;
        if (globalDocument && typeof globalDocument.removeEventListener === 'function') {
          if (localHandleMouseMove) {
            globalDocument.removeEventListener('mousemove', localHandleMouseMove);
          }
          if (localHandleMouseUp) {
            globalDocument.removeEventListener('mouseup', localHandleMouseUp);
          }
        }
        
        // é‡ç½®æ ·å¼
        if (globalDocument && globalDocument.body) {
          globalDocument.body.style.cursor = '';
          globalDocument.body.style.userSelect = '';
        }
      } catch (error) {
        // é™é»˜å¤„ç†æ¸…ç†é”™è¯¯ï¼Œé¿å…å½±å“åº”ç”¨è¿è¡Œ
        console.warn('æ¸…ç†äº‹ä»¶ç›‘å¬å™¨æ—¶å‡ºé”™:', error);
      }
    };
  }, [isDragging]);

  return {
    tocPanelWidth,
    leftPanelWidth,
    isDragging,
    dragTarget,
    showToc,
    setShowToc,
    containerRef,
    handleMouseDown
  };
};
</file>

<file path="frontend/src/index.js">
import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);
</file>

<file path="frontend/src/utils/dataConverter.js">
/**
 * å°†APIæ•°æ®è½¬æ¢ä¸ºReact Flowæ ¼å¼
 * @param {Object} apiData - åŒ…å«mermaid_stringå’Œnode_mappingsçš„å¯¹è±¡
 * @returns {Object} åŒ…å«nodeså’Œedgesæ•°ç»„çš„å¯¹è±¡
 */
export const convertDataToReactFlow = (apiData) => {
  console.log('ğŸ”§ [æ•°æ®è½¬æ¢] å¼€å§‹è½¬æ¢æ•°æ®:', apiData);
  
  if (!apiData || !apiData.node_mappings || !apiData.mermaid_string) {
    console.log('ğŸ”§ [æ•°æ®è½¬æ¢] æ•°æ®æ— æ•ˆï¼Œè¿”å›ç©ºæ•°ç»„');
    console.log('ğŸ”§ [æ•°æ®è½¬æ¢] apiDataå­˜åœ¨:', !!apiData);
    console.log('ğŸ”§ [æ•°æ®è½¬æ¢] node_mappingså­˜åœ¨:', !!(apiData && apiData.node_mappings));
    console.log('ğŸ”§ [æ•°æ®è½¬æ¢] mermaid_stringå­˜åœ¨:', !!(apiData && apiData.mermaid_string));
    return { nodes: [], edges: [] };
  }

  const { mermaid_string, node_mappings } = apiData;
  
  console.log('ğŸ”§ [æ•°æ®è½¬æ¢] node_mappings:', node_mappings);
  console.log('ğŸ”§ [æ•°æ®è½¬æ¢] mermaid_string:', mermaid_string);

  // åˆ›å»ºnodesæ•°ç»„
  const nodes = Object.keys(node_mappings).map((nodeId, index) => ({
    id: nodeId,
    data: { 
      label: node_mappings[nodeId].text_snippet || nodeId,
      paragraph_ids: node_mappings[nodeId].paragraph_ids || []
    },
    position: { x: 0, y: 0 }, // åˆå§‹ä½ç½®ï¼Œå°†åœ¨å¸ƒå±€é˜¶æ®µæ›´æ–°
    type: 'default'
  }));
  
  console.log('ğŸ”§ [æ•°æ®è½¬æ¢] åˆ›å»ºçš„èŠ‚ç‚¹:', nodes);

  // ä»mermaid_stringè§£æè¿æ¥å…³ç³»åˆ›å»ºedgesæ•°ç»„
  const edges = [];
  
  // åŒ¹é…Mermaidå›¾è¡¨ä¸­çš„è¿æ¥å…³ç³»ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
  // A --> B, A -> B, A --- B, A -- B
  // æ”¯æŒå¸¦æ ‡ç­¾çš„èŠ‚ç‚¹ï¼Œå¦‚ï¼šA[æ ‡ç­¾] --> B[æ ‡ç­¾]
  const connectionRegex = /([A-Za-z0-9_]+)(?:\[[^\]]*\])?\s*(-{1,2}>?|={1,2}>?)\s*([A-Za-z0-9_]+)(?:\[[^\]]*\])?/g;
  let match;
  let edgeIndex = 0;

  while ((match = connectionRegex.exec(mermaid_string)) !== null) {
    const [, source, connector, target] = match;
    
    console.log('ğŸ”§ [æ•°æ®è½¬æ¢] æ‰¾åˆ°è¿æ¥:', source, connector, target);
    
    // ç¡®ä¿æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹éƒ½å­˜åœ¨äºnode_mappingsä¸­
    if (node_mappings[source] && node_mappings[target]) {
      const edge = {
        id: `edge-${edgeIndex++}`,
        source: source,
        target: target,
        type: 'smoothstep', // ä½¿ç”¨å¹³æ»‘çš„è¾¹ç±»å‹
        animated: false
      };
      edges.push(edge);
    } else {
      console.warn('ğŸ”§ [æ•°æ®è½¬æ¢] è·³è¿‡æ— æ•ˆè¾¹:', source, '->', target, 
        '(æºèŠ‚ç‚¹å­˜åœ¨:', !!node_mappings[source], 'ç›®æ ‡èŠ‚ç‚¹å­˜åœ¨:', !!node_mappings[target], ')');
    }
  }
  
  console.log('ğŸ”§ [æ•°æ®è½¬æ¢] æœ€ç»ˆç»“æœ:');
  console.log('ğŸ”§ [æ•°æ®è½¬æ¢] èŠ‚ç‚¹æ•°é‡:', nodes.length);
  console.log('ğŸ”§ [æ•°æ®è½¬æ¢] è¾¹æ•°é‡:', edges.length);

  return { nodes, edges };
};
</file>

<file path="frontend/src/utils/flowDiagramExample.js">
import React, { useRef } from 'react';
import FlowDiagram from '../components/FlowDiagram';

/**
 * FlowDiagramä½¿ç”¨ç¤ºä¾‹
 * å±•ç¤ºå¦‚ä½•æ›¿æ¢ç°æœ‰çš„MermaidDiagramç»„ä»¶
 */
const FlowDiagramExample = () => {
  const flowDiagramRef = useRef(null);

  // ç¤ºä¾‹Mermaidä»£ç 
  const sampleMermaidCode = `
    graph TD
      A[å¼€å§‹åˆ†æ] --> B[è¯†åˆ«æ ¸å¿ƒè®ºç‚¹]
      B --> C[åˆ†æè®ºè¯ç»“æ„]
      C --> D[è¯„ä¼°è¯æ®è´¨é‡]
      D --> E[æ£€æŸ¥é€»è¾‘å…³ç³»]
      E --> F[å¾—å‡ºç»“è®º]
      F --> G[æ’°å†™æ€»ç»“]
      
      B --> H[è¯†åˆ«åå¯¹è§‚ç‚¹]
      H --> I[åˆ†æåé©³ç­–ç•¥]
      I --> E
  `;

  // èŠ‚ç‚¹ç‚¹å‡»å¤„ç†å‡½æ•°ï¼ˆä¸MermaidDiagramå…¼å®¹ï¼‰
  const handleNodeClick = (nodeId, event) => {
    console.log('èŠ‚ç‚¹è¢«ç‚¹å‡»:', nodeId, event);
    alert(`ç‚¹å‡»äº†èŠ‚ç‚¹: ${nodeId}`);
  };

  // æµ‹è¯•refæ–¹æ³•
  const handleEnsureVisible = () => {
    if (flowDiagramRef.current) {
      // è°ƒç”¨ä¸MermaidDiagramå…¼å®¹çš„æ–¹æ³•
      flowDiagramRef.current.ensureNodeVisible('C');
    }
  };

  const handleFitView = () => {
    if (flowDiagramRef.current) {
      flowDiagramRef.current.fitView();
    }
  };

  return (
    <div style={{ width: '100%', height: '600px', padding: '20px' }}>
      <h2>React Flow å›¾è¡¨ç¤ºä¾‹</h2>
      
      <div style={{ marginBottom: '10px' }}>
        <button onClick={handleEnsureVisible} style={{ marginRight: '10px' }}>
          èšç„¦åˆ°èŠ‚ç‚¹C
        </button>
        <button onClick={handleFitView}>
          é€‚åº”è§†å›¾
        </button>
      </div>

      <div style={{ width: '100%', height: '500px', border: '1px solid #ccc' }}>
        <FlowDiagram 
          ref={flowDiagramRef}
          code={sampleMermaidCode}
          onNodeClick={handleNodeClick}
          layoutOptions={{
            direction: 'TB', // ä¸Šåˆ°ä¸‹å¸ƒå±€
            nodeSpacing: 100,
            rankSpacing: 150
          }}
        />
      </div>
    </div>
  );
};

/**
 * åœ¨ç°æœ‰é¡¹ç›®ä¸­æ›¿æ¢MermaidDiagramçš„æ­¥éª¤ï¼š
 * 
 * 1. å¯¼å…¥æ–°ç»„ä»¶ï¼š
 *    import FlowDiagram from './FlowDiagram';
 *    // æ›¿æ¢åŸæ¥çš„ï¼š
 *    // import MermaidDiagram from './MermaidDiagram';
 * 
 * 2. æ›¿æ¢ç»„ä»¶ä½¿ç”¨ï¼š
 *    <FlowDiagram 
 *      ref={mermaidDiagramRef}  // ä¿æŒç›¸åŒçš„ref
 *      code={document.mermaid_code_demo}  // ä¿æŒç›¸åŒçš„props
 *      onNodeClick={handleNodeClick}      // ä¿æŒç›¸åŒçš„å›è°ƒ
 *    />
 * 
 * 3. refæ–¹æ³•ä¿æŒå…¼å®¹ï¼š
 *    - ensureNodeVisible(nodeId) 
 *    - fitView()
 *    - getReactFlowInstance() (æ–°å¢)
 * 
 * 4. åœ¨ViewerPageRefactored.jsä¸­çš„å…·ä½“æ›¿æ¢ï¼š
 *    æ‰¾åˆ°è¿™è¡Œï¼š
 *    <MermaidDiagram 
 *      ref={mermaidDiagramRef}
 *      code={document.mermaid_code_demo}
 *      onNodeClick={handleNodeClick}
 *    />
 *    
 *    æ›¿æ¢ä¸ºï¼š
 *    <FlowDiagram 
 *      ref={mermaidDiagramRef}
 *      code={document.mermaid_code_demo}
 *      onNodeClick={handleNodeClick}
 *    />
 */

export default FlowDiagramExample;
</file>

<file path="frontend/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./src/**/*.{js,jsx,ts,tsx}",
  ],
  theme: {
    extend: {
      colors: {
        primary: {
          50: '#eff6ff',
          100: '#dbeafe',
          200: '#bfdbfe',
          300: '#93c5fd',
          400: '#60a5fa',
          500: '#3b82f6',
          600: '#2563eb',
          700: '#1d4ed8',
          800: '#1e40af',
          900: '#1e3a8a',
        },
      },
      typography: {
        DEFAULT: {
          css: {
            maxWidth: 'none',
            color: '#374151',
            a: {
              color: '#3b82f6',
              '&:hover': {
                color: '#2563eb',
              },
            },
            'h1, h2, h3, h4': {
              color: '#111827',
            },
            code: {
              color: '#111827',
              backgroundColor: '#f3f4f6',
              paddingLeft: '0.25rem',
              paddingRight: '0.25rem',
              paddingTop: '0.125rem',
              paddingBottom: '0.125rem',
              borderRadius: '0.25rem',
              fontSize: '0.875em',
            },
            'code::before': {
              content: '""',
            },
            'code::after': {
              content: '""',
            },
          },
        },
      },
    },
  },
  plugins: [
    require('@tailwindcss/typography'),
  ],
}
</file>

<file path="markdown.md">
å†™åœ¨å‰é¢ï¼š

æœ¬æ–‡æ˜¯å¯¹ For they know not what they do ç¬¬äº”ç« ç¬¬ä¸€èŠ‚â€œWhy Should a Dialectician Learn toCount to Four?â€ï¼ˆP179-197ï¼‰é‡æ–°ç¿»è¯‘ã€‚è¯‘è€…æ°´å¹³æœ‰é™ï¼Œæ•…æ¬¢è¿å¹¶å¸Œæœ›è¯»è€…æŒ‡å‡ºå…¶ä¸­ä¸é€šé¡ºä¸ä¸ç†è§£åœ°æ–¹å¹¶æå‡ºä¿®æ”¹å»ºè®®ï¼Œå…ˆè¡Œæ„Ÿè°¢ã€‚

è“è‰²éƒ¨åˆ†æ˜¯ç¿»è¯‘æœ‰å¾…è¿›ä¸€æ­¥æ”¹è¿›çš„åœ°æ–¹ã€‚æ–œä½“ä¸ºåŸæ–‡æ‰€åŠ ï¼Œè€Œæ·±è“è‰²åŠ ç²—çš„æ–œä½“çœç•¥å·ç”¨ä»¥è¡¨ç¤ºåœ¨æœ¬è¯‘æ–‡æ‰€ç•¥å»æœªç¿»çš„æ–‡æ®µï¼ˆåˆ«é—®ä¸ºä»€ä¹ˆä¸ç¿»å…¨ï¼Œé—®å°±æ˜¯æ‡’[æ»‘ç¨½]ï¼‰ã€‚æœ¯è¯­å¯¹ç…§è¡¨å¦‚ä¸‹ï¼š

ç”¨è¯­å¯¹ç…§è¡¨  

<html><body><table><tr><td>actual</td><td>å®é™…çš„</td></tr><tr><td>virtual</td><td>è™šæ‹Ÿçš„</td></tr><tr><td>real</td><td>çœŸå®çš„</td></tr><tr><td>reality</td><td>ç°å®</td></tr><tr><td>the real</td><td>çœŸå®ã€å®åœ¨ç•Œ</td></tr><tr><td>realization</td><td>å®ç°</td></tr><tr><td>actualization</td><td>å®é™…åŒ–</td></tr><tr><td>the symbolic</td><td>è±¡å¾ç•Œã€ç¬¦å·ç•Œ</td></tr><tr><td>the imaginary</td><td>æƒ³è±¡ç•Œ</td></tr><tr><td>viewpoint</td><td>è§†ç‚¹</td></tr><tr><td>normal</td><td>å¸¸æ€çš„</td></tr><tr><td>normality</td><td>å¸¸æ€</td></tr><tr><td>inversion</td><td>é¢ å€’ã€å€’ç½®ã€åè½¬</td></tr><tr><td>image</td><td>åƒã€å½¢è±¡</td></tr><tr><td>reflexion</td><td>åå°„ã€åæ˜ ï¼ˆå¾·å¤ï¼šæŠ˜è¿”ã€åæ€ï¼‰</td></tr><tr><td>moment</td><td>ç¯èŠ‚ (å¾·å¤æ¢—)</td></tr><tr><td>excess</td><td>æº¢å‡º/è¿‡å‰©</td></tr><tr><td>gap</td><td>è£‚éš™</td></tr><tr><td>reduce</td><td>ç¼©å‡</td></tr></table></body></html>

è¯‘è€…ï¼šCicada

# ä¸ºä»€ä¹ˆä¸€ä½è¾©è¯å­¦å®¶åº”è¯¥å­¦ç€æ•°åˆ°å››ï¼Ÿ

ä¸‰å…ƒç»„/ä¸‰ä½ä¸€ä½“ä¸å…¶æº¢å‡º/è¿‡å‰©â€”â€”æ–°æ•™ã€é›…å„å®¾ä¸»ä¹‰â€¦â€¦ä¸å…¶ä»–â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€â€”â€œä½ æ‰‹æŒ‡çš„ä¸€æ•²â€â€¦â€¦â€” ä¸ºä»€ä¹ˆçœŸç†æ€»æ˜¯æ”¿æ²»æ€§çš„ï¼Ÿ

Â·ä¸‰å…ƒç»„/ä¸‰ä½ä¸€ä½“ä¸å…¶æº¢å‡º/è¿‡å‰©

ä¸€ä½é»‘æ ¼å°”æ´¾çš„è¾©è¯å­¦å®¶å¿…é¡»å­¦ç€æ•°åˆ°å¤šå°‘å‘¢ï¼Ÿå¤§å¤šæ•°é»‘æ ¼å°”çš„è§£é‡Šè€…ï¼Œæ›´ä¸ç”¨è¯´ä»–çš„æ‰¹è¯„è€…ï¼Œéƒ½è¯•å›¾ä¸€è‡´åœ°è¯´æœæˆ‘ä»¬ï¼Œæ­£ç¡®çš„ç­”æ¡ˆæ˜¯ï¼šåˆ°ä¸‰ï¼ˆè¾©è¯çš„ä¸‰å…ƒç»„[the dialectical triad]ï¼Œç­‰ç­‰ï¼‰ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜äº’ç›¸äº‰å¤ºè°èƒ½æ›´æœ‰è¯´æœåŠ›åœ°å”¤èµ·æˆ‘ä»¬å¯¹â€œç¬¬å››æ–¹â€çš„æ³¨æ„ã€‚è¿™ä¸ªç¬¬å››æ–¹å°±æ˜¯ä¸å¯è¾©è¯çš„æº¢å‡º/è¿‡å‰©ï¼Œæ˜¯æ­»äº¡ä¹‹å¤„æ‰€ï¼ˆâ€¦ ï¼‰ã€‚æ®è¯´å®ƒé€ƒç¦»è¾©è¯æ³•çš„æŒæ¡ï¼Œå°½ç®¡ï¼ˆæˆ–è€…æ›´å‡†ç¡®åœ°è¯´ï¼Œå› ä¸ºï¼‰å®ƒæ˜¯è¾©è¯æ³•è¿åŠ¨çš„å†…åœ¨å¯èƒ½æ€§æ¡ä»¶ï¼šåœ¨å…¶ç»“æœï¼ˆResultï¼‰ä¸­ä¸èƒ½è¢«æ‰¬å¼ƒ[aufgehoben]ã€ä¸èƒ½é‡æ–°è¢«æ”¶å…¥çš„ï¼ˆre-collectedï¼‰çº¯æ”¯å‡ºæ€§çš„å¦å®šæ€§ã€‚

ä¸å¹¸çš„æ˜¯ï¼ŒæŒ‰ç…§è¿™ç§å¯¹é»‘æ ¼å°”çš„æ‰¹è¯„çš„æƒ¯å¸¸ï¼Œé‚£é»‘æ ¼å°”åœ¨æ­¤çš„é—®é¢˜ä¸å¸ŒåŒºæŸ¯å…‹çš„ç”µå½±ã€Šå“ˆé‡Œã€‹ä¸­çš„åŒåä¸»äººå…¬çš„é—®é¢˜ä¸€æ ·ï¼šä»–ä¸é‚£ä¹ˆå®¹æ˜“åœ°åŒæ„ä»–çš„è‘¬ç¤¼â€”â€”ä»”ç»†ä¸€çœ‹ï¼Œä»¥ä¸‹æƒ…å½¢å¾ˆå¿«å°±ä¼šå˜å¾—æ˜¾è€Œæ˜“è§ï¼šæ‰¹è¯„å®¶ä»ä»–ä»¬çš„å¸½å­ä¸­å¾—å‡ºçš„æ‰€è°“æ¯ç­æ€§çš„è´£éš¾å®é™…ä¸Šæ„æˆäº†è¾©è¯è¿åŠ¨æœ¬èº«çš„å…³é”®æ–¹é¢ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»†å¿ƒçš„è¯»è€…ä¸ä»…ä¼šç«‹å³æƒ³èµ·è®¸å¤šç‰¹æ®Šçš„æ¡ˆä¾‹ï¼Œåƒâ€œä¸»è§‚é€»è¾‘â€ç¬¬ä¸€éƒ¨åˆ†ä¸­çš„å››ç§åˆ¤æ–­ï¼Œè€Œä¸”è¿˜ä¼šæƒ³èµ·è¿™ä¸€äº‹å®â€”â€”é»‘æ ¼å°”å°†è¾©è¯æ³•è¿åŠ¨æœ¬èº«æ‰€ç‰¹æœ‰çš„å››é‡ç»“æ„ï¼ˆquadruplicityï¼‰ä¸»é¢˜åŒ–äº†ï¼šåœ¨æœ€ç»ˆçš„ç»“æœï¼ˆResultï¼‰ä¸­ï¼Œé‚£ä¸å¤å­˜åœ¨ã€å˜å¾—ä¸å¯è§çš„è‡ªç›¸å…³å¦å®šæ€§çš„çº¯ç²¹è™šæ— çš„æº¢å‡º/è¿‡å‰©ï¼ˆthe excess of the pure nothingness ofself-relating negativityï¼‰ã€‚åœ¨ã€Šé€»è¾‘å­¦ã€‹çš„æœ€åä¸€ç« å…³äºè¾©è¯è¿‡ç¨‹çš„åŸºæœ¬çŸ©é˜µä¸­ï¼Œä»–æŒ‡å‡ºï¼Œéšç€ä¸»ä½“ä½œä¸ºâ€œä»€ä¹ˆéƒ½ä¸ç®—ï¼ˆcounts for nothingï¼‰â€çš„å¤šä½™ç¯èŠ‚ï¼ˆthesurplus-momentï¼‰ï¼Œè¿™ä¸€è¿‡ç¨‹çš„ç¯èŠ‚å¯ä»¥æ•°åˆ°ä¸‰æˆ–å››ã€‚

åœ¨æ–¹æ³•çš„è¿™ä¸ªè½¬æŠ˜ç‚¹ä¸Šï¼Œè®¤è¯†è¿‡ç¨‹åŒæ—¶è¿”å›åˆ°è‡ªèº«å»äº†ã€‚è¿™ä¸ªå¦å®šæ€§ä½œä¸ºè‡ªèº«æ‰¬å¼ƒç€çš„ï¼ˆself-sublatingï¼‰çŸ›ç›¾ï¼Œæ˜¯ç¬¬ä¸€ä¸ªç›´æ¥æ€§ä¸ç®€å•æ™®éæ€§çš„æ¢å¤ï¼›å› ä¸ºä»–è€…çš„ä»–è€…ã€å¦å®šçš„å¦å®šï¼Œç›´æ¥å°±æ˜¯è‚¯å®šï¼ˆthe positiveï¼‰ã€åŒä¸€ï¼ˆtheidenticalï¼‰ã€æ™®éï¼ˆthe universalï¼‰ã€‚å¦‚æœæœ‰äººåšæŒå»æ•°çš„è¯ï¼Œè¿™ç¬¬äºŒä¸ªç›´æ¥ä¹‹ç‰©ï¼ˆthis second immediateï¼‰ï¼Œåœ¨ä½œä¸ºæ•´ä½“çš„æ–¹æ³•ä¹‹è¿‡ç¨‹ä¸­ï¼Œç›¸å¯¹äºç¬¬ä¸€ä¸ªç›´æ¥ä¹‹ç‰©ï¼ˆthe first immediateï¼‰å’Œå—ä¸­ä»‹ä¹‹ç‰©ï¼ˆthe mediatedï¼‰ï¼Œå°±æ˜¯ç¬¬ä¸‰é¡¹ã€‚ç„¶è€Œï¼Œå®ƒç›¸å¯¹äºç¬¬ä¸€ä¸ªæˆ–å½¢å¼çš„å¦å®šï¼ˆthe first or formal negativeï¼‰ä¸ç»å¯¹çš„å¦å®šæ€§æˆ–ç¬¬äºŒä¸ªå¦å®šæ¥è¯´ï¼Œä¹Ÿæ˜¯ç¬¬ä¸‰é¡¹ï¼›ç°åœ¨ï¼Œç”±äºç¬¬ä¸€ä¸ªå¦å®šå·²ç»æ˜¯ç¬¬äºŒé¡¹ï¼Œè¢«æ•°ä½œç¬¬ä¸‰çš„é¡¹ä¹Ÿå¯ä»¥æ•°ä½œç¬¬å››ï¼Œè€ŒæŠ½è±¡å½¢å¼ä¹Ÿå¯ä»¥è¢«å½“ä½œä¸€ä¸ªå››é‡ç»“æ„ï¼ˆquadruplicityï¼‰ï¼Œè€Œä¸æ˜¯ä¸‰é‡ç»“æ„ï¼ˆtriplicityï¼‰ï¼›å¦å®šï¼ˆthe negativeï¼‰æˆ–å·®å¼‚ï¼Œä¾¿æ•°ä½œä¸¤é‡ï¼ˆdualityï¼‰ã€‚1

è¯‘è€…æ ¹æ®æ‰€å¼•ç”¨è‹±è¯‘çš„ç¿»è¯‘

åœ¨æ–¹æ³•çš„è¿™ä¸ªè½¬æŠ˜ç‚¹ä¸Šï¼Œè®¤è¯†è¿‡ç¨‹å‰ç«‹åˆ»è½¬å›åˆ°è‡ªèº«å»äº†ã€‚è¿™ä¸ªå¦å®šæ€§ï¼Œä½œä¸ºè‡ªèº«æ‰¬å¼ƒçš„çŸ›ç›¾ï¼Œæ˜¯ç¬¬ä¸€ä¸ªç›´æ¥æ€§ã€å³å•çº¯æ™®éæ€§ä¹‹æ¢å¤ï¼›å› ä¸ºä»–ç‰©çš„ä»–ç‰©ã€å¦å®šçš„å¦å®šï¼Œç›´æ¥å°±æ˜¯è‚¯å®šçš„ã€åŒä¸€çš„ã€æ™®é€¼çš„ã€‚è¿™ç¬¬äºŒä¸ªç›´æ¥çš„ä¸œè¥¿ï¼Œåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œ\*å‡å¦‚äººä»¬æ€»æ˜¯æ„¿æ„è®¡æ•°çš„è¯ï¼Œå¯¹ç¬¬ä¸€ä¸ªç›´æ¥çš„ä¸œè¥¿å’Œå¯¹æœ‰ä¸­ä»‹çš„ä¸œè¥¿è¯´ï¼Œå°±æ˜¯ç¬¬ä¸‰ä¸ªä¸œè¥¿ã€‚ä½†å®ƒå¯¹ç¬¬ä¸€ä¸ªæˆ–è¯´å½¢å¼çš„å¦å®šå¹·å¯¹ç»å¯¹çš„å¦å®šæ€§æˆ–è¯´ç¬¬äºŒä¸ªå¦å®šæ¥è¯´ï¼Œä¹Ÿæ˜¯ç¬¬ä¸‰ä¸ªï¼›å¦‚æœé‚£ç¬¬ä¸€ä¸ªå¦å®šå·²ç»æ˜¯ç¬¬äºŒé¡¹ï¼Œé‚£æœ«ï¼Œé‚£è¢«æ•°ä¸ºç¬¬ä¸‰çš„ï¼Œä¹Ÿå¯ä»¥æ•°ä½œç¬¬å››ï¼›\*æŠ½è±¡çš„å½¢å¼ä¹Ÿå°†ä¸ç”¨ä¸‰åˆ†æ³•è€Œè¢«å½“ä½œæ˜¯ä¸€ä¸ªå››åˆ†æ³•ã€‚å¦å®šçš„ä¸œè¥¿æˆ–åŒºåˆ«ï¼Œä»¥è¿™ç§æ–¹å¼ï¼Œä¾¿æ•°ä½œä¸¤åˆ†ã€‚ä¸€ä¸€ç¬¬ä¸‰ä¸ªæˆ–ç¬¬å››ä¸ªæ€»æ˜¯ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªç¯èŠ‚ï¼Œå³ç›´æ¥â€”ã€Šå¤§é€»è¾‘ï¼ˆä¸‹ï¼‰ã€‹æ¨ä¸€ä¹‹è¯‘æœ¬ P544

ç¬¬ä¸€ä¸ªç¯èŠ‚æ˜¯èµ·å§‹ç‚¹çš„ç›´æ¥è‚¯å®šæ€§ï¼ˆthe immediate positivity of the startingpointï¼‰ï¼›ç¬¬äºŒä¸ªç¯èŠ‚ï¼Œå…¶ä¸­ä»‹ï¼Œå¹¶éä»…ä»…æ˜¯å…¶ç›´æ¥çš„åé¢ã€å¤–åœ¨çš„å¯¹ç«‹é¢â€”â€”å®ƒå°±å‡ºç°åœ¨æˆ‘ä»¬åŠªåŠ›å»æŠŠæ¡ç¬¬ä¸€ä¸ªç¯èŠ‚ã€ç›´æ¥ä¹‹ç‰©ï¼ˆthe immediateï¼‰ã€è‡ªåœ¨è‡ªä¸ºï¼ˆinand for itselfï¼‰ã€æœ¬èº«ï¼ˆas suchï¼‰ä¹‹æ—¶ï¼šé€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å·²ç»å°†å…¶ä¸­ä»‹ï¼ˆmediatizeï¼‰ï¼Œè€Œå®ƒä¸çŸ¥ä¸è§‰ä¸­å˜æˆäº†è‡ªå·±çš„å¯¹ç«‹é¢ã€‚ç¬¬äºŒä¸ªç¯èŠ‚å› æ­¤ä¸æ˜¯ç¬¬ä¸€ä¸ªç¯èŠ‚çš„å¦å®šï¼ˆthe negativeï¼‰ï¼Œä¸æ˜¯å…¶ä»–å¼‚æ€§ï¼ˆothernessï¼‰ï¼›å®ƒä½œä¸ºç¬¬ä¸€ä¸ªç¯èŠ‚è‡ªå·±çš„ä»–è€…ã€è‡ªèº«çš„å¦å®šï¼ˆthe negativeï¼‰ï¼Œå°±æ˜¯ç¬¬ä¸€ä¸ªç¯èŠ‚ï¼šä¸€æ—¦æˆ‘ä»¬æ„æƒ³è¿™ä¸ªæŠ½è±¡-ç›´æ¥çš„èµ·å§‹ç‚¹ï¼ˆä¸€æ—¦æˆ‘ä»¬è§„å®šå…¶é¢„è®¾å’Œå«æ„çš„å…·ä½“ç½‘ç»œã€é˜é‡Šå…¶å†…å®¹ï¼‰ï¼Œå®ƒå°±è½¬å˜ä¸ºå®ƒè‡ªå·±çš„å¯¹ç«‹é¢ã€‚ç”šè‡³åœ¨æœ€æŠ½è±¡çš„å±‚æ¬¡ä¸Šï¼Œâ€œæ— â€ä¹Ÿä¸æ˜¯â€œæœ‰â€çš„å¤–åœ¨å¯¹ç«‹ï¼šæˆ‘ä»¬ä»…ä»…æ˜¯é€šè¿‡å°è¯•å¯¹â€œæœ‰â€è¿™ä¸€æ¦‚å¿µè¿›è¡Œå…·ä½“è¯´æ˜ä¸è§„å®šå°±åˆ°è¾¾äº†â€œæ— â€ã€‚â€œå†…åœ¨å¦å®šæ€§â€è¿™ä¸€åŸºæœ¬çš„è¾©è¯æ€æƒ³ï¼šä¸€ä¸ªå®ä½“ï¼ˆentityï¼‰è¢«å¦å®šã€æ¶ˆé€è¿›å…¥ï¼ˆpass over intoï¼‰å…¶ä½œä¸ºå…¶æœ¬èº«æ½œèƒ½ä¹‹å‘å±•çš„å¯¹ç«‹é¢ã€‚

â€¦ è¿™å°±æ˜¯å¦å®šæ€§å¿…é¡»è¢«æ•°ä¸¤æ¬¡çš„åŸå› ã€‚ä¸ºäº†åœ¨å®é™…ä¸Šå¦å®šå…¶èµ·å§‹ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»å¦å®šå®ƒè‡ªå·±çš„â€œå†…åœ¨å¦å®šï¼ˆnegationï¼‰â€â€”â€”åœ¨å…¶å†…åœ¨å¦å®šä¸­ï¼Œå®ƒçš„å†…å®¹æˆä¸ºäº†å®ƒçš„â€œçœŸç›¸â€ï¼ˆæ³•è¥¿æ–¯ä¸»ä¹‰è™½ç„¶åå¯¹è‡ªç”±ä¸»ä¹‰èµ„æœ¬ä¸»ä¹‰ï¼Œä½†ä¹Ÿä¸æ˜¯å®ƒå®é™…ä¸Šçš„å¦å®šï¼ˆnegationï¼‰è€Œä»…ä»…æ˜¯å…¶â€œå†…éƒ¨çš„â€å¦å®šï¼ˆnegationï¼‰ï¼šå› æ­¤ï¼Œä¸ºäº†å®é™…ä¸Šå¦å®šè‡ªç”±ä¸»ä¹‰èµ„æœ¬ä¸»ä¹‰ï¼Œæˆ‘ä»¬å¿…é¡»å¦å®šå®ƒçš„è¿™ä¸ªå¦å®šï¼ˆnegationï¼‰ï¼‰ã€‚è¿™ä¸€ç¬¬äºŒä¸ªå¦å®šã€è‡ªç›¸å…³çš„ï¼ˆself-relating negationï¼‰å¦å®šæˆ–ï¼ˆå¦‚é»‘æ ¼å°”ä¼šè¯´çš„ï¼‰åå°„è¿›è‡ªèº«çš„ä»–å¼‚æ€§ï¼Œæ˜¯ç»å¯¹å¦å®šæ€§å’Œâ€œçº¯ç²¹å·®å¼‚â€çš„æ¶ˆå¤±çš„ç‚¹/æ²¡å½±ç‚¹ï¼ˆvanishing pointï¼‰â€”â€”è¿™ä¸€è‡ªç›¸çŸ›ç›¾çš„ç¯èŠ‚å°±æ˜¯ç¬¬ä¸‰ä¸ªç¯èŠ‚ï¼Œå› ä¸ºå®ƒå·²ç»æ˜¯æ¶ˆé€è¿›å®ƒè‡ªå·±ä¹‹ä»–è€…çš„ç¬¬ä¸€ä¸ªç¯èŠ‚ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€å¾—åˆ°çš„ä¸œè¥¿ä¹Ÿå¯ä»¥è¢«æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªå›æº¯æ€§å†³å®šï¼ˆretroactive determinationï¼‰çš„æ¡ˆä¾‹ï¼šå½“åå¯¹å…¶å½»åº•çš„å¦å®šï¼ˆNegativeï¼‰æ—¶ï¼Œç¬¬ä¸€ä¸ªç¯èŠ‚æœ¬èº«å›æº¯æ€§åœ°è½¬å˜ä¸ºå…¶å¯¹ç«‹é¢ã€‚è‡ªåœ¨çš„èµ„æœ¬ä¸»ä¹‰ï¼ˆcapitalism-in-itselfï¼‰å¹¶ä¸ç­‰åŒäºåå¯¹å…±äº§ä¸»ä¹‰çš„èµ„æœ¬ä¸»ä¹‰ï¼ˆcapitalism-as-opposed-to-Communismï¼‰ï¼šå½“é¢å¯¹å…¶è§£ä½“çš„è¶‹åŠ¿æ—¶ï¼Œèµ„æœ¬ä¸»ä¹‰å¦‚æœè¦ç”Ÿå­˜ï¼Œå°±ä¸å¾—ä¸â€œä»å†…éƒ¨ï¼ˆfrom withinï¼‰â€å¦å®šè‡ªå·±ï¼ˆè¿›å…¥æ³•è¥¿æ–¯ä¸»ä¹‰ï¼‰ã€‚è¿™ç§è¾©è¯ç”±é˜¿å¤šè¯ºè®ºéŸ³ä¹å²æ—¶å¾—åˆ°äº†é˜è¿°ï¼š

â€¦â€¦2

åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæœ‰å…³ç»“æ„ä¸»ä¹‰ç§°ä½œâ€œç¼ºå¸­å†³å®šâ€çš„ä¸œè¥¿çš„èŒƒä¾‹ï¼šä¸å’Œè°éŸ³ï¼ˆdissonancesï¼‰å‡ºç°ä¹‹åï¼Œä¸‰å…¨éŸ³ï¼ˆtritoneï¼‰çš„æ„ä¹‰æ”¹å˜äº†ï¼Œå› ä¸ºä¸‰å…¨éŸ³è¿›ä¸€æ­¥çš„ä½¿ç”¨æš—å«æœ‰å¯¹ä¸å’Œè°éŸ³çš„å¦å®šï¼ˆnegationï¼‰â€”â€”å…¶æ–°æ„ä¹‰äº§ç”Ÿäºï¼šä¸å’Œè°éŸ³çš„ç¼ºå¸­ï¼ˆabsenceï¼‰ç°å­˜äºï¼ˆis present inï¼‰å¯¹ä¸‰å…¨éŸ³çš„ä½¿ç”¨ä¸­ã€‚åœ¨å…¶ç›´æ¥çš„åœ¨åœº/å‘ˆç°ï¼ˆpresenceï¼‰ä¸­ï¼Œä¸‰å…¨éŸ³ä¿æŒåŸæ ·ï¼›å…¶å†å²æ€§çš„ä¸­ä»‹ç”±è¿™æ ·ä¸€ä¸ªäº‹å®æ¥æ­éœ²ï¼šå®ƒæ° å› å®ƒä¿æŒåŸæ ·è€Œæ”¹å˜3ã€‚ä»Šæ—¥â€œå›å½’ä¼ ç»Ÿä»·å€¼â€è¿™ä¸€å‘¼åçš„é”™è¯¯ä¹Ÿåœ¨äºæ­¤ï¼šä¼ ç»Ÿä»·å€¼å› æˆ‘ä»¬é‡å»ºäº†å®ƒä»¬è€Œä¸å†ç›¸åŒï¼Œå› ä¸ºå®ƒä»¬åˆæ³•åŒ–äº†çš„é‚£ä¸ªç¤¾ä¼šç§©åºæ˜¯å®ƒä»¬çš„å¯¹ç«‹é¢ã€‚4

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¢è¡¥æ€§çš„è¦ç´ æ˜¯å¦‚ä½•å‡ºç°çš„ï¼šä¸€æ—¦æˆ‘ä»¬å°†ç›´æ¥ä¹‹ç‰©ï¼ˆtheimmediateï¼‰çš„å¦å®šï¼ˆnegationï¼‰æ·»åŠ ç»™ç›´æ¥ä¹‹ç‰©ï¼Œè¿™ä¸€å¦å®šï¼ˆnegationï¼‰å°±å›æº¯æ€§åœ°æ”¹å˜äº†ç›´æ¥æ€§ï¼ˆimmediacyï¼‰çš„æ„ä¹‰ï¼Œæ‰€ä»¥æˆ‘ä»¬è™½ç„¶å®é™…ä¸Šä»…æ‹¥æœ‰ä¸¤ä¸ªè¦ç´ å´å¿…é¡»æ•°åˆ°ä¸‰ã€‚æˆ–è€…ï¼Œå¦‚æœæˆ‘ä»¬è®¾æƒ³è¾©è¯è¿‡ç¨‹çš„å®Œæ•´å¾ªç¯ï¼Œè¿™é‡Œåªæœ‰ä¸‰ä¸ªâ€œè‚¯å®šï¼ˆpositiveï¼‰â€çš„ç¯èŠ‚ï¼ˆç›´æ¥æ€§ã€å…¶ä¸­ä»‹å’Œæœ€åå¯¹è¢«ä¸­ä»‹çš„ç›´æ¥æ€§çš„å¤å½’ï¼‰è¦å»æ•°â€”â€”æˆ‘ä»¬æ¼æ‰çš„æ˜¯çº¯ç²¹å·®å¼‚çš„é‚£éš¾ä»¥ç†è§£çš„å‰©ä½™ç‰©ï¼ˆsurplusï¼‰ï¼Œå®ƒè™½ä½¿å¾—æ•´ä¸ªè¿‡ç¨‹å¾—ä»¥è¿›è¡Œå´â€œä»€ä¹ˆä¹Ÿä¸ç®—ï¼ˆcounts for nothingï¼‰â€ï¼›æˆ‘ä»¬æ¼æ‰çš„æ˜¯è¿™ä¸€â€œå®ä½“çš„è™šç©ºâ€ï¼Œï¼ˆå¦‚é»‘æ ¼å°”æ‰€è¨€ï¼‰å®ƒåŒæ—¶ä¹Ÿæ˜¯æ‰€æœ‰ä¸€åˆ‡ï¼ˆall and everythingï¼‰çš„â€œå®¹å™¨ï¼ˆreceptacle [Rezeptakulum]ï¼‰â€ã€‚

# Â·æ–°æ•™ã€é›…å„å®¾ä¸»ä¹‰â€¦â€¦

ç„¶è€Œï¼Œåœ¨é‚£å¯¹â€œè¾©è¯æ–¹æ³•â€è¿›è¡Œæƒ¹äººæ¼æ€’çš„æŠ½è±¡åæ˜ ï¼ˆabstract reflectionsï¼‰çš„æœ€ä½³ä¼ ç»Ÿä¸­ï¼Œè¿™ç§æ€è€ƒï¼ˆruminationsï¼‰æœ‰ç€ä¸€ç§çº¯ç²¹å½¢å¼çš„æœ¬æ€§ï¼›å®ƒä»¬æ‰€ç¼ºä¹çš„æ˜¯ä¸å…·ä½“å†å²å†…å®¹å†…åœ¨çš„ç›¸äº’è”ç³»ï¼ˆrelatednessï¼‰ã€‚ä¸€æ—¦æˆ‘ä»¬åˆ°è¾¾è¿™ç§å±‚æ¬¡ï¼Œç¬¬å››çš„å‰©ä½™ç‰©-ç¯èŠ‚ï¼ˆsurplus-momentï¼‰ä½œä¸ºç¬¬äºŒä¸ªç¯èŠ‚ï¼ˆåˆ†è£‚ã€æŠ½è±¡å¯¹ç«‹ï¼‰ä¸æœ€ç»ˆç»“æœ[Result]ï¼ˆå’Œè§£[reconciliation]ï¼‰ä¹‹é—´â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€è¿™ç§æƒ³æ³•ç«‹åˆ»è·å¾—äº†å…·ä½“çš„è½®å»“â€”â€”äººä»¬åªéœ€æƒ³æƒ³è©¹æ˜ä¿¡åœ¨å…¶è®ºé©¬å…‹æ–¯Â·éŸ¦ä¼¯5çš„æ–‡ç« ï¼ˆè¿™ç¯‡æ–‡ç« æœ‰å…³éŸ¦ä¼¯å…³äºæ–°æ•™åœ¨èµ„æœ¬ä¸»ä¹‰å´›èµ·ä¸­çš„ä½œç”¨çš„ç†è®ºï¼‰ä¸­é˜æ˜â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€è¿™ä¸€æ¦‚å¿µçš„æ–¹å¼ã€‚éŸ¦ä¼¯çš„è¿™ä¸ªç†è®ºç»å¸¸è¢«è§£è¯»ä¸ºï¼ˆå¹¶ä¸”ä¹Ÿæ˜¯éŸ¦ä¼¯æœ¬äººçš„æ„æ€ï¼‰ä¸€ç§å¯¹é©¬å…‹æ€å…³äºç»æµåŸºç¡€ä¹‹é¦–è¦æ€§è®ºç‚¹çš„æ‰¹è¯„ï¼šæœ€ç»ˆï¼ŒéŸ¦ä¼¯çš„è§‚ç‚¹æ˜¯ï¼Œæ–°æ•™æ˜¯èµ„æœ¬ä¸»ä¹‰çš„æ¡ä»¶ã€‚ç›¸åï¼Œè©¹æ˜ä¿¡å°†éŸ¦ä¼¯çš„ç†è®ºè§£é‡Šå¾—å®Œå…¨ç¬¦åˆé©¬å…‹æ€ä¸»ä¹‰ï¼šéŸ¦ä¼¯çš„ç†è®ºæ˜¯å¯¹è¾©è¯å¿…ç„¶æ€§çš„è¯¦å°½é˜è¿°â€”â€”ç»ç”±è¿™ç§è¾©è¯å¿…ç„¶æ€§ï¼Œâ€œåŸºç¡€â€å’Œ â€œä¸Šå±‚å»ºç­‘â€ çš„â€œæ­£å¸¸â€å…³ç³»åœ¨å°å»ºä¸»ä¹‰è¿›å…¥èµ„æœ¬ä¸»ä¹‰çš„è¿‡ç¨‹ä¸­è¢«é¢ å€’äº†ã€‚

è¿™ç§è¾©è¯çš„å¿…ç„¶æ€§ä½äºä½•å¤„å‘¢ï¼Ÿæ¢è¨€ä¹‹ï¼šå…·ä½“æ¥è¯´ï¼Œæ–°æ•™æ˜¯æ€æ ·ä¸ºèµ„æœ¬ä¸»ä¹‰çš„å‡ºç°åˆ›é€ æ¡ä»¶çš„ï¼Ÿå¹¶éå¦‚äººä»¬ä¼šæœŸå¾…çš„é‚£æ ·ï¼Œé€šè¿‡é™åˆ¶å®—æ•™æ„è¯†å½¢æ€çš„å½±å“èŒƒå›´æˆ–é€šè¿‡åŠ¨æ‘‡å…¶åœ¨ä¸­ä¸–çºªç¤¾ä¼šæ— å¤„ä¸åœ¨çš„ç‰¹å¾ï¼Œè€Œæ˜¯ç›¸åé€šè¿‡å°†å…¶æ„ä¹‰ï¼ˆrelevanceï¼‰æ™®éåŒ–ï¼šè·¯å¾·åå¯¹ç”¨ä¸€é“é¸¿æ²Ÿå°†ä¿®é“é™¢ï¼ˆcloistersï¼‰ä¸ç¤¼æ‹œï¼ˆchurchï¼‰ä½œä¸ºä¸€ç§ç‹¬ç«‹çš„åˆ¶åº¦ï¼ˆinstitutionï¼‰åŒç¤¾ä¼šçš„å…¶ä»–éƒ¨åˆ†éš”ç»å¼€æ¥ï¼Œå› ä¸ºä»–å¸Œæœ›åŸºç£æ•™çš„æ€åº¦èƒ½å¤Ÿæ¸—é€å¹¶å†³å®šæˆ‘ä»¬æ•´ä¸ªçš„ä¸–ä¿—æ—¥å¸¸ç”Ÿæ´»ã€‚ä¼ ç»Ÿï¼ˆå‰æ–°æ•™ï¼‰çš„ç«‹åœºåŸºæœ¬ä¸Šå°†å®—æ•™çš„æ„ä¹‰é™åˆ¶äºæˆ‘ä»¬å¿…é¡»è¶‹å‘çš„ç›®æ ‡ï¼Œè€Œå°†æ‰‹æ®µâ€”â€”ä¸–ä¿—ç»æµæ´»åŠ¨çš„é¢†åŸŸâ€”â€”ç•™ç»™éå®—æ•™çš„å…±åŒåˆ¤æ–­ã€‚è€Œä¸æ­¤ç›¸åï¼Œæ–°æ•™çš„â€œå·¥ä½œä¼¦ç†â€å°†éå¸¸ä¸–ä¿—çš„æ´»åŠ¨ï¼ˆç»æµè·ç›Šï¼‰è®¾æƒ³ä¸ºæ­ç¤ºä¸Šå¸æ©å…¸çš„é¢†åŸŸã€‚

ç¦æ¬²ä¸»ä¹‰çš„åœ°ä½å˜åŒ–å¯ä»¥ä¾‹ç¤ºè¿™ä¸€è½¬å˜ä»¥ä¸ºä¾‹ï¼šåœ¨ä¼ ç»Ÿçš„å¤©ä¸»æ•™ä¸–ç•Œé‡Œï¼Œç¦æ¬²ä¸»ä¹‰æ¶‰åŠä¸€ä¸ªåˆ†ç¦»äºæ—¥å¸¸ä¸–ä¿—ç”Ÿæ´»çš„é˜¶å±‚ï¼Œä»–ä»¬è‡´åŠ›äºåœ¨è¿™ä¸ªä¸–ç•Œä¸Šæç»˜å…¶æ¥ä¸–ï¼Œä¹Ÿå°±æ˜¯åœ°ä¸Šå¤©å›½ï¼ˆåœ£äººã€ä¿®é“å£«çš„ç¦æ¬²ï¼‰ã€‚ç„¶è€Œï¼Œæ–°æ•™è¦æ±‚æ¯ä½åŸºç£å¾’åœ¨ä¸–ä¿—ç”Ÿæ´»ä¸­é‡‡å–ç¦æ¬²çš„è¡ŒåŠ¨â€”â€”è¦ç§¯ç´¯è´¢å¯Œè€Œä¸è¦è½»ç‡èŠ±é’±ï¼Œè¦èŠ‚åˆ¶ã€è°¦é€Šåœ°ç”Ÿæ´»â€”è¦åœ¨â€œé“­è®°ä¸Šå¸â€çš„åŒæ—¶å®Œæˆå…¶å·¥å…·æ€§ç»æµæ´»åŠ¨ï¼›å› æ­¤ä½œä¸ºä¸€ä¸ªé˜¶å±‚ä¹‹äº‹åŠ¡çš„ç¦æ¬²ä¸»ä¹‰å˜å¾—å¤šä½™èµ·æ¥ã€‚

è¿™ç§å¯¹åŸºç£æ•™ç«‹åœºçš„æ™®éåŒ–ï¼Œä¹Ÿå³è‚¯å®šåŸºç£æ•™ç«‹åœºå¯¹äºä¸–ä¿—ç»æµæ´»åŠ¨çš„æ„ä¹‰ï¼Œæ»‹ç”Ÿå‡ºäº†â€œæ–°æ•™å·¥ä½œä¼¦ç†â€çš„ç‰¹å¾ï¼ˆå°†ä¸Šç˜¾çš„å·¥ä½œï¼ˆcompulsiveï¼‰å’Œè´¢å¯Œç§¯ç´¯â€”â€”å£°ç§°æ”¾å¼ƒæ¶ˆè´¹â€”â€”ä½œä¸ºåœ¨å…¶è‡ªèº«çš„æœ€ç»ˆç›®çš„ï¼‰ï¼›ç„¶è€ŒåŒæ—¶ï¼Œå®ƒä¹Ÿåœ¨ä¸çŸ¥ä¸è§‰ä¸­éµå¾ªç€â€œç†æ€§çš„ç‹¡è®¡â€ï¼Œå¼€å¯äº†è´¬ä½å®—æ•™ä¹‹è·¯ï¼Œå°†å®—æ•™é™åˆ¶äºä¸å›½å®¶å’Œå…¬å…±äº‹åŠ¡åˆ†ç¦»çš„ç§äººé¢†åŸŸã€‚å› æ­¤ï¼Œæ–°æ•™å¯¹åŸºç£æ•™ç«‹åœºçš„æ™®éåŒ–ä»…ä»…æ˜¯é€šå‘èµ„äº§é˜¶çº§â€œå¸¸æ€â€ç¤¾ä¼šé€”ä¸­çš„æš‚æ—¶é˜¶æ®µï¼Œåœ¨è¿™é‡Œå®—æ•™è¢«ç¼©å‡ä¸ºâ€œæ‰‹æ®µâ€ï¼Œæˆä¸ºèƒ½ä½¿ä¸»ä½“åœ¨ä¸ºç”Ÿå­˜è¿›è¡Œçš„ç»æµæ–—äº‰ä¸­å‘ç°æ–°åŠ›é‡å’Œæ¯…åŠ›çš„åª’ä»‹ï¼Œå°±åƒé‚£äº› â€œè‡ªæˆ‘ç»éªŒâ€æŠ€å·§ä¸€æ ·ï¼Œå®ƒä»¬å°†æˆ‘ä»¬ä¸â€œçœŸæˆ‘ï¼ˆture Selfï¼‰â€çš„é­é‡æœåŠ¡äºå¯¹æˆ‘ä»¬çš„é€‚å½“æ€§ï¼ˆfitnessï¼‰ã€‚

å½“ç„¶ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å¯¹æ–°æ•™çš„å¹»è§‰ä¿æŒä¸€ç§åè®½çš„è·ç¦»ï¼Œå¹¶æŒ‡å‡ºæ–°æ•™åŠªåŠ›åºŸé™¤å®—æ•™ä¸æ—¥å¸¸ç”Ÿæ´»ä¹‹é—´å·®è·çš„æœ€ç»ˆç»“æœæ˜¯å¦‚ä½•å°†å®—æ•™è´¬ä½ä¸ºä¸€ç§â€œæ²»ç–—æ€§ï¼ˆtherapeuticï¼‰â€çš„æ‰‹æ®µï¼›æ›´å›°éš¾çš„åˆ™æ˜¯è¦å»æ„æƒ³æ–°æ•™ä½œä¸ºä¸­ä¸–çºªç¤¾å›¢ä¸»ä¹‰å’Œèµ„æœ¬ä¸»ä¹‰ä¸ªäººä¸»ä¹‰é—´â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…ï¼ˆvanishing mediatorï¼‰â€çš„å¿…ç„¶æ€§ã€‚æ¢å¥è¯è¯´ï¼Œä¸å¯å¿½è§†çš„ä¸€ç‚¹æ˜¯ï¼Œå¦‚æœï¼Œäººä»¬ä¸å¯èƒ½ç›´æ¥åœ°ï¼Œä¹Ÿå°±æ˜¯ç¼ºå°‘æ–°æ•™ä½œä¸º â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€çš„è°ƒè§£ï¼ˆintercessionï¼‰è€Œä»ä¸­ä¸–çºªçš„â€œå°é—­â€ç¤¾ä¼šè¿›å…¥èµ„äº§é˜¶çº§ç¤¾ä¼šï¼šæ­£æ˜¯æ–°æ•™é€šè¿‡å…¶å¯¹åŸºç£æ•™æ€§ï¼ˆChristianityï¼‰çš„æ™®éåŒ–ï¼Œä¸ºå…¶æ’¤å›åˆ°ç§å¯†é¢†åŸŸé¢„å¤‡äº†åŸºç¡€ã€‚

åœ¨æ”¿æ²»é¢†åŸŸï¼Œé›…å„å®¾ä¸»ä¹‰æ‰®æ¼”äº†åŒæ ·çš„è§’è‰²ï¼Œå®ƒç”šè‡³å¯ä»¥è¢«å®šä¹‰ä¸ºâ€œæ”¿æ²»çš„æ–°æ•™â€ã€‚â€¦

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¹Ÿå¾ˆå®¹æ˜“ä¿æŒä¸€ç§åè®½çš„è·ç¦»ï¼Œå¹¶æŒ‡å‡ºé›…å„å®¾ä¸»ä¹‰å¦‚ä½•å¿…ç„¶ä¼šé€šè¿‡å°†ç¤¾ä¼šæ•´ä½“ç²—æš´åœ°ç¼©å‡ä¸ºæŠ½è±¡çš„å¹³ç­‰åŸåˆ™è€Œåœ¨ææ€–ä¸»ä¹‰ä¸­ç»“æŸï¼Œå› ä¸ºè¿™ç§ç¼©å‡å—åˆ°äº†åˆ†æ”¯çš„ï¼ˆramifiedï¼‰å…·ä½“å…³ç³»ä¹‹ç½‘çš„æŠµåˆ¶ï¼ˆè§é»‘æ ¼å°”åœ¨ã€Šç²¾ç¥ç°è±¡å­¦ã€‹ä¸­å¯¹é›…å„å®¾ä¸»ä¹‰çš„ç»å…¸æ‰¹è¯„ï¼‰ã€‚æ›´éš¾åšåˆ°çš„æ˜¯ï¼Œè¦è¯æ˜ä¸ºä»€ä¹ˆä¸å¯èƒ½ä»æ—§åˆ¶åº¦ç›´æ¥è¿›å…¥è‡ªæˆ‘æœ¬ä½çš„èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»â€”â€”ä¸ºä»€ä¹ˆï¼Œæ­£æ˜¯å› ä¸ºä»–ä»¬è™šå¹»åœ°å°†ç¤¾ä¼šæ•´ä½“è¿˜åŸä¸ºæ°‘ä¸»æ”¿æ²»æ–¹æ¡ˆï¼Œé›…å„å®¾ä¸»ä¹‰æ˜¯ä¸€ä¸ªå¿…è¦çš„â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ï¼ˆé»‘æ ¼å°”æ‰¹è¯„å¾—å®é™…è¦ç‚¹å¹¶ä¸åœ¨äºè¯´é›…å„å®¾ä¸»ä¹‰æ–¹æ¡ˆæœ‰ä¹Œæ‰˜é‚¦ï¼ææ€–ä¸»ä¹‰ç‰¹å¾è¿™æ ·çš„è€ç”Ÿå¸¸è°ˆä¸­ï¼Œè€Œæ˜¯åœ¨äºæ­¤ï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨é›…å„å®¾ä¸»ä¹‰ä¸­å‘ç°ç°ä»£â€œææƒä¸»ä¹‰â€çš„æ ¹æºå’Œç¬¬ä¸€ä¸ªå½¢å¼æ˜¯å¾ˆå®¹æ˜“çš„ï¼›è€Œè¦å®Œå…¨æ‰¿è®¤å’Œé‡‡çº³æ²¡æœ‰é›…å„å®¾ä¸»ä¹‰çš„â€œæº¢å‡º/è¿‡å‰©â€å°±ä¸ä¼šæœ‰â€œå¸¸æ€çš„â€å¤šå…ƒæ°‘ä¸»è¿™æ ·ä¸€ä¸ªäº‹å®åˆ™è¦æ›´åŠ å›°éš¾å¹¶ä»¤äººä¸å®‰ã€‚6

ä¹Ÿå°±æ˜¯è¯´ï¼Œæ–°æ•™å’Œé›…å„å®¾ä¸»ä¹‰æ‰€é™·å…¥çš„å¹»è§‰ï¼Œæ¯”ä¹çœ‹ä¹‹ä¸‹è¦å¤æ‚å¾—å¤šï¼šå®ƒå¹¶ä¸ç®€å•åœ°åœ¨äºä»–ä»¬å¯¹åŸºç£æ•™æˆ–å¹³ç­‰ä¸»ä¹‰æ°‘ä¸»æ–¹æ¡ˆï¼ˆegalitarian-democratic projectï¼‰çš„é‚£æœ´ç´ é“å¾·ä¸»ä¹‰å¼çš„æ™®éåŒ–ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¹¶ä¸ç®€å•åœ°åœ¨äºä»–ä»¬å¿½ç•¥äº†æŠµåˆ¶è¿™ç§ç›´æ¥æ™®éåŒ–çš„ç¤¾ä¼šå…³ç³»çš„å…·ä½“è´¢å¯Œï¼ˆconcrete wealth of social relationsï¼‰ã€‚ä»–ä»¬çš„å¹»è§‰è¦æ¿€è¿›å¾—å¤šï¼šå®ƒåŒæ‰€æœ‰åœ¨å†å²ä¸Šç›¸å…³çš„æœ‰å…³æ”¿æ²»ä¹Œæ‰˜é‚¦çš„å¹»è§‰å…·æœ‰ç›¸åŒçš„æœ¬æ€§ã€‚é©¬å…‹æ€åœ¨è°ˆåˆ°æŸæ‹‰å›¾çš„å›½å®¶ï¼ˆStateï¼‰æ—¶æè¯·æˆ‘ä»¬æ³¨æ„è¿™ç§å¹»è§‰ï¼Œä»–è¯´ï¼ŒæŸæ‹‰å›¾æ²¡æœ‰çœ‹åˆ°ä»–äº‹å®ä¸Šæ‰€æè¿°çš„ä¸æ˜¯ä¸€ä¸ªå°šæœªå®ç°çš„ç†æƒ³ï¼ˆidealï¼‰ï¼Œè€Œæ˜¯ç°å­˜å¸Œè…Šå›½å®¶çš„åŸºæœ¬ç»“æ„ã€‚æ¢å¥è¯è¯´ï¼Œä¹Œæ‰˜é‚¦ï¼ˆutopiasï¼‰ä¹‹æ‰€ä»¥æ˜¯â€œä¹Œæ‰˜é‚¦çš„â€ï¼Œä¸æ˜¯å› ä¸ºå®ƒä»¬æç»˜äº†ä¸€ä¸ªâ€œä¸å¯èƒ½çš„ç†æƒ³ï¼ˆIdealï¼‰â€ï¼Œä¸€ä¸ªä¸å±äºè¿™ä¸ªä¸–ç•Œçš„æ¢¦æƒ³ï¼Œè€Œæ˜¯å› ä¸ºå®ƒä»¬æ²¡æœ‰è®¤å‡ºå®ƒä»¬çš„ç†æƒ³å›½ï¼ˆideal stateï¼‰åœ¨å…¶åŸºæœ¬å†…å®¹æ–¹é¢ï¼ˆé»‘æ ¼å°”ä¼šè¯´ï¼Œâ€œåœ¨å…¶æ¦‚å¿µæ–¹é¢â€ï¼‰å¦‚ä½•å·²ç„¶å®ç°äº†ã€‚

å½“ç¤¾ä¼šç°å®è¢«æ„é€ æˆä¸€ä¸ªâ€œæ–°æ•™ä¸–ç•Œâ€çš„æ—¶å€™ï¼Œæ–°æ•™å°±å˜å¾—å¤šä½™ï¼Œå¯ä»¥ä½œä¸ºä¸€ä¸ªä¸­ä»‹æ¶ˆå¤±äº†ï¼šèµ„æœ¬ä¸»ä¹‰å…¬æ°‘ç¤¾ä¼šçš„æ¦‚å¿µç»“æ„ï¼ˆnotional structureï¼‰æ˜¯ä¸€ä¸ªç”±â€œè´ªå¾—çš„ç¦æ¬²ä¸»ä¹‰â€ï¼ˆâ€œä½ æ‹¥æœ‰çš„è¶Šå¤šï¼Œä½ å°±è¶Šè¦æ”¾å¼ƒæ¶ˆè´¹â€ï¼‰è¿™ä¸ªæ‚–è®ºæ‰€å®šä¹‰çš„åŸå­åŒ–ä¸ªäººçš„ä¸–ç•Œâ€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œç¼ºå°‘æ–°æ•™ä¹‹ç§¯æå®—æ•™å½¢å¼è€Œåªæœ‰æ–°æ•™ä¹‹å†…å®¹çš„ç»“æ„ã€‚é›…å„å®¾ä¸»ä¹‰ä¹Ÿæ˜¯å¦‚æ­¤ï¼šé›…å„å®¾æ´¾æ‰€å¿½è§†çš„äº‹å®æ˜¯ï¼Œä»–ä»¬åŠªåŠ›è¿½æ±‚çš„ç†æƒ³åœ¨å…¶æ¦‚å¿µç»“æ„ä¸­åœ¨â€œè‚®è„çš„â€è´ªå¾—æ´»åŠ¨ï¼ˆacquisitive activityï¼‰ä¸­å·²ç„¶å®ç°ï¼Œè€Œè¿™ç§æ´»åŠ¨åœ¨ä»–ä»¬çœ‹æ¥æ˜¯å¯¹å…¶å´‡é«˜ç†æƒ³çš„èƒŒå›ã€‚åº¸ä¿—çš„ã€åˆ©å·±ä¸»ä¹‰çš„èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»æ˜¯è‡ªç”±ã€å¹³ç­‰å’Œåšçˆ±çš„ç°å®æ€§ï¼ˆactualityï¼‰ï¼šè‡ªç”±è´¸æ˜“çš„è‡ªç”±ï¼Œæ³•å¾‹é¢å‰çš„å½¢å¼å¹³ç­‰ï¼Œç­‰ç­‰ã€‚

â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€â€”â€”æ–°æ•™å¾’ã€é›…å„å®¾ä¸»ä¹‰â€”â€”æ‰€ç‰¹æœ‰çš„å¹»è§‰æ­£æ˜¯é»‘æ ¼å°”å¼çš„â€œç¾ä¸½çµé­‚â€çš„å¹»è§‰ï¼šä»–ä»¬æ‹’ç»åœ¨ä»–ä»¬æ‰€å“€å¹çš„è…è´¥ç°å®ä¸­æ‰¿è®¤ä»–ä»¬è‡ªå·±çš„è¡Œä¸ºçš„æœ€ç»ˆç»“æœâ€”â€”å¦‚æ‹‰åº·æ‰€è¯´ï¼Œä»–ä»¬è‡ªå·±çš„ä¿¡æ¯ä»¥å…¶çœŸå®è€Œé¢ å€’çš„å½¢å¼å‡ºç°ã€‚è€Œä½œä¸ºæ–°æ•™å’Œé›…å„å®¾ä¸»ä¹‰çš„â€œæ¸…é†’çš„â€ ç»§æ‰¿è€…ï¼Œæˆ‘ä»¬çš„å¹»è§‰ä¹Ÿä¸å°‘ï¼šæˆ‘ä»¬æŠŠé‚£äº›â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€è§†ä¸ºåå¸¸ï¼ˆaberrationsï¼‰æˆ–æº¢å‡º/è¿‡å‰©ï¼Œæ²¡èƒ½æ³¨æ„åˆ°æˆ‘ä»¬ä½•ä»¥åªæ˜¯â€œæ²¡æœ‰é›…å„å®¾å½¢å¼çš„é›…å„å®¾æ´¾â€ä¸â€œæ²¡æœ‰æ–°æ•™å½¢å¼çš„æ–°æ•™å¾’â€ã€‚

# Â·â€¦â€¦ä¸å…¶ä»–æ¶ˆå¤±çš„ä¸­ä»‹è€…

å½¢å¼å’Œå…¶æ¦‚å¿µå†…å®¹é—´çš„è£‚éš™ï¼Œä¹Ÿç»™æˆ‘ä»¬æä¾›äº†é€šå‘â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€çš„å¿…ç„¶æ€§çš„å…³é”®ï¼šä»å°å»ºä¸»ä¹‰åˆ°æ–°æ•™çš„è·¯å¾„ä¸ä»æ–°æ•™åˆ°å…·æœ‰å®—æ•™ç§äººåŒ–ç‰¹å¾çš„èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»çš„è·¯å¾„æ²¡æœ‰ç›¸åŒçš„ç‰¹å¾ã€‚ç¬¬ä¸€ä¸ªè·¯å¾„å…³ç³»åˆ°â€œå†…å®¹â€ï¼ˆåœ¨ä¿æŒæˆ–è€…ç”šè‡³åŠ å¼ºå®—æ•™å½¢å¼çš„ä¼ªè£…ä¸‹ï¼Œå‘ç”Ÿäº†å…³é”®æ€§çš„å˜åŒ–â€”â€”ç»æµæ´»åŠ¨ä¸­ç¦æ¬²å¼è´ªå¾—[asceticacquisitive]çš„æ€åº¦è¢«æ˜ç¡®è‚¯å®šä¸ºå±•ç¤ºæ©å…¸çš„åŠ¿åŠ›èŒƒå›´ï¼‰ï¼Œè€Œç¬¬äºŒä¸ªè·¯å¾„åˆ™æ˜¯ä¸€ä¸ªçº¯ç²¹å½¢å¼çš„è¡ŒåŠ¨ï¼Œä¸€ç§å½¢å¼çš„å˜åŒ–ï¼ˆä¸€æ—¦æ–°æ•™ä½œä¸ºç¦æ¬²å¼è´ªå¾—[ascetic-acquisitive]çš„æ€åº¦å¾—åˆ°å®ç°ï¼Œå®ƒå°±ä¼šä½œä¸ºå½¢å¼è€Œè„±è½ï¼‰ã€‚

å› æ­¤ï¼Œâ€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ä¹‹æ‰€ä»¥å‡ºç°ï¼Œæ˜¯å› ä¸ºåœ¨ä¸€ä¸ªè¾©è¯çš„è¿‡ç¨‹ä¸­ï¼Œå½¢å¼åœç•™åœ¨å†…å®¹åé¢çš„æ–¹å¼ï¼šé¦–å…ˆï¼Œå…³é”®æ€§çš„è½¬å˜å‘ç”Ÿåœ¨æ—§å½¢å¼çš„é™åº¦å†…ï¼Œç”šè‡³å‘ˆç°å‡ºå…¶å¤å…´çš„ä¸»å¼ è¿™ä¸€å¤–è¡¨ï¼ˆå¯¹åŸºç£æ•™æ€§çš„æ™®éåŒ–ï¼Œå›åˆ°å…¶â€œçœŸæ­£çš„å†…å®¹â€ï¼Œç­‰ç­‰ï¼‰ï¼›ç„¶åï¼Œä¸€æ—¦â€œç²¾ç¥çš„æ— å£°ç¼–ç»‡ï¼ˆsilent weavingï¼‰â€å®Œæˆå…¶å·¥ä½œï¼Œæ—§å½¢å¼å°±ä¼šè„±è½ã€‚è¿™ä¸€è¿‡ç¨‹çš„åŒé‡èŠ‚å¥ï¼ˆscansionï¼‰æ‰©å±•ä½¿æˆ‘ä»¬èƒ½å¤Ÿå…·ä½“åœ°æŒæ¡â€œå¦å®šä¹‹å¦å®š

ï¼ˆnegation of negationï¼‰â€è¿™ä¸€é™ˆæ—§çš„å…¬å¼ï¼šç¬¬ä¸€ä¸ªå¦å®šåœ¨äºå®è´¨æ€§å†…å®¹ç¼“æ…¢ã€ç§˜å¯†ä¸”æ— å½¢çš„å˜åŒ–ï¼Œè€Œè‡ªç›¸çŸ›ç›¾çš„æ˜¯ï¼Œè¿™ç§å˜åŒ–å‘ç”Ÿåœ¨å…¶è‡ªèº«å½¢å¼çš„åä¹‰ä¸‹çš„ï¼›é‚£ä¹ˆï¼Œä¸€æ—¦å½¢å¼å¤±å»äº†å®ƒçš„å®è´¨æ€§æƒåˆ©ï¼ˆsubstantial rightï¼‰ï¼Œå®ƒå°±ä¼šè‡ªå·±æ‘”å¾—ç²‰ç¢â€”â€”å¦å®šçš„å½¢å¼è¢«å¦å®šäº†ï¼Œæˆ–è€…ç”¨é»‘æ ¼å°”çš„ç»å…¸å¯¹å­æ¥è¯´ï¼Œå‘ç”Ÿâ€œåœ¨å…¶è‡ªèº«ä¸­çš„â€ï¼ˆin itselfï¼‰å˜åŒ–å˜æˆäº†â€œå¯¹äºå…¶è‡ªèº«çš„â€ï¼ˆfor itselfï¼‰ã€æˆ–ï¼Œâ€œè‡ªåœ¨â€å‘ç”Ÿçš„å˜åŒ–å˜æˆäº†â€œè‡ªä¸ºçš„â€â€”â€”è¯‘æ³¨ã€‘ã€‚

æˆ‘ä»¬åº”è¯¥è¿›ä¸€æ­¥å¤æ‚åŒ–è¿™å‰¯å›¾æ™¯ï¼šä»”ç»†è§‚å¯Ÿå¯ä»¥å‘ç°ï¼Œåœ¨ä»å°å»ºæ”¿æ²»ç»“æ„åˆ°èµ„äº§é˜¶çº§æ”¿æ²»ç»“æ„çš„è¿‡ç¨‹ä¸­ï¼Œå­˜åœ¨ç€ä¸¤ä¸ªâ€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ï¼šç»å¯¹å›ä¸»åˆ¶å’Œé›…å„å®¾ä¸»ä¹‰ã€‚ç¬¬ä¸€ä¸ªæ˜¯æœ‰å…³ä¸€ä¸ªæ‚–è®ºå¼å¦¥åçš„æ ‡å¿—ä¸ä½“ç°ï¼ˆembodimentï¼‰ï¼šè¿™ç§æ”¿æ²»å½¢å¼ä½¿å´›èµ·çš„èµ„äº§é˜¶çº§èƒ½å¤Ÿé€šè¿‡æ‰“ç ´å°å»ºä¸»ä¹‰ã€å…¶è¡Œä¼šå’Œç¤¾å›¢ï¼ˆcorporationsï¼‰çš„ç»æµåŠ›é‡æ¥åŠ å¼ºå…¶ç»æµéœ¸æƒâ€”â€”å½“ç„¶ï¼Œå®ƒçš„è‡ªç›¸çŸ›ç›¾ä¹‹å¤„åœ¨äºï¼Œå°å»ºä¸»ä¹‰æ­£æ˜¯é€šè¿‡å°†è‡ªå·±çš„æœ€é«˜ç‚¹ï¼ˆcrowning pointï¼‰ç»å¯¹åŒ–â€”â€”å°†ç»å¯¹æƒåŠ›èµ‹äºˆå›ä¸»â€”â€”æ¥â€œè‡ªæ˜åŸå¢“â€çš„ï¼›å› æ­¤ï¼Œç»å¯¹å›ä¸»åˆ¶çš„ç»“æœæ˜¯æ”¿æ²»ç§©åºä¸ç»æµåŸºç¡€ç›¸â€œåˆ†ç¦»â€ã€‚åŒæ ·çš„â€œè„±èŠ‚ï¼ˆdisconnectionï¼‰â€ä¹Ÿæ˜¯é›…å„å®¾ä¸»ä¹‰çš„ç‰¹å¾ï¼šæŠŠé›…å„å®¾ä¸»ä¹‰è§„å®šä¸ºä¸€ç§æ¿€è¿›æ„è¯†å½¢æ€å·²ç»æ˜¯é™ˆè¯æ»¥è°ƒäº†ï¼Œå®ƒâ€œä»å­—é¢ä¸Šâ€æ¥å—äº†èµ„äº§é˜¶çº§çš„æ”¿æ²»çº²é¢†ï¼ˆå¹³ç­‰ã€è‡ªç”±ã€åšçˆ±[brotherhood]ï¼‰ï¼Œå¹¶åŠªåŠ›å®ç°å®ƒï¼Œè€Œä¸è€ƒè™‘åŒå…¬æ°‘ç¤¾ä¼šçš„å…·ä½“è¡”æ¥ã€‚

ä¸¤è€…éƒ½ä¸ºä»–ä»¬çš„å¹»æƒ³ä»˜å‡ºäº†æ²‰é‡çš„ä»£ä»·ï¼šä¸“åˆ¶å›ä¸»å¾ˆæ™šæ‰æ³¨æ„åˆ°ï¼Œç¤¾ä¼šç§°èµä»–æ˜¯ä¸‡èƒ½çš„ï¼Œåªæ˜¯ä¸ºäº†è®©ä¸€ä¸ªé˜¶çº§æ¨ç¿»å¦ä¸€ä¸ªé˜¶çº§ï¼›é›…å„å®¾æ´¾ä¸€æ—¦å®Œæˆäº†æ‘§æ¯æ—§åˆ¶åº¦çš„æœºå™¨çš„å·¥ä½œï¼Œä¹Ÿå°±å˜å¾—å¤šä½™äº†ã€‚ä¸¤è€…éƒ½è¢«å…³äºæ”¿æ²»é¢†åŸŸè‡ªä¸»æ€§ï¼ˆautonomyï¼‰çš„å¹»æƒ³æ‰€è¿·æƒ‘ï¼Œéƒ½ç›¸ä¿¡è‡ªå·±çš„æ”¿æ²»ä½¿å‘½ï¼šä¸€ä¸ªç›¸ä¿¡çš‡æƒçš„ä¸å¯è´¨ç–‘æ€§ï¼Œå¦ä¸€ä¸ªç›¸ä¿¡å…¶æ”¿æ²»æ–¹æ¡ˆçš„æ°å½“æ€§ï¼ˆpertinenceï¼‰ã€‚åœ¨å¦ä¸€ä¸ªå±‚é¢ä¸Šï¼Œæˆ‘ä»¬ä¸æ˜¯ä¹Ÿå¯ä»¥è¿™æ ·è¯´æ³•è¥¿æ–¯ä¸»ä¹‰å’Œå…±äº§ä¸»ä¹‰ï¼Œå³â€œå®é™…ç°å­˜çš„ç¤¾ä¼šä¸»ä¹‰ï¼ˆactually existing socialismï¼‰â€å—ï¼Ÿæ³•è¥¿æ–¯ä¸»ä¹‰éš¾é“ä¸æ˜¯ä¸€ç§èµ„æœ¬ä¸»ä¹‰å›ºæœ‰çš„è‡ªæˆ‘å¦å®šï¼Œä¸æ˜¯è¯•å›¾é€šè¿‡ä¸€ç§ä½¿ç»æµä»å±äºæ„è¯†å½¢æ€-æ”¿æ²»ï¼ˆideological-politicalï¼‰é¢†åŸŸçš„æ„è¯†å½¢æ€æ¥â€œæ”¹å˜ä¸€äº›ä¸œè¥¿ï¼Œä»¥ä¾¿æ²¡æœ‰çœŸæ­£çš„æ”¹å˜â€å—ï¼Ÿåˆ—å®ä¸»ä¹‰çš„â€œå®é™…å­˜åœ¨çš„ç¤¾ä¼šä¸»ä¹‰â€éš¾é“ä¸æ˜¯ä¸€ç§â€œç¤¾ä¼šä¸»ä¹‰çš„é›…å„å®¾ä¸»ä¹‰â€ï¼Œä¸æ˜¯è¯•å›¾ä½¿æ•´ä¸ªç¤¾ä¼šç»æµç”Ÿæ´»ä»å±äºç¤¾ä¼šä¸»ä¹‰å›½å®¶çš„ç›´æ¥æ”¿æ²»è°ƒèŠ‚å—ï¼Ÿå®ƒä¸¤è€…éƒ½æ˜¯â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ï¼Œä½†è¿›å…¥äº†ä»€ä¹ˆå‘¢ï¼Ÿé€šå¸¸çš„çŠ¬å„’å¼ç­”æ¡ˆâ€œä»èµ„æœ¬ä¸»ä¹‰å›åˆ°èµ„æœ¬ä¸»ä¹‰â€ä¼¼ä¹æœ‰ç‚¹å¤ªå®¹æ˜“äº†â€¦â€¦

å› æ­¤ï¼Œè¿™ç§å¯¹â€œå†…å®¹â€ï¼ˆâ€œç»æµåŸºç¡€â€ï¼‰ä¸æ„è¯†å½¢æ€â€œå½¢å¼â€ä¹‹â€œå¸¸æ€â€å…³ç³»çš„é¢ å€’ï¼ˆå¦‚å‰æ‰€è¿°ï¼Œå®ƒä½¿éŸ¦ä¼¯çš„åé©¬å…‹æ€ä¸»ä¹‰è§£è¯»æˆä¸ºå¯èƒ½ï¼‰ï¼Œå°±åœ¨äºä¸Šè¿°ä½œä¸ºâ€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ä¹‹ç‰¹ç‚¹çš„â€œè§£æ”¾ï¼ˆemancipationï¼‰â€â€”â€”å®ƒå°†å½¢å¼ä»å…¶å†…å®¹â€œè§£æ”¾â€å‡ºæ¥ï¼šæ–°æ•™åŒä¸­ä¸–çºªæ•™ä¼šçš„å†³è£‚å¹¶ä¸â€œåæ˜ â€æ–°çš„ç¤¾ä¼šå†…å®¹ï¼Œè€Œç›¸åæ˜¯ä»¥å°å»ºè‡ªèº«æ„è¯†å½¢æ€å½¢å¼çš„æ¿€è¿›ç‰ˆæœ¬çš„åä¹‰å¯¹æ—§çš„å°å»ºå†…å®¹è¿›è¡Œæ‰¹åˆ¤ï¼›æ­£æ˜¯è¿™ç§å°†åŸºç£æ•™å½¢å¼ä»åŸºç£æ•™è‡ªèº«ç¤¾ä¼šå†…å®¹ä¸­é‡Šæ”¾å‡ºæ¥çš„â€œè§£æ”¾â€ï¼Œä¸ºæ—§å†…å®¹é€æ¸è½¬åŒ–ä¸ºæ–°ï¼ˆèµ„æœ¬ä¸»ä¹‰ï¼‰å†…å®¹å¼€è¾Ÿäº†ç©ºé—´ã€‚å› æ­¤ï¼Œè©¹æ˜ä¿¡å¾ˆå®¹æ˜“åœ°è¯æ˜äº†éŸ¦ä¼¯å…³äºæ–°æ•™åœ¨èµ„æœ¬ä¸»ä¹‰å…´èµ·è¿‡ç¨‹ä¸­çš„å…³é”®ä½œç”¨çš„ç†è®ºå¦‚ä½•ä»…ä»…å½±å“åˆ°åº¸ä¿—çš„ç»æµä¸»ä¹‰ï¼Œè€Œåˆå¦‚ä½•ä¸â€œåŸºç¡€â€å’Œæ„è¯†å½¢æ€â€œä¸Šå±‚å»ºç­‘â€çš„è¾©è¯æ³•ç›¸å½“å…¼å®¹ã€‚æ ¹æ®è¿™ç§è¾©è¯æ³•ï¼Œäººä»¬é€šè¿‡ä¸€ä¸ªâ€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ä»ä¸€ä¸ªç¤¾ä¼šå½¢æ€è¿›å…¥å¦ä¸€ä¸ªç¤¾ä¼šå½¢æ€ï¼Œè¿™ç§ä¸­ä»‹è€…é¢ å€’äº†â€œåŸºç¡€â€å’Œâ€œä¸Šå±‚å»ºç­‘â€ä¹‹é—´çš„å…³ç³»ï¼šé€šè¿‡å°†è‡ªå·±ä»è‡ªå·±çš„â€œåŸºç¡€â€ä¸­è§£æ”¾å‡ºæ¥ï¼Œæ—§â€œä¸Šå±‚å»ºç­‘â€ä¸ºâ€œåŸºç¡€â€çš„è½¬å˜å‡†å¤‡äº†åœ°å½¢ï¼ˆterrainï¼‰ã€‚ç»å…¸é©¬å…‹æ€ä¸»ä¹‰ç†è®ºå¤§å¦å°±è¿™æ ·è¢«æ‹¯æ•‘äº†ï¼Œæ„è¯†å½¢æ€å½¢å¼çš„â€œè§£æ”¾â€ä»â€œåŸºç¡€â€æœ¬èº«çš„å†…åœ¨å¯¹ç«‹ä¸­å¾—åˆ°äº†è§£é‡Šï¼šå½“è¿™äº›å¯¹ç«‹å˜å¾—å¦‚æ­¤æ¿€çƒˆï¼Œä»¥è‡³äºå®ƒä»¬ä¸å†èƒ½è¢«è‡ªå·±çš„æ„è¯†å½¢æ€å½¢å¼åˆæ³•åŒ–æ—¶ï¼Œè§£æ”¾å°±å‡ºç°äº†ã€‚

è¿™ç§å¯¹æ„è¯†å½¢æ€ä¸Šå±‚å»ºç­‘çš„â€œè§£æ”¾â€æœ‰ä¸€ä¸ªå›ºæœ‰çš„æ‚²å‰§æ€§çš„ä¼¦ç†ç»´åº¦ï¼šå®ƒæå‡ºäº†ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„ç‚¹ï¼Œåœ¨è¿™ä¸ªç‚¹ä¸Šï¼Œä¸€ç§æ„è¯†å½¢æ€â€œåœ¨å­—é¢ä¸Šæ¥å—äº†è‡ªèº«â€å¹¶ä¸å†ä½œä¸ºâ€œå®¢è§‚ä¸ŠçŠ¬å„’çš„â€ï¼ˆé©¬å…‹æ€ï¼‰å¯¹ç°å­˜æƒåŠ›å…³ç³»çš„åˆæ³•åŒ–å‘æŒ¥ä½œç”¨ã€‚è®©æˆ‘ä»¬æåˆ°å¦ä¸€ä¸ªæ›´å½“ä»£çš„æ¡ˆä¾‹ï¼šåœ¨ä¸œæ¬§â€œå®é™…å­˜åœ¨çš„ç¤¾ä¼šä¸»ä¹‰â€çš„æœ€åå‡ å¹´é—´å‡ºç°çš„â€œæ–°ç¤¾ä¼šè¿åŠ¨â€ï¼Œè¿™äº›è¿åŠ¨çš„å…¸èŒƒä»£è¡¨æ˜¯å‰ä¸œå¾·çš„â€œæ–°è®ºå›ï¼ˆNeues Forumï¼‰â€ï¼šä¸€ç¾¤å……æ»¡æ¿€æƒ…çš„çŸ¥è¯†åˆ†å­ï¼Œä»–ä»¬â€œè®¤çœŸå¯¹å¾…ç¤¾ä¼šä¸»ä¹‰â€ï¼Œå‡†å¤‡å†’ä¸€åˆ‡é£é™©æ¥æ‘§æ¯å¦¥ååˆ¶åº¦ï¼Œå¹¶ç”¨è¶…è¶Šäº†èµ„æœ¬ä¸»ä¹‰å’Œâ€œå®é™…å­˜åœ¨çš„â€ç¤¾ä¼šä¸»ä¹‰çš„ä¹Œæ‰˜é‚¦å¼çš„â€œç¬¬ä¸‰æ¡é“è·¯â€æ¥å–ä»£å®ƒã€‚ä»–ä»¬çœŸè¯šåœ°ç›¸ä¿¡å¹¶åšæŒä»–ä»¬ä¸æ˜¯åœ¨ä¸ºæ¢å¤è¥¿æ–¹èµ„æœ¬ä¸»ä¹‰è€Œå·¥ä½œï¼Œå½“ç„¶ï¼Œäº‹å®è¯æ˜è¿™ä¸è¿‡æ˜¯ä¸€ç§æ²¡æœ‰å®è´¨å†…å®¹çš„å¹»è§‰ï¼›ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œæ°æ°æ˜¯è¿™æ ·ï¼ˆä½œä¸ºä¸€ç§æ²¡æœ‰å®è´¨å†…å®¹çš„å½»åº•å¹»è§‰ï¼‰ï¼Œå®ƒæ‰åœ¨ä¸¥æ ¼æ„ä¹‰ä¸Šæ˜¯éæ„è¯†å½¢æ€çš„ï¼šå®ƒå¹¶æ²¡æœ‰ä»¥é¢ å€’çš„æ„è¯†å½¢æ€å½¢å¼â€œåæ˜ â€ä»»ä½•å®é™…çš„æƒåŠ›å…³ç³»ã€‚

åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬åº”è¯¥çº æ­£é©¬å…‹æ€ä¸»ä¹‰çš„å…¬è®¤æ–‡æœ¬ï¼ˆVulgateï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸åŒäºå¦‚ä¸‹è¿™ç§è€ç”Ÿå¸¸è°ˆï¼šåœ¨ä¸€ä¸ªç¤¾ä¼šå½¢æ€çš„â€œé¢“åºŸï¼ˆdecadenceï¼‰â€æ—¶æœŸï¼Œæ„è¯†å½¢æ€ä¼šå˜å¾—â€œçŠ¬å„’â€ï¼ˆæ¥å—â€œè¨€â€ä¸â€œè¡Œâ€ä¹‹é—´çš„å·®è·ï¼Œä¸å†â€œç›¸ä¿¡è‡ªå·±â€ï¼Œä¸å†è¢«è§†ä¸ºçœŸç†ï¼Œè€Œæ˜¯æŠŠè‡ªå·±å½“ä½œä½¿æƒåŠ›åˆæ³•åŒ–çš„çº¯ç²¹å·¥å…·æ€§æ‰‹æ®µï¼‰ã€‚

å¯ä»¥è¯´ï¼Œæ°æ°æ˜¯â€œé¢“åºŸâ€æ—¶æœŸä¸ºç»Ÿæ²»æ„è¯†å½¢æ€ï¼ˆruling ideologyï¼‰æ‰“å¼€äº†â€œè®¤çœŸå¯¹å¾…è‡ªå·±â€çš„å¯èƒ½æ€§ï¼Œå¹¶æœ‰æ•ˆåœ°å°†è‡ªå·±ä¸è‡ªå·±çš„ç¤¾ä¼šåŸºç¡€å¯¹ç«‹èµ·æ¥ï¼ˆå¯¹äºæ–°æ•™ï¼ŒåŸºç£å®—æ•™åå¯¹å°å»ºä¸»ä¹‰ä½œä¸ºå…¶ç¤¾ä¼šåŸºç¡€ï¼Œå°±åƒæ–°è®ºå›ä»¥â€œçœŸæ­£çš„ç¤¾ä¼šä¸»ä¹‰â€çš„åä¹‰åå¯¹ç°æœ‰çš„ç¤¾ä¼šä¸»ä¹‰ï¼‰ã€‚ä»¥è¿™ç§æ–¹å¼ï¼Œå®ƒå°±ä¸çŸ¥ä¸è§‰åœ°è§£å¼€äº†è‡´ä½¿è‡ªå·±æœ€ç»ˆæ¯ç­çš„åŠ›é‡ï¼šä¸€æ—¦ä»–ä»¬çš„å·¥ä½œå®Œæˆï¼Œä»–ä»¬å°±ä¼šè¢«â€œå†å²æ‰€æ·¹æ²¡â€ï¼ˆã€Šæ–°è®ºå›ã€‹åœ¨é€‰ä¸¾ä¸­çš„å¾—ç¥¨ç‡ä¸º $3 \%$ ï¼‰ï¼Œä¸€ä¸ªæ–°çš„â€œæ¶æ£æ—¶ä»£â€æ¥ä¸´äº†ï¼Œé‚£äº›åœ¨å…±äº§å…šé•‡å‹æœŸé—´å¤§å¤šä¿æŒæ²‰é»˜çš„å½“æƒè€…ï¼Œç°åœ¨å´æŠŠæ–°è®ºå›è¾±éª‚æˆâ€œç§˜å¯†çš„å…±äº§å…šå‘˜â€ã€‚

Â·ä½ æ‰‹æŒ‡çš„ä¸€æ•²â€¦â€¦

â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€å®é™…ä¸Šä»…æ˜¾ç°ä¸ºä¸€ä¸ªä¸­ä»‹è€…ï¼Œä¸€ä¸ªä»‹äºä¸¤ç§â€œå¸¸æ€â€äº‹ç‰©çŠ¶æ€ä¹‹é—´çš„ä¸­é—´å½¢è±¡ï¼ˆfigureï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§è§£è¯»æ˜¯å”¯ä¸€å¯èƒ½çš„è§£è¯»å—ï¼Ÿç”±â€œåé©¬å…‹æ€ä¸»ä¹‰â€æ”¿æ²»ç†è®ºï¼ˆClaude Lefort, Ernesto Laclauï¼‰æ‰€é˜è¿°çš„æ¦‚å¿µè£…ç½®å…è®¸å¦ä¸€ç§è§£è¯»ï¼Œè€Œè¿™ç§è§£è¯»ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è§†è§’ã€‚åœ¨è¿™ä¸ªé¢†åŸŸä¸­ï¼Œâ€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€è¿™ä¸€ç¯èŠ‚è¢«é˜¿å…°Â·å·´è¿ªæ¬§7å®šä¹‰ä¸ºâ€œäº‹ä»¶â€ï¼ˆå®ƒæœ‰å…³å·²ç¡®ç«‹çš„ç»“æ„ï¼‰çš„ç¯èŠ‚ï¼šå…¶çœŸç›¸åœ¨å…¶ä¸­å‡ºç°çš„ç¯èŠ‚ã€æœ‰å…³â€œå¼€æ”¾æ€§ï¼ˆopennessï¼‰â€çš„ç¯èŠ‚â€”â€”ä¸€æ—¦â€œäº‹ä»¶â€çš„çˆ†å‘è¢«åˆ¶åº¦åŒ–ä¸ºä¸€ç§æ–°çš„è‚¯å®šæ€§ï¼ˆpositivityï¼‰ï¼Œå®ƒå°±ä¼šæ¶ˆå¤±ï¼Œæˆ–è€…æ›´ç¡®åˆ‡åœ°è¯´ï¼Œåœ¨å­—é¢ä¸Šä¸å¯è§äº†ã€‚

æ ¹æ®ä¼—æ‰€å‘¨çŸ¥çš„è€ç”Ÿå¸¸è°ˆï¼ˆå®ƒä¸é€šå¸¸çš„æ¨¡å¼ç›¸åï¼Œå¹¶ä¸æ˜¯æŠ«ç€æ™ºæ…§å¤–è¡£çš„æ„šè ¢ï¼‰ï¼Œâ€œåœ¨äº‹å®ä¹‹åâ€ï¼Œå¾€å›çœ‹ï¼Œå†å²ï¼ˆHistoryï¼‰æ€»æ˜¯å¯ä»¥è¢«è§£è¯»ä¸ºä¸€ä¸ªå—è§„å¾‹æ”¯é…çš„è¿‡ç¨‹ï¼›è¢«è§£è¯»ä¸ºä¸€ä¸ªå…³äºå„é˜¶æ®µçš„æœ‰æ„ä¹‰çš„è¿ç»­ä½“ï¼›ç„¶è€Œï¼Œå°±æˆ‘ä»¬æ˜¯å…¶æ–½åŠ¨è€…ï¼ˆagentsï¼‰ï¼Œè¢«åµŒå…¥ã€å·å…¥äº†è¿™ä¸€è¿›ç¨‹è€Œè¨€ï¼Œæƒ…å†µâ€”â€”è‡³å°‘åœ¨â€œæŸäº‹æ­£åœ¨å‘ç”Ÿâ€çš„è½¬æŠ˜ç‚¹â€”â€”ä¼¼ä¹æ˜¯å¼€æ”¾è€Œä¸ç¡®å®šçš„ï¼ˆundecidableï¼‰ï¼Œç»ä¸æ˜¯ä¸€ä¸ªæ½œè—äºå…¶ä¸‹çš„ï¼ˆunderlyingï¼‰å¿…ç„¶æ€§çš„å±•ç°ï¼šæˆ‘ä»¬å‘ç°æˆ‘ä»¬è‡ªå·±é¢ä¸´ç€è´£ä»»ï¼Œå†³å®šçš„é‡è´Ÿå‹åœ¨æˆ‘ä»¬çš„è‚©ä¸Šã€‚

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹åæœˆé©å‘½ï¼šè¿½æº¯èµ·æ¥ï¼Œå¾ˆå®¹æ˜“å°†å…¶ç½®äºæ›´å¹¿æ³›çš„å†å²è¿›ç¨‹ä¸­ï¼Œè¯´æ˜å®ƒæ˜¯å¦‚ä½•ä»ä¿„å›½çš„å…·ä½“æƒ…å†µâ€”â€”ä¿„ç½—æ–¯å¤±è´¥çš„ç°ä»£åŒ–ä¸åŒæ—¶å­˜åœ¨çš„â€œç°ä»£æ€§çš„å²›å±¿â€ï¼ˆåœ¨å­¤ç«‹åœ°åŒºé«˜åº¦å‘è¾¾çš„å·¥äººé˜¶çº§ï¼‰â€”â€”ä¸­äº§ç”Ÿçš„ã€‚æ€»ä¹‹ï¼Œå°±è¿™ä¸ªä¸»é¢˜å†™ä¸€ç¯‡ç¤¾ä¼šå­¦è®ºæ–‡å¹¶ä¸éš¾ã€‚ç„¶è€Œï¼Œåªè¦é‡è¯»ä¸€ä¸‹åˆ—å®ã€æ‰˜æ´›èŒ¨åŸºã€å­Ÿä»€ç»´å…‹å’Œå…¶ä»–å‚ä¸è€…ä¹‹é—´çš„æ¿€æƒ…è®ºæˆ˜ï¼Œå°±ä¼šå‘ç°è‡ªå·±é¢å¯¹è¿™ç§â€œå®¢è§‚â€çš„å†å²å™è¿°æ‰€å¤±å»çš„ä¸œè¥¿ï¼šåœ¨ä¸€ç§æ–°æƒ…åŠ¿â€”â€”å®ƒå¯ä»¥è¯´æ˜¯è¿«ä½¿æ–½åŠ¨è€…ï¼ˆagentsï¼‰åœ¨æ²¡æœ‰ä»»ä½•â€œå†å²å‘å±•çš„ä¸€èˆ¬è§„å¾‹â€ä¿è¯çš„æƒ…å†µä¸‹å»å‘æ˜æ–°çš„è§£å†³æ–¹æ¡ˆå¹¶åšå‡ºå‰æ‰€æœªé—»çš„è¡ŒåŠ¨â€”â€”ä¸‹å†³å®šçš„é‡è´Ÿï¼ˆthe burden of decisionï¼‰ã€‚

è¿™ä¸€æœ‰å…³å¼€æ”¾æ€§ï¼ˆopennessï¼‰çš„â€œä¸å¯èƒ½çš„â€ç¯èŠ‚æ„æˆäº†ä¸»ä½“æ€§çš„ç¯èŠ‚ï¼šâ€œä¸»ä½“â€æ˜¯ä¸€ä¸ªåç§°ï¼ŒæŒ‡çš„æ˜¯é‚£ä¸ªè¢«å¬å”¤çš„ã€çªç„¶é—´è´Ÿæœ‰è´£ä»»çš„æ·±ä¸å¯æµ‹ï¼ˆunfathomableï¼‰çš„ Xï¼Œå®ƒåœ¨è¿™æ ·ä¸€ä¸ªæœ‰å…³ä¸ç¡®å®šæ€§ï¼ˆundecidabilityï¼‰çš„æ—¶åˆ»è¢«æŠ›å…¥ä¸€ä¸ªè´£ä»»çš„ä½ç½®ï¼Œè¢«æŠ›å…¥è¿™å…³äºå†³å®šï¼ˆdecisionï¼‰çš„ç´§æ€¥äº‹æ€ä¹‹ä¸­ã€‚è¿™å°±æ˜¯æˆ‘ä»¬è§£è¯»é»‘æ ¼å°”çš„è¿™ä¸€å‘½é¢˜â€”â€”â€œçœŸç†ï¼ˆTrueï¼‰ä¸ä»…è¦è¢«ç†è§£ä¸ºå®ä½“ï¼Œè€Œä¸”åŒæ ·è¦è¢«ç†è§£ä¸ºä¸»ä½“â€8â€”â€”ä¸å¾—ä¸é‡‡å–çš„æ–¹å¼ï¼šä¸ä»…è¦è¢«ç†è§£ä¸ºä¸€ä¸ªå—æŸç§éšè—çš„ç†æ€§å¿…ç„¶æ€§æ”¯é…çš„å®¢è§‚è¿‡ç¨‹ï¼ˆå³ä½¿è¿™ç§å¿…ç„¶æ€§å…·æœ‰é»‘æ ¼å°”å¼â€œç†æ€§çš„ç‹¡è®¡â€çš„ï¼‰ï¼Œè€Œä¸”è¦è¢«ç†è§£ä¸ºä¸€ä¸ªè¢«æœ‰å…³å¼€æ”¾æ€§ï¼ä¸ç¡®å®šæ€§ï¼ˆundecidabilityï¼‰çš„ç¯èŠ‚æ‰€æ‰“æ–­å¹¶å®¡è§†ï¼ˆscanï¼‰çš„è¿‡ç¨‹â€”â€”ä¸»ä½“çš„ä¸å¯è¿˜åŸçš„å¶ç„¶è¡Œä¸ºå»ºç«‹äº†ä¸€ä¸ªæ–°çš„å¿…ç„¶æ€§ã€‚æ ¹æ®ä¸€ä¸ªè‘—åçš„æ„è§ï¼ˆdoxaï¼‰ï¼Œè¾©è¯æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿç©¿é€è¯¸å¶ç„¶æ€§çš„è¡¨é¢æˆå‰§ï¼Œè¾¾è‡³åœ¨ä¸»ä½“èƒŒåâ€œæ“çºµç€è¡¨æ¼”â€çš„æ ¹æœ¬çš„ï¼ˆunderlyingï¼‰ç†æ€§å¿…ç„¶æ€§ã€‚ä¸€ä¸ªæ°å½“çš„é»‘æ ¼å°”å¼çš„è¾©è¯è¿åŠ¨å‡ ä¹æ˜¯è¿™ä¸€ç¨‹åºçš„å®Œå…¨é¢ å€’ï¼šå®ƒé©±æ•£äº†å¯¹â€œå®¢è§‚å†å²è¿›ç¨‹â€çš„è¿·ä¿¡å¹¶è®©æˆ‘ä»¬çœ‹åˆ°å®ƒçš„èµ·æºï¼šå†å²ä¸Šçš„å¿…ç„¶æ€§å‡ºç°çš„æ–¹å¼â€”â€”å®ƒæ˜¯ä¸€ç§å®è¯åŒ–ï¼ˆpositivizationï¼‰ã€ä¸»ä½“åœ¨ä¸€ä¸ªå¼€æ”¾çš„ã€ä¸ç¡®å®šçš„æƒ…åŠ¿ä¸‹çš„æ ¹æœ¬å¶ç„¶å†³å®šçš„ä¸€ä¸ªâ€œå‡ç»“ï¼ˆcoagulationï¼‰â€ã€‚æ ¹æ®å®šä¹‰ï¼Œâ€œè¾©è¯çš„å¿…ç„¶æ€§â€æ€»æ˜¯äº‹åçš„ï¼ˆaprÃ¨s coupï¼‰å¿…ç„¶æ€§ï¼šä¸€ä¸ªé€‚å½“çš„è¾©è¯è§£è¯»è´¨ç–‘å¯¹â€œå®é™…ä¸Šå‘ç”Ÿçš„äº‹æƒ…â€çš„è‡ªæˆ‘è¯æ˜ï¼Œå¹¶å°†å…¶ä¸æ²¡æœ‰å‘ç”Ÿçš„äº‹æƒ…å¯¹ç«‹èµ·æ¥â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒè®¤ä¸ºæ²¡æœ‰å‘ç”Ÿçš„äº‹æƒ…ï¼ˆä¸€ç³»åˆ—é”™è¿‡çš„æœºä¼šã€ä¸€ç³»åˆ—â€œæ›¿ä»£æ€§å†å²â€ï¼‰æ˜¯â€œå®é™…ä¸Šå‘ç”Ÿçš„äº‹æƒ…â€çš„æ„æˆéƒ¨åˆ†ã€‚

å› æ­¤ï¼Œè¾©è¯æ³•å¯¹â€œå¯èƒ½ä¸–ç•Œâ€è¿™ä¸€é—®é¢˜å¼ï¼ˆproblematicï¼‰çš„æ€åº¦æ¯”å®ƒçœ‹èµ·æ¥æ›´åŠ å…·æœ‰æ‚–è®ºæ€§ï¼šæ—¢ç„¶ç°åœ¨åœ¨æˆ‘ä»¬çš„ç°å®ä¸­å‘ç”Ÿçš„äº‹æƒ…æ˜¯ä¸€ç³»åˆ—æ ¹æœ¬æ€§çš„å¶ç„¶è¡Œä¸ºçš„ç»“æœï¼Œé‚£ä¹ˆå®šä¹‰æˆ‘ä»¬çš„ç°å®ä¸–ç•Œï¼ˆactual worldï¼‰çš„å”¯ä¸€æ°å½“æ–¹å¼å°±æ˜¯åœ¨å…¶å®šä¹‰ä¸­åŒ…æ‹¬å¯¹åœ¨å…¶ä½ç½®ä¸­æ‰€åŒ…å«çš„â€œå¯èƒ½ä¸–ç•Œâ€çš„å¦å®šâ€”â€”æˆ‘ä»¬å¤±å»çš„å„ç§æœºä¼šï¼ˆopportunitiesï¼‰æ˜¯æˆ‘ä»¬ä¹‹æ‰€æ˜¯ï¼ˆwhat we areï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä»¬ qualifyï¼ˆåœ¨è¿™ä¸ªè¯çš„æ‰€æœ‰æ„ä¹‰ä¸Š $\textcircled{1}$ ä½¿å…¶å…·å¤‡èµ„æ ¼ï¼› $\textcircled{2}$ ä¿®é¥°é™å®šâ€”â€”è¯‘æ³¨]ï¼‰äº†å®ƒã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬è§£è¯»è¿‡å»çš„è§†é‡æ˜¯ç”±æˆ‘ä»¬æ‰€åšçš„å¶ç„¶è¡ŒåŠ¨æ‰€å†³å®šçš„ï¼Œè¿™äº›è¡Œä¸ºå¼ºåŠ äº†å¯¹â€œå¿…ç„¶æ€§â€çš„å›æº¯æ€§å¹»è§‰ï¼›ç”±äºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬ä¸å¯èƒ½å æ®ä¸€ä¸ªçº¯ç²¹å…ƒè¯­è¨€çš„ä¸­ç«‹ä½ç½®ï¼Œä»é‚£é‡Œæˆ‘ä»¬å¯ä»¥çºµè§ˆæ‰€æœ‰çš„â€œå¯èƒ½ä¸–ç•Œâ€ã€‚è¿™æ„å‘³ç€ï¼Œç”±äºå®šä¹‰æˆ‘ä»¬è‡ªå·±çš„ç°å®ä¸–ç•Œçš„å”¯ä¸€æ–¹æ³•æ˜¯å€ŸåŠ©å®ƒä¸å®ƒçš„æ›¿ä»£é€‰é¡¹çš„å¦å®šå…³ç³»ï¼Œæˆ‘ä»¬æ°¸è¿œæ— æ³•ç¡®å®šï¼ˆdetermineï¼‰æˆ‘ä»¬å®é™…ç”Ÿæ´»äºå…¶ä¸­çš„ä¸–ç•Œã€‚æ¢å¥è¯è¯´ï¼ŒæŠŠè¿™ä¸ªæ‚–è®ºå‘æŒ¥åˆ°æè‡´ï¼šå½“ç„¶ï¼Œåªæœ‰ä¸€ä¸ªä¸–ç•Œæ˜¯çœŸæ­£å¯èƒ½çš„ï¼Œå³æˆ‘ä»¬å®é™…ç”Ÿæ´»äºå…¶ä¸­çš„ä¸–ç•Œï¼Œä½†ç”±äºæˆ‘ä»¬æ— æ³•è·å¾—ä¸€ä¸ªä¸­ç«‹è§‚å¯Ÿè€…çš„ä½ç½®ï¼Œæˆ‘ä»¬ä¸çŸ¥é“è¿™ä¸ªä¸–ç•Œæ˜¯å“ªä¸€ä¸ªï¼›æˆ‘ä»¬ä¸çŸ¥é“æˆ‘ä»¬å®é™…ç”Ÿæ´»åœ¨å“ªä¸ªâ€œå¯èƒ½ä¸–ç•Œâ€ã€‚é—®é¢˜çš„å…³é”®ä¸æ˜¯â€œæˆ‘ä»¬æ°¸è¿œæ— æ³•å¾—çŸ¥ä»€ä¹ˆæœºä¼šæ˜¯æˆ‘ä»¬å·²ç„¶å¤±å»çš„â€ï¼Œè€Œæ˜¯æˆ‘ä»¬æ°¸è¿œæ— æ³•çœŸæ­£å¾—çŸ¥çŸ¥é“ä»€ä¹ˆæ˜¯æˆ‘ä»¬å·²ç„¶å¾—åˆ°çš„ã€‚è¿™ç§ç«‹åœºå¯èƒ½çœ‹èµ·æ¥å¾ˆæç«¯ï¼Œéš¾é“æˆ‘ä»¬ä¸æ˜¯å¯ä»¥åœ¨â€œä»–ä¸çŸ¥é“è‡ªå·±çš„è¿æ°”â€è¿™å¥æˆ‘ä»¬ç”¨æ¥æŒ‡ç§°æŸäººä¸çŸ¥é“ä»–æœ‰å¤šå¹¸è¿é”™è¿‡äº†ä¸€ç³»åˆ—å¯èƒ½çš„ç¾éš¾çš„æ—¥å¸¸ç”¨è¯­ä¸­çœ‹å‡ºè¿™ä¸€ç‚¹å—ï¼Ÿå¦‚æœâ€œè¾©è¯æ³•â€ä¸æ„å‘³ç€è¿™ä¸€ç‚¹ï¼Œé‚£ä¹ˆæ‰€æœ‰å…³äºâ€œå®ä½“å³ä¸»ä½“â€çš„è®¨è®ºæœ€ç»ˆéƒ½æ˜¯æ— æ•ˆçš„ï¼Œè€Œæˆ‘ä»¬åˆå›åˆ°äº†ä½œä¸ºå®è´¨æ€§å¿…ç„¶æ€§ï¼ˆsubstantialNecessityï¼‰çš„ç†æ€§ï¼ˆReasonï¼‰ï¼Œåœ¨å¹•åæ“çºµç€â€¦â€¦

æ­£æ˜¯é¢å¯¹è¿™æ ·çš„èƒŒæ™¯ï¼Œæˆ‘ä»¬æ‰å¿…é¡»ç†è§£é»‘æ ¼å°”æœ‰å…³â€œè®¾å®šé¢„è®¾ï¼ˆpositing ofpresuppositionsï¼‰â€çš„è®ºé¢˜ï¼šè¿™ç§å›æº¯æ€§çš„è®¾å®šæ°æ°æ˜¯å¿…ç„¶æ€§ä»å¶ç„¶æ€§ä¸­å‡ºç°çš„æ–¹å¼ã€‚ä¸»ä½“â€œè®¾å®šå…¶é¢„è®¾â€çš„ç¯èŠ‚ï¼Œæ­£æ˜¯ä»–ä½œä¸ºä¸»ä½“è¢«æŠ¹å»çš„ç¯èŠ‚ï¼Œä»–ä½œä¸ºä¸­ä»‹è€…æ¶ˆå¤±çš„ç¯èŠ‚ï¼šå½“ä¸»ä½“çš„å†³å®šè¡Œä¸ºï¼ˆact of decisionï¼‰å˜æˆå®ƒçš„åé¢æ—¶çš„é‚£ä¸ªç»“æŸçš„ç¯èŠ‚ï¼›å»ºç«‹ä¸€ä¸ªæ–°çš„è±¡å¾ç½‘ç»œï¼Œè€Œå†å²å€ŸåŠ©è¿™ä¸€ç½‘ç»œå†æ¬¡è·å¾—äº†çº¿æ€§æ¼”è¿›çš„è‡ªæˆ‘è¯æ˜ã€‚è®©æˆ‘ä»¬å›åˆ°åæœˆé©å‘½ï¼šå…¶â€œé¢„è®¾â€åœ¨å®ƒçš„èƒœåˆ©å’Œæ–°æ”¿æƒçš„å·©å›ºä¹‹åã€å½¢åŠ¿çš„å¼€æ”¾æ€§å†æ¬¡ä¸§å¤±ä¹‹æ—¶æ‰è¢«â€œè®¾å®šâ€â€”â€”ä»¥â€œå®¢è§‚è§‚å¯Ÿè€…â€çš„èº«ä»½å™è¿°äº‹ä»¶çš„çº¿æ€§å‘å±•ï¼ˆç¡®å®šè‹ç»´åŸƒæ”¿æƒå¦‚ä½•åœ¨å…¶æœ€è–„å¼±çš„ç¯èŠ‚æ‰“ç ´å¸å›½ä¸»ä¹‰é“¾æ¡å¹¶ä»è€Œå¼€å¯ä¸–ç•Œå†å²çš„æ–°çºªå…ƒï¼Œç­‰ç­‰ï¼‰åœ¨è¿™ä¸ªæ—¶å€™æ‰åˆä¸€æ¬¡å¾—ä»¥å¯èƒ½ã€‚åœ¨æ­¤ä¸¥æ ¼çš„æ„ä¹‰ä¸Šï¼Œä¸»ä½“æ˜¯ä¸€ä¸ªâ€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ï¼šå®ƒçš„è¡Œä¸ºé€šè¿‡å˜å¾—ä¸å¯è§è€ŒæˆåŠŸâ€”â€”é€šè¿‡åœ¨ä¸€ä¸ªæ–°çš„è±¡å¾ç½‘ç»œä¸­â€œå®è¯åŒ–ï¼ˆpositivizingï¼‰â€è‡ªå·±ï¼Œå®ƒå°†è‡ªå·±å®šä½åœ¨æ­¤ç½‘ç»œä¸­å¹¶åœ¨å…¶ä¸­å°†è‡ªå·±è§£é‡Šä¸ºå†å²è¿›ç¨‹çš„ç»“æœï¼Œä»è€Œå°†è‡ªå·±é™ä¸ºå…¶è‡ªèº«è¡Œä¸ºæ‰€äº§ç”Ÿçš„æ•´ä½“ä¸­çš„ä¸€ä¸ªå•çº¯çš„ç¯èŠ‚ã€‚çœ‹çœ‹æ–¯å¤§æ—ä¸»ä¹‰çš„å…ƒè¯­è¨€çš„ç«‹åœºå°±çŸ¥é“äº†ï¼Œåœ¨é‚£é‡Œï¼ˆä¸å…³äºâ€œæ— äº§é˜¶çº§ç§‘å­¦â€ç­‰çš„é™ˆè¯æ»¥è°ƒç›¸åï¼‰ï¼Œé©¬å…‹æ€ä¸»ä¹‰ç†è®ºåœ¨æ— äº§é˜¶çº§è¿™ä¸€ä¾§çš„å‚ä¸ã€å®ƒçš„â€œå…šæ€§â€ã€å®ƒçš„â€œé€‰è¾¹ç«™â€ï¼Œå¹¶æœªè¢«æ„æƒ³ä¸ºç†è®ºæœ¬èº«å›ºæœ‰çš„ä¸œè¥¿â€”â€”é©¬å…‹æ€ä¸»ä¹‰è€…å¹¶æ²¡æœ‰ä»æ— äº§é˜¶çº§çš„ä¸»è§‚ç«‹åœºè¯´è¯ï¼Œä»–ä»¬ä»å¤–éƒ¨çš„ã€ä¸­ç«‹çš„ã€â€œå®¢è§‚çš„â€ç«‹åœºâ€œå°†ä»–ä»¬çš„åŸºæœ¬å–å‘ï¼ˆorientationï¼‰å»ºç«‹åœ¨æ— äº§é˜¶çº§çš„åŸºç¡€ä¸Šâ€ï¼š

åœ¨ä¸Šä¸ªä¸–çºªå…«åå¹´ä»£ï¼Œé©¬å…‹æ€ä¸»ä¹‰è€…å’Œæ°‘ç²¹æ´¾ä¹‹é—´çš„æ–—äº‰æ—¶æœŸï¼Œä¿„å›½çš„æ— äº§é˜¶çº§åœ¨äººå£ä¸­åªå å¾®ä¸è¶³é“çš„å°‘æ•°ï¼Œè€Œä¸ªä½“å†œæ°‘åˆ™å äººå£çš„ç»å¤§å¤šæ•°ã€‚ä½†æ— äº§é˜¶çº§ä½œä¸ºä¸€ä¸ªé˜¶çº§åœ¨å‘å±•ï¼Œè€Œå†œæ°‘ä½œä¸ºä¸€ä¸ªé˜¶çº§åœ¨ç“¦è§£ã€‚æ­£å› ä¸ºæ— äº§é˜¶çº§ä½œä¸ºä¸€ä¸ªé˜¶çº§åœ¨å‘å±•ï¼Œæ‰€ä»¥é©¬å…‹æ€ä¸»ä¹‰è€…å°†ä»–ä»¬çš„åŸºæœ¬å–å‘ï¼ˆorientationï¼‰å»ºç«‹åœ¨æ— äº§é˜¶çº§çš„åŸºç¡€ä¸Šã€‚ä»–ä»¬æ²¡æœ‰é”™ï¼Œå› ä¸ºæ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ— äº§é˜¶çº§åæ¥ä»ä¸€ä¸ªå¾®ä¸è¶³é“çš„åŠ›é‡å‘å±•æˆä¸ºä¸€æµçš„å†å²å’Œæ”¿æ²»åŠ›é‡ã€‚9

å½“ç„¶ï¼Œè¿™é‡Œè¦é—®çš„å…³é”®é—®é¢˜æ˜¯ï¼šåœ¨ä¸æ°‘ç²¹æ´¾æ–—äº‰çš„æ—¶å€™ï¼Œé©¬å…‹æ€ä¸»ä¹‰è€…æ˜¯ä»å“ªé‡Œè¯´åœ¨ä»–ä»¬é€‰æ‹©æ— äº§é˜¶çº§ä½œä¸ºå…¶æ–¹å‘çš„åŸºç¡€æ—¶ï¼Œä¼šå—é”™è¯¯å½±å“ï¼Ÿæ˜¾ç„¶æ˜¯ä»ä¸€ä¸ªå¤–åœ¨çš„ç‚¹ï¼šè¿™ä¸ªç‚¹åŒ…å«äº†ä½œä¸ºå®¢è§‚åŠ›é‡é¢†åŸŸçš„å†å²è¿›ç¨‹ï¼Œåœ¨å…¶ä¸­ï¼Œäººä»¬å¿…é¡»â€œå°å¿ƒä¸è¦å¼„é”™â€ï¼Œå¹¶ä¸”â€œè¢«æ­£ä¹‰çš„åŠ›é‡æ‰€å¼•å¯¼â€â€”â€”é‚£äº›å°†ä¼šè·èƒœçš„åŠ›é‡ã€‚ç®€è€Œè¨€ä¹‹ï¼Œäººä»¬å¿…é¡»â€œèµŒå¯¹äº†é©¬â€ã€‚

ä»¥è¿™ç§æ–¹å¼è§£è¯»â€”â€”ä¹Ÿå°±æ˜¯å›æº¯æ€§åœ°è§£è¯»â€”â€”å…³äºå¦‚ä½•å»è¡ŒåŠ¨çš„å†³å®šéµå¾ªâ€œå®¢è§‚â€è¯„ä»·ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªä¸­ç«‹çš„ã€â€œå®¢è§‚çš„â€ç«‹åœºçœ‹å¾…æƒ…åŠ¿ï¼›ç„¶åï¼Œåœ¨ç¡®å®šå“ªäº›æ˜¯å¯èƒ½è·èƒœçš„åŠ›é‡ä¹‹åï¼Œæˆ‘ä»¬å†³å®šâ€œå°†æˆ‘ä»¬çš„åŸºæœ¬å–å‘ï¼ˆorientationï¼‰å»ºç«‹åœ¨ä»–ä»¬çš„åŸºç¡€ä¸Šâ€â€¦â€¦

ç„¶è€Œï¼Œè¿™ç§å›æº¯æ€§çš„å™äº‹é™·å…¥äº†ä¸€ç§è§†è§’çš„å¹»è§‰ï¼šå®ƒé”™è¯¯åœ°è®¤è¯†äº†ä¸€ä¸ªå…³é”®çš„äº‹å®ï¼Œé‚£å°±æ˜¯â€œå†³å®šçš„çœŸæ­£ç†ç”±åªæœ‰åœ¨å†³å®šè¢«ä½œå‡ºä¹‹åï¼Œæ‰ä¼šæ˜¾ç°å‡ºæ¥â€ã€‚10æ¢å¥è¯è¯´ï¼Œåªæœ‰å¯¹é‚£äº›å·²ç»ä»æ— äº§é˜¶çº§ä¸»è§‚ç«‹åœºä¸Šè¯´è¯çš„äººï¼Œå°†æˆ‘ä»¬çš„åŸºæœ¬å–å‘

ï¼ˆorientationï¼‰å»ºç«‹åœ¨æ— äº§é˜¶çº§çš„åŸºç¡€ä¸Šâ€çš„ç†ç”±æ‰ä¼šæ˜¾ç°å‡ºæ¥â€”â€”æˆ–è€…ï¼Œæ­£å¦‚ç²¾æ˜çš„ç¥å­¦å®¶ä¼šè¯´çš„ï¼Œå½“ç„¶æœ‰å¾ˆå¥½çš„ç†ç”±ç›¸ä¿¡è€¶ç¨£åŸºç£ï¼Œä½†è¿™äº›ç†ç”±åªæœ‰é‚£äº›å·²ç»ç›¸ä¿¡ä»–çš„äººæ‰èƒ½å®Œå…¨ç†è§£ã€‚åˆ—å®ä¸»ä¹‰å…³äºå¸å›½ä¸»ä¹‰é“¾æ¡ä¸­â€œæœ€è–„å¼±ç¯èŠ‚â€çš„è‘—åç†è®ºä¹Ÿæ˜¯å¦‚æ­¤ï¼šäººä»¬ä¸ä¼šé¦–å…ˆé€šè¿‡å®¢è§‚çš„æ–¹æ³•ç¡®å®šå“ªä¸ªç¯èŠ‚æ˜¯æœ€è–„å¼±çš„ï¼Œç„¶åå†³å®šæ‰“å‡»è¿™ä¸ªç¯èŠ‚â€”â€”é‚£ä¸ªå†³å®šçš„è¡ŒåŠ¨æœ¬èº«å®šä¹‰äº†â€œæœ€è–„å¼±ç¯èŠ‚â€ã€‚è¿™å°±æ˜¯æ‹‰åº·æ‰€è¯´çš„è¡ŒåŠ¨ï¼šå¯ä»¥è¯´ï¼Œè¿™ä¸€è¡ŒåŠ¨å®šä¹‰äº†å®ƒè‡ªå·±çš„æ¡ä»¶ï¼›å›æº¯æ€§åœ°äº§ç”Ÿäº†è¯æ˜å…¶åˆç†çš„ç†ç”±ï¼š

å¯¹ï¼ˆé‚£äº›æŒ‡æœ›å®¢è§‚è¯„ä»·æ¡ä»¶çš„äººï¼‰æ¥è¯´ä¸å¯èƒ½çš„æ˜¯ï¼Œä¸€ç§å§¿æ€å¯ä»¥åˆ›é€ å‡ºä¸€äº›å›æº¯æ€§åœ°è¯æ˜å…¶åˆç†å¹¶ä½¿å…¶å˜å¾—æ°å½“çš„æ¡ä»¶ã€‚ç„¶è€Œï¼Œæœ‰è¯æ®è¡¨æ˜ï¼Œè¿™äº‹å‘ç”Ÿäº†ï¼Œè€Œä¸”ç›®çš„ä¸æ˜¯ä¸ºäº†çœ‹ï¼ˆæ­£ç¡®çœ‹å¾…äº‹ç‰©ï¼‰ï¼Œè€Œæ˜¯ä¸ºäº†ä½¿è‡ªå·±è¶³å¤Ÿç›²ç›®ï¼Œä»¥ä¾¿èƒ½å¤Ÿè¡Œè¿›åœ¨æ­£ç¡®çš„è·¯ï¼Œå³åˆ†ç¦»çš„è·¯ï¼ˆthe way that dispersesï¼‰ã€‚11

è¿™ä¸ªè¡ŒåŠ¨å› è€Œæ˜¯â€œè¿°è¡Œæ€§â€çš„ï¼Œåœ¨è¶…å‡ºäº†ï¼ˆexceedsï¼‰â€œè¨€è¯­è¡Œä¸ºâ€çš„æ„ä¹‰ä¸Šï¼šå…¶è¿°è¡Œæ€§æ˜¯â€œå›æº¯æ€§çš„â€ï¼šå®ƒé‡æ–°å®šä¹‰äº†å…¶è¯¸é¢„è®¾çš„ç½‘ç»œã€‚è¡ŒåŠ¨çš„å›æº¯è¿°è¡Œæ€§è¿™ä¸€â€œæº¢å‡º/è¿‡å‰©â€ä¹Ÿå¯ä»¥å€ŸåŠ©é»‘æ ¼å°”å…³äºæ³•å¾‹ä¸å…¶é€¾è¶Šï¼ˆtransgressionï¼‰ã€çŠ¯ç½ªçš„è¾©è¯æ³•å¾—åˆ°é˜é‡Šï¼šä»ä¸€ä¸ªè±¡å¾æ€§å…±åŒä½“çš„ç°å­˜çš„ã€ç§¯æçš„ï¼ˆpositiveï¼‰æ³•å¾‹çš„è§†è§’æ¥çœ‹ï¼Œä¸€ä¸ªè¡Œä¸ºæ ¹æ®å®šä¹‰æ˜¯çŠ¯ç½ªï¼Œå› ä¸ºå®ƒè¿åäº†å…¶è±¡å¾çš„é™åº¦ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªé—»æ‰€æœªé—»çš„ï¼ˆunheard-ofï¼‰å…ƒç´ ï¼Œä½¿ä¸€åˆ‡éƒ½é¢ å€’äº†è¿‡æ¥â€”â€”åœ¨ä¸€ä¸ªè¡Œä¸ºä¸­æ—¢ä¸å­˜åœ¨éŸµå¾‹ï¼ˆrhymeï¼‰ä¹Ÿæ²¡æœ‰ç†ç”±ï¼›ä¸€ä¸ªè¡Œä¸ºåœ¨å…¶æœ¬æ€§ä¸Šæ˜¯ä¸‘æ¶çš„ï¼Œæ­£å¦‚åŸºç£çš„å‡ºç°åœ¨ç°æœ‰å¾‹æ³•ï¼ˆLawï¼‰çš„å®ˆå«è€…çœ¼ä¸­é‚£æ ·â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨åŸºç£è¢«â€œåŸºç£æ•™åŒ–â€ä¹‹å‰ï¼Œæˆä¸ºåŸºç£æ•™ä¼ ç»Ÿçš„æ–°å¾‹æ³•çš„ä¸€éƒ¨åˆ†ä¹‹å‰ã€‚è¾©è¯çš„èµ·æºä½¿ç°æœ‰æ³•å¾‹çš„â€œä¸‘æ¶â€èµ·æºå†æ¬¡å¯è§â€¦

â€¦ 12

è¾©è¯æ³•å°†å¾‹æ³•çš„è¿™ä¸€è¢«é—å¿˜çš„åé¢å¸¦åˆ°äº†é˜³å…‰ä¸‹ï¼šå¾‹æ³•æœ¬èº«ä¸æœ€é«˜çš„çŠ¯ç½ªè¶Šè½¨è¡Œä¸ºç›¸ä¸€è‡´çš„æ–¹å¼ã€‚å½“ä¸€ä¸ªè¡ŒåŠ¨é‡æ–°â€œç¼åˆâ€å®ƒè‡ªå·±çš„è¿‡å»ã€å®ƒè‡ªå·±çš„æ¡ä»¶ï¼Œæ¶ˆé™¤äº†å…¶â€œä¸‘æ¶â€ç‰¹å¾çš„æ—¶å€™ï¼Œå®ƒå°±â€œæˆåŠŸâ€äº†â€”â€”è¿™ä¸ªè¡Œä¸ºæ˜¯ä¸€ä¸ªæ–°çš„ä¸»äººèƒ½æŒ‡ã€é‚£ä¸ªè¡¥å……æ€§çš„â€œä½ æ‰‹æŒ‡çš„æ•²å‡»â€çš„å‡ºç°ï¼Œå®ƒå¥‡è¿¹èˆ¬åœ°æŠŠå…ˆå‰çš„æ··ä¹±å˜æˆâ€œæ–°çš„å’Œè°â€ï¼š

Abeat of your finger onthedrumdischarges the soundsand beginsthenew harmony.   
Astep by you,and newmen arise and set ontheir march.   
Yourhead turnsaway:thenewlove!Yourheadturns back: thenewlove! (Rimbaud,Auneraison)

åœ¨â€œæ–°çš„å’Œè°â€å¼€å§‹ä¹‹åï¼Œæ–°ä¸»äººèƒ½æŒ‡çš„å½»åº•å¶ç„¶çš„ã€â€œä¸‘æ¶çš„â€ã€æ·±æ¸Šæ€§çš„ç‰¹å¾å°±å¤±å»äº†â€”â€”ä¾‹å¦‚ï¼Œçœ‹çœ‹åˆ—å®åœ¨åˆ—å®ä¸»ä¹‰ï¼ˆä¹ŸåŒ…æ‹¬æ–¯å¤§æ—ä¸»ä¹‰ï¼‰çš„å¶åƒåŒ–ä¼ è®°ä¸­å˜æˆäº†ä¸€ä¸ªâ€œçœ‹åˆ°äº†ä¸€åˆ‡å¹¶é¢„æ–™åˆ°äº†ï¼ˆforesawï¼‰ä¸€åˆ‡â€çš„æ˜æ™ºäººç‰©å°±çŸ¥é“äº†ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåªæœ‰åœ¨åˆ—å®ä¸»ä¹‰å´©æºƒåçš„ä»Šå¤©ï¼Œäººä»¬æ‰æœ‰å¯èƒ½æ¥è¿‘ä½œä¸ºä¸€åå†å²å‰§ä¸­çš„æ¼”å‘˜çš„åˆ—å®ï¼Œä»–æœ‰èƒ½åŠ›åšå‡ºæœªé¢„æ–™åˆ°çš„ï¼ˆunforeseenï¼‰ä¸¾åŠ¨ï¼Œè€Œè¿™äº›ä¸¾åŠ¨ï¼Œæ­£å¦‚Leszek Kolakowski çš„æ‰€ç®€æ´è¡¨è¾¾çš„é‚£æ ·ï¼Œå°±æ˜¯åœ¨æ­£ç¡®çš„æ—¶é—´çŠ¯ä¸‹æ­£ç¡®çš„é”™è¯¯ã€‚13

Â·ä¸ºä»€ä¹ˆçœŸç†æ€»æ˜¯æ”¿æ²»æ€§ï¼ˆpoliticalï¼‰çš„ï¼Ÿ

è¡ŒåŠ¨çš„æ¦‚å¿µç›´æ¥ç›¸å…³äºç¤¾ä¼šå’Œæ”¿æ²»ï¼ˆSocial and Politicalï¼‰ä¹‹é—´çš„å…³ç³»â€”â€”ç›¸å…³äºâ€œæ”¿æ²»æ€§ï¼ˆthe Politicalï¼‰â€å’Œâ€œæ”¿æ²»ï¼ˆpoliticsï¼‰â€ä¹‹é—´çš„åŒºåˆ«ï¼Œæ­£å¦‚ Lefort14å’ŒLaclau15æ‰€é˜è¿°çš„é‚£æ ·ï¼šâ€œæ”¿æ²»â€æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç¤¾ä¼šç»¼åˆä½“ï¼ˆseparate socialcomplexï¼‰ã€ä¸€ä¸ªä¸å…¶ä»–å­ç³»ç»Ÿï¼ˆç»æµã€æ–‡åŒ–å½¢å¼ï¼‰ç›¸äº’ä½œç”¨çš„ã€è¢«è‚¯å®šè§„å®šçš„ï¼ˆpositively determinedï¼‰ç¤¾ä¼šå…³ç³»çš„å­ç³»ç»Ÿï¼Œè€Œâ€œæ”¿æ²»æ€§â€[le Politique]åˆ™æ˜¯æœ‰å…³å¼€æ”¾æ€§çš„ã€ä¸ç¡®å®šçš„ç¯èŠ‚ï¼ˆæ­¤æ—¶ï¼Œç¤¾ä¼šçš„ç»“æ„æ€§åŸåˆ™ã€ç¤¾ä¼šå¥‘çº¦çš„åŸºæœ¬å½¢å¼è¢«è´¨ç–‘ï¼‰â€”â€”ç®€è€Œè¨€ä¹‹ï¼Œå°±æ˜¯é€šè¿‡å»ºç«‹â€œæ–°å’Œè°â€çš„è¡ŒåŠ¨æ¥å…‹æœå…¨çƒå±æœºçš„ç¯èŠ‚ã€‚å› æ­¤ï¼Œâ€œæ”¿æ²»æ€§â€çš„ç»´åº¦å¾—åˆ°äº†åŒé‡çš„åˆ»ç”»ï¼šå®ƒæ˜¯ç¤¾ä¼šæ•´ä½“çš„ä¸€ä¸ªç¯èŠ‚ï¼Œæ˜¯å…¶å­ç³»ç»Ÿä¸­çš„ä¸€ä¸ªï¼Œå¹¶ä¸”ä¹Ÿæ˜¯æ•´ä½“ä¹‹å‘½è¿åœ¨å…¶ä¸­è¢«å†³å®šâ€”â€”æ–°çš„å¥‘çº¦åœ¨å…¶ä¸­è¢«è®¾è®¡å¹¶ç¼”ç»“â€”â€”çš„åœ°å¸¦ã€‚16

åœ¨ç¤¾ä¼šç†è®ºä¸­ï¼Œäººä»¬é€šå¸¸è®¤ä¸ºæ”¿æ²»ç»´åº¦ç›¸å¯¹äºç¤¾ä¼šï¼ˆthe Socialï¼‰æœ¬èº«è€Œè¨€æ˜¯æ¬¡è¦çš„ã€‚åœ¨å®è¯ä¸»ä¹‰ç¤¾ä¼šå­¦ä¸­ï¼Œæ”¿æ²»æ˜¯ç¤¾ä¼šç»„ç»‡ç”¨ä»¥ç»„ç»‡å…¶è‡ªæˆ‘è°ƒèŠ‚çš„ä¸€ä¸ªå­ç³»ç»Ÿï¼›åœ¨ç»å…¸é©¬å…‹æ€ä¸»ä¹‰ä¸­ï¼Œæ”¿æ²»æ˜¯ç¤¾ä¼šé˜¶çº§åˆ†åŒ–æ‰€å¯¼è‡´çš„å¼‚åŒ–æ™®éæ€§ï¼ˆalienatedUniversalityï¼‰çš„ç‹¬ç«‹é¢†åŸŸï¼ˆå…¶åŸºæœ¬å«ä¹‰æ˜¯ï¼Œæ— é˜¶çº§ç¤¾ä¼šå°†æ„å‘³ç€ä½œä¸ºä¸€ä¸ªç‹¬ç«‹é¢†åŸŸçš„æ”¿æ²»æ€§ï¼ˆthe Politicalï¼‰çš„ç»ˆç»“ï¼‰ï¼›ç”šè‡³åœ¨ä¸€äº›â€œæ–°ç¤¾ä¼šè¿åŠ¨â€çš„æ„è¯†å½¢æ€ä¸­ï¼Œæ”¿æ²»æ€§ï¼ˆthe Politicalï¼‰è¢«åˆ’å®šä¸ºå›½å®¶æƒåŠ›çš„é¢†åŸŸï¼Œå…¬æ°‘ç¤¾ä¼šå¿…é¡»ç»„ç»‡å…¶è‡ªå«è°ƒèŠ‚æœºåˆ¶åå¯¹å®ƒã€‚é’ˆå¯¹è¿™äº›æ¦‚å¿µï¼Œäººä»¬å¯ä»¥å†’é™©æå‡ºè¿™æ ·çš„å‡è®¾ï¼šç¤¾ä¼šçš„èµ·æºæ€»æ˜¯â€œæ”¿æ²»æ€§çš„ï¼ˆpoliticalï¼‰â€â€”â€”ä¸€ä¸ªç§¯æï¼ˆpositivelyï¼‰ç°å­˜çš„ç¤¾ä¼šä½“ç³»åªä¸è¿‡æ˜¯ä¸€ç§å½¢å¼ï¼Œåœ¨è¿™ç§å½¢å¼ä¸­ï¼Œä¸€ä¸ªå½»åº•å¶ç„¶ä¹‹å†³å®šçš„å¦å®šæ€§è·å¾—äº†ï¼ˆassumesï¼‰ç§¯æçš„ï¼ˆpositiveï¼‰ã€æœ‰è§„å®šçš„ï¼ˆdeterminateï¼‰å®å­˜ã€‚

é›…å„å®¾æ´¾ï¼Œè¿™äº›â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€çš„æœ€æ°å‡ºè€…ï¼Œâ€œå°†æ”¿æ²»ç»å¯¹åŒ–â€ç»éæ„å¤–ï¼›æŒ‡è´£ä»–ä»¬å¤±è´¥æ˜¯å› ä¸ºä»–ä»¬æƒ³ä½¿æ”¿æ²»è¿™ä¸ªå„ä¸ªç¤¾ä¼šå­ç³»ç»Ÿçš„å…¶ä¸­ä¹‹ä¸€æˆä¸ºæ•´ä¸ªç¤¾ä¼šå¤§å¦çš„ç»“æ„æ€§åŸåˆ™ï¼Œå°±å¿½è§†äº†ä¸€ä¸ªå…³é”®çš„äº‹å®ï¼Œå³å¯¹é›…å„å®¾æ´¾æ¥è¯´ï¼Œæ”¿æ²»å±‚é¢ä¸æ˜¯è®¸å¤šå­ç³»ç»Ÿä¸­çš„ä¸€ä¸ªï¼Œè€Œæ˜¯æŒ‡æ˜äº†ä¸€ç§æ¿€è¿›å¦å®šæ€§çš„å‡ºç°ï¼Œå®ƒä½¿ç¤¾ä¼šç»“æ„çš„æ–°åŸºç¡€æˆä¸ºå¯èƒ½â€”â€”ä»–ä»¬æ¶ˆå¤±ä¸æ˜¯å› ä¸ºä»–ä»¬çš„è™šå¼±ï¼Œè€Œæ˜¯å› ä¸ºä»–ä»¬çš„æˆåŠŸâ€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œä»–ä»¬æ¶ˆå¤±çš„æ—¶å€™æ­£æ˜¯ä»–ä»¬å·¥ä½œå®Œæˆçš„æ—¶å€™ã€‚

ç”¨æ›´åŠ â€œç¬¦å·å­¦â€çš„æœ¯è¯­ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œä½œä¸ºå­ç³»ç»Ÿçš„æ”¿æ²»æ˜¯æœ‰å…³æ”¿æ²»ä¸»ä½“çš„éšå–»ï¼Œæ˜¯ä½œä¸ºä¸»ä½“çš„æ”¿æ²»æ€§ï¼ˆthe Politicalï¼‰çš„éšå–»ï¼šåœ¨è¢«æ„æˆçš„ç¤¾ä¼šç©ºé—´ä¸­ï¼Œå æ®äº†ä½œä¸ºå¦å®šæ€§çš„æ”¿æ²»æ€§çš„ä½ç½®ï¼ˆplace of the Politicalï¼‰çš„é‚£ä¸ªå…ƒç´ ï¼Œå¦å®šæ€§ä¸­æ­¢äº†å®ƒï¼Œå¹¶é‡æ–°å»ºç«‹äº†å®ƒã€‚æ¢å¥è¯è¯´ï¼Œä½œä¸ºâ€œå­ç³»ç»Ÿâ€çš„â€œæ”¿æ²»â€ï¼Œä½œä¸ºç¤¾ä¼šçš„ä¸€ä¸ªç‹¬ç«‹é¢†åŸŸï¼Œåœ¨ç¤¾ä¼šä¸­è¡¨å¾ç€ï¼ˆrepresentsï¼‰å®ƒè‡ªå·±è¢«é—å¿˜çš„åŸºç¡€ï¼Œå®ƒèµ·æºäºä¸€ç§æš´åŠ›çš„ã€æ·±æ¸Šæ€§çš„è¡ŒåŠ¨â€”â€”å®ƒåœ¨ç¤¾ä¼šç©ºé—´ä¸­è¡¨å¾ç€å¦‚æœè¿™ä¸ªç©ºé—´è¦æ„æˆå®ƒè‡ªå·±å°±å¿…é¡»æ‰å‡ºæ¥çš„ä¸œè¥¿ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è®¤è¯†åˆ°æ‹‰åº·å¯¹èƒ½æŒ‡çš„å®šä¹‰ï¼ˆå³â€œä¸ºå¦ä¸€ä¸ªèƒ½æŒ‡è¡¨å¾ç€ä¸»ä½“â€çš„ä¸œè¥¿ï¼‰ï¼šä½œä¸ºå­ç³»ç»Ÿçš„æ”¿æ²»ä¸ºå…¶ä»–æ‰€æœ‰ç¤¾ä¼šå­ç³»ç»Ÿè¡¨å¾ç€æ”¿æ²»æ€§ï¼ˆä¸»ä½“ï¼‰ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®è¯ä¸»ä¹‰ç¤¾ä¼šå­¦å®¶æ‹¼å‘½åœ°è¯•å›¾è¯´æœæˆ‘ä»¬æ”¿æ²»åªæ˜¯ä¸€ä¸ªå­ç³»ç»Ÿï¼šè¿™ç§åŠè¯´çš„è¯­æ°”ç»æœ›è€Œæ€¥è¿«ï¼Œä¼¼ä¹å‘¼åº”äº†ä¸€ç§è¿«åœ¨çœ‰ç«çš„â€œçˆ†ç‚¸â€çš„å±é™©ï¼Œæ”¿æ²»å°†å†æ¬¡â€œæˆä¸ºå…¨éƒ¨â€â€”â€”è½¬å˜ä¸ºâ€œæ”¿æ²»æ€§çš„ï¼ˆpoliticalï¼‰â€ã€‚

è¿™ç§åŠè¯´æœ‰ä¸€ç§ç¡®å®šæ— ç–‘çš„è§„èŒƒæ€§å¯“æ„ï¼Œç»™å®ƒå¢æ·»äº†ä¸€ç§å·«æœ¯çš„æ°”æ¯ï¼ˆair ofconjurationï¼‰ï¼šå®ƒå¿…é¡»ä¿æŒä¸ºä¸€ä¸ªå•çº¯çš„å­ç³»ç»Ÿâ€¦â€¦

å› æ­¤ï¼Œåœ¨å¯¹â€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€è¿™ä¸€æ‚–è®ºçš„ä¸¤ç§å¯èƒ½çš„è§£è¯»ä¸­ï¼Œè¦ç´§çš„æ˜¯ç¤¾ä¼šå¯¹æŠ—æ€§å³å¦å®šæ€§çš„åœ°ä½ï¼šæ˜¯è¯´ï¼Œå¦å®šæ€§åœ¨ç¤¾ä¼šç©ºé—´ä¸­çš„å‡ºç°ä»…ä»…æ˜¯ä»ä¸€ç§è‚¯å®šæ€§ï¼ˆpositivityï¼‰å½¢å¼åˆ°å¦ä¸€ç§è‚¯å®šæ€§å½¢å¼çš„è·¯å¾„ä¸­çš„ä¸€ä¸ªä¸­ä»‹äººï¼ˆintermediaryï¼‰ï¼Œæ˜¯æç»˜äº†ä»ä¸€ç§â€œå¸¸æ€ï¼ˆnormalityï¼‰â€å‘å¦ä¸€ç§â€œå¸¸æ€â€è¿‡æ¸¡ä¹‹ç‰¹å¾çš„â€œä¾‹å¤–â€ï¼›è¿˜æ˜¯è¯´ï¼Œè¿™ç§â€œå¸¸æ€â€åªä¸è¿‡æ˜¯è¢«é—å¿˜çš„å¦å®šæ€§çš„æº¢å‡º/è¿‡å‰©ï¼ˆexcess of negativityï¼‰çš„åæœã€â€œç»…å£«åŒ–ï¼ˆgentrificationï¼‰â€ï¼Ÿç¬¬äºŒç§è§£å†³æ–¹æ¡ˆé¢ è¦†äº†æ•´ä¸ªè§†è§’ï¼šâ€œå­ç³»ç»Ÿâ€çš„ç¨³å®šç½‘ç»œæ­£æ˜¯ç¤¾ä¼šå¯¹ç«‹ä¸­çš„ä¸€æçš„éœ¸æƒçš„å½¢å¼ï¼Œè€Œâ€œé˜¶çº§å’Œå¹³â€æ­£æ˜¯é˜¶çº§æ–—äº‰ä¸­ä¸€ä¸ªé˜¶çº§çš„éœ¸æƒçš„æ ‡å¿—â€¦â€¦

ä¸€æ—¦â€œå­ç³»ç»Ÿâ€çš„ç½‘ç»œç¨³å®šä¸‹æ¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ—¦â€œæ–°çš„å’Œè°â€å»ºç«‹èµ·æ¥ï¼Œä¸€æ—¦æ–°ç§©åºï¼ˆOrderï¼‰â€œè®¾å®šäº†å®ƒçš„é¢„è®¾â€ï¼Œâ€œç¼åˆäº†â€å®ƒçš„åœºåŸŸï¼Œè¡¨å¾å…¶èµ·æºçš„é‚£ä¸ªå…ƒç´ çš„éšå–»æ€§ï¼ˆmetaphoricityï¼‰å°±å¤±å»äº†ï¼šè¿™ä¸ªå…ƒç´ è¢«ç¼©å‡ä¸ºâ€œå…¶ä»–å…ƒç´ ä¸­çš„ä¸€ä¸ªâ€ï¼›å®ƒå¤±å»äº†å æ®ç€ï¼ˆå½»åº•å¦å®šæ€§çš„ï¼‰â€œæ— â€çš„ä½ç½®çš„â€œä¸€â€ï¼ˆOne whichholds the place of Nothingï¼‰è¿™ä¸€ç‰¹å¾ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥å›åˆ°è‡­åæ˜­è‘—çš„é»‘æ ¼å°”ä¸‰å…ƒç»„ï¼šä¸»ä½“æ˜¯è¿™ä¸ªâ€œæ¶ˆå¤±çš„ä¸­ä»‹è€…â€ã€è¿™ä¸ªç¬¬å››ç¯èŠ‚ï¼Œå¯ä»¥è¯´ï¼Œå®ƒé¢å¸ƒäº†è‡ªå·±çš„æ¶ˆå¤±ï¼›å®ƒçš„æ¶ˆå¤±æ­£æ˜¯è¡¡é‡å…¶â€œæˆåŠŸâ€çš„æ ‡å‡†ä¹Ÿæ˜¯è‡ªæˆ‘å…³è”çš„å¦å®šæ€§çš„è™šç©ºï¼Œä¸€æ—¦æˆ‘ä»¬ä»å…¶ç»“æœâ€œå›å¤´â€çœ‹è¿™ä¸ªè¿‡ç¨‹ï¼Œå®ƒå°±å˜å¾—ä¸å¯è§äº†ã€‚å¯¹é»‘æ ¼å°”ä¸‰å…ƒç»„ä¸­è¿™ä¸€æº¢å‡ºçš„ç¬¬å››ç¯èŠ‚çš„è€ƒå¯Ÿï¼Œä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ ¼é›·é©¬æ–¯çš„â€œç¬¦å·å­¦çŸ©é˜µâ€çš„èƒŒæ™¯ä¸‹è§£è¯»å®ƒï¼š

![](https://cdn-mineru.openxlab.org.cn/extract/37e3a677-8d78-46c9-9253-53ee427260a8/25d5185a40bbe06833ea2b8159437fb9425fc19035b1e9993438a5354267689f.jpg)

å¿…ç„¶æ€§ï¼ˆnecessityï¼‰å’Œä¸å¯èƒ½æ€§ï¼ˆimpossibilityï¼‰çš„å¯¹ç«‹æœ¬èº«æº¶è§£è¿›å…¥å¯èƒ½æ€§ï¼ˆpossibilityï¼‰çš„é¢†åŸŸï¼ˆå¯ä»¥è¯´ï¼Œå¯èƒ½æ€§æ˜¯å¯¹å¿…ç„¶æ€§çš„â€œå¦å®šä¹‹å¦å®šâ€ï¼‰â€”â€”éšä¹‹æ¶ˆå¤±çš„æ˜¯ç¬¬å››ä¸ªæœ¯è¯­ï¼Œå³ç»ä¸å¯èƒ½ç­‰åŒäºå¯èƒ½ï¼ˆPossibleï¼‰çš„å¶ç„¶ï¼ˆtheContingentï¼‰ã€‚åœ¨å¶ç„¶æ€§ï¼ˆcontingencyï¼‰ä¸­æ€»å­˜åœ¨æŸäº›â€œä¸å®åœ¨ç•Œé­é‡â€çš„ä¸œè¥¿ï¼ŒæŸäº›å‰æ‰€æœªé—»çš„å®ä½“çš„çŒ›ç„¶å‡ºç°ï¼Œå®ƒè¿æŠ—äº†äººä»¬å¯¹â€œå¯èƒ½â€æ‰€æŒçš„æ—¢å®šåœºåŸŸçš„é™åº¦ï¼Œè€Œâ€œå¯èƒ½â€å¯ä»¥è¯´æ˜¯ä¸€ç§â€œæ¸©å’Œçš„â€ã€å¹³å’Œçš„å¶ç„¶æ€§ï¼Œä¸€ç§è¢«æ‹”æ‰äº†åˆºçš„å¶ç„¶æ€§ã€‚

ä¾‹å¦‚ï¼Œåœ¨ç²¾ç¥åˆ†æä¸­ï¼ŒçœŸç†å±äºå¶ç„¶æ€§çš„ç§©åº17ï¼šæˆ‘ä»¬åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­è¿‡ç€æ— èŠçš„ç”Ÿæ´»ï¼Œæ·±é™·äºç»“æ„å®ƒçš„æ™®éçš„è°è¨€ï¼ˆuniversal Lieï¼‰ä¹‹ä¸­ï¼Œè€Œçªç„¶é—´ï¼Œä¸€äº›å®Œå…¨å¶ç„¶çš„é­é‡â€”â€”æœ‹å‹çš„ä¸€å¥é—²è¯ï¼Œæˆ‘ä»¬ç›®ç¹çš„ä¸€ä»¶äº‹æ•…â€”â€”å”¤èµ·äº†å…³äºè¢«å‹æŠ‘çš„æ—§åˆ›ä¼¤çš„è®°å¿†ï¼Œæ‰“ç ´äº†æˆ‘ä»¬çš„è‡ªæˆ‘æ¬ºéª—ã€‚ç²¾ç¥åˆ†æåœ¨è¿™é‡Œæ˜¯å½»åº•åæŸæ‹‰å›¾çš„ï¼šæ™®éæ€§æ˜¯æœ€å“è¶Šçš„è™šå‡æ€§ï¼ˆFalsity par excellenceï¼‰çš„é¢†åŸŸï¼Œè€ŒçœŸç†åˆ™æ˜¯ä½œä¸ºä¸€ç§ç‰¹æ®Šçš„å¶ç„¶é­é‡å‡ºç°çš„ï¼Œè¿™ç§é­é‡ä½¿å…¶â€œè¢«å‹æŠ‘â€çš„ä¸œè¥¿å˜å¾—å¯è§ã€‚18åœ¨â€œå¯èƒ½æ€§â€ä¸­æ‰€å¤±å»çš„ç»´åº¦æ­£æ˜¯è¿™ç§æœ‰å…³çœŸç†ä¹‹å‡ºç°çš„åˆ›ä¼¤æ€§çš„ã€æ— ä¿è¯çš„ï¼ˆunwarrantedï¼‰ç‰¹æ€§ï¼šå½“ä¸€ä¸ªçœŸç†å˜å¾—â€œå¯èƒ½â€æ—¶ï¼Œå®ƒå¤±å»äº†â€œäº‹ä»¶â€çš„ç‰¹æ€§ï¼Œå®ƒå˜æˆäº†ä¸€ä¸ªå•çº¯çš„æœ‰å…³äº‹å®çš„ï¼ˆfactualï¼‰å‡†ç¡®æ€§ï¼Œä»è€Œæˆä¸ºç»Ÿæ²»æ€§çš„æ™®éè°è¨€çš„ç»„æˆéƒ¨åˆ†ã€‚19

ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ‹‰åº·çš„ç²¾ç¥åˆ†æä¸ç½—è’‚é‚£ç§å¤šå…ƒå®ç”¨ä¸»ä¹‰çš„â€œè‡ªç”±ä¸»ä¹‰â€æœ‰å¤šè¿œã€‚æ‹‰åº·çš„æœ€åä¸€è¯¾ä¸æ˜¯çœŸç†ï¼ˆtruthsï¼‰çš„ç›¸å¯¹æ€§å’Œå¤šå…ƒæ€§ï¼Œè€Œæ˜¯åšç¡¬çš„ã€åˆ›ä¼¤æ€§çš„äº‹å®ï¼Œå³åœ¨æ¯ä¸€ä¸ªå…·ä½“çš„æ˜Ÿä¸›ä¸­ï¼ŒçœŸç†ï¼ˆtruthï¼‰å¿…ç„¶ä¼šä»¥æŸç§å¶ç„¶çš„ç»†èŠ‚å‡ºç°ã€‚æ¢å¥è¯è¯´ï¼Œå°½ç®¡çœŸç†æ˜¯ä¾èµ–äºè¯­å¢ƒçš„â€”â€”å°½ç®¡ä¸€èˆ¬æ„ä¹‰ä¸Šçš„çœŸç†å¹¶ä¸å­˜åœ¨ï¼Œæœ‰çš„æ€»æ˜¯æŸç§æƒ…å†µçš„çœŸç†â€”â€”ä½†åœ¨æ¯ä¸€ä¸ªå¤šå…ƒåœºåŸŸä¸­éƒ½ä¾ç„¶æœ‰ä¸€ä¸ªé˜æ˜å…¶çœŸç†å¹¶ä¸”æœ¬èº«ä¸èƒ½è¢«ç›¸å¯¹åŒ–çš„ç‰¹æ®Šçš„ç‚¹ï¼›åœ¨è¿™ä¸ªç¡®åˆ‡çš„æ„ä¹‰ä¸Šï¼ŒçœŸç†æ€»æ˜¯ä¸€ã€‚å¦‚æœæˆ‘ä»¬æŠŠâ€œæœ¬ä½“è®ºâ€çŸ©é˜µæ¢æˆâ€œä¹‰åŠ¡è®ºâ€çŸ©é˜µï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçš„ç›®æ ‡å°±ä¼šæ›´æ¸…æ¥šï¼š

![](https://cdn-mineru.openxlab.org.cn/extract/37e3a677-8d78-46c9-9253-53ee427260a8/3dcd220f4366e5bd9cfd37929a776bfd8b5d735d04ef935d9bcb2ad8e2ab91d4.jpg)

æˆ‘ä»¬ç”šè‡³ç¼ºä¹ä¸€ä¸ªåˆé€‚çš„æœ¯è¯­æ¥å½¢å®¹è¿™ä¸ªXï¼Œæ¥å½¢å®¹è¿™â€œä¸æ˜¯å‘½ä»¤çš„ï¼ˆnotprescribedï¼‰â€ã€â€œå®¹è®¸çš„ï¼ˆfacultativeï¼‰â€ï¼Œä½†åˆä¸æ˜¯ç®€å•çš„â€œå…è®¸çš„ï¼ˆpermittedï¼‰â€ä¸œè¥¿çš„å¥‡æ€ªçŠ¶æ€â€”â€”ä¾‹å¦‚ï¼Œåœ¨ç²¾ç¥åˆ†æç–—æ³•ä¸­å‡ºç°äº†ä¸€äº›è¿„ä»Šä¸ºæ­¢è¢«ç¦æ­¢çš„çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯¹ç¦ä»¤è¿›è¡Œäº†å˜²å¼„ï¼Œæš´éœ²äº†å…¶éšè—æœºåˆ¶ï¼Œä½†å¹¶æ²¡æœ‰å› æ­¤è€Œå˜æˆä¸€ç§ä¸­æ€§çš„â€œå…è®¸ï¼ˆpermissivenessï¼‰â€ã€‚ä¸¤è€…ä¹‹é—´çš„åŒºåˆ«æ¶‰åŠåˆ°å¯¹æ™®éç§©åºçš„ä¸åŒå…³ç³»ï¼šâ€œå…è®¸ï¼ˆpermissivenessï¼‰â€æ˜¯ç”±å®ƒä¿è¯çš„ï¼ˆwarrantedï¼‰ï¼Œè€Œè¿™ç§ä¿è¯åœ¨â€œä½ å¯ä»¥ï¼ˆmayï¼‰â€¦â€¦â€çš„æƒ…å†µä¸‹æ˜¯ç¼ºä¹çš„ï¼Œæ‹‰åº·ç§°è¿™ç§æƒ…å†µä¸ºscilicetï¼šä½ å¯ä»¥çŸ¥é“ï¼ˆå…³äºä½ çš„æ¬²æœ›çš„çœŸç›¸ï¼‰â€”â€”å¦‚æœä½ ä¸ºè‡ªå·±æ‰¿æ‹…é£é™©ã€‚è¿™ä¸ªscilicet ä¹Ÿè®¸æ˜¯æ‰¹åˆ¤æ€§æ€ç»´çš„æœ€ç»ˆè¿½ç´¢ã€‚
</file>

<file path="package.json">
{
  "dependencies": {
    "repomix": "^1.0.0"
  }
}
</file>

<file path="requirements-web.txt">
# Core project dependencies
requests>=2.31.0
python-dotenv>=1.0.0
openai>=1.3.0
anthropic>=0.5.0
google-generativeai>=0.3.0
numpy>=1.24.0
scikit-learn>=1.3.0
aiohttp>=3.8.0
asyncio-throttle>=1.0.2

# Web backend dependencies
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
python-multipart>=0.0.6
python-jose[cryptography]>=3.3.0
passlib[bcrypt]>=1.7.4

# File processing
aiofiles>=23.2.0

# Development tools
pytest>=7.4.0
pytest-asyncio>=0.21.0
</file>

<file path="sample_input_document_as_markdown__durnovo_memo.md">
# The Future Anglo-German War Will Transform into an Armed Conflict Between Two Groups of Powers

## By Pyotr Nikolayevich Durnovo, February 1914

The central factor of our current period in world history is the rivalry between England and Germany. This rivalry must inevitably lead to an armed conflict between them, the outcome of which will likely be fatal for the defeated side. The interests of these two states are far too incompatible, and their simultaneous existence as great powers will, sooner or later, prove impossible.

Indeed, on one side stands an island nation whose global significance rests on its dominion over the seas, world trade, and countless colonies. On the other side stands a powerful continental power whose limited territory is insufficient for its growing population. Therefore, Germany has directly and openly declared that its future lies on the seas, has developed enormous world trade with miraculous speed, and built a formidable navy to protect it. With its famous "Made in Germany" mark, Germany has created a mortal danger to its rival's industrial and economic well-being.

Naturally, England cannot surrender without a fight, and a life-and-death struggle between it and Germany is inevitable. The impending armed conflict resulting from this rivalry cannot possibly be reduced to a duel between England and Germany. Their forces are too unequal, and at the same time, they are insufficiently vulnerable to each other.

Germany can incite rebellion in India, South America, and especially a dangerous uprising in Ireland, paralyze English maritime trade through privateering, and perhaps submarine warfare, thereby creating food supply difficulties for Great Britain. However, despite the boldness of German military leaders, they would hardly risk a landing in England unless a lucky chance helps them destroy or significantly weaken the English navy.

As for England, Germany is completely invulnerable to it. All that is accessible to England is to seize German colonies, halt German maritime trade, and, in the most favorable case, destroy the German navy, but nothing more--and this alone cannot force the opponent to make peace. Therefore, England will undoubtedly try to resort to its previously successful means and decide on armed intervention only after securing the participation of strategically stronger powers on its side.

Since Germany, in turn, will certainly not remain isolated, the future Anglo-German war will transform into an armed conflict between two groups of powers: one following German orientation and the other English.

## It Is Difficult to Discern Any Real Benefits Russia Has Gained from Its Rapprochement with England

### Until the Russo-Japanese War

Russian policy adhered to neither orientation. Since the reign of Emperor Alexander III, Russia had been in a defensive alliance with France, one solid enough to ensure joint action by both states in case of an attack on either one, but not so close as to obligate them to necessarily support with armed force all political actions and demands of the ally.

Simultaneously, the Russian court maintained traditionally friendly relations, based on family ties, with Berlin. Thanks to this configuration, peace between the great powers remained undisturbed for many years, despite the abundance of combustible material in Europe. France was protected from German attack by its alliance with Russia, Germany was protected from French revanchist aspirations by Russia's proven peacefulness and friendship, and Russia was protected from excessive Austro-Hungarian machinations in the Balkan Peninsula by Germany's need to maintain good neighborly relations with it.

Finally, isolated England, restrained by rivalry with Russia in Persia, traditional English diplomatic fears of our offensive movements toward India, and poor relations with France (particularly evident during the famous Fashoda incident), watched the strengthening of German naval power with alarm yet hesitated to take active measures.

The Russo-Japanese War fundamentally changed the relationships between the great powers and brought England out of its isolated position. Throughout the Russo-Japanese War, England and America maintained a favorable neutrality toward Japan, while we enjoyed equally benevolent neutrality from France and Germany. This would seem to have been the seed of the most natural political combination for us.

However, after the war, our diplomacy made a sharp turn and definitively set a course for rapprochement with England. France was drawn into England's political orbit, forming the Triple Entente group with predominant English influence, and a collision with the powers grouping around Germany became, sooner or later, inevitable.

What advantages did we expect from abandoning our traditional policy of distrust toward England and breaking our tested, if not friendly, then neighborly relations with Germany? Looking somewhat carefully and examining the events that occurred after the Portsmouth Treaty, it is difficult to discern any real benefits we have gained from rapprochement with England. The only plus--improved relations with Japan--can hardly be considered a consequence of Russian-English rapprochement.

In essence, Russia and Japan are created to live in peace, as they have absolutely nothing to divide. All of Russia's properly understood tasks in the Far East are fully compatible with Japan's interests. These tasks are essentially limited to very modest bounds. A too broad flight of fantasy by overzealous executors, which had no foundation in actual state interests on one side, and the excessive nervousness and sensitivity of Japan, which mistakenly took these fantasies for a consistently implemented plan on the other side, caused a collision that more skillful diplomacy could have avoided.

Russia needs neither Korea nor even Port Arthur. Access to the open sea is undoubtedly useful, but the sea itself is not a market; it is merely a route for more advantageous delivery of goods to consuming markets. Meanwhile, in our Far East, we have not and will not for a long time have valuables promising any significant profits from their export. There are no markets there for our exports. We cannot count on broadly supplying our export goods to either developed America (industrially and agriculturally), nor poor and also industrial Japan, nor even coastal China and more distant markets, where our exports would inevitably meet with goods from industrially stronger competing powers.

What remains is inner China, with which our trade is conducted primarily by land. Thus, an open port would contribute more to the import of foreign goods to us than to the export of our domestic products. On the other hand, Japan, whatever may be said, does not covet our Far Eastern possessions. The Japanese are by nature a southern people, and the harsh conditions of our Far Eastern region cannot tempt them.

It is known that even in Japan itself, the northern Yezo is sparsely populated; Japanese colonization is proceeding unsuccessfully even in the southern part of Sakhalin that was ceded to Japan by the Portsmouth Treaty. Having taken Korea and Formosa, Japan is unlikely to go further north, and its aspirations will likely be directed toward the Philippine Islands, Indochina, Java, Sumatra, and Borneo. At most, what it might strive for is to acquire, purely for commercial reasons, some further sections of the Manchurian railway.

In short, *peaceful coexistence*--I will say more--*close rapprochement* between Russia and Japan in the Far East is entirely natural, regardless of any English mediation. The ground for agreement suggests itself. Japan is not a wealthy country; maintaining both a strong army and a powerful fleet simultaneously is difficult for it.

Its island position pushes it toward strengthening specifically its naval power. An alliance with Russia would allow it to focus all its attention on the fleet, which is necessary given its already emerging rivalry with America, while leaving the protection of its interests on the mainland to Russia. On the other hand, we, having the Japanese fleet for naval defense of our Pacific coast, would be able to abandon our unrealistic dream of creating a navy in the Far East.

Thus, in terms of relations with Japan, rapprochement with England has brought us no real benefit. It has given us nothing in terms of strengthening our position in Manchuria, Mongolia, or even in the Uriankhai region, where the uncertainty of our position testifies that agreement with England has, in any case, not freed our diplomacy's hands. On the contrary, our attempt to establish relations with Tibet met with sharp resistance from England.

Our position in Persia has not changed for the better since the agreement. Everyone remembers our predominant influence in that country under Shah Nasr-ed-Din during the period of greatest tension in our relations with England. From the moment of rapprochement with the larivalry with America, while leaving the protection of its interests on the mainland to Russia.

From the moment of rapprochement with the latter, we found ourselves involved in a series of incomprehensible attempts to impose on the Persian population a constitution that was completely unnecessary for them, contributing to the overthrow of a monarch loyal to Russia.

In short, we not only gained nothing, but on the contrary, lost all along the line, destroying both our prestige and many millions of rubles, and even the precious blood of Russian soldiers, treacherously murdered and, to please England, not even avenged.

But the most negative consequences of rapprochement with England--and consequently a fundamental divergence with Germany--have manifested in the Near East. As is known, Bismarck once made the famous statement that for Germany the Balkan question was not worth the bones of a single Pomeranian grenadier.

Subsequently, Balkan complications began to attract incomparably more attention from German diplomacy, which took the "sick man" under its protection, but even then Germany long showed no inclination to risk relations with Russia over Balkan affairs. The proof is before us. Indeed, how easy it would have been for Austria, during the Russo-Japanese War and our subsequent turmoil, to realize its cherished aspirations in the Balkan Peninsula.

But Russia at that time had not yet tied its fate to England, and Austria-Hungary was forced to miss the most advantageous moment for its goals. However, it was enough for us to take the path of close rapprochement with England for the annexation of Bosnia and Herzegovina to immediately follow--which could have been accomplished so easily and painlessly in 1905 or 1906--then the Albanian question arose and the combination with Prince Wied.

Russian diplomacy tried to answer Austrian intrigues by forming the Balkan League, but this combination proved completely ephemeral.

Under what conditions will this collision occur, and what will be its likely consequences? The basic groupings in the future war are obvious: Russia, France, and England on one side, Germany, Austria, and Turkey on the other. It is more than likely that other powers will also take part in the war, depending on the various conditions under which war breaks out. But whether the immediate cause of war is a new collision of opposing interests in the Balkans, or a colonial incident like Algeciras, the basic grouping will remain the same. Italy, if it properly understands its interests, will not side with Germany. Due to political and economic reasons, Italy undoubtedly strives to expand its current territory, which can only be achieved at the expense of Austria on one side and Turkey on the other. Therefore, it is natural that Italy will not side with those who guarantee the territorial integrity of the states at whose expense it wishes to realize its aspirations.

Moreover, the possibility is not excluded that Italy might join the anti-German coalition if the fortunes of war inclined in its favor, in order to secure for itself the most advantageous conditions for participation in the subsequent division of spoils. In this respect, Italy's position aligns with the probable position of Romania, which will likely remain neutral until the scales of fortune tip to one side or the other. Then, guided by healthy political egoism, it will join the victors to be rewarded either at Russia's expense or at Austria's. Among other Balkan states, Serbia and Montenegro will undoubtedly side against Austria, while Bulgaria and Albania (if it has not formed at least an embryo of a state) will side against Serbia. Greece will probably remain neutral or side against Turkey, but only when the outcome is more or less predetermined. The participation of other states will be incidental, and we should fear Sweden, which will naturally be among our opponents.

Under such conditions, a struggle with Germany presents enormous difficulties for us and will require incalculable sacrifices. The war will not catch the opponent off guard, and the degree of its preparedness will probably exceed our most exaggerated expectations. One should not think that this preparedness stems from Germany's own desire for war. War is unnecessary for it, as long as it could achieve its goal--ending sole dominion over the seas--without it. But since this vital goal meets opposition from the coalition, Germany will not retreat from war and will certainly try to provoke it, choosing the most advantageous moment for itself.

## THE MAIN BURDEN OF WAR WILL FALL ON RUSSIA

The main burden of war will undoubtedly fall on us, since England is hardly capable of broad participation in a continental war, while France, poor in human material, given the colossal losses that will accompany war under modern conditions of military technology, will probably adhere to strictly defensive tactics. The role of the battering ram, breaking through the very thickness of German defense, will fall to us, and yet how many factors will be against us and how much strength and attention we will have to spend on them. From these unfavorable factors, we should exclude the Far East. America and Japan, the first by essence and the second by virtue of its current political orientation, are both hostile to Germany, and there is no basis to expect them to side with it.

Moreover, the war, regardless of its outcome, will weaken Russia and divert its attention to the West, which serves Japanese and American interests. Therefore, our rear is sufficiently secured from the Far East, and at most they will extort from us some concessions of an economic nature for their benevolent neutrality. The possibility is not excluded of America or Japan joining the anti-German side, but of course only as seizers of one or another poorly defended German colony. On the other hand, an outbreak of hostility against us in Persia is certain, along with probable unrest among Muslims in the Caucasus and Turkestan, and the possibility of Afghanistan moving against us in connection with the latter. Finally, we should foresee very unpleasant complications in Poland and Finland.

In Finland, an uprising will inevitably break out if Sweden proves to be among our opponents. As for Poland, we should expect that we will not be able to hold it during the war. When it falls into the power of our opponents, they will undoubtedly attempt to provoke an uprising, which is not very dangerous for us but which we will still have to count among the unfavorable factors, especially since the influence of our allies may induce us to take steps in our relations with Poland that are more dangerous than any open uprising.

Are we prepared for such a stubborn struggle as the future war of European peoples will undoubtedly prove to be? To this question, one must, without equivocation, answer innfold. Our young legislative institutions are largely to blame for this insufficiency, having taken a dilettantish interest in our defense, but far from grasping the full seriousness of the political situation developing under the influence of the orientation which, with society's sympathetic attitude, our Ministry of Foreign Affairs has followed in recent years.

Proof of this is the huge number of military and naval ministry bills remaining unconsidered, and in particular, the plan for organizing our state defense that was presented to the Duma under State Secretary Stolypin. Undisputedly, in the realm of troop training, we have, according to specialists, achieved substantial improvement compared to the time preceding the Japanese War. Our field artillery leaves nothing to be desired; the rifle is quite satisfactory; the equipment is comfortable and practical. However, it is also undisputed that there are essential deficiencies in our defense organization.

In this regard, we must first note the inadequacy of our military supplies, which cannot be blamed on the military department, since the planned procurement programs are far from fully implemented due to the low productivity of our factories. This insufficiency of ammunition supplies is significant because, given the embryonic state of our industry, we will not have the ability during the war to make up for identified shortfalls by domestic means. Furthermore, with both the Baltic and Black Seas closed to us, the import of defense items we lack from abroad will prove impossible.

Additionally, an unfavorable circumstance for our defense is its excessive dependence on foreign industry, which, combined with the cessation of any convenient foreign communications, will create a series of difficulties that are hard to overcome. The quantity of heavy artillery we have is far from sufficient, as proven by the experience of the Japanese War, and we have few machine guns. We have barely begun organizing our fortress defense, and even the Revel fortress, which protects access to the capital, is not yet complete. The network of strategic railways is insufficient, and the railways possess rolling stock that may be adequate for normal traffic but is inadequate for the colossal demands that will be placed on us in case of a European war. Finally, we should not lose sight of the fact that in the upcoming war, the most cultured, technically developed nations will be fighting. Every war has invariably been accompanied until now by new developments in military technology, and the technical backwardness of our industry does not create favorable conditions for us to adopt new inventions.

## GERMANY'S AND RUSSIA'S VITAL INTERESTS NOWHERE COLLIDE

All these factors are hardly being given due consideration by our diplomacy, whose conduct toward Germany is not devoid, to a certain degree, of even some aggressiveness that could excessively hasten the moment of armed collision with Germany, which, given the English orientation, is essentially inevitable.

But is this orientation correct, and does even a favorable period of war promise us such advantages that would compensate for all the difficulties and sacrifices inevitable in an exceptionally intense war?

The vital interests of Russia and Germany nowhere collide and provide a complete basis for peaceful coexistence between these two states. Germany's future lies on the seas, that is, where Russia, essentially the most continental of all the great powers, has no interests.

We have no overseas colonies and probably never will, and communication between different parts of the empire is easier by land than by sea. We do not feel an excess of population requiring territorial expansion, but even from the point of view of new conquests, what can victory over Germany give us?

PoznaÅ„, East Prussia? But why do we need these areas, densely populated by Poles, when we already have difficulty managing Russian Poles? Why revive centrifugal aspirations, not yet extinct in the Vistula region, by bringing into the Russian state restless PoznaÅ„ and East Prussian Poles, whose national demands even the firmer German authority, compared to Russian, cannot suppress?

## IN THE REALM OF ECONOMIC INTERESTS, RUSSIAN BENEFITS AND NEEDS DO NOT CONTRADICT GERMAN ONES

| Region                  | Potential Acquisitions                    | Notes                                                                                      |
|------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------|
| Transcaucasus          | Armenian-populated areas                  | Desirable due to revolutionary sentiments and dreams of a great Armenia. |
| Persia                 | Economic and territorial expansion        | Interests do not collide with Germany. |
| Kashgaria              | Economic and territorial expansion        | Interests do not collide with Germany. |
| Urianhai region        | Economic and territorial expansion        | Interests do not collide with Germany. |
| Vistula region         | Areas of little value, poorly suited for colonization | Polish-Lithuanian population is restless and hostile to Germans. |
| Baltic provinces       | Areas of little value, poorly suited for colonization | Latvian-Estonian population is equally restless and hostile to Germans. |

But one might object that territorial acquisitions, under modern conditions of national life, take second place and economic interests come to the fore. However, even in this realm, Russian benefits and needs hardly contradict German ones as much as is commonly thought. It is beyond doubt that the current Russian-German trade treaties are disadvantageous for our agriculture and advantageous for German agriculture, but it is hardly correct to attribute this circumstance to Germany's cunning and unfriendliness.

We should not lose sight of the fact that these treaties are advantageous to us in many of their parts. The Russian delegates who concluded these treaties at the time were convinced supporters of developing Russian industry at whatever cost and, undoubtedly, consciously sacrificed, at least partially, the interests of Russian agriculture in favor of Russian industry's interests.

Furthermore, we must not lose sight of the fact that Germany itself is far from being a direct consumer of most items of our agricultural foreign exports. For most products of our agricultural industry, Germany is only an intermediary, and consequently, it depends on us and the consuming markets to establish direct relations and thereby avoid expensive German intermediation.

Finally, it is necessary to consider that the conditions of trade relations can change depending on the conditions of political coexistence between the contracting states, since no country benefits from the economic weakening of an ally, but conversely benefits from the ruin of a political opponent.

In short, although it is undoubtedly true that the current Russian-German trade treaties are disadvantageous for us and that Germany, in concluding them, successfully exploited circumstances that developed favorably for it--that is, simply put, squeezed us--this behavior cannot be counted as hostile and is a worthy-of-emulation act of healthy national egoism, which could not have been unexpected from Germany and which should have been taken into account.

In any case, we see in Austria-Hungary an agricultural country in incomparably greater economic dependence on Germany than we are, which nevertheless does not prevent it from achieving such development in agriculture as we can only dream about. Given all the above, concluding a trade treaty with Germany that is fully acceptable for Russia would seem to not at all require Germany's prior defeat. Good neighborly relations with it, thoughtful weighing of our real economic interests in various branches of the national economy, and long persistent bargaining with German delegates, who are undoubtedly called to protect the interests of their, not our, fatherland, are quite sufficient.**Economic Relations with Germany: Analysis and Recommendations**

Germany's actions, which successfully exploited circumstances that developed favorably for it--squeezing us in the process--cannot be counted as hostile. This behavior exemplifies a worthy act of healthy national egoism, which, while not unexpected from Germany, should have been taken into account.

In any case, we observe that Austria-Hungary, an agricultural country with incomparably greater economic dependence on Germany than we have, nevertheless achieves agricultural development that we can only dream about. Given this context, concluding a trade treaty with Germany that is fully acceptable to Russia does not necessarily require Germany's prior defeat. Good neighborly relations, thoughtful consideration of our real economic interests across various branches of the national economy, and prolonged negotiations with German delegates--who are undoubtedly tasked with protecting the interests of their own country--are quite sufficient.

I will elaborate: Germany's defeat in terms of our trade exchange would be disadvantageous for us. Such a defeat would likely culminate in a peace dictated by England's economic interests. England would exploit the situation to the fullest extent, and in a devastated Germany that has lost its sea routes, we would lose a valuable consumer market for our products, which have no other outlet.

Regarding Germany's economic future, the interests of Russia and England are directly opposed to one another. It is advantageous for England to undermine German maritime trade and industry, reducing Germany to a poor agricultural country if possible. Conversely, it is in our interest for Germany to develop its maritime trade and the industries that support it, thereby opening its internal market to our agricultural products to supply its numerous working population.

However, independent of trade treaties, there are concerns regarding the dominance of German influence in Russian economic life and the systematic implementation of German colonization, which is purportedly a clear danger to the Russian state. These fears, however, seem largely exaggerated. The infamous Drang nach Osten was a natural and understandable phenomenon at the time, as Germany's territory could not accommodate its growing population, which was pushed toward the direction of least resistance--namely, into the less densely populated neighboring countries.

The German government was compelled to acknowledge the inevitability of this movement but could hardly see it as serving its interests. After all, German citizens were departing the sphere of German statehood, thereby diminishing their country's living strength. Of course, the German government made every effort to maintain the settlers' connection with their former homeland and even allowed the possibility of dual citizenship.

Nonetheless, it is undeniable that a significant number of German emigrants ultimately settled permanently and irrevocably in their new locations, gradually severing ties with their homeland. This situation, clearly unaligned with Germany's state interests, likely motivated it to pursue a path of colonial policy and maritime trade, which was previously foreign to it.

As German colonies proliferate and German industry and maritime trade develop in connection with them, the wave of German colonists is receding. The day is not far off when the Drang nach Osten will become a matter of historical memory.

In any case, German colonization, which undoubtedly contradicts our state interests, must be halted, and friendly relations with Germany do not preclude this action. Advocating for a preference for German orientation does not imply endorsing Russia's vassal dependency on Germany. While maintaining friendly, neighborly ties with Germany, we must not sacrifice our state interests for this goal.

Germany itself would likely not object to efforts aimed at curtailing the further influx of German colonists into Russia. It is more advantageous for Germany to direct the wave of migration to its colonies. Furthermore, even in the absence of colonies and when German industry did not fully employ the population, the German government did not protest against the restrictive measures instituted during Alexander III's reign regarding foreign colonization.

Regarding German dominance in our economic life, this phenomenon hardly warrants the reproaches typically directed at it. Russia is too impoverished in both capital and industrial enterprise to manage without a broad influx of foreign capital. Thus, some dependence on foreign capital is inevitable until our industrial capabilities and the material means of the population develop sufficiently to eliminate the need for foreign entrepreneurs and their investments.

While we require foreign capital, German capital is more advantageous for us than any other option. Primarily, this capital is the least expensive, as it demands lower rates of entrepreneurial profit. This largely accounts for the comparative affordability of German products and their gradual displacement of English goods from the global market.

The lower profitability expectations associated with German capital allow it to enter ventures that other foreign investments might avoid due to relatively low profitability. Consequently, the influx of German capital into Russia results in smaller outflows of entrepreneurial profits compared to English and French capital, thus retaining more Russian rubles within the country.

Moreover, a significant portion of the profits generated by German capital invested in Russian industry does not leave Russia but is reinvested domestically. Unlike English or French capitalists, German capitalists tend to move to Russia along with their investments. This characteristic explains the notable presence of German industrialists, factory owners, and manufacturers compared to their English and French counterparts. The latter typically remain abroad, extracting profits from Russia without reinvesting in the country. In contrast, German entrepreneurs often reside in Russia for extended periods and frequently settle there permanently.

Even if we acknowledge the necessity of eradicating German dominance in our economic life--perhaps even at the cost of completely expelling German capital from Russian industry--it seems that such measures could be enacted without resorting to war with Germany.

The costs associated with such a war would far exceed any dubious benefits we might gain from liberation from German dominance. Moreover, the aftermath of this war would create an economic situation where the burden of German capital would feel light by comparison. It is beyond doubt that the war would necessitate expenditures that exceed Russia's limited financial resources, forcing us to seek credit from allied and neutral states, which would not be granted freely.

The potential consequences of an unsuccessful war are dire. The financial and economic ramifications of defeat cannot be calculated or foreseen and would likely lead to the complete collapse of our national economy. Even a victory would yield unfavorable financial prospects: a thoroughly devastated Germany would be unable to compensate us for our incurred costs. A peace treaty dictated by England's interests would prevent Germany from recovering sufficiently to cover our military expenses in the future. Any resources we might manage to extract would need to be shared with our allies, resulting in a meager portion for us compared to our military expenditures.

Additionally, the repayment of war loans would come under pressure from our allies. After the collapse of German power, our significance to them would diminish. Furthermore, our enhanced political power due to victory may prompt them to undermine us economically. Thus, even after a victorious conclusion to the war, we might find ourselves in a financial predicament with our creditors, making our current dependence on German capital seem ideal by comparison.

However bleak the economic prospects presented by an alliance with England and, consequently, a war with Germany may appear, they remain secondary to the political consequences of this fundamentally unnatural alliance.

The conflict between Russia and Germany is deeply undesirable for both nations, as it undermines the monarchist principle that both represent in the civilized world, standing in opposition to the democratic principle embodied elsewhere.will have to be paid, not without pressure from our allies. After all, after the collapse of German power, we will no longer be needed by them. Moreover, our political might, increased due to victory, will induce them to weaken us, at least economically. Thus, inevitably, even after a victorious conclusion to the war, we will find ourselves in a state of financial bondage to our creditors, compared to which our current dependence on German capital will seem ideal.

The struggle between Russia and Germany is deeply undesirable for both sides, as it amounts to weakening the monarchist principle.

We should not lose sight of the fact that Russia and Germany represent the conservative principle in the civilized world, which stands in opposition to the democratic principle embodied by England and, to a far lesser degree, France.

However strange it may seem, England, monarchist and conservative to the core at home, has always acted externally as the patron of the most demagogic aspirations, invariably indulging all popular movements aimed at weakening the monarchist principle.

From this perspective, the struggle between Germany and Russia, regardless of its outcome, is deeply undesirable for both sides, as it undoubtedly amounts to weakening the global conservative principle, for which these two great powers serve as the only reliable bulwark.

Moreover, one cannot help but foresee that, under the exceptional conditions of the approaching general European war, this war, again regardless of its outcome, will present a mortal danger for both Russia and Germany.

Based on a deep conviction formed through careful, many-year study of all contemporary anti-state currents, a social revolution will inevitably break out in the defeated country, which, by force of circumstances, will spread to the victorious country.

The channels by which both countries have been invisibly connected over many years of peaceful coexistence are too numerous for fundamental social upheavals occurring in one of them not to be reflected in the other.

That these upheavals will be specifically social rather than political in character--there can be no doubt, and this applies not only to Russia but also to Germany.

Russia presents an especially favorable ground for social upheavals, where the masses undoubtedly lean toward the principles of unconscious socialism.

Despite the oppositional nature of Russian society, as unconscious as the socialism of the broad strata of the population may be, political revolution in Russia is impossible, and any revolutionary movement will inevitably degenerate into a socialist revolution.

The Russian commoner, be they peasant or worker, does not seek political rights, which are both unnecessary and incomprehensible to him.

The peasant dreams of being freely granted someone else's land, while the worker longs to receive all the capitalist's capital and profits, and their aspirations do not extend beyond this.

If the government were to deny them support and leave elections to their natural course, the legislative institutions would not see within their very walls a single intellectual, aside from a few agitator-demagogues.

However, members of our legislative institutions might proclaim the people's trust in them; the peasant would sooner believe a landless government official than a landowner-Octobrist sitting in the Duma.

It is more than strange, under such conditions, to demand that governmental authority seriously reckon with the opposition, for its sake abandon the role of impartial regulator of social relations, and present itself before the broad popular masses as an obedient organ of the class aspirations of the intellectual-propertied minority of the population.

By demanding from governmental authority responsibility before class representation and obedience to an artificially created parliament, our opposition essentially demands from the government the psychology of a savage who crafts an idol with his own hands and then worships it with trepidation.

## RUSSIA WILL BE PLUNGED INTO HOPELESS ANARCHY, WHOSE OUTCOME IS DIFFICULT TO FORESEE

If the war ends victoriously, suppressing the socialist movement will ultimately not present insurmountable difficulties.

There will be agrarian disturbances based on agitation for the necessity of rewarding soldiers with additional land allotments, and there will be labor unrest in transitioning from the probably elevated wartime wages to normal rates--and one must hope it will be limited to this, until the wave of German social revolution reaches us.

But in case of failure, the possibility of which, in struggling with such an opponent as Germany, cannot be ignored--social revolution in its most extreme manifestations is inevitable for us.

A furious campaign against the government will begin in legislative institutions, resulting in revolutionary outbursts in the country.

These latter will immediately advance socialist slogans, the only ones that can raise and group together broad strata of the population--first land redistribution, and then general division of all valuables and properties.

The defeated army, having lost, moreover, during the war its most reliable cadre composition, captured largely by the general peasant aspiration for land, will prove too demoralized to serve as a bulwark of law and order.

The totality of all the above leads to the conclusion that rapprochement with England promises us no benefits, and the English orientation of our diplomacy is fundamentally mistaken in its essence. We have no common path with England; it should be left to its fate, and we need not quarrel with Germany over it.

## The Triple Entente

In this direction, and not in the fruitless seeking of ground for agreement with England that contradicts our state views and goals by its very essence, all efforts of our diplomacy should be concentrated. Germany must meet our aspirations to restore tested friendly-allied relations with it and work out, in closest agreement with us, conditions for our coexistence that would not give ground for anti-German agitation from our constitutional-liberal parties, which by their very nature are forced to adhere not to conservative-German, but to liberal-English orientation.
</file>

<file path="sample_input_document_as_markdown__small.md">
Whether you need to pay income taxes on a legal settlement for an accident depends on the purpose of the settlement and what the payments are compensating for. Hereâ€™s a breakdown:

### **1. Personal Physical Injury or Sickness**
- **Tax-Free:** Settlements or awards compensating for physical injuries or physical sickness are generally not taxable under U.S. tax law (IRC Â§104(a)(2)).
- **Exceptions:** If you deducted related medical expenses in prior years (e.g., via itemized deductions), the portion reimbursed by the settlement for those expenses is taxable.

### **2. Emotional Distress or Mental Anguish**
- **Taxable:** Payments for emotional distress or mental anguish are taxable unless they stem directly from a physical injury or sickness. 
- **Non-Taxable:** If the emotional distress is caused by or directly related to a physical injury, the payment is non-taxable.

### **3. Lost Wages**
- **Taxable:** Compensation for lost wages or lost income is taxable because it replaces taxable earnings.

### **4. Punitive Damages**
- **Taxable:** Punitive damages are always taxable, regardless of whether the underlying case involves physical injury.

### **5. Interest on the Settlement**
- **Taxable:** Any interest earned on the settlement amount (e.g., due to delays in payment) is taxable.

### **6. Property Damage**
- **Generally Non-Taxable:** If youâ€™re compensated for damage to property (e.g., vehicle repair or replacement), itâ€™s typically non-taxable unless the payment exceeds your adjusted basis (the value of the property).

### **Key Considerations:**
- **Attorneyâ€™s Fees:** If you hired a lawyer on a contingency fee basis, the full settlement amount is often reported as income before deducting fees, which can complicate your tax situation.
- **State Laws:** Check state-specific rules, as state income tax treatment may vary.

Always consult a tax professional to confirm how your specific settlement is taxed, as details matter (e.g., whether the settlement agreement explicitly allocates payments to different types of damages).
</file>

<file path="screenshots/mindmap-architecture.svg">
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 500">
  <style>
    .box-text {
      display: flex; align-items: center; justify-content: center;
      text-align: center; padding: 4px; box-sizing: border-box;
    }
    .loop-text {
      display: flex; align-items: center; justify-content: center;
      text-align: center; padding: 4px; box-sizing: border-box;
    }
  </style>
  <rect width="800" height="500" fill="#F0F4F8" rx="10" ry="10" />
  <text x="400" y="40" font-family="Arial" font-size="24" font-weight="bold" text-anchor="middle" fill="#333">
    Mindmap Generator Architecture
  </text>
  <rect x="320" y="70" width="160" height="60" rx="12" ry="12" fill="#5DADE2" />
  <foreignObject x="320" y="70" width="160" height="60">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 16px; font-weight: bold; color: white;">
      Document Analysis Engine
    </div>
  </foreignObject>
  <rect x="50" y="170" width="140" height="50" rx="10" ry="10" fill="#58D68D" />
  <foreignObject x="50" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Document Input
    </div>
  </foreignObject>
  <rect x="250" y="170" width="140" height="50" rx="10" ry="10" fill="#EC7063" />
  <foreignObject x="250" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Document Type<br/>Detection
    </div>
  </foreignObject>
  <rect x="450" y="170" width="140" height="50" rx="10" ry="10" fill="#F4D03F" />
  <foreignObject x="450" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Topic<br/>Extraction
    </div>
  </foreignObject>
  <rect x="650" y="170" width="140" height="50" rx="10" ry="10" fill="#AF7AC5" />
  <foreignObject x="650" y="170" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Reality<br/>Check
    </div>
  </foreignObject>
  <rect x="50" y="280" width="140" height="50" rx="10" ry="10" fill="#EC7063" />
  <foreignObject x="50" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Subtopic<br/>Processing
    </div>
  </foreignObject>
  <rect x="250" y="280" width="140" height="50" rx="10" ry="10" fill="#F4D03F" />
  <foreignObject x="250" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Detail<br/>Extraction
    </div>
  </foreignObject>
  <rect x="450" y="280" width="140" height="50" rx="10" ry="10" fill="#48C9B0" />
  <foreignObject x="450" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Redundancy<br/>Filter
    </div>
  </foreignObject>
  <rect x="650" y="280" width="140" height="50" rx="10" ry="10" fill="#58D68D" />
  <foreignObject x="650" y="280" width="140" height="50">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 14px; font-weight: bold; color: white;">
      Output<br/>Generation
    </div>
  </foreignObject>
  <rect x="230" y="390" width="100" height="40" rx="8" ry="8" fill="#9B59B6" stroke="#8E44AD" stroke-width="2" />
  <foreignObject x="230" y="390" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 12px; font-weight: bold; color: white;">
      Mermaid
    </div>
  </foreignObject>
  <rect x="350" y="390" width="100" height="40" rx="8" ry="8" fill="#9B59B6" stroke="#8E44AD" stroke-width="2" />
  <foreignObject x="350" y="390" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 12px; font-weight: bold; color: white;">
      HTML
    </div>
  </foreignObject>
  <rect x="470" y="390" width="100" height="40" rx="8" ry="8" fill="#9B59B6" stroke="#8E44AD" stroke-width="2" />
  <foreignObject x="470" y="390" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="box-text" style="font-family: Arial; font-size: 12px; font-weight: bold; color: white;">
      Markdown
    </div>
  </foreignObject>
  <line x1="120" y1="170" x2="320" y2="100" stroke="#555" stroke-width="2" />
  <line x1="400" y1="130" x2="320" y2="170" stroke="#555" stroke-width="2" />
  <line x1="400" y1="130" x2="520" y2="170" stroke="#555" stroke-width="2" />
  <line x1="400" y1="130" x2="720" y2="170" stroke="#555" stroke-width="2" />
  <line x1="390" y1="195" x2="450" y2="195" stroke="#555" stroke-width="2" />
  <line x1="590" y1="195" x2="650" y2="195" stroke="#555" stroke-width="2" />
  <line x1="320" y1="220" x2="120" y2="280" stroke="#555" stroke-width="2" />
  <line x1="520" y1="220" x2="320" y2="280" stroke="#555" stroke-width="2" />
  <line x1="720" y1="220" x2="520" y2="280" stroke="#555" stroke-width="2" />
  <line x1="190" y1="305" x2="250" y2="305" stroke="#555" stroke-width="2" />
  <line x1="390" y1="305" x2="450" y2="305" stroke="#555" stroke-width="2" />
  <line x1="590" y1="305" x2="650" y2="305" stroke="#555" stroke-width="2" />
  <line x1="720" y1="330" x2="280" y2="390" stroke="#555" stroke-width="2" />
  <line x1="720" y1="330" x2="400" y2="390" stroke="#555" stroke-width="2" />
  <line x1="720" y1="330" x2="520" y2="390" stroke="#555" stroke-width="2" />
  <path d="M 720 220 C 760 250, 760 290, 720 330" fill="none" stroke="#555" stroke-width="2" stroke-dasharray="5,3" />
  <foreignObject x="740" y="250" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="loop-text" style="font-family: Arial; font-size: 10px; color: #555;">
      Verification Loop
    </div>
  </foreignObject>
  <path d="M 120 220 C 80 250, 80 290, 120 330" fill="none" stroke="#555" stroke-width="2" stroke-dasharray="5,3" />
  <foreignObject x="0" y="250" width="100" height="40">
    <div xmlns="http://www.w3.org/1999/xhtml" class="loop-text" style="font-family: Arial; font-size: 10px; color: #555;">
      Exploration Loop
    </div>
  </foreignObject>
</svg>
</file>

<file path="frontend/src/components/LogicalDivider.js">
import React from 'react';
import { GripVertical } from 'lucide-react';

const LogicalDivider = ({ nodeInfo, dragHandleProps }) => {
  const { title, color = 'gray' } = nodeInfo;
  
  // æ ¹æ®é¢œè‰²è·å–å¯¹åº”çš„Tailwindç±»
  const getColorClasses = (color) => {
    const colorMap = {
      gray: 'border-gray-300 dark:border-gray-600 bg-gray-50 dark:bg-gray-700 text-gray-600 dark:text-gray-400',
      blue: 'border-blue-300 dark:border-blue-600 bg-blue-50 dark:bg-blue-700 text-blue-600 dark:text-blue-400',
      green: 'border-green-300 dark:border-green-600 bg-green-50 dark:bg-green-700 text-green-600 dark:text-green-400',
      purple: 'border-purple-300 dark:border-purple-600 bg-purple-50 dark:bg-purple-700 text-purple-600 dark:text-purple-400',
      red: 'border-red-300 dark:border-red-600 bg-red-50 dark:bg-red-700 text-red-600 dark:text-red-400',
      yellow: 'border-yellow-300 dark:border-yellow-600 bg-yellow-50 dark:bg-yellow-700 text-yellow-600 dark:text-yellow-400'
    };
    return colorMap[color] || colorMap.gray;
  };

  return (
    <div className="relative my-6">
      {/* æ°´å¹³åˆ†å‰²çº¿ */}
      <div className={`border-t-2 ${getColorClasses(color).split(' ')[0]} ${getColorClasses(color).split(' ')[1]}`} />
      
      {/* æ‹–æ‹½æ‰‹æŸ„å’Œæ ‡ç­¾å®¹å™¨ */}
      <div className="absolute top-0 left-1/2 transform -translate-x-1/2 -translate-y-1/2 flex items-center">
        {/* æ‹–æ‹½æ‰‹æŸ„ - å§‹ç»ˆå¯è§ */}
        <div 
          className={`
            flex items-center justify-center w-6 h-6 rounded-full border-2 
            ${getColorClasses(color)}
            cursor-grab hover:cursor-grabbing
            shadow-sm
            opacity-70 hover:opacity-100
            transition-opacity duration-200
          `}
          {...(dragHandleProps || {})}
        >
          <GripVertical className="w-3 h-3" />
        </div>
        
        {/* èŠ‚ç‚¹æ ‡ç­¾ */}
        <div className={`
          ml-2 px-3 py-1 rounded-full text-xs font-medium
          ${getColorClasses(color)}
          border-2 shadow-sm
          max-w-xs truncate
        `}>
          {title}
        </div>
      </div>
    </div>
  );
};

export default LogicalDivider;
</file>

<file path="frontend/src/hooks/useDocumentViewer.js">
import { useState, useEffect, useCallback } from 'react';
import { useParams } from 'react-router-dom';
import axios from 'axios';
import toast from 'react-hot-toast';

// è·å–é»˜è®¤çš„æ¼”ç¤ºæµç¨‹å›¾ä»£ç 
const getDefaultDemoMermaidCode = () => `---
config:
  layout: dagre
  theme: redux
  look: neo
---
%%{ init: { 'flowchart': { 'curve': 'basis' } } }%%
flowchart TD
    %% --- Link Style ---
    linkStyle default stroke:#6A99C9,stroke-width:2px,stroke-dasharray:3 3,color:#000000

    A[ä¸ºä»€ä¹ˆè¾©è¯å­¦å®¶è¦å­¦ç€æ•°åˆ°å››ï¼Ÿ] --å¼•è®º--> B{ç¬¬å››æ–¹ï¼šæº¢å‡º/è¿‡å‰©çš„å¦å®šæ€§ç¯èŠ‚}
    B --æ ¸å¿ƒåŒ–ä¸º--> C[æ ¸å¿ƒæ¦‚å¿µï¼šæ¶ˆå¤±çš„ä¸­ä»‹è€…]
    B --å¯¼å‘ç»“è®º--> J[ç»“è®ºï¼šä¸»ä½“ä½œä¸ºæ¶ˆå¤±çš„ä¸­ä»‹è€…]

    C --é˜è¿°æœºåˆ¶--> D[æ¶ˆå¤±çš„ä¸­ä»‹è€…çš„è¿ä½œæœºåˆ¶]
    C --ä¸¾ä¾‹è¯´æ˜--> E[å®ä¾‹åˆ†æ]
    C --æ­ç¤ºç‰¹æ€§--> F{ä¸­ä»‹è€…çš„å¹»è§‰ï¼š<br/>æœªè®¤è¯†åˆ°è‡ªèº«è¡Œä¸ºçš„çœŸå®ç»“æœ}
    C --å…³è”æ¦‚å¿µ--> H[æ¶ˆå¤±çš„ä¸­ä»‹è€…ä¸äº‹ä»¶åŠä¸»ä½“]

    D --é˜¶æ®µ1--> D1[1. æ—§å½¢å¼çš„æ™®éåŒ–ä¸æ¿€è¿›åŒ–]
    D1 --é˜¶æ®µ2--> D2[2. æ–°ç¤¾ä¼šå†…å®¹çš„å½¢æˆ]
    D2 --é˜¶æ®µ3--> D3[3. ä¸­ä»‹è€…å½¢å¼çš„æ¶ˆå¤±/å˜å¾—å¤šä½™]

    E --ä¾‹è¯ä¸€--> E1[æ–°æ•™ä¼¦ç†: å°å»ºä¸»ä¹‰ â†’ èµ„æœ¬ä¸»ä¹‰]
    E --ä¾‹è¯äºŒ--> E2[é›…å„å®¾ä¸»ä¹‰: æ—§åˆ¶åº¦ â†’ èµ„äº§é˜¶çº§æ°‘ä¸»]
    E --å…¶ä»–ä¾‹è¯--> E3[å…¶ä»–ä¾‹å­: ç»å¯¹å›ä¸»åˆ¶<br/>æ³•è¥¿æ–¯ä¸»ä¹‰ç­‰]

    F --å¥½æ¯”--> G[ä¸ç¾ä¸½çµé­‚çš„ç±»æ¯”]

    H --å®šä¹‰ä¸»ä½“--> H1[ä¸»ä½“ï¼šåœ¨å¼€æ”¾/ä¸ç¡®å®šæ—¶åˆ»<br/>è¢«å¬å”¤çš„X]
    H --å¼•å‡º--> I[çœŸç†çš„æ”¿æ²»æ€§]

    H1 --å…¶è¡ŒåŠ¨--> H2[è¡ŒåŠ¨ï¼šå›æº¯æ€§åœ°åˆ›é€ å…¶<br/>åˆç†æ€§ä¸æ¡ä»¶]
    H2 --å…¶ç»“æœ--> H3[è®¾å®šé¢„è®¾ï¼šä¸»ä½“è¡ŒåŠ¨æˆåŠŸå<br/>è¢«æ•´åˆè¿›æ–°ç§©åºå¹¶å˜å¾—ä¸å¯è§]

    I --å…·ä½“ä¸º--> I1[åŒºåˆ†æ”¿æ²»ä¸æ”¿æ²»æ€§]
    I1 --é˜é‡Š--> I2[æ”¿æ²»æ€§ï¼šç¤¾ä¼šç»“æ„è¢«è´¨ç–‘å’Œé‡å¡‘çš„<br/>å¼€æ”¾æ€§ç¯èŠ‚ï¼ŒçœŸç†åœ¨æ­¤æ˜¾ç°]
    I2 --å¼ºè°ƒ--> I3[ç¤¾ä¼šç§©åºçš„èµ·æºæ€»æ˜¯æ”¿æ²»æ€§çš„]

    J --è¿›ä¸€æ­¥é˜é‡Š--> J1[ä¸»ä½“æ˜¯è¾©è¯è¿‡ç¨‹çš„ç¬¬å››ç¯èŠ‚<br/>å…¶æ¶ˆå¤±æ˜¯å…¶æˆåŠŸçš„æ ‡å¿—]
    J1 --å…³è”è‡³--> K[çœŸç†çš„å¶ç„¶æ€§ä¸åˆ›ä¼¤æ€§]

    K --é€šè¿‡ç±»æ¯”--> K1[ç±»æ¯”æ ¼é›·é©¬æ–¯ç¬¦å·å­¦çŸ©é˜µ<br/>ä¸æ‹‰åº·ç²¾ç¥åˆ†æ]
    K1 --æ­ç¤ºçœŸç†--> K2[çœŸç†ä½œä¸ºç‰¹æ®Šçš„å¶ç„¶é­é‡<br/>æ‰“ç ´æ™®éçš„è°è¨€]

    %% é¢å¤–çš„åˆ†ææ¡†æ¶
    A --ç†è®ºåŸºç¡€--> L[é»‘æ ¼å°”è¾©è¯æ³•çš„å››é‡ç»“æ„]
    L --åŒ…å«--> L1[1. ç›´æ¥è‚¯å®šæ€§]
    L1 --å¯¼å‘--> L2[2. å†…åœ¨å¦å®šæ€§/ä¸­ä»‹]
    L2 --å‘å±•ä¸º--> L3[3. å¦å®šçš„å¦å®š]
    L3 --å®Œæˆäº--> L4[4. ä¸»ä½“ä½œä¸ºæ¶ˆå¤±çš„ç¯èŠ‚]

    %% å†å²å®ä¾‹çš„è¯¦ç»†åˆ†æ
    E1 --æœºåˆ¶åˆ†æ--> M1[æ–°æ•™ï¼šå®—æ•™æ™®éåŒ–â†’å®—æ•™ç§äººåŒ–]
    E2 --æœºåˆ¶åˆ†æ--> M2[é›…å„å®¾ï¼šæ”¿æ²»æ¿€è¿›åŒ–â†’èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»]
    M1 --> M3[å…±åŒç‚¹ï¼šå½¢å¼ä¸å†…å®¹çš„åˆ†ç¦»]
    M2 --> M3

    %% ç°ä»£ç›¸å…³æ€§
    E3 --å½“ä»£ä¾‹è¯--> N[ä¸œæ¬§æ–°ç¤¾ä¼šè¿åŠ¨]
    N --ç‰¹å¾--> N1[ç†æƒ³ä¸»ä¹‰çš„ç¬¬ä¸‰æ¡é“è·¯]
    N1 --ç»“æœ--> N2[ä¸ºèµ„æœ¬ä¸»ä¹‰å¤è¾Ÿé“ºè·¯]
    N2 --éªŒè¯--> C

    A:::concept
    B:::concept
    C:::concept
    J:::conclusion
    D:::mechanism
    E:::example
    F:::highlight
    H:::concept
    D1:::mechanism
    D2:::mechanism
    D3:::mechanism
    E1:::example
    E2:::example
    E3:::example
    G:::default
    H1:::default
    I:::concept
    H2:::default
    H3:::default
    I1:::default
    I2:::default
    I3:::default
    J1:::default
    K:::concept
    K1:::default
    K2:::default
    L:::theory
    L1:::theory
    L2:::theory
    L3:::theory
    L4:::theory
    M1:::analysis
    M2:::analysis
    M3:::analysis
    N:::modern
    N1:::modern
    N2:::modern`;

export const useDocumentViewer = () => {
  const { documentId } = useParams();
  
  const [document, setDocument] = useState(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);
  
  // æ–‡æ¡£æŸ¥çœ‹æ¨¡å¼ç›¸å…³çŠ¶æ€
  const [viewMode, setViewMode] = useState('markdown');
  const [isPdfFile, setIsPdfFile] = useState(false);
  
  // æ–‡æ¡£ç»“æ„å’Œç›®å½•ç›¸å…³çŠ¶æ€
  const [documentStructure, setDocumentStructure] = useState(null);
  const [toc, setToc] = useState([]);
  const [expandedTocItems, setExpandedTocItems] = useState(new Set());

  const loadDocument = async () => {
    try {
      setLoading(true);
      setError(null);
      
      // æ£€æŸ¥æ˜¯å¦ä¸ºçº¯ç¤ºä¾‹æ¨¡å¼ï¼ˆdemo-å‰ç¼€ + æ—¶é—´æˆ³IDï¼‰
      if (documentId.startsWith('demo-')) {
        const actualDocumentId = documentId.replace('demo-', '');
        console.log('ğŸ¨ [ç¤ºä¾‹æ¨¡å¼] æ£€æµ‹åˆ°ç¤ºä¾‹æ¨¡å¼ï¼ŒåŸå§‹ID:', documentId, 'å®é™…ID:', actualDocumentId);
        
        // æ£€æŸ¥æ˜¯å¦ä¸ºçº¯ç¤ºä¾‹æ¨¡å¼ï¼ˆåŸºäºæ—¶é—´æˆ³çš„è™šæ‹ŸIDï¼‰
        if (actualDocumentId.length > 10 && /^\d+$/.test(actualDocumentId)) {
          console.log('ğŸ“ [çº¯ç¤ºä¾‹æ¨¡å¼] æ£€æµ‹åˆ°çº¯ç¤ºä¾‹æ¨¡å¼ï¼Œæ˜¾ç¤ºé¢„è®¾å†…å®¹');
          // åˆ›å»ºè™šæ‹Ÿæ–‡æ¡£å¯¹è±¡ç”¨äºçº¯ç¤ºä¾‹æ¨¡å¼
          setDocument({
            document_id: actualDocumentId,
            content: null, // æ ‡è®°ä¸ºç¤ºä¾‹æ¨¡å¼
            mermaid_code_demo: getDefaultDemoMermaidCode(),
            filename: 'è®ºè¯ç»“æ„åˆ†æç¤ºä¾‹',
            file_type: '.md',
            pdf_base64: null,
          });
          setLoading(false);
          return;
        }
      }
      
      // å¯¹äºä¸Šä¼ çš„æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨documentId
      const statusResponse = await axios.get(`http://localhost:8000/api/document-status/${documentId}`);
      
      if (statusResponse.data.success) {
        const docData = statusResponse.data;
        setDocument({
          document_id: docData.document_id,
          content: docData.content,
          content_with_ids: docData.content_with_ids, // æ·»åŠ å¸¦æ®µè½IDçš„å†…å®¹
          mermaid_code: docData.mermaid_code,
          mermaid_code_demo: docData.mermaid_code_demo,
          node_mappings_demo: docData.node_mappings_demo,
          filename: docData.filename,
          file_type: docData.file_type,
          pdf_base64: docData.pdf_base64,
        });
        
        // æ£€æŸ¥æ˜¯å¦ä¸ºPDFæ–‡ä»¶
        const isPDF = docData.file_type === '.pdf';
        setIsPdfFile(isPDF);
        
        // å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œé»˜è®¤æ˜¾ç¤ºè½¬æ¢åçš„Markdown
        if (isPDF) {
          setViewMode('markdown');
        }
        
        console.log('ğŸ“„ [æ–‡æ¡£åŠ è½½] æˆåŠŸåŠ è½½æ–‡æ¡£');
      } else {
        const response = await axios.get(`http://localhost:8000/api/document/${documentId}`);
        
        if (response.data.success) {
          const docData = response.data;
          setDocument({
            document_id: docData.document_id,
            content: docData.content,
            content_with_ids: docData.content_with_ids, // æ·»åŠ å¸¦æ®µè½IDçš„å†…å®¹
            mermaid_code: docData.mermaid_code,
            mermaid_code_demo: docData.mermaid_code_demo,
            node_mappings_demo: docData.node_mappings_demo,
            filename: docData.filename,
            file_type: docData.file_type,
            pdf_base64: docData.pdf_base64,
          });
        } else {
          setError('åŠ è½½æ–‡æ¡£å¤±è´¥');
        }
      }
    } catch (error) {
      console.error('Load document error:', error);
      const errorMessage = error.response?.data?.detail || 'åŠ è½½æ–‡æ¡£å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥';
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  const loadDocumentStructure = useCallback(async () => {
    try {
      console.log('ğŸ“„ [å¼€å§‹åŠ è½½] å¼€å§‹åŠ è½½æ–‡æ¡£ç»“æ„ï¼ŒdocumentId:', documentId);
      
      // è·å–æ–‡æ¡£ç»“æ„ä¿¡æ¯ï¼ˆåŒ…å«chunksï¼‰
      const structureResponse = await axios.get(`http://localhost:8000/api/document-structure/${documentId}`);
      console.log('ğŸ“„ [ç»“æ„å“åº”]', structureResponse.data);
      
      if (structureResponse.data.success) {
        setDocumentStructure(structureResponse.data.structure);
        const chunks = structureResponse.data.chunks || [];
        
        console.log('ğŸ“„ [æ–‡æ¡£ç»“æ„] æˆåŠŸåŠ è½½äº†', chunks.length, 'ä¸ªå†…å®¹å—');
        console.log('ğŸ“„ [å†…å®¹å—è¯¦æƒ…]', chunks.map(c => ({ 
          id: c.chunk_id, 
          heading: c.heading,
          content_length: c.content?.length || 0 
        })));
        
        // å¦‚æœæœ‰tocæ•°æ®ï¼Œä¹Ÿè®¾ç½®å®ƒ
        if (structureResponse.data.toc) {
          setToc(structureResponse.data.toc);
          // é»˜è®¤å±•å¼€æ‰€æœ‰ä¸€çº§ç›®å½•
          const topLevelItems = new Set(structureResponse.data.toc.filter(item => item.level === 1).map(item => item.id));
          setExpandedTocItems(topLevelItems);
          console.log('ğŸ“„ [ç›®å½•] è®¾ç½®äº†', structureResponse.data.toc.length, 'ä¸ªç›®å½•é¡¹');
        } else {
          // å¦‚æœæ²¡æœ‰tocï¼Œå°è¯•å•ç‹¬è·å–
          try {
            const tocResponse = await axios.get(`http://localhost:8000/api/document-toc/${documentId}`);
            if (tocResponse.data.success) {
              setToc(tocResponse.data.toc);
              const topLevelItems = new Set(tocResponse.data.toc.filter(item => item.level === 1).map(item => item.id));
              setExpandedTocItems(topLevelItems);
              console.log('ğŸ“„ [ç›®å½•å•ç‹¬åŠ è½½] æˆåŠŸåŠ è½½', tocResponse.data.toc.length, 'ä¸ªç›®å½•é¡¹');
            }
          } catch (tocError) {
            console.warn('ğŸ“„ [ç›®å½•åŠ è½½å¤±è´¥]', tocError);
          }
        }
        
        return chunks;
      } else {
        console.warn('ğŸ“„ [ç»“æ„åŠ è½½å¤±è´¥]', structureResponse.data.message);
      }
    } catch (error) {
      console.error('ğŸ“„ [åŠ è½½æ–‡æ¡£ç»“æ„é”™è¯¯]', error);
      if (error.response) {
        console.error('ğŸ“„ [å“åº”é”™è¯¯]', error.response.data);
      }
    }
    return [];
  }, [documentId]);

  const toggleTocItem = (itemId) => {
    setExpandedTocItems(prev => {
      const newSet = new Set(prev);
      if (newSet.has(itemId)) {
        newSet.delete(itemId);
      } else {
        newSet.add(itemId);
      }
      return newSet;
    });
  };

  useEffect(() => {
    loadDocument();
  }, [documentId]);

  return {
    documentId,
    document,
    setDocument,
    loading,
    error,
    viewMode,
    setViewMode,
    isPdfFile,
    documentStructure,
    toc,
    expandedTocItems,
    toggleTocItem,
    loadDocument,
    loadDocumentStructure
  };
};
</file>

<file path="frontend/src/hooks/useMindmapGeneration.js">
import { useState, useEffect } from 'react';
import { useLocation } from 'react-router-dom';
import axios from 'axios';
import toast from 'react-hot-toast';

export const useMindmapGeneration = (documentId, document, setDocument) => {
  const location = useLocation();
  
  const [demoMindmapStatus, setDemoMindmapStatus] = useState('not_started');
  const [autoStarted, setAutoStarted] = useState(false);

  // é»˜è®¤æ¼”ç¤ºæµç¨‹å›¾ä»£ç  - ç°ä»£åŒ–æ ·å¼
  const defaultDemoMermaidCode = `---
config:
  layout: dagre
  theme: redux
  look: neo
---
%%{ init: { 'flowchart': { 'curve': 'basis' } } }%%
flowchart TD
    %% --- Link Style ---
    linkStyle default stroke:#6A99C9,stroke-width:2px,stroke-dasharray:3 3,color:#000000

    A[ä¸ºä»€ä¹ˆè¾©è¯å­¦å®¶è¦å­¦ç€æ•°åˆ°å››ï¼Ÿ] --å¼•è®º--> B{ç¬¬å››æ–¹ï¼šæº¢å‡º/è¿‡å‰©çš„å¦å®šæ€§ç¯èŠ‚}
    B --æ ¸å¿ƒåŒ–ä¸º--> C[æ ¸å¿ƒæ¦‚å¿µï¼šæ¶ˆå¤±çš„ä¸­ä»‹è€…]
    B --å¯¼å‘ç»“è®º--> J[ç»“è®ºï¼šä¸»ä½“ä½œä¸ºæ¶ˆå¤±çš„ä¸­ä»‹è€…]

    C --é˜è¿°æœºåˆ¶--> D[æ¶ˆå¤±çš„ä¸­ä»‹è€…çš„è¿ä½œæœºåˆ¶]
    C --ä¸¾ä¾‹è¯´æ˜--> E[å®ä¾‹åˆ†æ]
    C --æ­ç¤ºç‰¹æ€§--> F{ä¸­ä»‹è€…çš„å¹»è§‰ï¼š<br/>æœªè®¤è¯†åˆ°è‡ªèº«è¡Œä¸ºçš„çœŸå®ç»“æœ}
    C --å…³è”æ¦‚å¿µ--> H[æ¶ˆå¤±çš„ä¸­ä»‹è€…ä¸äº‹ä»¶åŠä¸»ä½“]

    D --é˜¶æ®µ1--> D1[1. æ—§å½¢å¼çš„æ™®éåŒ–ä¸æ¿€è¿›åŒ–]
    D1 --é˜¶æ®µ2--> D2[2. æ–°ç¤¾ä¼šå†…å®¹çš„å½¢æˆ]
    D2 --é˜¶æ®µ3--> D3[3. ä¸­ä»‹è€…å½¢å¼çš„æ¶ˆå¤±/å˜å¾—å¤šä½™]

    E --ä¾‹è¯ä¸€--> E1[æ–°æ•™ä¼¦ç†: å°å»ºä¸»ä¹‰ â†’ èµ„æœ¬ä¸»ä¹‰]
    E --ä¾‹è¯äºŒ--> E2[é›…å„å®¾ä¸»ä¹‰: æ—§åˆ¶åº¦ â†’ èµ„äº§é˜¶çº§æ°‘ä¸»]
    E --å…¶ä»–ä¾‹è¯--> E3[å…¶ä»–ä¾‹å­: ç»å¯¹å›ä¸»åˆ¶<br/>æ³•è¥¿æ–¯ä¸»ä¹‰ç­‰]

    F --å¥½æ¯”--> G[ä¸ç¾ä¸½çµé­‚çš„ç±»æ¯”]

    H --å®šä¹‰ä¸»ä½“--> H1[ä¸»ä½“ï¼šåœ¨å¼€æ”¾/ä¸ç¡®å®šæ—¶åˆ»<br/>è¢«å¬å”¤çš„X]
    H --å¼•å‡º--> I[çœŸç†çš„æ”¿æ²»æ€§]

    H1 --å…¶è¡ŒåŠ¨--> H2[è¡ŒåŠ¨ï¼šå›æº¯æ€§åœ°åˆ›é€ å…¶<br/>åˆç†æ€§ä¸æ¡ä»¶]
    H2 --å…¶ç»“æœ--> H3[è®¾å®šé¢„è®¾ï¼šä¸»ä½“è¡ŒåŠ¨æˆåŠŸå<br/>è¢«æ•´åˆè¿›æ–°ç§©åºå¹¶å˜å¾—ä¸å¯è§]

    I --å…·ä½“ä¸º--> I1[åŒºåˆ†æ”¿æ²»ä¸æ”¿æ²»æ€§]
    I1 --é˜é‡Š--> I2[æ”¿æ²»æ€§ï¼šç¤¾ä¼šç»“æ„è¢«è´¨ç–‘å’Œé‡å¡‘çš„<br/>å¼€æ”¾æ€§ç¯èŠ‚ï¼ŒçœŸç†åœ¨æ­¤æ˜¾ç°]
    I2 --å¼ºè°ƒ--> I3[ç¤¾ä¼šç§©åºçš„èµ·æºæ€»æ˜¯æ”¿æ²»æ€§çš„]

    J --è¿›ä¸€æ­¥é˜é‡Š--> J1[ä¸»ä½“æ˜¯è¾©è¯è¿‡ç¨‹çš„ç¬¬å››ç¯èŠ‚<br/>å…¶æ¶ˆå¤±æ˜¯å…¶æˆåŠŸçš„æ ‡å¿—]
    J1 --å…³è”è‡³--> K[çœŸç†çš„å¶ç„¶æ€§ä¸åˆ›ä¼¤æ€§]

    K --é€šè¿‡ç±»æ¯”--> K1[ç±»æ¯”æ ¼é›·é©¬æ–¯ç¬¦å·å­¦çŸ©é˜µ<br/>ä¸æ‹‰åº·ç²¾ç¥åˆ†æ]
    K1 --æ­ç¤ºçœŸç†--> K2[çœŸç†ä½œä¸ºç‰¹æ®Šçš„å¶ç„¶é­é‡<br/>æ‰“ç ´æ™®éçš„è°è¨€]

    %% é¢å¤–çš„åˆ†ææ¡†æ¶
    A --ç†è®ºåŸºç¡€--> L[é»‘æ ¼å°”è¾©è¯æ³•çš„å››é‡ç»“æ„]
    L --åŒ…å«--> L1[1. ç›´æ¥è‚¯å®šæ€§]
    L1 --å¯¼å‘--> L2[2. å†…åœ¨å¦å®šæ€§/ä¸­ä»‹]
    L2 --å‘å±•ä¸º--> L3[3. å¦å®šçš„å¦å®š]
    L3 --å®Œæˆäº--> L4[4. ä¸»ä½“ä½œä¸ºæ¶ˆå¤±çš„ç¯èŠ‚]

    %% å†å²å®ä¾‹çš„è¯¦ç»†åˆ†æ
    E1 --æœºåˆ¶åˆ†æ--> M1[æ–°æ•™ï¼šå®—æ•™æ™®éåŒ–â†’å®—æ•™ç§äººåŒ–]
    E2 --æœºåˆ¶åˆ†æ--> M2[é›…å„å®¾ï¼šæ”¿æ²»æ¿€è¿›åŒ–â†’èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»]
    M1 --> M3[å…±åŒç‚¹ï¼šå½¢å¼ä¸å†…å®¹çš„åˆ†ç¦»]
    M2 --> M3

    %% ç°ä»£ç›¸å…³æ€§
    E3 --å½“ä»£ä¾‹è¯--> N[ä¸œæ¬§æ–°ç¤¾ä¼šè¿åŠ¨]
    N --ç‰¹å¾--> N1[ç†æƒ³ä¸»ä¹‰çš„ç¬¬ä¸‰æ¡é“è·¯]
    N1 --ç»“æœ--> N2[ä¸ºèµ„æœ¬ä¸»ä¹‰å¤è¾Ÿé“ºè·¯]
    N2 --éªŒè¯--> C

    A:::concept
    B:::concept
    C:::concept
    J:::conclusion
    D:::mechanism
    E:::example
    F:::highlight
    H:::concept
    D1:::mechanism
    D2:::mechanism
    D3:::mechanism
    E1:::example
    E2:::example
    E3:::example
    G:::default
    H1:::default
    I:::concept
    H2:::default
    H3:::default
    I1:::default
    I2:::default
    I3:::default
    J1:::default
    K:::concept
    K1:::default
    K2:::default
    L:::theory
    L1:::theory
    L2:::theory
    L3:::theory
    L4:::theory
    M1:::analysis
    M2:::analysis
    M3:::analysis
    N:::modern
    N1:::modern
    N2:::modern`;

  // MindmapStatusDisplay ç»„ä»¶å®šä¹‰
  const MindmapStatusDisplay = () => {
    const getStatusInfo = () => {
      if (demoMindmapStatus === 'generating') {
        return { 
          text: 'åˆ†æä¸­...', 
          color: 'text-yellow-600',
          bgColor: 'bg-yellow-50',
          borderColor: 'border-yellow-200'
        };
      }
      
      if (demoMindmapStatus === 'error') {
        return { 
          text: 'åˆ†æå¤±è´¥', 
          color: 'text-red-600',
          bgColor: 'bg-red-50',
          borderColor: 'border-red-200'
        };
      }
      
      if (demoMindmapStatus === 'completed' && document?.mermaid_code_demo) {
        return { 
          text: 'è®ºè¯ç»“æ„å·²ç”Ÿæˆ', 
          color: 'text-green-600',
          bgColor: 'bg-green-50',
          borderColor: 'border-green-200'
        };
      }
      
      return { 
        text: 'æœªå¼€å§‹', 
        color: 'text-gray-600',
        bgColor: 'bg-gray-50',
        borderColor: 'border-gray-200'
      };
    };

    const statusInfo = getStatusInfo();
    
    return (
      <div className={`inline-flex items-center px-2 py-1 text-xs rounded border ${statusInfo.bgColor} ${statusInfo.color} ${statusInfo.borderColor}`}>
        {demoMindmapStatus === 'generating' && (
          <div className="animate-spin rounded-full h-3 w-3 border-b border-current mr-1"></div>
        )}
        {statusInfo.text}
      </div>
    );
  };

  const startMindmapGeneration = async (method = 'demo') => {
    try {
      // å¦‚æœæ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œç›´æ¥è®¾ç½®æ¼”ç¤ºä»£ç æˆ–è°ƒç”¨API
      if (method === 'demo') {
        setDemoMindmapStatus('generating');
        
        // å¦‚æœæ˜¯çœŸæ­£çš„demoæ–‡æ¡£ï¼ˆä»¥demo-å¼€å¤´ä½†æ˜¯æ—¶é—´æˆ³å½¢å¼ï¼‰ï¼Œç›´æ¥æ˜¾ç¤ºç¤ºä¾‹
        if (documentId.includes(Date.now().toString().slice(0, 8))) {
          // æ¨¡æ‹ŸåŠ è½½è¿‡ç¨‹
          setTimeout(() => {
            setDocument(prev => ({
              ...prev,
              mermaid_code_demo: defaultDemoMermaidCode
            }));
            setDemoMindmapStatus('completed');
            toast.success('è®ºè¯ç»“æ„æµç¨‹å›¾åŠ è½½å®Œæˆï¼');
          }, 1000);
          
          toast.success('æ­£åœ¨åŠ è½½é¢„è®¾çš„è®ºè¯ç»“æ„ç¤ºä¾‹...');
          return;
        }
        
        // å¯¹äºä¸Šä¼ çš„æ–‡ä»¶ï¼Œè°ƒç”¨åç«¯API
        const response = await axios.post(`http://localhost:8000/api/generate-argument-structure/${documentId}`);
        
        if (response.data.success) {
          toast.success('å¼€å§‹åˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„...');
          
          if (response.data.status === 'completed' && response.data.mermaid_code) {
            setDemoMindmapStatus('completed');
            setDocument(prev => ({
              ...prev,
              mermaid_code_demo: response.data.mermaid_code,
              node_mappings_demo: response.data.node_mappings || {}
            }));
            toast.success('è®ºè¯ç»“æ„æµç¨‹å›¾ç”Ÿæˆå®Œæˆï¼');
          }
        } else {
          throw new Error(response.data.message || 'å¼€å§‹åˆ†æå¤±è´¥');
        }
      }
    } catch (error) {
      console.error(`Start argument structure generation error:`, error);
      
      setDemoMindmapStatus('error');
      toast.error('åˆ†æè®ºè¯ç»“æ„å¤±è´¥');
    }
  };

  // æ–‡æ¡£åŠ è½½å®Œæˆåè‡ªåŠ¨å¼€å§‹ç”Ÿæˆè®ºè¯ç»“æ„ï¼ˆåªè¿è¡Œä¸€æ¬¡ï¼‰
  useEffect(() => {
    if (document && !autoStarted && documentId.startsWith('demo-')) {
      setAutoStarted(true);
      setTimeout(() => {
        startMindmapGeneration('demo');
      }, 1000);
    }
  }, [document, autoStarted, documentId]);

  // è½®è¯¢æ£€æŸ¥è®ºè¯ç»“æ„ç”ŸæˆçŠ¶æ€
  useEffect(() => {
    let interval;
    if (demoMindmapStatus === 'generating' && !documentId.includes(Date.now().toString().slice(0, 8))) {
      interval = setInterval(async () => {
        try {
          // å¯¹äºä¸Šä¼ çš„çœŸå®æ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨documentIdï¼ˆå·²ç»ä¸å¸¦demo-å‰ç¼€äº†ï¼‰
          const actualDocumentId = documentId;
            
          const response = await axios.get(`http://localhost:8000/api/document-status/${actualDocumentId}`);
          if (response.data.success) {
            if (response.data.status_demo === 'completed' && response.data.mermaid_code_demo) {
              setDemoMindmapStatus('completed');
              setDocument(prev => ({
                ...prev,
                mermaid_code_demo: response.data.mermaid_code_demo,
                node_mappings_demo: response.data.node_mappings_demo || {}
              }));
              toast.success('è®ºè¯ç»“æ„æµç¨‹å›¾ç”Ÿæˆå®Œæˆï¼');
            } else if (response.data.status_demo === 'error') {
              setDemoMindmapStatus('error');
              toast.error('è®ºè¯ç»“æ„åˆ†æå¤±è´¥');
            }
          }
        } catch (error) {
          console.error('Status polling error:', error);
        }
      }, 2000);
    }

    return () => {
      if (interval) clearInterval(interval);
    };
  }, [demoMindmapStatus, documentId, setDocument]);

  const handleDownloadMarkdown = () => {
    if (!document || !document.content) return;
    
    try {
      const blob = new Blob([document.content], { type: 'text/plain;charset=utf-8' });
      const url = URL.createObjectURL(blob);
      
      if (typeof window !== 'undefined' && window.document && typeof window.document.createElement === 'function') {
        const a = window.document.createElement('a');
        a.href = url;
        a.download = `${document.filename || documentId}_content.md`;
        if (window.document.body) {
          window.document.body.appendChild(a);
          a.click();
          window.document.body.removeChild(a);
        }
      }
      
      URL.revokeObjectURL(url);
      toast.success('Markdownæ–‡æ¡£ä¸‹è½½æˆåŠŸ');
    } catch (error) {
      console.error('Download markdown error:', error);
      toast.error('ä¸‹è½½å¤±è´¥ï¼š' + error.message);
    }
  };

  const handleDownloadMermaid = (mode = 'demo') => {
    if (!document || !document.mermaid_code_demo) return;
    
    try {
      const blob = new Blob([document.mermaid_code_demo], { type: 'text/plain' });
      const url = URL.createObjectURL(blob);
      
      if (typeof window !== 'undefined' && window.document && typeof window.document.createElement === 'function') {
        const a = window.document.createElement('a');
        a.href = url;
        a.download = `${documentId}_argument_structure.mmd`;
        if (window.document.body) {
          window.document.body.appendChild(a);
          a.click();
          window.document.body.removeChild(a);
        }
      }
      
      URL.revokeObjectURL(url);
      toast.success('è®ºè¯ç»“æ„æµç¨‹å›¾ä»£ç ä¸‹è½½æˆåŠŸ');
    } catch (error) {
      toast.error('ä¸‹è½½å¤±è´¥ï¼š' + error.message);
    }
  };

  const handleOpenMermaidEditor = (mode = 'demo') => {
    if (!document || !document.mermaid_code_demo) return;
    
    try {
      const safeBtoa = (str) => {
        return btoa(unescape(encodeURIComponent(str)));
      };
      
      const mermaidConfig = {
        code: document.mermaid_code_demo,
        mermaid: { theme: 'default' }
      };
      
      const configJson = JSON.stringify(mermaidConfig);
      const encodedConfig = safeBtoa(configJson);
      const url = `https://mermaid.live/edit#pako:${encodedConfig}`;
      
      window.open(url, '_blank');
    } catch (error) {
      console.error('Error opening Mermaid editor:', error);
      
          const mermaidEditorUrl = `https://mermaid.live/edit#base64:${encodeURIComponent(document.mermaid_code_demo)}`;
    window.open(mermaidEditorUrl, '_blank');
      
      if (navigator.clipboard && navigator.clipboard.writeText) {
        navigator.clipboard.writeText(document.mermaid_code_demo).then(() => {
          toast.success('æµç¨‹å›¾ä»£ç å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼Œå¯æ‰‹åŠ¨ç²˜è´´åˆ°ç¼–è¾‘å™¨ä¸­');
        }).catch(() => {
          toast.error('æ— æ³•æ‰“å¼€åœ¨çº¿ç¼–è¾‘å™¨ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶ä»£ç ');
        });
      } else {
        toast.error('æ— æ³•æ‰“å¼€åœ¨çº¿ç¼–è¾‘å™¨ï¼Œè¯·ä½¿ç”¨ä¸‹è½½åŠŸèƒ½è·å–ä»£ç ');
      }
    }
  };

  return {
    demoMindmapStatus,
    startMindmapGeneration,
    handleDownloadMarkdown,
    handleDownloadMermaid,
    handleOpenMermaidEditor,
    MindmapStatusDisplay
  };
};
</file>

<file path="frontend/src/index.css">
@import 'tailwindcss/base';
@import 'tailwindcss/components';
@import 'tailwindcss/utilities';

html, body {
  height: 100%;
  margin: 0;
  padding: 0;
}

#root {
  height: 100%;
}

body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}

* {
  box-sizing: border-box;
}
</file>

<file path="frontend/src/utils/api.js">
/**
 * API å·¥å…·å‡½æ•°
 */

const API_BASE_URL = process.env.REACT_APP_API_BASE_URL || '';

/**
 * æ›´æ–°èŠ‚ç‚¹æ ‡ç­¾
 * @param {string} documentId - æ–‡æ¡£ID
 * @param {string} nodeId - èŠ‚ç‚¹ID
 * @param {string} newLabel - æ–°æ ‡ç­¾
 * @returns {Promise<Object>} APIå“åº”
 */
export const updateNodeLabel = async (documentId, nodeId, newLabel) => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${nodeId}/label`, {
      method: 'PATCH',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ newLabel })
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('âŒ [API] æ›´æ–°èŠ‚ç‚¹æ ‡ç­¾å¤±è´¥:', error);
    throw error;
  }
};

/**
 * æ·»åŠ å­èŠ‚ç‚¹
 * @param {string} documentId - æ–‡æ¡£ID
 * @param {string} parentNodeId - çˆ¶èŠ‚ç‚¹ID
 * @param {string} newNodeLabel - æ–°èŠ‚ç‚¹æ ‡ç­¾
 * @returns {Promise<Object>} APIå“åº”
 */
export const addChildNode = async (documentId, parentNodeId, newNodeLabel = 'æ–°èŠ‚ç‚¹') => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${parentNodeId}/child`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ label: newNodeLabel })
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('âŒ [API] æ·»åŠ å­èŠ‚ç‚¹å¤±è´¥:', error);
    throw error;
  }
};

/**
 * æ·»åŠ åŒçº§èŠ‚ç‚¹
 * @param {string} documentId - æ–‡æ¡£ID
 * @param {string} siblingNodeId - åŒçº§èŠ‚ç‚¹ID
 * @param {string} newNodeLabel - æ–°èŠ‚ç‚¹æ ‡ç­¾
 * @returns {Promise<Object>} APIå“åº”
 */
export const addSiblingNode = async (documentId, siblingNodeId, newNodeLabel = 'æ–°èŠ‚ç‚¹') => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${siblingNodeId}/sibling`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ label: newNodeLabel })
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('âŒ [API] æ·»åŠ åŒçº§èŠ‚ç‚¹å¤±è´¥:', error);
    throw error;
  }
};

/**
 * åˆ é™¤èŠ‚ç‚¹
 * @param {string} documentId - æ–‡æ¡£ID
 * @param {string} nodeId - è¦åˆ é™¤çš„èŠ‚ç‚¹ID
 * @returns {Promise<Object>} APIå“åº”
 */
export const deleteNode = async (documentId, nodeId) => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/document/${documentId}/node/${nodeId}`, {
      method: 'DELETE',
      headers: {
        'Content-Type': 'application/json',
      }
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.message || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('âŒ [API] åˆ é™¤èŠ‚ç‚¹å¤±è´¥:', error);
    throw error;
  }
};

/**
 * é€šç”¨ API é”™è¯¯å¤„ç†
 * @param {Error} error - é”™è¯¯å¯¹è±¡
 * @returns {string} ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
 */
export const handleApiError = (error) => {
  if (error.name === 'TypeError' && error.message.includes('fetch')) {
    return 'ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®';
  }
  
  if (error.message.includes('404')) {
    return 'è¯·æ±‚çš„èµ„æºæœªæ‰¾åˆ°';
  }
  
  if (error.message.includes('500')) {
    return 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•';
  }
  
  return error.message || 'æ“ä½œå¤±è´¥ï¼Œè¯·é‡è¯•';
};
</file>

<file path="frontend/src/utils/layoutHelper.js">
import dagre from 'dagre';

// å®šä¹‰èŠ‚ç‚¹çš„é»˜è®¤å°ºå¯¸ï¼Œä¸CSSä¿æŒä¸€è‡´
const DEFAULT_NODE_WIDTH = 200;
const DEFAULT_NODE_HEIGHT = 50;

/**
 * ä½¿ç”¨Dagreç®—æ³•è®¡ç®—èŠ‚ç‚¹å¸ƒå±€
 * @param {Array} nodes - React Flowæ ¼å¼çš„èŠ‚ç‚¹æ•°ç»„
 * @param {Array} edges - React Flowæ ¼å¼çš„è¾¹æ•°ç»„
 * @param {Object} options - å¸ƒå±€é€‰é¡¹
 * @returns {Object} åŒ…å«å¸ƒå±€åçš„nodeså’ŒåŸå§‹edgesçš„å¯¹è±¡
 */
export const getLayoutedElements = (nodes, edges, options = {}) => {
  console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] å¼€å§‹å¸ƒå±€è®¡ç®—');
  console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] è¾“å…¥èŠ‚ç‚¹æ•°é‡:', nodes.length);
  console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] è¾“å…¥è¾¹æ•°é‡:', edges.length);
  console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] å¸ƒå±€é€‰é¡¹:', options);

  if (nodes.length === 0) {
    console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] æ²¡æœ‰èŠ‚ç‚¹ï¼Œè¿”å›ç©ºæ•°ç»„');
    return { nodes: [], edges: [] };
  }

  try {
    // åˆ›å»ºæœ‰å‘å›¾
    const graph = new dagre.graphlib.Graph();
    
    // è®¾ç½®å›¾çš„é»˜è®¤å±æ€§
    graph.setDefaultEdgeLabel(() => ({}));
    graph.setGraph({
      rankdir: options.direction || 'TB', // TB: ä¸Šåˆ°ä¸‹, LR: å·¦åˆ°å³
      nodesep: options.nodeSpacing || 100, // èŠ‚ç‚¹é—´è·
      ranksep: options.rankSpacing || 150, // å±‚çº§é—´è·
      marginx: options.marginX || 50,
      marginy: options.marginY || 50
    });

    // ä½¿ç”¨ä¸CSSä¸€è‡´çš„èŠ‚ç‚¹å°ºå¯¸
    const nodeWidth = options.nodeWidth || DEFAULT_NODE_WIDTH;
    const nodeHeight = options.nodeHeight || DEFAULT_NODE_HEIGHT;

    // æ·»åŠ èŠ‚ç‚¹åˆ°å›¾ä¸­
    nodes.forEach((node) => {
      graph.setNode(node.id, { width: nodeWidth, height: nodeHeight });
    });

    // æ·»åŠ è¾¹åˆ°å›¾ä¸­
    edges.forEach((edge) => {
      graph.setEdge(edge.source, edge.target);
    });

    // è®¡ç®—å¸ƒå±€
    dagre.layout(graph);
    console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] Dagreå¸ƒå±€è®¡ç®—å®Œæˆ');

    // åº”ç”¨è®¡ç®—å‡ºçš„ä½ç½®åˆ°èŠ‚ç‚¹
    const layoutedNodes = nodes.map((node) => {
      const nodeWithPosition = graph.node(node.id);
      
      if (!nodeWithPosition) {
        console.error('ğŸ”§ [å¸ƒå±€è®¡ç®—] èŠ‚ç‚¹ä½ç½®è®¡ç®—å¤±è´¥:', node.id);
        return {
          ...node,
          position: { x: 0, y: 0 }
        };
      }

      const finalPosition = {
        // dagreè¿”å›çš„æ˜¯èŠ‚ç‚¹ä¸­å¿ƒç‚¹åæ ‡ï¼Œéœ€è¦è½¬æ¢ä¸ºå·¦ä¸Šè§’åæ ‡
        // ç¡®ä¿åæ ‡æ˜¯æ•°å­—ç±»å‹
        x: Number(nodeWithPosition.x - nodeWidth / 2),
        y: Number(nodeWithPosition.y - nodeHeight / 2)
      };
      
      return {
        ...node,
        position: finalPosition,
        // ç¡®ä¿React Flowéœ€è¦çš„å…¶ä»–å±æ€§
        width: nodeWidth,
        height: nodeHeight
      };
    });

    console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] å¸ƒå±€è®¡ç®—å®Œæˆï¼Œè¿”å›èŠ‚ç‚¹æ•°é‡:', layoutedNodes.length);
    console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] æ‰€æœ‰èŠ‚ç‚¹ä½ç½®:', layoutedNodes.map(n => ({ id: n.id, position: n.position })));

    return {
      nodes: layoutedNodes,
      edges: edges
    };
  } catch (error) {
    console.error('ğŸ”§ [å¸ƒå±€è®¡ç®—] å¸ƒå±€è®¡ç®—å¤±è´¥:', error);
    // å¦‚æœå¸ƒå±€è®¡ç®—å¤±è´¥ï¼Œè¿”å›èŠ‚ç‚¹çš„é»˜è®¤ä½ç½®
    const fallbackNodes = nodes.map((node, index) => ({
      ...node,
      position: { 
        x: (index % 3) * 250, // ç®€å•çš„ç½‘æ ¼å¸ƒå±€
        y: Math.floor(index / 3) * 150 
      }
    }));
    console.log('ğŸ”§ [å¸ƒå±€è®¡ç®—] ä½¿ç”¨å›é€€å¸ƒå±€:', fallbackNodes.map(n => ({ id: n.id, position: n.position })));
    return {
      nodes: fallbackNodes,
      edges: edges
    };
  }
};

/**
 * é‡æ–°å¸ƒå±€ç°æœ‰çš„å…ƒç´ 
 * @param {Array} nodes - å½“å‰çš„èŠ‚ç‚¹æ•°ç»„
 * @param {Array} edges - å½“å‰çš„è¾¹æ•°ç»„
 * @param {Object} options - å¸ƒå±€é€‰é¡¹
 * @returns {Object} é‡æ–°å¸ƒå±€åçš„nodeså’Œedges
 */
export const relayoutElements = (nodes, edges, options = {}) => {
  return getLayoutedElements(nodes, edges, options);
};
</file>

<file path="mindmap_generator.py">
import re
import os
import random
import json
import time
import asyncio
import hashlib
import base64
import zlib
import logging
import copy
from datetime import datetime
from enum import Enum, auto
from typing import Dict, Any, List, Union, Optional, Tuple, Set
from termcolor import colored
import aiofiles
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from fuzzywuzzy import fuzz
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import Google Generative AI with error handling
try:
    import google.generativeai as genai
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    GOOGLE_AI_AVAILABLE = False
    genai = None

def get_logger():
    """Mindmap-specific logger with colored output for generation stages."""
    logger = logging.getLogger("mindmap_generator")
    if not logger.handlers:
        handler = logging.StreamHandler()
        
        # Custom formatter that adds colors specific to mindmap generation stages
        def colored_formatter(record):
            message = record.msg
            
            # Color-code specific mindmap generation stages and metrics
            if "Starting mindmap generation" in message:
                message = colored("ğŸš€ " + message, "cyan", attrs=["bold"])
            elif "Detected document type:" in message:
                doc_type = message.split(": ")[1]
                message = f"ğŸ“„ Document Type: {colored(doc_type, 'yellow', attrs=['bold'])}"
            elif "Extracting main topics" in message:
                message = colored("ğŸ“Œ " + message, "blue")
            elif "Processing topic" in message:
                # Highlight topic name and progress
                parts = message.split("'")
                if len(parts) >= 3:
                    topic_name = parts[1]
                    message = f"ğŸ” Processing: {colored(topic_name, 'green')} {colored(parts[2], 'white')}"
            elif "Successfully extracted" in message:
                if "topics" in message:
                    message = colored("âœ… " + message, "green")
                elif "subtopics" in message:
                    message = colored("â• " + message, "cyan")
                elif "details" in message:
                    message = colored("ğŸ“ " + message, "blue")
            elif "Approaching word limit" in message:
                message = colored("âš ï¸ " + message, "yellow")
            elif "Error" in message or "Failed" in message:
                message = colored("âŒ " + message, "red", attrs=["bold"])
            elif "Completion status:" in message:
                # Highlight progress metrics
                message = message.replace("Completion status:", colored("ğŸ“Š Progress:", "cyan", attrs=["bold"]))
                metrics = message.split("Progress:")[1]
                parts = metrics.split(",")
                colored_metrics = []
                for part in parts:
                    if ":" in part:
                        label, value = part.split(":")
                        colored_metrics.append(f"{label}:{colored(value, 'yellow')}")
                message = "ğŸ“Š Progress:" + ",".join(colored_metrics)
            elif "Mindmap generation completed" in message:
                message = colored("ğŸ‰ " + message, "green", attrs=["bold"])
                
            # Format timestamp and add any extra attributes
            timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')
            log_message = f"{colored(timestamp, 'white')} {message}"
            
            # Add any extra attributes in grey
            if hasattr(record, 'extra') and record.extra:
                extra_str = ' '.join(f"{k}={v}" for k, v in record.extra.items())
                log_message += f" {colored(f'[{extra_str}]', 'grey')}"
                
            return log_message
            
        class MindmapFormatter(logging.Formatter):
            def format(self, record):
                return colored_formatter(record)
                
        handler.setFormatter(MindmapFormatter())
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger

logger = get_logger()

class Config:
    """Minimal configuration for document processing."""
    # API configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")  # æ·»åŠ è‡ªå®šä¹‰base_urlæ”¯æŒ
    ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')  # Add Gemini API key
    API_PROVIDER = os.getenv('API_PROVIDER') # "OPENAI", "CLAUDE", "DEEPSEEK", or "GEMINI"
    
    # Model settings
    CLAUDE_MODEL_STRING = "claude-3-5-haiku-latest"
    OPENAI_COMPLETION_MODEL = "gpt-4o-mini-2024-07-18"
    DEEPSEEK_COMPLETION_MODEL = "deepseek-chat"  # "deepseek-reasoner" or "deepseek-chat"
    DEEPSEEK_CHAT_MODEL = "deepseek-chat"
    DEEPSEEK_REASONER_MODEL = "deepseek-reasoner"
    GEMINI_MODEL_STRING = "gemini-2.0-flash-lite"  # Add Gemini model string
    CLAUDE_MAX_TOKENS = 200000
    OPENAI_MAX_TOKENS = 8192
    DEEPSEEK_MAX_TOKENS = 8192
    GEMINI_MAX_TOKENS = 8192  # Add Gemini max tokens
    TOKEN_BUFFER = 500
    
    # Cost tracking (prices in USD per token)
    OPENAI_INPUT_TOKEN_PRICE = 0.15/1000000  # GPT-4o-mini input price
    OPENAI_OUTPUT_TOKEN_PRICE = 0.60/1000000  # GPT-4o-mini output price
    ANTHROPIC_INPUT_TOKEN_PRICE = 0.80/1000000  # Claude 3.5 Haiku input price
    ANTHROPIC_OUTPUT_TOKEN_PRICE = 4.00/1000000  # Claude 3.5 Haiku output price
    DEEPSEEK_CHAT_INPUT_PRICE = 0.27/1000000  # Chat input price (cache miss)
    DEEPSEEK_CHAT_OUTPUT_PRICE = 1.10/1000000  # Chat output price
    DEEPSEEK_REASONER_INPUT_PRICE = 0.14/1000000  # Reasoner input price (cache miss)
    DEEPSEEK_REASONER_OUTPUT_PRICE = 2.19/1000000  # Reasoner output price (includes CoT)
    GEMINI_INPUT_TOKEN_PRICE = 0.075/1000000  # Gemini 2.0 Flash Lite input price estimate
    GEMINI_OUTPUT_TOKEN_PRICE = 0.30/1000000  # Gemini 2.0 Flash Lite output price estimate

class TokenUsageTracker:
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost = 0
        self.call_counts = {}
        self.token_counts_by_task = {}
        self.cost_by_task = {}
        
        # Categorize tasks for better reporting
        self.task_categories = {
            'topics': ['extracting_main_topics', 'consolidating_topics', 'detecting_document_type'],
            'subtopics': ['extracting_subtopics', 'consolidate_subtopics'],
            'details': ['extracting_details', 'consolidate_details'],
            'similarity': ['checking_content_similarity'],
            'verification': ['verifying_against_source'],
            'emoji': ['selecting_emoji'],
            'other': []  # Catch-all for uncategorized tasks
        }
        
        # Initialize counters for each category
        self.call_counts_by_category = {category: 0 for category in self.task_categories}
        self.token_counts_by_category = {category: {'input': 0, 'output': 0} for category in self.task_categories}
        self.cost_by_category = {category: 0 for category in self.task_categories}
        
    def update(self, input_tokens: int, output_tokens: int, task: str):
        """Update token usage with enhanced task categorization."""
        # Update base metrics
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        
        # Calculate cost based on provider
        task_cost = 0
        if Config.API_PROVIDER == "CLAUDE":
            task_cost = (
                input_tokens * Config.ANTHROPIC_INPUT_TOKEN_PRICE + 
                output_tokens * Config.ANTHROPIC_OUTPUT_TOKEN_PRICE
            )
        elif Config.API_PROVIDER == "DEEPSEEK":
            # Different pricing for chat vs reasoner model
            if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
                task_cost = (
                    input_tokens * Config.DEEPSEEK_CHAT_INPUT_PRICE + 
                    output_tokens * Config.DEEPSEEK_CHAT_OUTPUT_PRICE
                )
            else:  # reasoner model
                task_cost = (
                    input_tokens * Config.DEEPSEEK_REASONER_INPUT_PRICE + 
                    output_tokens * Config.DEEPSEEK_REASONER_OUTPUT_PRICE
                )
        elif Config.API_PROVIDER == "GEMINI":
            task_cost = (
                input_tokens * Config.GEMINI_INPUT_TOKEN_PRICE + 
                output_tokens * Config.GEMINI_OUTPUT_TOKEN_PRICE
            )
        else:  # OPENAI
            task_cost = (
                input_tokens * Config.OPENAI_INPUT_TOKEN_PRICE + 
                output_tokens * Config.OPENAI_OUTPUT_TOKEN_PRICE
            )
            
        self.total_cost += task_cost
        
        # Update task-specific metrics
        if task not in self.token_counts_by_task:
            self.token_counts_by_task[task] = {'input': 0, 'output': 0}
            self.cost_by_task[task] = 0
            
        self.token_counts_by_task[task]['input'] += input_tokens
        self.token_counts_by_task[task]['output'] += output_tokens
        self.call_counts[task] = self.call_counts.get(task, 0) + 1
        self.cost_by_task[task] = self.cost_by_task.get(task, 0) + task_cost
        
        # Update category metrics
        category_found = False
        for category, tasks in self.task_categories.items():
            if any(task.startswith(t) for t in tasks) or (category == 'other' and not category_found):
                self.call_counts_by_category[category] += 1
                self.token_counts_by_category[category]['input'] += input_tokens
                self.token_counts_by_category[category]['output'] += output_tokens
                self.cost_by_category[category] += task_cost
                category_found = True
                break
    
    def get_enhanced_summary(self) -> Dict[str, Any]:
        """Get enhanced usage summary with category breakdowns and percentages."""
        total_calls = sum(self.call_counts.values())
        total_cost = sum(self.cost_by_task.values())
        
        # Calculate percentages for call counts by category
        call_percentages = {}
        for category, count in self.call_counts_by_category.items():
            call_percentages[category] = (count / total_calls * 100) if total_calls > 0 else 0
            
        # Calculate percentages for token counts by category
        token_percentages = {}
        for category, counts in self.token_counts_by_category.items():
            total_tokens = counts['input'] + counts['output']
            token_percentages[category] = (total_tokens / (self.total_input_tokens + self.total_output_tokens) * 100) if (self.total_input_tokens + self.total_output_tokens) > 0 else 0
            
        # Calculate percentages for cost by category
        cost_percentages = {}
        for category, cost in self.cost_by_category.items():
            cost_percentages[category] = (cost / total_cost * 100) if total_cost > 0 else 0
        
        return {
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": self.total_input_tokens + self.total_output_tokens,
            "total_cost_usd": round(self.total_cost, 6),
            "total_calls": total_calls,
            "calls_by_task": dict(self.call_counts),
            "token_counts_by_task": self.token_counts_by_task,
            "cost_by_task": {task: round(cost, 6) for task, cost in self.cost_by_task.items()},
            "categories": {
                category: {
                    "calls": count,
                    "calls_percentage": round(call_percentages[category], 2),
                    "tokens": self.token_counts_by_category[category],
                    "tokens_percentage": round(token_percentages[category], 2),
                    "cost_usd": round(self.cost_by_category[category], 6),
                    "cost_percentage": round(cost_percentages[category], 2)
                }
                for category, count in self.call_counts_by_category.items()
            }
        }
        
    def print_usage_report(self):
        """Print a detailed usage report to the console."""
        summary = self.get_enhanced_summary()
        
        # Helper to format USD amounts
        def fmt_usd(amount):
            return f"${amount:.6f}"
        
        # Helper to format percentages
        def fmt_pct(percentage):
            return f"{percentage:.2f}%"
        
        # Helper to format numbers with commas
        def fmt_num(num):
            return f"{num:,}"
        
        # Find max task name length for proper column alignment
        max_task_length = max([len(task) for task in summary['calls_by_task'].keys()], default=30)
        task_col_width = max(max_task_length + 2, 30)
        
        report = [
            "\n" + "="*80,
            colored("ğŸ“Š TOKEN USAGE AND COST REPORT", "cyan", attrs=["bold"]),
            "="*80,
            "",
            f"Total Tokens: {fmt_num(summary['total_tokens'])} (Input: {fmt_num(summary['total_input_tokens'])}, Output: {fmt_num(summary['total_output_tokens'])})",
            f"Total Cost: {fmt_usd(summary['total_cost_usd'])}",
            f"Total API Calls: {fmt_num(summary['total_calls'])}",
            "",
            colored("BREAKDOWN BY CATEGORY", "yellow", attrs=["bold"]),
            "-"*80,
            "Category".ljust(15) + "Calls".rjust(10) + "Call %".rjust(10) + "Tokens".rjust(12) + "Token %".rjust(10) + "Cost".rjust(12) + "Cost %".rjust(10),
            "-"*80
        ]
        
        for category, data in summary['categories'].items():
            if data['calls'] > 0:
                tokens = data['tokens']['input'] + data['tokens']['output']
                report.append(
                    category.ljust(15) + 
                    fmt_num(data['calls']).rjust(10) + 
                    fmt_pct(data['calls_percentage']).rjust(10) + 
                    fmt_num(tokens).rjust(12) + 
                    fmt_pct(data['tokens_percentage']).rjust(10) + 
                    fmt_usd(data['cost_usd']).rjust(12) + 
                    fmt_pct(data['cost_percentage']).rjust(10)
                )
                
        report.extend([
            "-"*80,
            "",
            colored("DETAILED BREAKDOWN BY TASK", "yellow", attrs=["bold"]),
            "-"*80,
            "Task".ljust(task_col_width) + "Calls".rjust(8) + "Input".rjust(12) + "Output".rjust(10) + "Cost".rjust(12),
            "-"*80
        ])
        
        # Sort tasks by cost (highest first)
        sorted_tasks = sorted(
            summary['cost_by_task'].items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        for task, cost in sorted_tasks:
            if cost > 0:
                report.append(
                    task.ljust(task_col_width) + 
                    fmt_num(summary['calls_by_task'][task]).rjust(8) + 
                    fmt_num(summary['token_counts_by_task'][task]['input']).rjust(12) + 
                    fmt_num(summary['token_counts_by_task'][task]['output']).rjust(10) + 
                    fmt_usd(cost).rjust(12)
                )
                
        report.extend([
            "-"*80,
            "",
            f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "="*80,
        ])
        
        logger.info("\n".join(report))
        
class DocumentOptimizer:
    """Minimal document optimizer that only implements what's needed for mindmap generation."""
    def __init__(self):
        self.openai_client = AsyncOpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        self.anthropic_client = AsyncAnthropic(api_key=Config.ANTHROPIC_API_KEY)
        self.deepseek_client = AsyncOpenAI(
            api_key=Config.DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        # Initialize Google GenAI client only if needed
        self.gemini_client = None
        if Config.API_PROVIDER == "GEMINI" and Config.GEMINI_API_KEY and GOOGLE_AI_AVAILABLE:
            try:
                # Configure Google Generative AI
                genai.configure(api_key=Config.GEMINI_API_KEY)
                # Create a GenerativeModel instance
                self.gemini_client = genai.GenerativeModel(Config.GEMINI_MODEL_STRING)
                logger.info("Gemini API client initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize Gemini client: {e}")
        elif Config.API_PROVIDER == "GEMINI" and not GOOGLE_AI_AVAILABLE:
            logger.error("Gemini API provider selected but google-generativeai package not installed")
        elif Config.API_PROVIDER == "GEMINI" and not Config.GEMINI_API_KEY:
            logger.error("Gemini API provider selected but no API key provided")
        self.token_tracker = TokenUsageTracker()
        
    async def generate_completion(self, prompt: str, max_tokens: int = 5000, request_id: str = None, task: Optional[str] = None) -> Optional[str]:
        try:
            # Log the start of the request with truncated prompt
            prompt_preview = " ".join(prompt.split()[:40])  # Get first 40 words
            logger.info(
                f"\n{colored('ğŸ”„ API Request', 'cyan', attrs=['bold'])}\n"
                f"Task: {colored(task or 'unknown', 'yellow')}\n"
                f"Provider: {colored(Config.API_PROVIDER, 'blue')}\n"
                f"Prompt preview: {colored(prompt_preview + '...', 'white')}"
            )
            if Config.API_PROVIDER == "CLAUDE":
                async with self.anthropic_client.messages.stream(
                    model=Config.CLAUDE_MODEL_STRING,
                    max_tokens=max_tokens,
                    temperature=0.7,
                    messages=[{"role": "user", "content": prompt}]
                ) as stream:
                    message = await stream.get_final_message()
                    response_preview = " ".join(message.content[0].text.split()[:30])
                    self.token_tracker.update(
                        message.usage.input_tokens,
                        message.usage.output_tokens,
                        task or "unknown"
                    )
                    logger.info(
                        f"\n{colored('âœ… API Response', 'green', attrs=['bold'])}\n"
                        f"Response preview: {colored(response_preview + '...', 'white')}\n"
                        f"Tokens: {colored(f'Input={message.usage.input_tokens}, Output={message.usage.output_tokens}', 'yellow')}"
                    )
                    return message.content[0].text
            elif Config.API_PROVIDER == "DEEPSEEK":
                kwargs = {
                    "model": Config.DEEPSEEK_COMPLETION_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "stream": False
                }
                if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
                    kwargs["temperature"] = 0.7
                response = await self.deepseek_client.chat.completions.create(**kwargs)
                response_preview = " ".join(response.choices[0].message.content.split()[:30])
                self.token_tracker.update(
                    response.usage.prompt_tokens,
                    response.usage.completion_tokens,
                    task or "unknown"
                )
                logger.info(
                    f"\n{colored('âœ… API Response', 'green', attrs=['bold'])}\n"
                    f"Response preview: {colored(response_preview + '...', 'white')}\n"
                    f"Tokens: {colored(f'Input={response.usage.prompt_tokens}, Output={response.usage.completion_tokens}', 'yellow')}"
                )
                return response.choices[0].message.content
            elif Config.API_PROVIDER == "GEMINI":
                if not self.gemini_client:
                    logger.error("Gemini client not initialized")
                    return None
                
                try:
                    # Generate content using Gemini model
                    response = await asyncio.to_thread(
                        self.gemini_client.generate_content,
                        prompt,
                        generation_config=genai.GenerationConfig(
                            max_output_tokens=min(max_tokens, Config.GEMINI_MAX_TOKENS),
                            temperature=0.7,
                        )
                    )
                    
                    response_text = response.text
                    response_preview = " ".join(response_text.split()[:30])
                    
                    # Extract token usage from response metadata
                    input_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0) if hasattr(response, 'usage_metadata') else 0
                    output_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0) if hasattr(response, 'usage_metadata') else 0
                    
                    self.token_tracker.update(
                        input_tokens,
                        output_tokens,
                        task or "unknown"
                    )
                    
                    logger.info(
                        f"\n{colored('âœ… API Response', 'green', attrs=['bold'])}\n"
                        f"Response preview: {colored(response_preview + '...', 'white')}\n"
                        f"Tokens: {colored(f'Input={input_tokens}, Output={output_tokens}', 'yellow')}"
                    )
                    
                    return response_text
                    
                except Exception as e:
                    logger.error(f"Gemini API error: {str(e)}")
                    return None
            elif Config.API_PROVIDER == "OPENAI":
                response = await self.openai_client.chat.completions.create(
                    model=Config.OPENAI_COMPLETION_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                response_preview = " ".join(response.choices[0].message.content.split()[:30])
                self.token_tracker.update(
                    response.usage.prompt_tokens,
                    response.usage.completion_tokens,
                    task or "unknown"
                )
                logger.info(
                    f"\n{colored('âœ… API Response', 'green', attrs=['bold'])}\n"
                    f"Response preview: {colored(response_preview + '...', 'white')}\n"
                    f"Tokens: {colored(f'Input={response.usage.prompt_tokens}, Output={response.usage.completion_tokens}', 'yellow')}"
                )
                return response.choices[0].message.content
            else:
                raise ValueError(f"Invalid API_PROVIDER: {Config.API_PROVIDER}")
        except Exception as e:
            logger.error(
                f"\n{colored('âŒ API Error', 'red', attrs=['bold'])}\n"
                f"Error: {colored(str(e), 'red')}"
            )
            return None
    
class MinimalDatabaseStub:
    """Minimal database stub that provides just enough for the mindmap generator."""
    @staticmethod
    async def get_document_by_id(document_id: str) -> Dict[str, Any]:
        """Stub that returns minimal document info."""
        return {
            "id": document_id,
            "original_file_name": f"document_{document_id}.txt",
            "sanitized_filename": document_id,
            "status": "processing",
            "progress_percentage": 0
        }
        
    @staticmethod
    async def get_optimized_text(document_id: str, request_id: str) -> Optional[str]:
        """In our simplified version, this just returns the raw text content."""
        return MinimalDatabaseStub._stored_text
        
    @staticmethod
    async def update_document_status(*args, **kwargs) -> Dict[str, Any]:
        """Stub that just returns success."""
        return {"status": "success"}
        
    @staticmethod
    async def add_token_usage(*args, **kwargs) -> None:
        """Stub that does nothing."""
        pass

    # Add a way to store the text content
    _stored_text = ""
    
    @classmethod
    def store_text(cls, text: str):
        """Store text content for later retrieval."""
        cls._stored_text = text

async def initialize_db():
    """Minimal DB initialization that just returns our stub."""
    return MinimalDatabaseStub()

class DocumentType(Enum):
    """Enumeration of supported document types."""
    TECHNICAL = auto()
    SCIENTIFIC = auto()
    NARRATIVE = auto()
    BUSINESS = auto()
    ACADEMIC = auto()
    LEGAL = auto()      
    MEDICAL = auto()    
    INSTRUCTIONAL = auto() 
    ANALYTICAL = auto() 
    PROCEDURAL = auto() 
    GENERAL = auto()

    @classmethod
    def from_str(cls, value: str) -> 'DocumentType':
        """Convert string to DocumentType enum."""
        try:
            return cls[value.upper()]
        except KeyError:
            return cls.GENERAL

class NodeShape(Enum):
    """Enumeration of node shapes for the mindmap structure."""
    ROOT = '(())'        # Double circle for root node (ğŸ“„)
    TOPIC = '(())'       # Double circle for main topics
    SUBTOPIC = '()'      # Single circle for subtopics
    DETAIL = '[]'        # Square brackets for details

    def apply(self, text: str) -> str:
        """Apply the shape to the text."""
        return {
            self.ROOT: f"(({text}))",
            self.TOPIC: f"(({text}))",
            self.SUBTOPIC: f"({text})",
            self.DETAIL: f"[{text}]"
        }[self]

class MindMapGenerationError(Exception):
    """Custom exception for mindmap generation errors."""
    pass

class ContentItem:
    """Class to track content items with their context information."""
    def __init__(self, text: str, path: List[str], node_type: str, importance: str = None):
        self.text = text
        self.path = path
        self.path_str = ' â†’ '.join(path)
        self.node_type = node_type
        self.importance = importance
        
    def __str__(self):
        return f"{self.text} ({self.node_type} at {self.path_str})"

class MindMapGenerator:
    def __init__(self):
        self.optimizer = DocumentOptimizer()
        self.config = {
            'max_summary_length': 2500,
            'max_tokens': 3000,
            'valid_types': [t.name.lower() for t in DocumentType],
            'default_type': DocumentType.GENERAL.name.lower(),
            'max_retries': 3,
            'request_timeout': 30,  # seconds
            'chunk_size': 8192,     # bytes for file operations
            'max_topics': 6,        # Maximum main topics
            'max_subtopics': 4,     # Maximum subtopics per topic
            'max_details': 8,       # Maximum details per subtopic
            'similarity_threshold': {
                'topic': 75,        # Allow more diverse main topics
                'subtopic': 70,     # Allow more nuanced subtopics
                'detail': 65        # Allow more specific details
            },
            'reality_check': {
                'batch_size': 8,    # Number of nodes to verify in parallel
                'min_verified_topics': 4,  # Minimum verified topics needed
                'min_verified_ratio': 0.6  # Minimum ratio of verified content
            }
        }
        self.verification_stats = {
            'total_nodes': 0,
            'verified_nodes': 0,
            'topics': {'total': 0, 'verified': 0},
            'subtopics': {'total': 0, 'verified': 0},
            'details': {'total': 0, 'verified': 0}
        }
        self._emoji_cache = {}
        self.retry_config = {
            'max_retries': 3,
            'base_delay': 1,
            'max_delay': 10,
            'jitter': 0.1,
            'timeout': 30
        }
        self._initialize_prompts()
        self.numbered_pattern = re.compile(r'^\s*\d+\.\s*(.+)$')
        self.parentheses_regex = re.compile(r'(\((?!\()|(?<!\))\))')
        self.control_chars_regex = re.compile(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]')
        self.unescaped_quotes_regex = re.compile(r'(?<!\\)"(?!,|\s*[}\]])')
        self.percentage_regex1 = re.compile(r'(\d+(?:\.\d+)?)\s+(?=percent|of\s|share|margin|CAGR)', re.IGNORECASE)
        self.percentage_regex2 = re.compile(r'\s+percent\b', re.IGNORECASE)
        self.backslash_regex = re.compile(r'\\{2,}')
        self.special_chars_regex = re.compile(r'[^a-zA-Z0-9\s\[\]\(\)\{\}\'_\-.,`*%\\]')
        self.paren_replacements = {
            '(': 'â¨',  # U+2768 MEDIUM LEFT PARENTHESIS ORNAMENT
            ')': 'â©',  # U+2769 MEDIUM RIGHT PARENTHESIS ORNAMENT
            # Backup alternatives if needed:
            # '(': 'âŸ®',  # U+27EE MATHEMATICAL LEFT FLATTENED PARENTHESIS
            # ')': 'âŸ¯',  # U+27EF MATHEMATICAL RIGHT FLATTENED PARENTHESIS
            # Or:
            # '(': 'ï¹™',  # U+FE59 SMALL LEFT PARENTHESIS
            # ')': 'ï¹š',  # U+FE5A SMALL RIGHT PARENTHESIS
        }
        self._emoji_file = os.path.join(os.path.dirname(__file__), "emoji_cache.json")
        self._load_emoji_cache()
        
    def _load_emoji_cache(self):
        """Load emoji cache from disk if available."""
        try:
            if os.path.exists(self._emoji_file):
                with open(self._emoji_file, 'r', encoding='utf-8') as f:
                    loaded_cache = json.load(f)
                    # Convert tuple string keys back to actual tuples
                    self._emoji_cache = {tuple(eval(k)): v for k, v in loaded_cache.items()}
                    logger.info(f"Loaded {len(self._emoji_cache)} emoji mappings from cache")
            else:
                self._emoji_cache = {}
        except Exception as e:
            logger.warning(f"Failed to load emoji cache: {str(e)}")
            self._emoji_cache = {}

    def _save_emoji_cache(self):
        """Save emoji cache to disk for reuse across runs."""
        try:
            # Convert tuple keys to strings for JSON serialization
            serializable_cache = {str(k): v for k, v in self._emoji_cache.items()}
            with open(self._emoji_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_cache, f)
            logger.info(f"Saved {len(self._emoji_cache)} emoji mappings to cache")
        except Exception as e:
            logger.warning(f"Failed to save emoji cache: {str(e)}")
                
    async def _retry_with_exponential_backoff(self, func, *args, **kwargs):
        """Enhanced retry mechanism with jitter and circuit breaker."""
        retries = 0
        max_retries = self.retry_config['max_retries']
        base_delay = self.retry_config['base_delay']
        max_delay = self.retry_config['max_delay']
        
        while retries < max_retries:
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                retries += 1
                if retries >= max_retries:
                    raise
                    
                delay = min(base_delay * (2 ** (retries - 1)), max_delay)
                actual_delay = random.uniform(0, delay)
                
                logger.warning(f"Attempt {retries}/{max_retries} failed: {str(e)}. "
                            f"Retrying in {actual_delay:.2f}s")
                
                await asyncio.sleep(actual_delay)

    def _validate_parsed_response(self, parsed: Any, expected_type: str) -> Union[List[Any], Dict[str, Any]]:
        """Validate and normalize parsed JSON response."""
        if expected_type == "array":
            if isinstance(parsed, list):
                return parsed
            elif isinstance(parsed, dict):
                # Try to extract array from common fields
                for key in ['items', 'topics', 'elements', 'data']:
                    if isinstance(parsed.get(key), list):
                        return parsed[key]
                logger.debug("No array found in dictionary fields")
                return []
            else:
                logger.debug(f"Unexpected type for array response: {type(parsed)}")
                return []
        
        return parsed if isinstance(parsed, dict) else {}

    def _clean_detail_response(self, response: str) -> List[Dict[str, str]]:
        """Clean and validate detail responses."""
        try:
            # Remove markdown code blocks if present
            if '```' in response:
                matches = re.findall(r'```(?:json)?(.*?)```', response, re.DOTALL)
                if matches:
                    response = matches[0].strip()
                    
            # Basic cleanup
            response = response.strip()
            
            try:
                parsed = json.loads(response)
            except json.JSONDecodeError:
                # Try cleaning quotes and parse again
                response = response.replace("'", '"')
                try:
                    parsed = json.loads(response)
                except json.JSONDecodeError:
                    return []
                    
            # Handle both array and single object responses
            if isinstance(parsed, dict):
                parsed = [parsed]
                
            # Validate each detail
            valid_details = []
            seen_texts = set()
            
            for item in parsed:
                try:
                    text = str(item.get('text', '')).strip()
                    importance = str(item.get('importance', 'medium')).lower()
                    
                    # Skip empty text or duplicates
                    if not text or text in seen_texts:
                        continue
                        
                    if importance not in ['high', 'medium', 'low']:
                        importance = 'medium'
                        
                    seen_texts.add(text)
                    valid_details.append({
                        'text': text,
                        'importance': importance
                    })
                    
                except Exception as e:
                    logger.debug(f"Error processing detail item: {str(e)}")
                    continue
                    
            return valid_details
            
        except Exception as e:
            logger.error(f"Error in detail cleaning: {str(e)}")
            return []

    def _clean_json_response(self, response: str) -> str:
        """Enhanced JSON response cleaning with advanced recovery and validation."""
        if not response or not isinstance(response, str):
            logger.warning("Empty or invalid response type received")
            return "[]"  # Return empty array as safe default
            
        try:
            # First try to find complete JSON structure
            def find_json_structure(text: str) -> Optional[str]:
                # Look for array pattern
                array_match = re.search(r'\[[\s\S]*?\](?=\s*$|\s*[,}\]])', text)
                if array_match:
                    return array_match.group(0)
                    
                # Look for object pattern
                object_match = re.search(r'\{[\s\S]*?\}(?=\s*$|\s*[,\]}])', text)
                if object_match:
                    return object_match.group(0)
                
                return None

            # Handle markdown code blocks first
            if '```' in response:
                code_blocks = re.findall(r'```(?:json)?([\s\S]*?)```', response)
                if code_blocks:
                    for block in code_blocks:
                        if json_struct := find_json_structure(block):
                            response = json_struct
                            break
            else:
                if json_struct := find_json_structure(response):
                    response = json_struct

            # Advanced character cleaning
            def clean_characters(text: str) -> str:
                # Remove control characters while preserving valid whitespace
                text = self.control_chars_regex.sub('', text)
                
                # Normalize quotes and apostrophes
                text = text.replace('"', '"').replace('"', '"')  # Smart double quotes to straight double quotes
                text = text.replace("'", "'").replace("'", "'")  # Smart single quotes to straight single quotes
                text = text.replace("'", '"')  # Convert single quotes to double quotes
                
                # Normalize whitespace
                text = ' '.join(text.split())
                
                # Escape unescaped quotes within strings
                text = self.unescaped_quotes_regex.sub('\\"', text)
                
                return text

            response = clean_characters(response)

            # Fix common JSON syntax issues
            def fix_json_syntax(text: str) -> str:
                # Fix trailing/multiple commas
                text = re.sub(r',\s*([\]}])', r'\1', text)  # Remove trailing commas
                text = re.sub(r',\s*,', ',', text)  # Remove multiple commas
                
                # Fix missing quotes around keys
                text = re.sub(r'(\{|\,)\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', text)
                
                # Ensure proper array/object closure
                brackets_stack = []
                for char in text:
                    if char in '[{':
                        brackets_stack.append(char)
                    elif char in ']}':
                        if not brackets_stack:
                            continue  # Skip unmatched closing brackets
                        if (char == ']' and brackets_stack[-1] == '[') or (char == '}' and brackets_stack[-1] == '{'):
                            brackets_stack.pop()
                        
                # Close any unclosed brackets
                while brackets_stack:
                    text += ']' if brackets_stack.pop() == '[' else '}'
                
                return text

            response = fix_json_syntax(response)

            # Validate and normalize structure
            def normalize_structure(text: str) -> str:
                try:
                    # Try parsing to validate
                    parsed = json.loads(text)
                    
                    # Ensure we have an array
                    if isinstance(parsed, dict):
                        # Convert single object to array
                        return json.dumps([parsed])
                    elif isinstance(parsed, list):
                        return json.dumps(parsed)
                    else:
                        return json.dumps([str(parsed)])
                        
                except json.JSONDecodeError:
                    # If still invalid, attempt emergency recovery
                    if text.strip().startswith('{'):
                        return f"[{text.strip()}]"  # Wrap object in array
                    elif not text.strip().startswith('['):
                        return f"[{text.strip()}]"  # Wrap content in array
                    return text
            
            response = normalize_structure(response)

            # Final validation
            try:
                json.loads(response)  # Verify we have valid JSON
                return response
            except json.JSONDecodeError as e:
                logger.warning(f"Final JSON validation failed: {str(e)}")
                # If all cleaning failed, return empty array
                return "[]"

        except Exception as e:
            logger.error(f"Error during JSON response cleaning: {str(e)}")
            return "[]"

    def _parse_llm_response(self, response: str, expected_type: str = "array") -> Union[List[Any], Dict[str, Any]]:
        """Parse and validate LLM response."""
        if not response or not isinstance(response, str):
            logger.warning("Empty or invalid response type received")
            return [] if expected_type == "array" else {}

        try:
            # Extract JSON from markdown code blocks if present
            if '```' in response:
                matches = re.findall(r'```(?:json)?(.*?)```', response, re.DOTALL)
                if matches:
                    response = matches[0].strip()

            # Basic cleanup
            response = response.strip()
            
            try:
                parsed = json.loads(response)
                return self._validate_parsed_response(parsed, expected_type)
            except json.JSONDecodeError:
                # Try cleaning quotes and parse again
                response = response.replace("'", '"')
                try:
                    parsed = json.loads(response)
                    return self._validate_parsed_response(parsed, expected_type)
                except json.JSONDecodeError:
                    # If we still can't parse, try emergency extraction for arrays
                    if expected_type == "array":
                        items = re.findall(r'"([^"]+)"', response)
                        if items:
                            return items

                        # Try line-by-line extraction
                        lines = response.strip().split('\n')
                        items = [line.strip().strip(',"\'[]{}') for line in lines 
                                if line.strip() and not line.strip().startswith(('```', '{', '}'))]
                        if items:
                            return items

                    return [] if expected_type == "array" else {}

        except Exception as e:
            logger.error(f"Unexpected error in JSON parsing: {str(e)}")
            return [] if expected_type == "array" else {}

    def _get_importance_marker(self, importance: str) -> str:
        """Get the appropriate diamond marker based on importance level."""
        markers = {
            'high': 'â™¦ï¸',    # Red diamond for high importance
            'medium': 'ğŸ”¸',  # Orange diamond for medium importance
            'low': 'ğŸ”¹'      # Blue diamond for low importance
        }
        return markers.get(importance.lower(), 'ğŸ”¹')

    async def _save_emoji_cache_async(self):
        """Asynchronous version of save_emoji_cache to avoid blocking."""
        try:
            # Convert to a non-blocking call
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._save_emoji_cache)
        except Exception as e:
            logger.warning(f"Failed to save emoji cache asynchronously: {str(e)}")
            
    async def _select_emoji(self, text: str, node_type: str = 'topic') -> str:
        """Select appropriate emoji for node content with persistent cache."""
        cache_key = (text, node_type)
        
        # First check in-memory cache
        if cache_key in self._emoji_cache:
            return self._emoji_cache[cache_key]
            
        # If not in cache, generate emoji
        try:
            prompt = f"""Select the single most appropriate emoji to represent this {node_type}: "{text}"

            Requirements:
            1. Return ONLY the emoji character - no explanations or other text
            2. Choose an emoji that best represents the concept semantically
            3. For abstract concepts, use metaphorical or symbolic emojis
            4. Default options if unsure:
            - Topics: ğŸ“„ (document)
            - Subtopics: ğŸ“Œ (pin)
            - Details: ğŸ”¹ (bullet point)
            5. Be creative but clear - the emoji should intuitively represent the concept

            Examples:
            - "Market Growth" â†’ ğŸ“ˆ
            - "Customer Service" â†’ ğŸ‘¥
            - "Financial Report" â†’ ğŸ’°
            - "Product Development" â†’ âš™ï¸
            - "Global Expansion" â†’ ğŸŒ
            - "Research and Development" â†’ ğŸ”¬
            - "Digital Transformation" â†’ ğŸ’»
            - "Supply Chain" â†’ ğŸ”„
            - "Healthcare Solutions" â†’ ğŸ¥
            - "Security Measures" â†’ ğŸ”’

            Return ONLY the emoji character without any explanation."""
            
            response = await self._retry_generate_completion(
                prompt,
                max_tokens=20,
                request_id='',
                task="selecting_emoji"
            )
            
            # Clean the response to get just the emoji
            emoji = response.strip()
            
            # If no emoji was returned or response is too long, use defaults
            if not emoji or len(emoji) > 4:  # Most emojis are 2 chars, some are 4
                defaults = {
                    'topic': 'ğŸ“„',
                    'subtopic': 'ğŸ“Œ',
                    'detail': 'ğŸ”¹'
                }
                emoji = defaults.get(node_type, 'ğŸ“„')
                
            # Add to in-memory cache
            self._emoji_cache[cache_key] = emoji
            
            # Save cache to disk periodically (every 10 new emojis)
            if len(self._emoji_cache) % 10 == 0:
                await asyncio.create_task(self._save_emoji_cache_async())
                
            return emoji
            
        except Exception as e:
            logger.warning(f"Error selecting emoji: {str(e)}")
            return 'ğŸ“„' if node_type == 'topic' else 'ğŸ“Œ' if node_type == 'subtopic' else 'ğŸ”¹'

    def _initialize_prompts(self) -> None:
        """Initialize type-specific prompts from a configuration file or define them inline."""
        self.type_specific_prompts = {
            DocumentType.TECHNICAL: {
                'topics': """Analyze this technical document focusing on core system components and relationships.
                
    First, identify the major architectural or technical components that form complete, independent units of functionality.
    Each component should be:
    - A distinct technical system, module, or process
    - Independent enough to be understood on its own
    - Critical to the overall system functionality
    - Connected to at least one other component

    Avoid topics that are:
    - Too granular (implementation details)
    - Too broad (entire system categories)
    - Isolated features without system impact
    - Pure documentation elements

    Think about:
    1. What are the core building blocks?
    2. How do these pieces fit together?
    3. What dependencies exist between components?
    4. What are the key technical boundaries?

    Format: Return a JSON array of component names that represent the highest-level technical building blocks.""",

                'subtopics': """For the technical component '{topic}', identify its essential sub-components and interfaces.

    Each subtopic should:
    - Represent a crucial aspect of this component
    - Have clear technical responsibilities
    - Interface with other parts of the system
    - Contribute to the component's core purpose

    Consider:
    1. What interfaces does this component expose?
    2. What are its internal subsystems?
    3. How does it process data or handle requests?
    4. What services does it provide to other components?
    5. What technical standards or protocols does it implement?

    Format: Return a JSON array of technical subtopic names that form this component's architecture.""",

                'details': """For the technical subtopic '{subtopic}', identify specific implementation aspects and requirements.

    Focus on:
    1. Key algorithms or methods
    2. Data structures and formats
    3. Protocol specifications
    4. Performance characteristics
    5. Error handling approaches
    6. Security considerations
    7. Dependencies and requirements

    Include concrete technical details that are:
    - Implementation-specific
    - Measurable or testable
    - Critical for understanding
    - Relevant to integration

    Format: Return a JSON array of technical specifications and implementation details."""
            },

            DocumentType.SCIENTIFIC: {
                'topics': """Analyze this scientific document focusing on major research components and methodological frameworks.

    Identify main scientific themes that:
    - Represent complete experimental or theoretical units
    - Follow scientific method principles
    - Support the research objectives
    - Build on established scientific concepts

    Consider:
    1. What are the primary research questions?
    2. What methodological approaches are used?
    3. What theoretical frameworks are applied?
    4. What experimental designs are implemented?
    5. How do different research components interact?

    Avoid topics that are:
    - Too specific (individual measurements)
    - Too broad (entire fields of study)
    - Purely descriptive without scientific merit
    - Administrative or non-research elements

    Format: Return a JSON array of primary scientific themes or research components.""",

                'subtopics': """For the scientific theme '{topic}', identify key methodological elements and experimental components.

    Each subtopic should:
    - Represent a distinct experimental or analytical approach
    - Contribute to scientific rigor
    - Support reproducibility
    - Connect to research objectives

    Consider:
    1. What specific methods were employed?
    2. What variables were measured?
    3. What controls were implemented?
    4. What analytical techniques were used?
    5. How were data validated?

    Format: Return a JSON array of scientific subtopics that detail the research methodology.""",

                'details': """For the scientific subtopic '{subtopic}', extract specific experimental parameters and results.

    Focus on:
    1. Measurement specifications
    2. Statistical analyses
    3. Data collection procedures
    4. Validation methods
    5. Error margins
    6. Equipment specifications
    7. Environmental conditions

    Include details that are:
    - Quantifiable
    - Reproducible
    - Statistically relevant
    - Methodologically important

    Format: Return a JSON array of specific scientific parameters and findings."""
            },
            
            DocumentType.NARRATIVE: {
            'topics': """Analyze this narrative document focusing on storytelling elements and plot development.

    Identify major narrative components that:
    - Represent complete story arcs or plot elements
    - Form essential narrative structures
    - Establish key story developments
    - Connect to the overall narrative flow

    Consider:
    1. What are the primary plot points?
    2. What character arcs are developed?
    3. What themes are explored?
    4. What settings are established?
    5. How do different narrative elements interweave?

    Avoid topics that are:
    - Too specific (individual scenes)
    - Too broad (entire genres)
    - Purely stylistic elements
    - Non-narrative content

    Format: Return a JSON array of primary narrative themes or story elements.""",

            'subtopics': """For the narrative theme '{topic}', identify key story elements and developments.

    Each subtopic should:
    - Represent a distinct narrative aspect
    - Support story progression
    - Connect to character development
    - Contribute to theme exploration

    Consider:
    1. What specific plot developments occur?
    2. What character interactions take place?
    3. What conflicts are presented?
    4. What thematic elements are developed?
    5. What setting details are important?

    Format: Return a JSON array of narrative subtopics that detail story components.""",

            'details': """For the narrative subtopic '{subtopic}', extract specific story details and elements.

    Focus on:
    1. Scene descriptions
    2. Character motivations
    3. Dialogue highlights
    4. Setting details
    5. Symbolic elements
    6. Emotional moments
    7. Plot connections

    Include details that are:
    - Story-advancing
    - Character-developing
    - Theme-supporting
    - Atmosphere-building

    Format: Return a JSON array of specific narrative details and elements."""
        },
        
            DocumentType.BUSINESS: {
            'topics': """Analyze this business document focusing on strategic initiatives and market opportunities.

    Identify major business components that:
    - Represent complete business strategies
    - Form essential market approaches
    - Establish key business objectives
    - Connect to organizational goals

    Consider:
    1. What are the primary business objectives?
    2. What market opportunities are targeted?
    3. What strategic initiatives are proposed?
    4. What organizational capabilities are required?
    5. How do different business elements align?

    Avoid topics that are:
    - Too specific (individual tactics)
    - Too broad (entire industries)
    - Administrative elements
    - Non-strategic content

    Format: Return a JSON array of primary business themes or strategic elements.""",

            'subtopics': """For the business theme '{topic}', identify key strategic elements and approaches.

    Each subtopic should:
    - Represent a distinct business aspect
    - Support strategic objectives
    - Connect to market opportunities
    - Contribute to business growth

    Consider:
    1. What specific strategies are proposed?
    2. What market segments are targeted?
    3. What resources are required?
    4. What competitive advantages exist?
    5. What implementation steps are needed?

    Format: Return a JSON array of business subtopics that detail strategic components.""",

            'details': """For the business subtopic '{subtopic}', extract specific strategic details and requirements.

    Focus on:
    1. Market metrics
    2. Financial projections
    3. Resource requirements
    4. Implementation timelines
    5. Success metrics
    6. Risk factors
    7. Growth opportunities

    Include details that are:
    - Measurable
    - Action-oriented
    - Resource-specific
    - Market-focused

    Format: Return a JSON array of specific business details and requirements."""
        },            

            DocumentType.ANALYTICAL: {
                'topics': """Analyze this analytical document focusing on key insights and data patterns.

    Identify major analytical themes that:
    - Represent complete analytical frameworks
    - Reveal significant patterns or trends
    - Support evidence-based conclusions
    - Connect different aspects of analysis

    Consider:
    1. What are the primary analytical questions?
    2. What major patterns emerge from the data?
    3. What key metrics drive the analysis?
    4. How do different analytical components relate?
    5. What are the main areas of investigation?

    Avoid topics that are:
    - Too granular (individual data points)
    - Too broad (entire analytical fields)
    - Purely descriptive without analytical value
    - Administrative or non-analytical elements

    Format: Return a JSON array of primary analytical themes or frameworks.""",

                'subtopics': """For the analytical theme '{topic}', identify key metrics and analytical approaches.

    Each subtopic should:
    - Represent a distinct analytical method or metric
    - Contribute to understanding patterns
    - Support data-driven insights
    - Connect to analytical objectives

    Consider:
    1. What specific analyses were performed?
    2. What metrics were calculated?
    3. What statistical approaches were used?
    4. What patterns were investigated?
    5. How were conclusions validated?

    Format: Return a JSON array of analytical subtopics that detail the investigation methods.""",

                'details': """For the analytical subtopic '{subtopic}', extract specific findings and supporting evidence.

    Focus on:
    1. Statistical results
    2. Trend analyses
    3. Correlation findings
    4. Significance measures
    5. Confidence intervals
    6. Data quality metrics
    7. Validation results

    Include details that are:
    - Quantifiable
    - Statistically significant
    - Evidence-based
    - Methodologically sound

    Format: Return a JSON array of specific analytical findings and metrics."""
            },
            DocumentType.LEGAL: {
                'topics': """Analyze this legal document focusing on key legal principles and frameworks.

    Identify major legal components that:
    - Represent complete legal concepts or arguments
    - Form foundational legal principles
    - Establish key rights, obligations, or requirements
    - Connect to relevant legal frameworks

    Consider:
    1. What are the primary legal issues or questions?
    2. What statutory frameworks apply?
    3. What precedential cases are relevant?
    4. What legal rights and obligations are established?
    5. How do different legal concepts interact?

    Avoid topics that are:
    - Too specific (individual clauses)
    - Too broad (entire bodies of law)
    - Administrative or non-legal elements
    - Purely formatting sections

    Format: Return a JSON array of primary legal themes or frameworks.""",

                'subtopics': """For the legal theme '{topic}', identify key legal elements and requirements.

    Each subtopic should:
    - Represent a distinct legal requirement or concept
    - Support legal compliance or enforcement
    - Connect to statutory or case law
    - Contribute to legal understanding

    Consider:
    1. What specific obligations arise?
    2. What rights are established?
    3. What procedures are required?
    4. What legal tests or standards apply?
    5. What exceptions or limitations exist?

    Format: Return a JSON array of legal subtopics that detail requirements and obligations.""",

                'details': """For the legal subtopic '{subtopic}', extract specific legal provisions and requirements.

    Focus on:
    1. Specific statutory references
    2. Case law citations
    3. Compliance requirements
    4. Procedural steps
    5. Legal deadlines
    6. Jurisdictional requirements
    7. Enforcement mechanisms

    Include details that are:
    - Legally binding
    - Procedurally important
    - Compliance-critical
    - Precedent-based

    Format: Return a JSON array of specific legal provisions and requirements."""
            },
            DocumentType.MEDICAL: {
                'topics': """Analyze this medical document focusing on key clinical concepts and patient care aspects.

    Identify major medical components that:
    - Represent complete clinical concepts
    - Form essential diagnostic or treatment frameworks
    - Establish key medical protocols
    - Connect to standard medical practices

    Consider:
    1. What are the primary medical conditions or issues?
    2. What treatment approaches are discussed?
    3. What diagnostic frameworks apply?
    4. What clinical outcomes are measured?
    5. How do different medical aspects interact?

    Avoid topics that are:
    - Too specific (individual symptoms)
    - Too broad (entire medical fields)
    - Administrative elements
    - Non-clinical content

    Format: Return a JSON array of primary medical themes or clinical concepts.""",

                'subtopics': """For the medical theme '{topic}', identify key clinical elements and protocols.

    Each subtopic should:
    - Represent a distinct clinical aspect
    - Support patient care decisions
    - Connect to medical evidence
    - Contribute to treatment planning

    Consider:
    1. What specific treatments are indicated?
    2. What diagnostic criteria apply?
    3. What monitoring is required?
    4. What contraindications exist?
    5. What patient factors are relevant?

    Format: Return a JSON array of medical subtopics that detail clinical approaches.""",

                'details': """For the medical subtopic '{subtopic}', extract specific clinical guidelines and parameters.

    Focus on:
    1. Dosage specifications
    2. Treatment protocols
    3. Monitoring requirements
    4. Clinical indicators
    5. Risk factors
    6. Side effects
    7. Follow-up procedures

    Include details that are:
    - Clinically relevant
    - Evidence-based
    - Treatment-specific
    - Patient-focused

    Format: Return a JSON array of specific medical parameters and guidelines."""
            },

            DocumentType.INSTRUCTIONAL: {
                'topics': """Analyze this instructional document focusing on key learning objectives and educational frameworks.

    Identify major instructional components that:
    - Represent complete learning units
    - Form coherent educational modules
    - Establish key competencies
    - Connect to learning outcomes

    Consider:
    1. What are the primary learning objectives?
    2. What skill sets are being developed?
    3. What knowledge areas are covered?
    4. What pedagogical approaches are used?
    5. How do different learning components build on each other?

    Avoid topics that are:
    - Too specific (individual facts)
    - Too broad (entire subjects)
    - Administrative elements
    - Non-educational content

    Format: Return a JSON array of primary instructional themes or learning modules.""",

                'subtopics': """For the instructional theme '{topic}', identify key learning elements and approaches.

    Each subtopic should:
    - Represent a distinct learning component
    - Support skill development
    - Connect to learning objectives
    - Contribute to competency building

    Consider:
    1. What specific skills are taught?
    2. What concepts are introduced?
    3. What practice activities are included?
    4. What assessment methods are used?
    5. What prerequisites are needed?

    Format: Return a JSON array of instructional subtopics that detail learning components.""",

                'details': """For the instructional subtopic '{subtopic}', extract specific learning activities and resources.

    Focus on:
    1. Practice exercises
    2. Examples and illustrations
    3. Assessment criteria
    4. Learning resources
    5. Key definitions
    6. Common mistakes
    7. Success indicators

    Include details that are:
    - Skill-building
    - Practice-oriented
    - Assessment-ready
    - Learning-focused

    Format: Return a JSON array of specific instructional elements and activities."""
            },

            DocumentType.ACADEMIC: {
                'topics': """Analyze this academic document focusing on scholarly arguments and theoretical frameworks.

    Identify major academic components that:
    - Represent complete theoretical concepts
    - Form scholarly arguments
    - Establish key academic positions
    - Connect to existing literature

    Consider:
    1. What are the primary theoretical frameworks?
    2. What scholarly debates are addressed?
    3. What research questions are explored?
    4. What methodological approaches are used?
    5. How do different theoretical elements interact?

    Avoid topics that are:
    - Too specific (individual citations)
    - Too broad (entire fields)
    - Administrative elements
    - Non-scholarly content

    Format: Return a JSON array of primary academic themes or theoretical frameworks.""",

                'subtopics': """For the academic theme '{topic}', identify key theoretical elements and arguments.

    Each subtopic should:
    - Represent a distinct theoretical aspect
    - Support scholarly analysis
    - Connect to literature
    - Contribute to academic discourse

    Consider:
    1. What specific arguments are made?
    2. What evidence is presented?
    3. What theoretical models apply?
    4. What counterarguments exist?
    5. What methodological approaches are used?

    Format: Return a JSON array of academic subtopics that detail theoretical components.""",

                'details': """For the academic subtopic '{subtopic}', extract specific scholarly evidence and arguments.

    Focus on:
    1. Research findings
    2. Theoretical implications
    3. Methodological details
    4. Literature connections
    5. Critical analyses
    6. Supporting evidence
    7. Scholarly debates

    Include details that are:
    - Theoretically relevant
    - Evidence-based
    - Methodologically sound
    - Literature-connected

    Format: Return a JSON array of specific academic elements and arguments."""
            },

            DocumentType.PROCEDURAL: {
                'topics': """Analyze this procedural document focusing on systematic processes and workflows.

    Identify major procedural components that:
    - Represent complete process units
    - Form coherent workflow stages
    - Establish key procedures
    - Connect to overall process flow

    Consider:
    1. What are the primary process phases?
    2. What workflow sequences exist?
    3. What critical paths are defined?
    4. What decision points occur?
    5. How do different process elements connect?

    Avoid topics that are:
    - Too specific (individual actions)
    - Too broad (entire systems)
    - Administrative elements
    - Non-procedural content

    Format: Return a JSON array of primary procedural themes or process phases.""",

                'subtopics': """For the procedural theme '{topic}', identify key process elements and requirements.

    Each subtopic should:
    - Represent a distinct process step
    - Support workflow progression
    - Connect to other steps
    - Contribute to process completion

    Consider:
    1. What specific steps are required?
    2. What inputs are needed?
    3. What outputs are produced?
    4. What conditions apply?
    5. What validations occur?

    Format: Return a JSON array of procedural subtopics that detail process steps.""",

                'details': """For the procedural subtopic '{subtopic}', extract specific step requirements and checks.

    Focus on:
    1. Step-by-step instructions
    2. Input requirements
    3. Quality checks
    4. Decision criteria
    5. Exception handling
    6. Success criteria
    7. Completion indicators

    Include details that are:
    - Action-oriented
    - Sequence-specific
    - Quality-focused
    - Process-critical

    Format: Return a JSON array of specific procedural steps and requirements."""
            },
                
            DocumentType.GENERAL: {
            'topics': """Analyze this document focusing on main conceptual themes and relationships.

    Identify major themes that:
    - Represent complete, independent ideas
    - Form logical groupings of related concepts
    - Support the document's main purpose
    - Connect to other important themes

    Consider:
    1. What are the fundamental ideas being presented?
    2. How do these ideas relate to each other?
    3. What are the key areas of focus?
    4. How is the information structured?

    Avoid topics that are:
    - Too specific (individual examples)
    - Too broad (entire subject areas)
    - Isolated facts without context
    - Purely formatting elements

    Format: Return a JSON array of primary themes or concept areas.""",

                'subtopics': """For the theme '{topic}', identify key supporting concepts and related ideas.

    Each subtopic should:
    - Represent a distinct aspect of the main theme
    - Provide meaningful context
    - Support understanding
    - Connect to the overall narrative

    Consider:
    1. What are the main points about this theme?
    2. What examples illustrate it?
    3. What evidence supports it?
    4. How does it develop through the document?

    Format: Return a JSON array of subtopics that develop this theme.""",

                'details': """For the subtopic '{subtopic}', extract specific supporting information and examples.

    Focus on:
    1. Concrete examples
    2. Supporting evidence
    3. Key definitions
    4. Important relationships
    5. Specific applications
    6. Notable implications
    7. Clarifying points

    Include details that:
    - Illustrate the concept
    - Provide evidence
    - Aid understanding
    - Connect to larger themes

    Format: Return a JSON array of specific supporting details and examples."""
            }
        }
        # Add default prompts for any missing document types
        for doc_type in DocumentType:
            if doc_type not in self.type_specific_prompts:
                self.type_specific_prompts[doc_type] = self.type_specific_prompts[DocumentType.GENERAL]

    async def detect_document_type(self, content: str, request_id: str) -> DocumentType:
        """Use LLM to detect document type with sophisticated analysis."""
        summary_content = content[:self.config['max_summary_length']]
        prompt = f"""You are analyzing a document to determine its primary type and structure. This document requires the most appropriate conceptual organization strategy.

    Key characteristics of each document type:

    TECHNICAL
    - Contains system specifications, API documentation, or implementation details
    - Focuses on HOW things work and technical implementation
    - Uses technical terminology, code examples, or system diagrams
    - Structured around components, modules, or technical processes
    Example indicators: API endpoints, code blocks, system requirements, technical specifications

    SCIENTIFIC
    - Presents research findings, experimental data, or scientific theories
    - Follows scientific method with hypotheses, methods, results
    - Contains statistical analysis or experimental procedures
    - References prior research or scientific literature
    Example indicators: methodology sections, statistical results, citations, experimental procedures

    NARRATIVE
    - Tells a story or presents events in sequence
    - Has character development or plot progression
    - Uses descriptive language and scene-setting
    - Organized chronologically or by story elements
    Example indicators: character descriptions, plot developments, narrative flow, dialogue

    BUSINESS
    - Focuses on business operations, strategy, or market analysis
    - Contains financial data or business metrics
    - Addresses organizational or market challenges
    - Includes business recommendations or action items
    Example indicators: market analysis, financial projections, strategic plans, ROI calculations

    ACADEMIC
    - Centers on scholarly research and theoretical frameworks
    - Engages with academic literature and existing theories
    - Develops theoretical arguments or conceptual models
    - Contributes to academic discourse in a field
    Example indicators: literature reviews, theoretical frameworks, scholarly arguments, academic citations

    LEGAL
    - Focuses on laws, regulations, or legal requirements
    - Contains legal terminology and formal language
    - References statutes, cases, or legal precedents
    - Addresses rights, obligations, or compliance
    Example indicators: legal citations, compliance requirements, jurisdictional references, statutory language

    MEDICAL
    - Centers on clinical care, diagnoses, or treatments
    - Uses medical terminology and protocols
    - Addresses patient care or health outcomes
    - Follows clinical guidelines or standards
    Example indicators: diagnostic criteria, treatment protocols, clinical outcomes, medical terminology

    INSTRUCTIONAL
    - Focuses on teaching or skill development
    - Contains learning objectives and outcomes
    - Includes exercises or practice activities
    - Structured for progressive learning
    Example indicators: learning objectives, practice exercises, assessment criteria, skill development

    ANALYTICAL
    - Presents data analysis or systematic examination
    - Contains trends, patterns, or correlations
    - Uses analytical frameworks or methodologies
    - Focuses on drawing conclusions from data
    Example indicators: data trends, analytical methods, pattern analysis, statistical insights

    PROCEDURAL
    - Provides step-by-step instructions or processes
    - Focuses on HOW to accomplish specific tasks
    - Contains clear sequential steps or workflows
    - Emphasizes proper order and procedures
    Example indicators: numbered steps, workflow diagrams, sequential instructions

    GENERAL
    - Contains broad or mixed content types
    - No strong alignment with other categories
    - Covers multiple topics or approaches
    - Uses general language and structure
    Example indicators: mixed content types, general descriptions, broad overviews, diverse topics

    Key Differentiators:

    1. TECHNICAL vs PROCEDURAL:
    - Technical focuses on system components and how they work
    - Procedural focuses on steps to accomplish tasks

    2. SCIENTIFIC vs ACADEMIC:
    - Scientific focuses on experimental methods and results
    - Academic focuses on theoretical frameworks and scholarly discourse

    3. ANALYTICAL vs SCIENTIFIC:
    - Analytical focuses on data patterns and insights
    - Scientific focuses on experimental validation of hypotheses

    4. INSTRUCTIONAL vs PROCEDURAL:
    - Instructional focuses on learning and skill development
    - Procedural focuses on task completion steps

    5. MEDICAL vs SCIENTIFIC:
    - Medical focuses on clinical care and treatment
    - Scientific focuses on research methodology

    Return ONLY the category name that best matches the document's structure and purpose.

    Document excerpt:
    {summary_content}"""
        try:
            response = await self._retry_generate_completion(
                prompt,
                max_tokens=50,
                request_id=request_id,
                task="detecting_document_type"
            )
            return DocumentType.from_str(response.strip().lower())
        except Exception as e:
            logger.error(f"Error detecting document type: {str(e)}", extra={"request_id": request_id})
            return DocumentType.GENERAL

    @staticmethod
    def _create_node(name: str, importance: str = 'high', emoji: str = "") -> Dict[str, Any]:
        """Create a node dictionary with the given parameters.
        
        Args:
            name (str): The name/text content of the node
            importance (str): The importance level ('high', 'medium', 'low')
            emoji (str): The emoji to represent this node
            
        Returns:
            Dict[str, Any]: Node dictionary with all necessary attributes
        """
        return {
            'name': name,
            'importance': importance.lower(),
            'emoji': emoji,
            'subtopics': [],  # Initialize empty lists for children
            'details': []
        }
        
    def _escape_text(self, text: str) -> str:
        """Replace parentheses with Unicode alternatives and handle other special characters."""
        # Replace regular parentheses in content text with Unicode alternatives
        for original, replacement in self.paren_replacements.items():
            text = text.replace(original, replacement)
            
        # Handle percentages
        text = self.percentage_regex1.sub(r'\1%', text)
        text = self.percentage_regex2.sub('%', text)
        
        # Replace special characters while preserving needed symbols
        text = self.special_chars_regex.sub('', text)
        
        # Clean up multiple backslashes
        text = self.backslash_regex.sub(r'\\', text)
        
        return text

    def _format_node_line(self, node: Dict[str, Any], indent_level: int) -> str:
        """Format a single node in Mermaid syntax."""
        indent = '    ' * indent_level
        
        # For root node, always return just the document emoji
        if indent_level == 1:
            return f"{indent}((ğŸ“„))"
        
        # Get the node text and escape it
        if 'text' in node:
            # For detail nodes
            importance = node.get('importance', 'low')
            marker = {'high': 'â™¦ï¸', 'medium': 'ğŸ”¸', 'low': 'ğŸ”¹'}[importance]
            text = self._escape_text(node['text'])
            return f"{indent}[{marker} {text}]"
        else:
            # For topic and subtopic nodes
            node_name = self._escape_text(node['name'])
            emoji = node.get('emoji', '')
            if emoji and node_name:
                node_name = f"{emoji} {node_name}"
            
            # For main topics (level 2)
            if indent_level == 2:
                return f"{indent}(({node_name}))"
            
            # For subtopics (level 3)
            return f"{indent}({node_name})"

    def _add_node_to_mindmap(self, node: Dict[str, Any], mindmap_lines: List[str], indent_level: int) -> None:
        """Recursively add a node and its children to the mindmap."""
        # Add the current node
        node_line = self._format_node_line(node, indent_level)
        mindmap_lines.append(node_line)
        
        # Add all subtopics first
        for subtopic in node.get('subtopics', []):
            self._add_node_to_mindmap(subtopic, mindmap_lines, indent_level + 1)
            
            # Then add details under each subtopic
            for detail in subtopic.get('details', []):
                detail_line = self._format_node_line({
                    'text': detail['text'],
                    'name': detail['text'],
                    'importance': detail['importance']  # Pass through the importance level
                }, indent_level + 2)
                mindmap_lines.append(detail_line)

    async def _batch_redundancy_check(self, items, content_type='topic', context_prefix='', batch_size=10):
        """Perform early batch redundancy checks to avoid wasting LLM calls.
        
        Args:
            items: List of items to check (topics or subtopics)
            content_type: Type of content ('topic' or 'subtopic')
            context_prefix: Optional context prefix for subtopics (e.g. parent topic name)
            batch_size: Maximum batch size for parallel processing
            
        Returns:
            List of non-redundant items
        """
        if not items or len(items) <= 1:
            return items
            
        # Process in batches for efficient parallel checking
        start_count = len(items)
        logger.info(f"Starting early redundancy check for {len(items)} {content_type}s...")
        
        # Track items to keep (non-redundant)
        unique_items = []
        seen_names = {}
        
        # First, use simple fuzzy matching to catch obvious duplicates
        for item in items:
            item_name = item['name']
            if not await self.is_similar_to_existing(item_name, seen_names, content_type):
                unique_items.append(item)
                seen_names[item_name] = item
        
        # If we still have lots of items, use more aggressive LLM-based similarity
        if len(unique_items) > 3 and len(unique_items) > len(items) * 0.8:  # Only if enough items and not much reduction yet
            try:
                # Create pairs for comparison
                pairs_to_check = []
                for i in range(len(unique_items)-1):
                    for j in range(i+1, len(unique_items)):
                        pairs_to_check.append((i, j))
                
                # Process in batches with semaphore for rate limiting
                redundant_indices = set()
                semaphore = asyncio.Semaphore(3)  # Limit concurrent LLM calls
                
                async def check_pair(i, j):
                    if i in redundant_indices or j in redundant_indices:
                        return None
                        
                    async with semaphore:
                        try:
                            context1 = context2 = content_type
                            if context_prefix:
                                context1 = context2 = f"{content_type} of {context_prefix}"
                                
                            is_redundant = await self.check_similarity_llm(
                                unique_items[i]['name'],
                                unique_items[j]['name'],
                                context1,
                                context2
                            )
                            
                            if is_redundant:
                                # Keep item with more detailed information
                                i_detail = len(unique_items[i].get('name', ''))
                                j_detail = len(unique_items[j].get('name', ''))
                                return (j, i) if i_detail > j_detail else (i, j)
                        except Exception as e:
                            logger.warning(f"Early redundancy check failed: {str(e)}")
                            
                    return None
                    
                # Process batches to maintain parallelism
                for batch_idx in range(0, len(pairs_to_check), batch_size):
                    batch = pairs_to_check[batch_idx:batch_idx + batch_size]
                    results = await asyncio.gather(*(check_pair(i, j) for i, j in batch))
                    
                    # Process results
                    for result in results:
                        if result:
                            redundant_idx, keep_idx = result
                            if redundant_idx not in redundant_indices:
                                redundant_indices.add(redundant_idx)
                                logger.info(f"Found redundant {content_type}: '{unique_items[redundant_idx]['name']}' similar to '{unique_items[keep_idx]['name']}'")
                
                # Filter out redundant items
                unique_items = [item for i, item in enumerate(unique_items) if i not in redundant_indices]
            except Exception as e:
                logger.error(f"Error in aggressive redundancy check: {str(e)}")
        
        reduction = start_count - len(unique_items)
        if reduction > 0:
            logger.info(f"Early redundancy check removed {reduction} redundant {content_type}s ({reduction/start_count*100:.1f}%)")
        
        return unique_items

    async def is_similar_to_existing(self, name: str, existing_names: Union[dict, set], content_type: str = 'topic') -> bool:
        """Check if name is similar to any existing names using stricter fuzzy matching thresholds.
        
        Args:
            name: Text to check for similarity
            existing_names: Dictionary or set of existing names to compare against
            content_type: Type of content being compared ('topic', 'subtopic', or 'detail')
            
        Returns:
            bool: True if similar content exists, False otherwise
        """
        # Lower thresholds to catch more duplicates
        base_threshold = {
            'topic': 75,      # Lower from 85 to catch more duplicates
            'subtopic': 70,   # Lower from 80 to catch more duplicates
            'detail': 65      # Lower from 75 to catch more duplicates
        }[content_type]
        
        # Get threshold for this content type
        threshold = base_threshold
        
        # Adjust threshold based on text length - be more lenient with longer texts
        if len(name) < 10:
            threshold = min(threshold + 10, 95)  # Stricter for very short texts
        elif len(name) > 100:
            threshold = max(threshold - 15, 55)  # More lenient for long texts
        
        # Make adjustments for content types to catch more duplicates
        if content_type == 'subtopic':
            threshold = max(threshold - 10, 60)  # Lower threshold to catch more duplicates
        elif content_type == 'detail':
            threshold = max(threshold - 10, 55)  # Lower threshold to catch more duplicates
        
        # Clean and normalize input text
        name = re.sub(r'\s+', ' ', name.lower().strip())
        name = re.sub(r'[^\w\s]', '', name)
        
        # Special handling for numbered items
        numbered_pattern = self.numbered_pattern
        name_without_number = numbered_pattern.sub(r'\1', name)
        
        # Handle both dict and set inputs
        existing_items = existing_names.keys() if isinstance(existing_names, dict) else existing_names
        
        for existing_name in existing_items:
            # Skip if lengths are vastly different
            existing_clean = re.sub(r'\s+', ' ', str(existing_name).lower().strip())
            existing_clean = re.sub(r'[^\w\s]', '', existing_clean)
            
            if abs(len(name) - len(existing_clean)) > len(name) * 0.7:  # Increased from 0.5
                continue
            
            # Calculate multiple similarity metrics
            basic_ratio = fuzz.ratio(name, existing_clean)
            partial_ratio = fuzz.partial_ratio(name, existing_clean)
            token_sort_ratio = fuzz.token_sort_ratio(name, existing_clean)
            token_set_ratio = fuzz.token_set_ratio(name, existing_clean)
            
            # For numbered items, compare without numbers
            existing_without_number = numbered_pattern.sub(r'\1', existing_clean)
            if name_without_number != name or existing_without_number != existing_clean:
                number_ratio = fuzz.ratio(name_without_number, existing_without_number)
                basic_ratio = max(basic_ratio, number_ratio)
            
            # Weight ratios differently based on content type - higher weights to catch more duplicates
            if content_type == 'topic':
                final_ratio = max(
                    basic_ratio,
                    token_sort_ratio * 1.1,  # Increased weight
                    token_set_ratio * 1.0    # Increased weight
                )
            elif content_type == 'subtopic':
                final_ratio = max(
                    basic_ratio,
                    partial_ratio * 1.0,     # Increased weight
                    token_sort_ratio * 0.95, # Increased weight
                    token_set_ratio * 0.9    # Increased weight
                )
            else:  # details
                final_ratio = max(
                    basic_ratio * 0.95,
                    partial_ratio * 0.9,
                    token_sort_ratio * 0.85,
                    token_set_ratio * 0.8
                )
            
            # Increase ratio for shorter texts to catch more duplicates
            if len(name) < 30:
                final_ratio *= 1.1  # Boost ratio for short texts
            
            # Check against adjusted threshold
            if final_ratio > threshold:
                logger.debug(
                    f"Found similar {content_type}:\n"
                    f"New: '{name}'\n"
                    f"Existing: '{existing_clean}'\n"
                    f"Ratio: {final_ratio:.2f} (threshold: {threshold})"
                )
                return True
        
        return False

    async def check_similarity_llm(self, text1: str, text2: str, context1: str, context2: str) -> bool:
        """LLM-based similarity check between two text elements with stricter criteria."""
        prompt = f"""Compare these two text elements and determine if they express similar core information, making one redundant in the mindmap.

        Text 1 (from {context1}):
        "{text1}"

        Text 2 (from {context2}):
        "{text2}"

        A text is REDUNDANT if ANY of these apply:
        1. It conveys the same primary information or main point as the other text
        2. It covers the same concept from a similar angle or perspective
        3. The semantic meaning overlaps significantly with the other text
        4. A reader would find having both entries repetitive or confusing
        5. One could be safely removed without losing important information

        A text is DISTINCT ONLY if ALL of these apply:
        1. It focuses on a clearly different aspect or perspective
        2. It provides substantial unique information not present in the other
        3. It serves a fundamentally different purpose in context
        4. Both entries together provide significantly more value than either alone
        5. The conceptual overlap is minimal

        When in doubt, mark as REDUNDANT to create a cleaner, more focused mindmap.

        Respond with EXACTLY one of these:
        REDUNDANT (overlapping information about X)
        DISTINCT (different aspect: X)

        where X is a very brief explanation."""

        try:
            response = await self._retry_generate_completion(
                prompt,
                max_tokens=50,
                request_id='similarity_check',
                task="checking_content_similarity"
            )
            
            # Consider anything not explicitly marked as DISTINCT to be REDUNDANT
            result = not response.strip().upper().startswith("DISTINCT")
            
            logger.info(
                f"\n{colored('ğŸ” Content comparison:', 'cyan')}\n"
                f"Text 1: {colored(text1[:100] + '...', 'yellow')}\n"
                f"Text 2: {colored(text2[:100] + '...', 'yellow')}\n"
                f"Result: {colored('REDUNDANT' if result else 'DISTINCT', 'green')}\n"
                f"LLM Response: {colored(response.strip(), 'white')}"
            )
            return result
        except Exception as e:
            logger.error(f"Error in LLM similarity check: {str(e)}")
            # Default to considering items similar if the check fails
            return True

    async def _process_content_batch(self, content_items: List[ContentItem]) -> Set[int]:
        """Process a batch of content items to identify redundant content with parallel processing.
        
        Args:
            content_items: List of ContentItem objects to process
            
        Returns:
            Set of indices identifying redundant items that should be removed
        """
        redundant_indices = set()
        comparison_tasks = []
        comparison_counter = 0
        
        # Create cache of preprocessed texts to avoid recomputing
        processed_texts = {}
        for idx, item in enumerate(content_items):
            # Normalize text for comparison
            text = re.sub(r'\s+', ' ', item.text.lower().strip())
            text = re.sub(r'[^\w\s]', '', text)
            processed_texts[idx] = text
        
        # Limit concurrent API calls
        semaphore = asyncio.Semaphore(10)  # Adjust based on API limits
        
        # Prepare all comparison tasks first
        for i in range(len(content_items)):
            item1 = content_items[i]
            text1 = processed_texts[i]
            
            for j in range(i + 1, len(content_items)):
                item2 = content_items[j]
                text2 = processed_texts[j]
                
                # Quick exact text match check - avoid API call
                if text1 == text2:
                    # Log this immediately since we're not using the API
                    comparison_counter += 1
                    logger.info(f"\nMaking comparison {comparison_counter}... (exact match found)")
                    
                    # Add to candidates for removal with perfect confidence
                    confidence = 1.0
                    
                    # Determine which to keep based on importance and path
                    item1_importance = self._get_importance_value(item1.importance)
                    item2_importance = self._get_importance_value(item2.importance)
                    
                    if ((item2_importance > item1_importance) or
                        (item2_importance == item1_importance and 
                        len(item2.path) < len(item1.path))):
                        redundant_indices.add(i)
                        confidence_text = f'{confidence:.2f}'
                        logger.info(
                            f"\n{colored('ğŸ”„ Removing redundant content:', 'yellow')}\n"
                            f"Keeping: {colored(item2.text[:100] + '...', 'green')}\n"
                            f"Removing: {colored(item1.text[:100] + '...', 'red')}\n"
                            f"Confidence: {colored(confidence_text, 'cyan')}"
                        )
                        break  # Stop processing this item if we're removing it
                    else:
                        redundant_indices.add(j)
                        confidence_text = f'{confidence:.2f}'
                        logger.info(
                            f"\n{colored('ğŸ”„ Removing redundant content:', 'yellow')}\n"
                            f"Keeping: {colored(item1.text[:100] + '...', 'green')}\n"
                            f"Removing: {colored(item2.text[:100] + '...', 'red')}\n"
                            f"Confidence: {colored(confidence_text, 'cyan')}"
                        )
                    continue
                
                # Skip if lengths are very different
                len_ratio = min(len(text1), len(text2)) / max(len(text1), len(text2))
                if len_ratio < 0.5:  # Texts differ in length by more than 50%
                    continue
                
                # Skip if one item is already marked for removal
                if i in redundant_indices or j in redundant_indices:
                    continue
                    
                # Add to parallel comparison tasks
                async def check_similarity_with_context(idx1, idx2):
                    """Run similarity check with semaphore and return context for logging"""
                    nonlocal comparison_counter
                    
                    # Atomically increment comparison counter
                    comparison_id = comparison_counter = comparison_counter + 1
                    
                    # Log start of comparison
                    logger.info(f"\nMaking comparison {comparison_id}...")
                    
                    # Run the LLM comparison with rate limiting
                    async with semaphore:
                        try:
                            is_redundant = await self.check_similarity_llm(
                                content_items[idx1].text, 
                                content_items[idx2].text,
                                content_items[idx1].path_str, 
                                content_items[idx2].path_str
                            )
                            
                            # Calculate confidence if redundant
                            confidence = 0.0
                            if is_redundant:
                                # Calculate fuzzy string similarity metrics
                                fuzz_ratio = fuzz.ratio(processed_texts[idx1], processed_texts[idx2]) / 100.0
                                token_sort_ratio = fuzz.token_sort_ratio(processed_texts[idx1], processed_texts[idx2]) / 100.0
                                token_set_ratio = fuzz.token_set_ratio(processed_texts[idx1], processed_texts[idx2]) / 100.0
                                
                                # Combine metrics for overall confidence
                                confidence = (fuzz_ratio * 0.4 + 
                                            token_sort_ratio * 0.3 + 
                                            token_set_ratio * 0.3)
                            
                            return {
                                'comparison_id': comparison_id,
                                'is_redundant': is_redundant,
                                'confidence': confidence,
                                'idx1': idx1,
                                'idx2': idx2,
                                'success': True
                            }
                        except Exception as e:
                            logger.error(f"Error in comparison {comparison_id}: {str(e)}")
                            return {
                                'comparison_id': comparison_id,
                                'success': False,
                                'error': str(e),
                                'idx1': idx1,
                                'idx2': idx2
                            }
                
                # Add task to our list
                comparison_tasks.append(check_similarity_with_context(i, j))
        
        # Run all comparison tasks in parallel
        if comparison_tasks:
            logger.info(f"Starting {len(comparison_tasks)} parallel similarity comparisons")
            results = await asyncio.gather(*comparison_tasks)
            
            # Process results
            # First, collect all redundancies with confidence scores
            redundancy_candidates = []
            for result in results:
                if not result['success']:
                    continue
                    
                if result['is_redundant'] and result['confidence'] > 0.8:  # High confidence threshold
                    redundancy_candidates.append(result)
            
            # Sort by confidence (highest first)
            redundancy_candidates.sort(key=lambda x: x['confidence'], reverse=True)
            
            # Process each redundancy candidate
            for result in redundancy_candidates:
                i, j = result['idx1'], result['idx2']
                
                # Skip if either item is already marked for removal
                if i in redundant_indices or j in redundant_indices:
                    continue
                    
                # Determine which to keep based on importance and path
                item1 = content_items[i]
                item2 = content_items[j]
                item1_importance = self._get_importance_value(item1.importance)
                item2_importance = self._get_importance_value(item2.importance)
                
                if ((item2_importance > item1_importance) or
                    (item2_importance == item1_importance and 
                    len(item2.path) < len(item1.path))):
                    redundant_indices.add(i)
                    confidence_text = f'{result["confidence"]:.2f}'
                    logger.info(
                        f"\n{colored('ğŸ”„ Removing redundant content:', 'yellow')}\n"
                        f"Keeping: {colored(item2.text[:100] + '...', 'green')}\n"
                        f"Removing: {colored(item1.text[:100] + '...', 'red')}\n"
                        f"Confidence: {colored(confidence_text, 'cyan')}"
                    )
                else:
                    redundant_indices.add(j)
                    confidence_text = f'{result["confidence"]:.2f}'
                    logger.info(
                        f"\n{colored('ğŸ”„ Removing redundant content:', 'yellow')}\n"
                        f"Keeping: {colored(item1.text[:100] + '...', 'green')}\n"
                        f"Removing: {colored(item2.text[:100] + '...', 'red')}\n"
                        f"Confidence: {colored(confidence_text, 'cyan')}"
                    )
        
        logger.info(f"\nBatch processing complete. Made {comparison_counter} comparisons.")
        return redundant_indices                

    def _get_importance_value(self, importance: str) -> int:
        """Convert importance string to numeric value for comparison."""
        return {'high': 3, 'medium': 2, 'low': 1}.get(importance.lower(), 0)

    def _extract_content_for_filtering(self, node: Dict[str, Any], current_path: List[str]) -> None:
        """Extract all content items with their full paths for filtering."""
        if not node:
            return

        # Process current node (including root node)
        if 'name' in node:
            current_node_path = current_path + ([node['name']] if node['name'] else [])
            
            # Add the node itself unless it's the root "Document Mindmap" node
            if len(current_path) > 0 or (node['name'] and node['name'] != 'Document Mindmap'):
                # Determine node type based on path depth
                node_type = 'root' if len(current_path) == 0 else 'topic' if len(current_path) == 1 else 'subtopic'
                
                content_item = ContentItem(
                    text=node['name'],
                    path=current_node_path,
                    node_type=node_type,
                    importance=node.get('importance', 'medium')
                )
                
                # Only add if path is non-empty
                if current_node_path:
                    path_tuple = tuple(current_node_path)
                    self.all_content.append(content_item)
                    self.content_by_path[path_tuple] = content_item

            # Process details at current level
            for detail in node.get('details', []):
                if isinstance(detail, dict) and 'text' in detail:
                    # Only add details if we have a valid parent path
                    if current_node_path:
                        detail_path = current_node_path + ['detail']
                        detail_item = ContentItem(
                            text=detail['text'],
                            path=detail_path,
                            node_type='detail',
                            importance=detail.get('importance', 'medium')
                        )
                        detail_path_tuple = tuple(detail_path)
                        self.all_content.append(detail_item)
                        self.content_by_path[detail_path_tuple] = detail_item

            # Process subtopics
            for subtopic in node.get('subtopics', []):
                self._extract_content_for_filtering(subtopic, current_node_path)
        else:
            # If no name but has subtopics, process them with current path
            for subtopic in node.get('subtopics', []):
                self._extract_content_for_filtering(subtopic, current_path)

    async def final_pass_filter_for_duplicative_content(self, mindmap_data: Dict[str, Any], batch_size: int = 50) -> Dict[str, Any]:
        """Enhanced filter for duplicative content with more aggressive detection and safer rebuilding."""
        USE_VERBOSE = True  # Toggle for verbose logging
        
        def vlog(message: str, color: str = 'white', bold: bool = False):
            """Helper for verbose logging"""
            if USE_VERBOSE:
                attrs = ['bold'] if bold else []
                logger.info(colored(message, color, attrs=attrs))
                
        vlog("\n" + "="*80, 'cyan', True)
        vlog("ğŸ” STARTING ENHANCED DUPLICATE CONTENT FILTER PASS", 'cyan', True)
        vlog("="*80 + "\n", 'cyan', True)
        
        # Debug input structure
        vlog("\nğŸ“¥ INPUT MINDMAP STRUCTURE:", 'blue', True)
        vlog(f"Mindmap keys: {list(mindmap_data.keys())}")
        if 'central_theme' in mindmap_data:
            central_theme = mindmap_data['central_theme']
            vlog(f"Central theme keys: {list(central_theme.keys())}")
            vlog(f"Number of initial topics: {len(central_theme.get('subtopics', []))}")
            topics = central_theme.get('subtopics', [])
            vlog("\nInitial topic names:")
            for i, topic in enumerate(topics, 1):
                vlog(f"{i}. {topic.get('name', 'UNNAMED')} ({len(topic.get('subtopics', []))} subtopics)")
        else:
            vlog("WARNING: No 'central_theme' found in mindmap!", 'red', True)
            return mindmap_data  # Return original if no central theme
        
        # Initialize instance variables for content tracking
        vlog("\nğŸ”„ Initializing content tracking...", 'yellow')
        self.all_content = []
        self.content_by_path = {}
        
        # Extract all content items for filtering
        vlog("\nğŸ“‹ Starting content extraction from central theme...", 'blue', True)
        try:
            # Fixed extraction method - should properly extract all content
            self._extract_content_for_filtering(mindmap_data.get('central_theme', {}), [])
            
            # Verify extraction worked
            vlog(f"âœ… Successfully extracted {len(self.all_content)} total content items:", 'green')
            content_types = {}
            for item in self.all_content:
                content_types[item.node_type] = content_types.get(item.node_type, 0) + 1
            for node_type, count in content_types.items():
                vlog(f"  - {node_type}: {count} items", 'green')
        except Exception as e:
            vlog(f"âŒ Error during content extraction: {str(e)}", 'red', True)
            return mindmap_data  # Return original data on error
        
        # Check if we have any content to filter
        initial_count = len(self.all_content)
        if initial_count == 0:
            vlog("âŒ No content extracted - mindmap appears empty", 'red', True)
            return mindmap_data  # Return original data
        
        # Process content in batches for memory efficiency
        vlog("\nğŸ”„ Processing content in batches...", 'yellow', True)
        content_batches = [
            self.all_content[i:i+batch_size] 
            for i in range(0, len(self.all_content), batch_size)
        ]
        
        all_to_remove = set()
        
        for batch_idx, batch in enumerate(content_batches):
            vlog(f"Processing batch {batch_idx+1}/{len(content_batches)} ({len(batch)} items)...", 'yellow')
            batch_to_remove = await self._process_content_batch(batch)
            
            # Adjust indices to global positions
            global_indices = {batch_idx * batch_size + i for i in batch_to_remove}
            all_to_remove.update(global_indices)
            
            vlog(f"Batch {batch_idx+1} complete: identified {len(batch_to_remove)} redundant items", 'green')
        
        # Get indices of items to keep
        keep_indices = set(range(len(self.all_content))) - all_to_remove
        
        # Convert to set of paths to keep
        vlog("\nğŸ”„ Converting to paths for rebuild...", 'blue')
        keep_paths = {tuple(self.all_content[i].path) for i in keep_indices}
        vlog(f"Keeping {len(keep_paths)} unique paths", 'blue')
        
        # Safety check - add at least one path if none remain
        if not keep_paths and len(self.all_content) > 0:
            vlog("âš ï¸ No paths remained after filtering! Adding at least one path", 'yellow', True)
            first_item = self.all_content[0]
            keep_paths.add(tuple(first_item.path))
        
        # Rebuild the mindmap with only the paths to keep
        vlog("\nğŸ—ï¸ Rebuilding mindmap...", 'yellow', True)
        
        def rebuild_mindmap(node: Dict[str, Any], current_path: List[str]) -> Optional[Dict[str, Any]]:
            """Recursively rebuild mindmap keeping only non-redundant content."""
            # Add special case for root node
            if not node:
                return None
                
            # For root node, always keep it and process its subtopics
            if not current_path:
                result = copy.deepcopy(node)
                result['subtopics'] = []
                
                # Process main topics
                for topic in node.get('subtopics', []):
                    if topic.get('name'):
                        topic_path = [topic['name']]
                        rebuilt_topic = rebuild_mindmap(topic, topic_path)
                        if rebuilt_topic:
                            result['subtopics'].append(rebuilt_topic)
                
                # Always return root node even if no subtopics remain
                return result
                    
            # For non-root nodes, check if current path should be kept
            path_tuple = tuple(current_path)
            if path_tuple not in keep_paths:
                return None
                
            result = copy.deepcopy(node)
            result['subtopics'] = []
            
            # Process subtopics
            for subtopic in node.get('subtopics', []):
                if subtopic.get('name'):
                    subtopic_path = current_path + [subtopic['name']]
                    rebuilt_subtopic = rebuild_mindmap(subtopic, subtopic_path)
                    if rebuilt_subtopic:
                        result['subtopics'].append(rebuilt_subtopic)
            
            # Filter details
            if 'details' in result:
                filtered_details = []
                for detail in result['details']:
                    if isinstance(detail, dict) and 'text' in detail:
                        detail_path = current_path + ['detail']
                        if tuple(detail_path) in keep_paths:
                            filtered_details.append(detail)
                result['details'] = filtered_details
            
            # Only return node if it has content
            if result['subtopics'] or result.get('details'):
                return result
            return None
        
        # Rebuild mindmap without redundant content
        filtered_data = rebuild_mindmap(mindmap_data.get('central_theme', {}), [])
        
        # Safety check - add the original data's central theme if rebuild failed completely
        if not filtered_data:
            vlog("âŒ Filtering removed all content - using original mindmap", 'red', True)
            return mindmap_data
            
        # Another safety check - ensure we have subtopics
        if not filtered_data.get('subtopics'):
            vlog("âŒ Filtering removed all subtopics - using original mindmap", 'red', True) 
            return mindmap_data
        
        # Put the central theme back into a complete mindmap structure
        result_mindmap = {'central_theme': filtered_data}
        
        # Calculate and log statistics
        removed_count = initial_count - len(keep_indices)
        reduction_percentage = (removed_count / initial_count * 100) if initial_count > 0 else 0
        
        vlog(
            f"\n{colored('âœ… Duplicate content filtering complete', 'green', attrs=['bold'])}\n"
            f"Original items: {colored(str(initial_count), 'yellow')}\n"
            f"Filtered items: {colored(str(len(keep_indices)), 'yellow')}\n"
            f"Removed {colored(str(removed_count), 'red')} duplicate items "
            f"({colored(f'{reduction_percentage:.1f}%', 'red')} reduction)"
        )
        
        return result_mindmap

    async def generate_mindmap(self, document_content: str, request_id: str) -> str:
        """Generate a complete mindmap from document content with balanced coverage of all topics.
        
        Args:
            document_content (str): The document content to analyze
            request_id (str): Unique identifier for request tracking
            
        Returns:
            str: Complete Mermaid mindmap syntax
            
        Raises:
            MindMapGenerationError: If mindmap generation fails
        """
        try:
            logger.info("Starting mindmap generation process...", extra={"request_id": request_id})
            
            # Initialize content caching and LLM call tracking
            self._content_cache = {}
            self._llm_calls = {
                'topics': 0,
                'subtopics': 0,
                'details': 0
            }
            
            # Initialize tracking of unique concepts
            self._unique_concepts = {
                'topics': set(),
                'subtopics': set(),
                'details': set()
            }
            
            # Enhanced completion tracking
            completion_status = {
                'total_topics': 0,
                'processed_topics': 0,
                'total_subtopics': 0,
                'processed_subtopics': 0,
                'total_details': 0
            }
            
            # Set strict LLM call limits with increased bounds
            max_llm_calls = {
                'topics': 20,      # Increased from 15
                'subtopics': 30,   # Increased from 20
                'details': 40      # Increased from 24
            }

            # Set minimum content requirements with better distribution
            min_requirements = {
                'topics': 4,       # Minimum topics to process
                'subtopics_per_topic': 2,  # Minimum subtopics per topic
                'details_per_subtopic': 3   # Minimum details per subtopic
            }
            
            # Calculate document word count and set limit 
            doc_words = len(document_content.split())
            word_limit = min(doc_words * 0.9, 8000)  # Cap at 8000 words
            current_word_count = 0
            
            logger.info(f"Document size: {doc_words} words. Generation limit: {word_limit:,} words", extra={"request_id": request_id})

            # Helper function to check if we have enough content with stricter enforcement
            def has_sufficient_content():
                if completion_status['processed_topics'] < min_requirements['topics']:
                    return False
                if completion_status['total_topics'] > 0:
                    avg_subtopics_per_topic = (completion_status['processed_subtopics'] / 
                                            completion_status['processed_topics'])
                    if avg_subtopics_per_topic < min_requirements['subtopics_per_topic']:
                        return False
                # Process at least 75% of available topics before considering early stop
                if completion_status['total_topics'] > 0:
                    topics_processed_ratio = completion_status['processed_topics'] / completion_status['total_topics']
                    if topics_processed_ratio < 0.75:
                        return False
                return True
                                        
            # Check cache first for document type with strict caching
            doc_type_key = hashlib.md5(document_content[:1000].encode()).hexdigest()
            if doc_type_key in self._content_cache:
                doc_type = self._content_cache[doc_type_key]
            else:
                doc_type = await self.detect_document_type(document_content, request_id)
                self._content_cache[doc_type_key] = doc_type
                self._llm_calls['topics'] += 1
                
            logger.info(f"Detected document type: {doc_type.name}", extra={"request_id": request_id})
            
            type_prompts = self.type_specific_prompts[doc_type]
            
            # Extract main topics with enhanced LLM call limit and uniqueness check
            if self._llm_calls['topics'] < max_llm_calls['topics']:
                logger.info("Extracting main topics...", extra={"request_id": request_id})
                main_topics = await self._extract_main_topics(document_content, type_prompts['topics'], request_id)
                self._llm_calls['topics'] += 1
                
                # NEW: Perform early redundancy check on main topics
                main_topics = await self._batch_redundancy_check(main_topics, 'topic')
                
                completion_status['total_topics'] = len(main_topics)
            else:
                logger.info("Using cached main topics to avoid excessive LLM calls")
                main_topics = self._content_cache.get('main_topics', [])
                completion_status['total_topics'] = len(main_topics)
            
            if not main_topics:
                raise MindMapGenerationError("No main topics could be extracted from the document")
                
            # Cache main topics with timestamp
            self._content_cache['main_topics'] = {
                'data': main_topics,
                'timestamp': time.time()
            }

            # Process topics with completion tracking
            processed_topics = {}
            # NEW: Track already processed topics for redundancy checking
            processed_topic_names = {}
            
            for topic_idx, topic in enumerate(main_topics, 1):
                # Don't stop early if we haven't processed minimum topics
                should_continue = (topic_idx <= min_requirements['topics'] or 
                                not has_sufficient_content() or
                                completion_status['processed_topics'] < len(main_topics) * 0.75)
                                
                if not should_continue:
                    logger.info(f"Stopping after processing {topic_idx} topics - sufficient content gathered")
                    break
        
                topic_name = topic['name']
                
                # NEW: Check if this topic is redundant with already processed topics
                is_redundant = False
                for processed_name in processed_topic_names:
                    if await self.is_similar_to_existing(topic_name, {processed_name: True}, 'topic'):
                        logger.info(f"Skipping redundant topic: '{topic_name}' (similar to '{processed_name}')")
                        is_redundant = True
                        break
                        
                if is_redundant:
                    continue
                    
                # Track this topic for future redundancy checks
                processed_topic_names[topic_name] = True
                
                # Enhanced word limit check with buffer
                if current_word_count > word_limit * 0.95:  # Increased from 0.9 to ensure more completion
                    logger.info(f"Approaching word limit at {current_word_count}/{word_limit:.0f} words")
                    break

                logger.info(f"Processing topic {topic_idx}/{len(main_topics)}: '{topic_name}' "
                        f"(Words: {current_word_count}/{word_limit:.0f})",
                        extra={"request_id": request_id})
                
                # Track unique concepts with validation
                if topic_name not in self._unique_concepts['topics']:
                    self._unique_concepts['topics'].add(topic_name)
                    completion_status['processed_topics'] += 1

                try:
                    # Enhanced subtopic processing with caching
                    topic_key = hashlib.md5(f"{topic_name}:{doc_type_key}".encode()).hexdigest()
                    if topic_key in self._content_cache:
                        subtopics = self._content_cache[topic_key]
                        logger.info(f"Using cached subtopics for topic: {topic_name}")
                    else:
                        if self._llm_calls['subtopics'] < max_llm_calls['subtopics']:
                            subtopics = await self._extract_subtopics(
                                topic, document_content, type_prompts['subtopics'], request_id
                            )
                            
                            # NEW: Perform early redundancy check on subtopics
                            subtopics = await self._batch_redundancy_check(
                                subtopics, 'subtopic', context_prefix=topic_name
                            )
                            
                            self._content_cache[topic_key] = subtopics
                            self._llm_calls['subtopics'] += 1
                        else:
                            logger.info("Reached subtopic LLM call limit")
                            break
                            
                    topic['subtopics'] = []
                    
                    if subtopics:
                        completion_status['total_subtopics'] += len(subtopics)
                        processed_subtopics = {}
                        
                        # NEW: Track already processed subtopics for redundancy checking
                        processed_subtopic_names = {}
                        
                        # Process each subtopic with completion tracking
                        for subtopic_idx, subtopic in enumerate(subtopics, 1):
                            if self._llm_calls['details'] >= max_llm_calls['details']:
                                logger.info("Reached maximum LLM calls for detail extraction")
                                break
                                
                            subtopic_name = subtopic['name']
                            
                            # NEW: Check redundancy with already processed subtopics
                            is_redundant = False
                            for processed_name in processed_subtopic_names:
                                if await self.is_similar_to_existing(subtopic_name, {processed_name: True}, 'subtopic'):
                                    logger.info(f"Skipping redundant subtopic: '{subtopic_name}' (similar to '{processed_name}')")
                                    is_redundant = True
                                    break
                                    
                            if is_redundant:
                                continue
                                
                            # Track this subtopic for future redundancy checks
                            processed_subtopic_names[subtopic_name] = True
                            
                            # Track word count for subtopics
                            subtopic_words = len(subtopic_name.split())
                            if current_word_count + subtopic_words > word_limit * 0.95:
                                logger.info("Approaching word limit during subtopic processing")
                                break
                                
                            current_word_count += subtopic_words
                            
                            # Track unique subtopics
                            self._unique_concepts['subtopics'].add(subtopic_name)
                            completion_status['processed_subtopics'] += 1

                            try:
                                # Enhanced detail processing with caching
                                subtopic_key = hashlib.md5(f"{subtopic_name}:{topic_key}".encode()).hexdigest()
                                if subtopic_key in self._content_cache:
                                    details = self._content_cache[subtopic_key]
                                    logger.info(f"Using cached details for subtopic: {subtopic_name}")
                                else:
                                    if self._llm_calls['details'] < max_llm_calls['details']:
                                        details = await self._extract_details(
                                            subtopic, document_content, type_prompts['details'], request_id
                                        )
                                        self._content_cache[subtopic_key] = details
                                        self._llm_calls['details'] += 1
                                    else:
                                        details = []
                                
                                subtopic['details'] = []
                                
                                if details:
                                    completion_status['total_details'] += len(details)
                                    
                                    # Process details with completion tracking
                                    seen_details = {}
                                    unique_details = []
                                    
                                    for detail in details:
                                        detail_words = len(detail['text'].split())
                                        
                                        if current_word_count + detail_words > word_limit * 0.98:
                                            logger.info("Approaching word limit during detail processing")
                                            break
                                            
                                        if not await self.is_similar_to_existing(detail['text'], seen_details, 'detail'):
                                            current_word_count += detail_words
                                            seen_details[detail['text']] = True
                                            unique_details.append(detail)
                                            self._unique_concepts['details'].add(detail['text'])
                                    
                                    subtopic['details'] = unique_details
                                
                                processed_subtopics[subtopic_name] = subtopic
                                
                            except Exception as e:
                                logger.error(f"Error processing details for subtopic '{subtopic_name}': {str(e)}")
                                processed_subtopics[subtopic_name] = subtopic
                                continue
                        
                        topic['subtopics'] = list(processed_subtopics.values())
                    
                    processed_topics[topic_name] = topic
                        
                except Exception as e:
                    logger.error(f"Error processing topic '{topic_name}': {str(e)}")
                    processed_topics[topic_name] = topic
                    continue
                
                # Log completion status
                logger.info(
                    f"Completion status: "
                    f"Topics: {completion_status['processed_topics']}/{completion_status['total_topics']}, "
                    f"Subtopics: {completion_status['processed_subtopics']}/{completion_status['total_subtopics']}, "
                    f"Details: {completion_status['total_details']}"
                )
            
            if not processed_topics:
                raise MindMapGenerationError("No topics could be processed")
            
            # Enhanced final statistics logging
            completion_stats = {
                'words_generated': current_word_count,
                'word_limit': word_limit,
                'completion_percentage': (current_word_count/word_limit)*100,
                'topics_processed': completion_status['processed_topics'],
                'total_topics': completion_status['total_topics'],
                'unique_topics': len(self._unique_concepts['topics']),
                'unique_subtopics': len(self._unique_concepts['subtopics']),
                'unique_details': len(self._unique_concepts['details']),
                'llm_calls': self._llm_calls,
                'early_stopping': has_sufficient_content()
            }
            
            logger.info(
                f"Mindmap generation completed:"
                f"\n- Words generated: {completion_stats['words_generated']}/{completion_stats['word_limit']:.0f} "
                f"({completion_stats['completion_percentage']:.1f}%)"
                f"\n- Topics processed: {completion_stats['topics_processed']}/{completion_stats['total_topics']}"
                f"\n- Unique topics: {completion_stats['unique_topics']}"
                f"\n- Unique subtopics: {completion_stats['unique_subtopics']}"
                f"\n- Unique details: {completion_stats['unique_details']}"
                f"\n- LLM calls: topics={completion_stats['llm_calls']['topics']}, "
                f"subtopics={completion_stats['llm_calls']['subtopics']}, "
                f"details={completion_stats['llm_calls']['details']}"
                f"\n- Early stopping: {completion_stats['early_stopping']}",
                extra={"request_id": request_id}
            )
            
            logger.info("Starting initial mindmap generation...")
            concepts = {
                'central_theme': self._create_node('Document Mindmap', 'high')
            }
            concepts['central_theme']['subtopics'] = list(processed_topics.values())
                
            logger.info("Starting duplicate content filtering...")
            try:
                # Explicitly await the filtering
                filtered_concepts = await self.final_pass_filter_for_duplicative_content(
                    concepts,
                    batch_size=25
                )
                
                if not filtered_concepts:
                    logger.warning("Filtering removed all content, using original mindmap")
                    filtered_concepts = concepts
                    
                # NEW: Perform reality check against original document
                logger.info("Starting reality check to filter confabulations...")
                verified_concepts = await self.verify_mindmap_against_source(
                    filtered_concepts, 
                    document_content
                )
                
                if not verified_concepts or not verified_concepts.get('central_theme', {}).get('subtopics'):
                    logger.warning("Reality check removed all content, using filtered mindmap with warning")
                    verified_concepts = filtered_concepts
                
                # Print enhanced usage report with detailed breakdowns
                self.optimizer.token_tracker.print_usage_report()

                try:
                    self._save_emoji_cache()  # Save cache at the end of processing
                except Exception as e:
                    logger.warning(f"Failed to save emoji cache: {str(e)}")
                                    
                logger.info("Successfully verified against source document, generating final mindmap...")
                return self._generate_mermaid_mindmap(verified_concepts)
                
            except Exception as e:
                logger.error(f"Error during content filtering or verification: {str(e)}")
                logger.warning("Using unfiltered mindmap due to filtering/verification error")
                
                # Print usage report even if verification fails
                self.optimizer.token_tracker.print_usage_report()
                
                return self._generate_mermaid_mindmap(concepts)

        except Exception as e:
            logger.error(f"Error in mindmap generation: {str(e)}", extra={"request_id": request_id})
            raise MindMapGenerationError(f"Failed to generate mindmap: {str(e)}")

    async def _extract_main_topics(self, content: str, topics_prompt: str, request_id: str) -> List[Dict[str, Any]]:
        """Extract main topics using LLM with more aggressive deduplication and content preservation.
        
        Args:
            content (str): The document content to analyze
            topics_prompt (str): The prompt template for topic extraction
            request_id (str): Unique identifier for the request
            
        Returns:
            List[Dict[str, Any]]: List of extracted topics with their metadata
            
        Raises:
            MindMapGenerationError: If topic extraction fails
        """
        MAX_TOPICS = 8  # Increased from 6 to ensure complete coverage
        MIN_TOPICS = 4  # Minimum topics to process
        MAX_CONCURRENT_TASKS = 50  # Limit concurrent LLM calls
        
        async def extract_from_chunk(chunk: str) -> List[Dict[str, Any]]:
            """Extract topics from a single content chunk."""
            consolidated_prompt = f"""You are an expert at identifying unique, distinct main topics within content.
                        
            {topics_prompt}

            Additional requirements:
            1. Each topic must be truly distinct from others - avoid overlapping concepts
            2. Combine similar themes into single, well-defined topics
            3. Ensure topics are specific enough to be meaningful but general enough to support subtopics
            4. Aim for 4-8 most significant topics that capture the key distinct areas
            5. Focus on conceptual separation - each topic should represent a unique aspect or dimension
            6. Avoid topics that are too similar or could be subtopics of each other
            7. Prioritize broader topics that can encompass multiple subtopics
            8. Eliminate redundancy - each topic should cover a distinct area with no overlap

            IMPORTANT: 
            1. DO NOT include specific statistics, percentages, or numerical data unless explicitly stated in the source text
            2. DO NOT refer to modern studies, surveys, or analyses that aren't mentioned in the document
            3. DO NOT make up correlation coefficients, growth rates, or other numerical relationships
            4. Keep your content strictly based on what's in the document, not general knowledge about the topic
            5. Use general descriptions rather than specific numbers if the document doesn't provide exact figures

            Current content chunk:
            {chunk}

            IMPORTANT: Respond with ONLY a JSON array of strings representing the main distinct topics.
            Example format: ["First Distinct Topic", "Second Distinct Topic"]"""

            try:
                response = await self.optimizer.generate_completion(
                    consolidated_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task="extracting_main_topics"
                )
                
                logger.debug(f"Raw topics response for chunk: {response}", 
                            extra={"request_id": request_id})
                
                parsed_response = self._parse_llm_response(response, "array")
                
                chunk_topics = []
                seen_names = set()
                
                for topic_name in parsed_response:
                    if isinstance(topic_name, str) and topic_name.strip():
                        cleaned_name = re.sub(r'[`*_#]', '', topic_name)
                        cleaned_name = ' '.join(cleaned_name.split())
                        
                        if cleaned_name and cleaned_name not in seen_names:
                            seen_names.add(cleaned_name)
                            # Select appropriate emoji for topic
                            emoji = await self._select_emoji(cleaned_name, 'topic')
                            chunk_topics.append({
                                'name': cleaned_name,
                                'emoji': emoji,
                                'processed': False,  # Track processing status
                                'importance': 'high',  # Main topics are always high importance
                                'subtopics': [],
                                'details': []
                            })
                
                return chunk_topics
                
            except Exception as e:
                logger.error(f"Error extracting topics from chunk: {str(e)}", 
                            extra={"request_id": request_id})
                return []

        try:
            # Create content chunks with overlap to ensure context preservation
            chunk_size = min(8000, len(content) // 3) if len(content) > 6000 else 4000
            overlap = 250  # Characters of overlap between chunks
            
            # Create overlapping chunks
            content_chunks = []
            start = 0
            while start < len(content):
                end = min(start + chunk_size, len(content))
                # Extend to nearest sentence end if possible
                if end < len(content):
                    next_period = content.find('.', end)
                    if next_period != -1 and next_period - end < 200:  # Don't extend too far
                        end = next_period + 1
                chunk = content[start:end]
                content_chunks.append(chunk)
                start = end - overlap if end < len(content) else end

            # Initialize concurrent processing controls
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
            topics_with_metrics = {}  # Track topic frequency and importance
            unique_topics_seen = set()
            max_chunks_to_process = 5  # Increased from 3

            async def process_chunk(chunk: str, chunk_idx: int) -> List[Dict[str, Any]]:
                """Process a single chunk with semaphore control."""
                if chunk_idx >= max_chunks_to_process:
                    return []
                    
                if len(unique_topics_seen) >= MAX_TOPICS * 1.5:
                    return []
                    
                async with semaphore:
                    return await self._retry_with_exponential_backoff(
                        lambda: extract_from_chunk(chunk)
                    )

            # Process chunks concurrently
            chunk_results = await asyncio.gather(
                *(process_chunk(chunk, idx) for idx, chunk in enumerate(content_chunks))
            )

            # Process results with more aggressive deduplication
            all_topics = []
            for chunk_topics in chunk_results:
                # Track topic frequency and merge similar topics
                for topic in chunk_topics:
                    topic_key = topic['name'].lower()
                    
                    # Check for similar existing topics with stricter criteria
                    similar_found = False
                    for existing_key in list(topics_with_metrics.keys()):
                        if await self.is_similar_to_existing(topic_key, {existing_key: True}, 'topic'):
                            topics_with_metrics[existing_key]['frequency'] += 1
                            similar_found = True
                            break
                    
                    if not similar_found:
                        topics_with_metrics[topic_key] = {
                            'topic': topic,
                            'frequency': 1,
                            'first_appearance': len(all_topics)
                        }
                
                # Only add unique topics
                for topic in chunk_topics:
                    if topic['name'] not in unique_topics_seen:
                        unique_topics_seen.add(topic['name'])
                        all_topics.append(topic)
                        
                        if len(unique_topics_seen) >= MAX_TOPICS * 1.5:
                            break

                # Early stopping checks
                if len(unique_topics_seen) >= MIN_TOPICS:
                    topic_frequencies = [metrics['frequency'] for metrics in topics_with_metrics.values()]
                    if len(topic_frequencies) >= MIN_TOPICS:
                        avg_frequency = sum(topic_frequencies) / len(topic_frequencies)
                        if avg_frequency >= 1.5:  # Topics appear in multiple chunks
                            break

            if not all_topics:
                error_msg = "No valid topics extracted from document"
                logger.error(error_msg, extra={"request_id": request_id})
                raise MindMapGenerationError(error_msg)

            # Add consolidation step when we have too many potential topics
            if len(all_topics) > MIN_TOPICS * 1.5:
                consolidation_prompt = f"""You are merging and consolidating similar topics from a document.

                Here are the current potential topics extracted:
                {json.dumps([topic['name'] for topic in all_topics], indent=2)}

                Requirements:
                1. Identify topics that cover the same or similar concepts
                2. Merge overlapping topics into a single, well-defined topic
                3. Choose the most representative, precise, and concise name for each topic
                4. Ensure each final topic is clearly distinct from others
                5. Aim for exactly {MIN_TOPICS}-{MAX_TOPICS} distinct topics that cover the key areas
                6. Completely eliminate redundancy - each topic should represent a unique conceptual area
                7. Broader topics are preferred over narrower ones if they can encompass the same content
                8. Choose clear, concise topic names that accurately represent the content

                Return ONLY a JSON array of consolidated topic names.
                Example: ["First Consolidated Topic", "Second Consolidated Topic"]"""

                try:
                    response = await self._retry_generate_completion(
                        consolidation_prompt,
                        max_tokens=1000,
                        request_id=request_id,
                        task="consolidating_topics"
                    )
                    
                    consolidated_names = self._parse_llm_response(response, "array")
                    
                    if consolidated_names and len(consolidated_names) >= MIN_TOPICS:
                        # Create new topics from consolidated names
                        consolidated_topics = []
                        seen_names = set()
                        
                        for name in consolidated_names:
                            if isinstance(name, str) and name.strip():
                                cleaned_name = re.sub(r'[`*_#]', '', name)
                                cleaned_name = ' '.join(cleaned_name.split())
                                
                                if cleaned_name and cleaned_name not in seen_names:
                                    emoji = await self._select_emoji(cleaned_name, 'topic')
                                    consolidated_topics.append({
                                        'name': cleaned_name,
                                        'emoji': emoji,
                                        'processed': False,
                                        'importance': 'high',
                                        'subtopics': [],
                                        'details': []
                                    })
                                    seen_names.add(cleaned_name)
                        
                        if len(consolidated_topics) >= MIN_TOPICS:
                            all_topics = consolidated_topics
                            logger.info(f"Successfully consolidated topics from {len(unique_topics_seen)} to {len(consolidated_topics)}")
                except Exception as e:
                    logger.warning(f"Topic consolidation failed: {str(e)}", extra={"request_id": request_id})

            # Sort and select final topics with stricter deduplication
            sorted_topics = sorted(
                topics_with_metrics.values(),
                key=lambda x: (-x['frequency'], x['first_appearance'])
            )

            final_topics = []
            seen_final = set()
            
            # Select final topics with more aggressive deduplication
            for topic_data in sorted_topics:
                topic = topic_data['topic']
                if len(final_topics) >= MAX_TOPICS:
                    break
                    
                if topic['name'] not in seen_final:
                    similar_exists = False
                    for existing_topic in final_topics:
                        if await self.is_similar_to_existing(topic['name'], {existing_topic['name']: True}, 'topic'):
                            similar_exists = True
                            break
                    
                    if not similar_exists:
                        seen_final.add(topic['name'])
                        final_topics.append(topic)

            # Add additional topics if needed 
            if len(final_topics) < MIN_TOPICS:
                for topic in all_topics:
                    if len(final_topics) >= MIN_TOPICS:
                        break
                        
                    if topic['name'] not in seen_final:
                        similar_exists = False
                        for existing_topic in final_topics:
                            if await self.is_similar_to_existing(topic['name'], {existing_topic['name']: True}, 'topic'):
                                similar_exists = True
                                break
                        
                        if not similar_exists:
                            seen_final.add(topic['name'])
                            final_topics.append(topic)

            # Final LLM-based deduplication when we have enough topics
            if len(final_topics) > MIN_TOPICS:
                for i in range(len(final_topics)-1, 0, -1):
                    if len(final_topics) <= MIN_TOPICS:
                        break
                        
                    for j in range(i-1, -1, -1):
                        try:
                            is_duplicate = await self.check_similarity_llm(
                                final_topics[i]['name'], 
                                final_topics[j]['name'],
                                "main topic", 
                                "main topic"
                            )
                            
                            if is_duplicate and len(final_topics) > MIN_TOPICS:
                                logger.info(f"LLM detected duplicate topics: '{final_topics[i]['name']}' and '{final_topics[j]['name']}'")
                                del final_topics[i]
                                break
                        except Exception as e:
                            logger.warning(f"LLM duplicate check failed: {str(e)}")
                            continue

            logger.info(
                f"Successfully extracted {len(final_topics)} main topics "
                f"(min: {MIN_TOPICS}, max: {MAX_TOPICS})",
                extra={"request_id": request_id}
            )

            return final_topics

        except Exception as e:
            error_msg = f"Failed to extract main topics: {str(e)}"
            logger.error(error_msg, extra={"request_id": request_id})
            raise MindMapGenerationError(error_msg)

    async def _extract_subtopics(self, topic: Dict[str, Any], content: str, subtopics_prompt_template: str, request_id: str) -> List[Dict[str, Any]]:
        """Extract subtopics using LLM with more aggressive deduplication and content preservation."""
        MAX_SUBTOPICS = self.config['max_subtopics']
        MAX_CONCURRENT_TASKS = 50  # Limit concurrent LLM calls
        
        content_hash = hashlib.md5(content.encode()).hexdigest()
        cache_key = f"subtopics_{topic['name']}_{content_hash}_{request_id}"
        
        if not hasattr(self, '_subtopics_cache'):
            self._subtopics_cache = {}
            
        if not hasattr(self, '_processed_chunks_by_topic'):
            self._processed_chunks_by_topic = {}
        
        if topic['name'] not in self._processed_chunks_by_topic:
            self._processed_chunks_by_topic[topic['name']] = set()

        async def extract_from_chunk(chunk: str) -> List[Dict[str, Any]]:
            chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
            if chunk_hash in self._processed_chunks_by_topic[topic['name']]:
                return []
                
            self._processed_chunks_by_topic[topic['name']].add(chunk_hash)
                
            enhanced_prompt = f"""You are an expert at identifying distinct, relevant subtopics that support a main topic.

            Topic: {topic['name']}

            {subtopics_prompt_template.format(topic=topic['name'])}

            Additional requirements:
            1. Each subtopic must provide unique value and perspective with NO conceptual overlap
            2. Include both high-level and specific subtopics that are clearly distinct
            3. Ensure strong connection to main topic without repeating the topic itself
            4. Focus on distinct aspects or dimensions that don't overlap with each other
            5. Include 4-6 important subtopics that cover different facets of the topic
            6. Balance breadth and depth of coverage with zero redundancy
            7. Choose clear, concise subtopic names that accurately represent the content
            8. Eliminate subtopics that could be merged without significant information loss

            IMPORTANT: 
            1. DO NOT include specific statistics, percentages, or numerical data unless explicitly stated in the source text
            2. DO NOT refer to modern studies, surveys, or analyses that aren't mentioned in the document
            3. DO NOT make up correlation coefficients, growth rates, or other numerical relationships
            4. Keep your content strictly based on what's in the document, not general knowledge about the topic
            5. Use general descriptions rather than specific numbers if the document doesn't provide exact figures

            Content chunk:
            {chunk}

            IMPORTANT: Return ONLY a JSON array of strings representing distinct subtopics.
            Example: ["First Distinct Subtopic", "Second Distinct Subtopic"]"""

            try:
                response = await self.optimizer.generate_completion(
                    enhanced_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"extracting_subtopics_{topic['name']}"
                )
                
                logger.debug(f"Raw subtopics response for {topic['name']}: {response}", 
                            extra={"request_id": request_id})
                
                parsed_response = self._parse_llm_response(response, "array")
                
                chunk_subtopics = []
                seen_names = {}
                
                for subtopic_name in parsed_response:
                    if isinstance(subtopic_name, str) and subtopic_name.strip():
                        cleaned_name = re.sub(r'[`*_#]', '', subtopic_name)
                        cleaned_name = ' '.join(cleaned_name.split())
                        
                        if cleaned_name and not await self.is_similar_to_existing(cleaned_name, seen_names, 'subtopic'):
                            emoji = await self._select_emoji(cleaned_name, 'subtopic')
                            node = self._create_node(
                                name=cleaned_name,
                                emoji=emoji
                            )
                            chunk_subtopics.append(node)
                            seen_names[cleaned_name] = node
                
                return chunk_subtopics
                
            except Exception as e:
                logger.error(f"Error extracting subtopics from chunk for {topic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                return []

        try:
            if cache_key in self._subtopics_cache:
                return self._subtopics_cache[cache_key]
                
            chunk_size = min(8000, len(content) // 3) if len(content) > 6000 else 4000
            content_chunks = [content[i:i + chunk_size] 
                            for i in range(0, len(content), chunk_size)]
            
            # Initialize concurrent processing controls
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
            seen_names = {}
            all_subtopics = []
            
            async def process_chunk(chunk: str) -> List[Dict[str, Any]]:
                """Process a single chunk with semaphore control."""
                async with semaphore:
                    return await self._retry_with_exponential_backoff(
                        lambda: extract_from_chunk(chunk)
                    )

            # Process chunks concurrently
            chunk_results = await asyncio.gather(
                *(process_chunk(chunk) for chunk in content_chunks)
            )

            # Process results with more aggressive deduplication
            for chunk_subtopics in chunk_results:
                for subtopic in chunk_subtopics:
                    if not await self.is_similar_to_existing(subtopic['name'], seen_names, 'subtopic'):
                        seen_names[subtopic['name']] = subtopic
                        all_subtopics.append(subtopic)

            if not all_subtopics:
                logger.warning(f"No subtopics found for topic {topic['name']}", 
                            extra={"request_id": request_id})
                return []

            # Always perform consolidation to reduce duplicative content
            consolidation_prompt = f"""You are consolidating subtopics for the main topic: {topic['name']}

            Current subtopics:
            {json.dumps([st['name'] for st in all_subtopics], indent=2)}

            Requirements:
            1. Aggressively merge subtopics that cover similar information or concepts
            2. Eliminate any conceptual redundancy between subtopics
            3. Choose the clearest and most representative name for each consolidated subtopic
            4. Each final subtopic must address a unique aspect of the main topic
            5. Select 3-5 truly distinct subtopics that together fully cover the topic
            6. Ensure zero information repetition between subtopics
            7. Prioritize broader subtopics that can encompass multiple narrower ones
            8. Choose clear, concise subtopic names that accurately represent the content

            Return ONLY a JSON array of consolidated subtopic names.
            Example: ["First Consolidated Subtopic", "Second Consolidated Subtopic"]"""

            try:
                consolidation_response = await self._retry_generate_completion(
                    consolidation_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"consolidate_subtopics_{topic['name']}"
                )
                
                consolidated_names = self._parse_llm_response(consolidation_response, "array")
                
                if consolidated_names:
                    seen_names = {}
                    consolidated_subtopics = []
                    
                    for name in consolidated_names:
                        if isinstance(name, str) and name.strip():
                            cleaned_name = re.sub(r'[`*_#]', '', name)
                            cleaned_name = ' '.join(cleaned_name.split())
                            
                            if cleaned_name and not await self.is_similar_to_existing(cleaned_name, seen_names, 'subtopic'):
                                emoji = await self._select_emoji(cleaned_name, 'subtopic')
                                node = self._create_node(
                                    name=cleaned_name,
                                    emoji=emoji
                                )
                                consolidated_subtopics.append(node)
                                seen_names[cleaned_name] = node
                    
                    if consolidated_subtopics:
                        all_subtopics = consolidated_subtopics
                        logger.info(f"Successfully consolidated subtopics for {topic['name']} from {len(all_subtopics)} to {len(consolidated_subtopics)}")
                        
            except Exception as e:
                logger.warning(f"Subtopic consolidation failed for {topic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                # If consolidation fails, do a simple deduplication pass
                seen = set()
                deduplicated_subtopics = []
                for subtopic in all_subtopics:
                    if subtopic['name'] not in seen:
                        seen.add(subtopic['name'])
                        deduplicated_subtopics.append(subtopic)
                all_subtopics = sorted(deduplicated_subtopics, 
                                    key=lambda x: len(x['name']), 
                                    reverse=True)[:MAX_SUBTOPICS]
            
            # Final LLM-based deduplication when we have enough subtopics
            if len(all_subtopics) > 3:  # Only if we have enough to potentially remove some
                for i in range(len(all_subtopics)-1, 0, -1):
                    if len(all_subtopics) <= 3:  # Ensure we keep at least 3 subtopics
                        break
                        
                    for j in range(i-1, -1, -1):
                        try:
                            is_duplicate = await self.check_similarity_llm(
                                all_subtopics[i]['name'], 
                                all_subtopics[j]['name'],
                                f"subtopic of {topic['name']}", 
                                f"subtopic of {topic['name']}"
                            )
                            
                            if is_duplicate:
                                logger.info(f"LLM detected duplicate subtopics: '{all_subtopics[i]['name']}' and '{all_subtopics[j]['name']}'")
                                del all_subtopics[i]
                                break
                        except Exception as e:
                            logger.warning(f"LLM duplicate check failed: {str(e)}")
                            continue
            
            final_subtopics = all_subtopics[:MAX_SUBTOPICS]
            self._subtopics_cache[cache_key] = final_subtopics
            
            logger.info(f"Successfully extracted {len(final_subtopics)} subtopics for {topic['name']}", 
                        extra={"request_id": request_id})
            return final_subtopics
            
        except Exception as e:
            logger.error(f"Failed to extract subtopics for topic {topic['name']}: {str(e)}", 
                        extra={"request_id": request_id})
            return []

    def _validate_detail(self, detail: Dict[str, Any]) -> bool:
        """Validate a single detail entry with more flexible constraints."""
        try:
            # Basic structure validation
            if not isinstance(detail, dict):
                logger.debug(f"Detail not a dict: {type(detail)}")
                return False
                
            # Required fields check
            if not all(k in detail for k in ['text', 'importance']):
                logger.debug(f"Missing required fields. Found keys: {detail.keys()}")
                return False
                
            # Text validation
            if not isinstance(detail['text'], str) or not detail['text'].strip():
                logger.debug("Invalid or empty text field")
                return False
                
            # Importance validation with case insensitivity
            valid_importance = ['high', 'medium', 'low']
            if detail['importance'].lower() not in valid_importance:
                logger.debug(f"Invalid importance: {detail['importance']}")
                return False
                
            # More generous length limit
            if len(detail['text']) > 500:  # Increased from 200
                logger.debug(f"Text too long: {len(detail['text'])} chars")
                return False
                
            return True
            
        except Exception as e:
            logger.debug(f"Validation error: {str(e)}")
            return False

    async def _extract_details(self, subtopic: Dict[str, Any], content: str, details_prompt_template: str, request_id: str) -> List[Dict[str, Any]]:
        """Extract details for a subtopic with more aggressive deduplication and content preservation."""
        MINIMUM_VALID_DETAILS = 5  # Early stopping threshold
        MAX_DETAILS = self.config['max_details']
        MAX_CONCURRENT_TASKS = 50  # Limit concurrent LLM calls
        
        # Create cache key
        content_hash = hashlib.md5(content.encode()).hexdigest()
        cache_key = f"details_{subtopic['name']}_{content_hash}_{request_id}"
        
        if not hasattr(self, '_details_cache'):
            self._details_cache = {}
        
        if not hasattr(self, '_processed_chunks_by_subtopic'):
            self._processed_chunks_by_subtopic = {}
        
        if subtopic['name'] not in self._processed_chunks_by_subtopic:
            self._processed_chunks_by_subtopic[subtopic['name']] = set()

        if not hasattr(self, '_current_details'):
            self._current_details = []

        async def extract_from_chunk(chunk: str) -> List[Dict[str, Any]]:
            chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
            if chunk_hash in self._processed_chunks_by_subtopic[subtopic['name']]:
                return []
                
            self._processed_chunks_by_subtopic[subtopic['name']].add(chunk_hash)
                
            enhanced_prompt = f"""You are an expert at identifying distinct, important details that support a specific subtopic.

            Subtopic: {subtopic['name']}

            {details_prompt_template.format(subtopic=subtopic['name'])}

            Additional requirements:
            1. Each detail MUST provide 3-5 sentences of specific, substantive information
            2. Include CONCRETE EXAMPLES, numbers, dates, or direct references from the text
            3. EXTRACT actual quotes or paraphrase specific passages from the source document
            4. Make each detail UNIQUELY VALUABLE - it should contain information not found in other details
            5. Focus on DEPTH rather than breadth - explore fewer ideas more thoroughly
            6. Include specific evidence, reasoning, or context that supports the subtopic
            7. Balance factual information with analytical insights
            8. Avoid generic statements that could apply to many documents

            Content chunk:
            {chunk}

            IMPORTANT: Return ONLY a JSON array where each object has:
            - "text": The detail text (3-5 sentences with specific examples and evidence)
            - "importance": "high", "medium", or "low" based on significance
            """

            try:
                response = await self.optimizer.generate_completion(
                    enhanced_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"extracting_details_{subtopic['name']}"
                )
                
                raw_details = self._clean_detail_response(response)
                chunk_details = []
                seen_texts = {}
                
                for detail in raw_details:
                    if self._validate_detail(detail) and not await self.is_similar_to_existing(detail['text'], seen_texts, 'detail'):
                        seen_texts[detail['text']] = True
                        
                        # Ensure importance is valid
                        detail['importance'] = detail['importance'].lower()
                        if detail['importance'] not in ['high', 'medium', 'low']:
                            detail['importance'] = 'medium'
                        
                        # Add to results
                        chunk_details.append({
                            'text': detail['text'],
                            'importance': detail['importance']
                        })
                        self._current_details.append(detail)
                        
                        if len(self._current_details) >= MINIMUM_VALID_DETAILS:
                            logger.info(f"Reached minimum required details ({MINIMUM_VALID_DETAILS}) during chunk processing")
                            return chunk_details
                
                return chunk_details
                    
            except Exception as e:
                logger.error(f"Error extracting details from chunk for {subtopic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                return chunk_details if 'chunk_details' in locals() else []

        try:
            if cache_key in self._details_cache:
                return self._details_cache[cache_key]

            self._current_details = []
            chunk_size = min(8000, len(content) // 3) if len(content) > 6000 else 4000
            content_chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]
            
            # Initialize concurrent processing controls
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
            seen_texts = {}
            all_details = []
            early_stop = asyncio.Event()

            async def process_chunk(chunk: str) -> List[Dict[str, Any]]:
                """Process a single chunk with semaphore control."""
                if early_stop.is_set():
                    return []
                    
                async with semaphore:
                    chunk_details = await self._retry_with_exponential_backoff(
                        lambda: extract_from_chunk(chunk)
                    )
                    
                    # Check if we've reached minimum details
                    if len(self._current_details) >= MINIMUM_VALID_DETAILS:
                        early_stop.set()
                    
                    return chunk_details

            # Process chunks concurrently
            chunk_results = await asyncio.gather(
                *(process_chunk(chunk) for chunk in content_chunks)
            )

            # Process results with more aggressive deduplication
            for chunk_details in chunk_results:
                for detail in chunk_details:
                    if not await self.is_similar_to_existing(detail['text'], seen_texts, 'detail'):
                        seen_texts[detail['text']] = True
                        all_details.append(detail)

                        if len(all_details) >= MINIMUM_VALID_DETAILS:
                            break

                if len(all_details) >= MINIMUM_VALID_DETAILS:
                    logger.info(f"Reached minimum required details ({MINIMUM_VALID_DETAILS})")
                    break

            # Always perform consolidation to reduce duplicative content
            consolidation_prompt = f"""You are consolidating details for the subtopic: {subtopic['name']}

            Current details:
            {json.dumps([d['text'] for d in all_details], indent=2)}

            Requirements:
            1. Aggressively merge details that convey similar information or concepts
            2. Eliminate all redundancy and repetitive information 
            3. Choose the most clear, concise, and informative phrasing for each detail
            4. Each final detail must provide unique information not covered by others
            5. Select 3-5 truly distinct details that together fully support the subtopic
            6. Ensure that even similar-sounding details have completely different content
            7. Choose clear, concise detail text that accurately represents the information
            8. Mark each detail with appropriate importance (high/medium/low)

            Return ONLY a JSON array of consolidated details with text and importance.
            Example:
            [
                {{"text": "First distinct detail", "importance": "high"}},
                {{"text": "Second distinct detail", "importance": "medium"}}
            ]"""

            try:
                consolidation_response = await self._retry_generate_completion(
                    consolidation_prompt,
                    max_tokens=1000,
                    request_id=request_id,
                    task=f"consolidate_details_{subtopic['name']}"
                )
                
                consolidated_raw = self._clean_detail_response(consolidation_response)
                
                if consolidated_raw:
                    seen_texts = {}
                    consolidated_details = []
                    
                    for detail in consolidated_raw:
                        if self._validate_detail(detail) and not await self.is_similar_to_existing(detail['text'], seen_texts, 'detail'):
                            seen_texts[detail['text']] = True
                            detail['importance'] = detail['importance'].lower()
                            if detail['importance'] not in ['high', 'medium', 'low']:
                                detail['importance'] = 'medium'
                            consolidated_details.append(detail)
                            
                    if consolidated_details:
                        all_details = consolidated_details
                        logger.info(f"Successfully consolidated details for {subtopic['name']} from {len(all_details)} to {len(consolidated_details)}")
                    
            except Exception as e:
                logger.warning(f"Detail consolidation failed for {subtopic['name']}: {str(e)}", 
                            extra={"request_id": request_id})
                # If consolidation fails, do a simple deduplication pass
                seen = set()
                deduplicated_details = []
                for detail in all_details:
                    if detail['text'] not in seen:
                        seen.add(detail['text'])
                        deduplicated_details.append(detail)
                all_details = deduplicated_details
                if len(self._current_details) >= MINIMUM_VALID_DETAILS:
                    logger.info(f"Using {len(self._current_details)} previously collected valid details")
                    all_details = self._current_details
                else:
                    importance_order = {"high": 0, "medium": 1, "low": 2}
                    all_details = sorted(
                        all_details,
                        key=lambda x: (importance_order.get(x["importance"].lower(), 3), -len(x["text"]))
                    )[:MAX_DETAILS]

            # Final LLM-based deduplication when we have enough details
            if len(all_details) > 3:  # Only if we have enough to potentially remove some
                details_to_remove = set()
                for i in range(len(all_details)-1):
                    if i in details_to_remove:
                        continue
                        
                    if len(all_details) - len(details_to_remove) <= 3:  # Ensure we keep at least 3 details
                        break
                        
                    for j in range(i+1, len(all_details)):
                        if j in details_to_remove:
                            continue
                        
                        try:
                            is_duplicate = await self.check_similarity_llm(
                                all_details[i]['text'], 
                                all_details[j]['text'],
                                f"detail of {subtopic['name']}", 
                                f"detail of {subtopic['name']}"
                            )
                            
                            if is_duplicate:
                                logger.info("LLM detected duplicate details")
                                
                                # Determine which to keep based on importance
                                importance_i = {"high": 3, "medium": 2, "low": 1}[all_details[i]['importance']]
                                importance_j = {"high": 3, "medium": 2, "low": 1}[all_details[j]['importance']]
                                
                                if importance_i >= importance_j:
                                    details_to_remove.add(j)
                                else:
                                    details_to_remove.add(i)
                                    break  # Break inner loop if we're removing i
                        except Exception as e:
                            logger.warning(f"LLM duplicate check failed: {str(e)}")
                            continue
                            
                # Apply removals
                all_details = [d for i, d in enumerate(all_details) if i not in details_to_remove]
            
            # Sort details by importance first, then by length (longer details typically have more substance)
            importance_order = {"high": 0, "medium": 1, "low": 2}
            final_details = sorted(
                all_details, 
                key=lambda x: (importance_order.get(x["importance"].lower(), 3), -len(x["text"]))
            )[:MAX_DETAILS]            
            self._details_cache[cache_key] = final_details
            
            logger.info(f"Successfully extracted {len(final_details)} details for {subtopic['name']}", 
                            extra={"request_id": request_id})
            return final_details
                
        except Exception as e:
            logger.error(f"Failed to extract details for subtopic {subtopic['name']}: {str(e)}", 
                        extra={"request_id": request_id})
            if hasattr(self, '_current_details') and len(self._current_details) > 0:
                logger.info(f"Returning {len(self._current_details)} collected details despite error")
                return self._current_details[:MAX_DETAILS]
            return []
            
    async def _retry_generate_completion(self, prompt: str, max_tokens: int, request_id: str, task: str) -> str:
        """Retry the LLM completion in case of failures with exponential backoff."""
        retries = 0
        base_delay = 1  # Start with 1 second delay
        
        while retries < self.config['max_retries']:
            try:
                response = await self.optimizer.generate_completion(
                    prompt,
                    max_tokens=max_tokens,
                    request_id=request_id,
                    task=task
                )
                return response
            except Exception as e:
                retries += 1
                if retries >= self.config['max_retries']:
                    logger.error(f"Exceeded maximum retries for {task}", extra={"request_id": request_id})
                    raise
                
                delay = min(base_delay * (2 ** (retries - 1)), 10)  # Cap at 10 seconds
                logger.warning(f"Retrying {task} ({retries}/{self.config['max_retries']}) after {delay}s: {str(e)}", extra={"request_id": request_id})
                await asyncio.sleep(delay)

    async def verify_mindmap_against_source(self, mindmap_data: Dict[str, Any], original_document: str) -> Dict[str, Any]:
        """Verify all mindmap nodes against the original document with lenient criteria and improved error handling."""
        try:
            logger.info("\n" + "="*80)
            logger.info(colored("ğŸ” STARTING REALITY CHECK TO IDENTIFY POTENTIAL CONFABULATIONS", "cyan", attrs=["bold"]))
            logger.info("="*80 + "\n")
            
            # Split document into chunks to handle context window limitations
            chunk_size = 8000  # Adjust based on model context window
            overlap = 250  # Characters of overlap between chunks
            
            # Create overlapping chunks
            doc_chunks = []
            start = 0
            while start < len(original_document):
                end = min(start + chunk_size, len(original_document))
                # Extend to nearest sentence end if possible
                if end < len(original_document):
                    next_period = original_document.find('.', end)
                    if next_period != -1 and next_period - end < 200:  # Don't extend too far
                        end = next_period + 1
                chunk = original_document[start:end]
                doc_chunks.append(chunk)
                start = end - overlap if end < len(original_document) else end
            
            logger.info(f"Split document into {len(doc_chunks)} chunks for verification")
            
            # Extract all nodes from mindmap for verification
            all_nodes = []
            
            def extract_nodes(node, path=None):
                """Recursively extract all nodes with their paths."""
                if path is None:
                    path = []
                
                if not node:
                    return
                    
                current_path = path.copy()
                
                # Add current node if it has a name
                if 'name' in node and node['name']:
                    node_type = 'root' if not path else 'topic' if len(path) == 1 else 'subtopic'
                    all_nodes.append({
                        'text': node['name'],
                        'path': current_path,
                        'type': node_type,
                        'verified': False,
                        'node_ref': node,  # Store reference to original node
                        'node_id': id(node),  # Store unique object ID as backup
                        'structural_importance': 'high' if node_type in ['root', 'topic'] else 'medium'
                    })
                    current_path = current_path + [node['name']]
                
                # Add details
                for detail in node.get('details', []):
                    if isinstance(detail, dict) and 'text' in detail:
                        all_nodes.append({
                            'text': detail['text'],
                            'path': current_path,
                            'type': 'detail',
                            'verified': False,
                            'node_ref': detail,  # Store reference to original node
                            'node_id': id(detail),  # Store unique object ID as backup
                            'structural_importance': 'low',
                            'importance': detail.get('importance', 'medium')
                        })
                
                # Process subtopics
                for subtopic in node.get('subtopics', []):
                    extract_nodes(subtopic, current_path)
            
            # Start extraction from central theme
            extract_nodes(mindmap_data.get('central_theme', {}))
            logger.info(f"Extracted {len(all_nodes)} nodes for verification")
            
            # Create verification batches to limit concurrent API calls
            batch_size = 5  # Number of nodes to verify in parallel
            node_batches = [all_nodes[i:i+batch_size] for i in range(0, len(all_nodes), batch_size)]
            
            # Track verification statistics
            verification_stats = {
                'total': len(all_nodes),
                'verified': 0,
                'not_verified': 0,
                'by_type': {
                    'topic': {'total': 0, 'verified': 0},
                    'subtopic': {'total': 0, 'verified': 0},
                    'detail': {'total': 0, 'verified': 0}
                }
            }
            
            for node_type in ['topic', 'subtopic', 'detail']:
                verification_stats['by_type'][node_type]['total'] = len([n for n in all_nodes if n.get('type') == node_type])
            
            # Function to verify a single node against a document chunk
            async def verify_node_in_chunk(node, chunk):
                """Verify if a node's content is actually present in or can be logically derived from a document chunk."""
                if not node or not chunk:
                    return False
                    
                # Check if node has required keys
                required_keys = ['type', 'text']
                if not all(key in node for key in required_keys):
                    logger.warning(f"Node missing required keys: {node}")
                    return True  # Consider verified if we can't properly check it
                    
                # Special handling for root node
                if node['type'] == 'root':
                    return True  # Always consider root node verified
                    
                node_text = node['text']
                node_type = node['type']
                path_str = ' â†’ '.join(node['path']) if node['path'] else 'root'
                
                prompt = f"""You are an expert fact-checker verifying if information in a mindmap can be reasonably derived from the original document.

            Task: Determine if this {node_type} is supported by the document text or could be reasonably inferred from it.

            {node_type.title()}: "{node_text}"
            Path: {path_str}

            Document chunk:
            ```
            {chunk}
            ```

            VERIFICATION GUIDELINES:
            1. The {node_type} can be EXPLICITLY mentioned OR reasonably inferred from the document, even through logical deduction
            2. Logical synthesis, interpretation, and summarization of concepts in the document are STRONGLY encouraged
            3. Content that represents a reasonable conclusion or implication from the document should be VERIFIED
            4. Content that groups, categorizes, or abstracts ideas from the document should be VERIFIED
            5. High-level insights that connect multiple concepts from the document should be VERIFIED
            6. Only mark as unsupported if it contains specific claims that DIRECTLY CONTRADICT the document
            7. GIVE THE BENEFIT OF THE DOUBT - if the content could plausibly be derived from the document, verify it
            8. When uncertain, LEAN TOWARDS VERIFICATION rather than rejection - mindmaps are meant to be interpretive, not literal
            9. For details specifically, allow for more interpretive latitude - they represent insights derived from the document
            10. Consider historical and domain context that would be natural to include in an analysis

            Answer ONLY with one of these formats:
            - "YES: [brief explanation of how it's supported or can be derived]" 
            - "NO: [brief explanation of why it contains information that directly contradicts the document]"

            IMPORTANT: Remember to be GENEROUS in your interpretation. If there's any reasonable way the content could be derived from the document, even through multiple logical steps, mark it as verified. Only reject content that introduces completely new facts not derivable from the document or directly contradicts it."""

                try:
                    response = await self._retry_generate_completion(
                        prompt,
                        max_tokens=150,
                        request_id='verify_node',
                        task="verifying_against_source"
                    )
                    
                    # Parse the response to get verification result
                    response = response.strip().upper()
                    is_verified = response.startswith("YES")
                    
                    # Log detailed verification result for debugging
                    logger.debug(
                        f"\n{colored('Verification result for', 'blue')}: {colored(node_text[:50] + '...', 'yellow')}\n"
                        f"Result: {colored('VERIFIED' if is_verified else 'NOT VERIFIED', 'green' if is_verified else 'red')}\n"
                        f"Response: {response[:100]}"
                    )
                    
                    return is_verified
                    
                except Exception as e:
                    logger.error(f"Error verifying node: {str(e)}")
                    # Be more lenient on errors - consider verified
                    return True
            
            # Process each node batch
            for batch_idx, batch in enumerate(node_batches):
                logger.info(f"Verifying batch {batch_idx+1}/{len(node_batches)} ({len(batch)} nodes)")
                
                # For each node, try to verify against any document chunk
                for node in batch:
                    if node.get('verified', False):
                        continue  # Skip if already verified
                        
                    node_verified = False
                    
                    # Try to verify against each chunk
                    for chunk_idx, chunk in enumerate(doc_chunks):
                        if await verify_node_in_chunk(node, chunk):
                            node['verified'] = True
                            node_verified = True
                            verification_stats['verified'] += 1
                            node_type = node.get('type', 'unknown')
                            if node_type in verification_stats['by_type']:
                                verification_stats['by_type'][node_type]['verified'] += 1
                            logger.info(
                                f"{colored('âœ… VERIFIED', 'green', attrs=['bold'])}: "
                                f"{node.get('type', 'NODE').upper()} '{node.get('text', '')[:50]}...' "
                                f"(Found in chunk {chunk_idx+1})"
                            )
                            break
                    
                    if not node_verified:
                        verification_stats['not_verified'] += 1
                        logger.info(
                            f"{colored('â“ NOT VERIFIED', 'yellow', attrs=['bold'])}: "
                            f"{node.get('type', 'NODE').upper()} '{node.get('text', '')[:50]}...' "
                            f"(Not found in any chunk)"
                        )
            
            # Calculate verification percentages
            verification_percentage = (verification_stats['verified'] / verification_stats['total'] * 100) if verification_stats['total'] > 0 else 0
            for node_type in ['topic', 'subtopic', 'detail']:
                type_stats = verification_stats['by_type'][node_type]
                type_stats['percentage'] = (type_stats['verified'] / type_stats['total'] * 100) if type_stats['total'] > 0 else 0
            
            # Log verification statistics
            logger.info("\n" + "="*80)
            logger.info(colored("ğŸ” REALITY CHECK RESULTS", "cyan", attrs=['bold']))
            logger.info(f"Total nodes checked: {verification_stats['total']}")
            logger.info(f"Verified: {verification_stats['verified']} ({verification_percentage:.1f}%)")
            logger.info(f"Not verified: {verification_stats['not_verified']} ({100-verification_percentage:.1f}%)")
            logger.info("\nBreakdown by node type:")
            for node_type in ['topic', 'subtopic', 'detail']:
                type_stats = verification_stats['by_type'][node_type]
                logger.info(f"  {node_type.title()}s: {type_stats['verified']}/{type_stats['total']} verified ({type_stats['percentage']:.1f}%)")
            logger.info("="*80 + "\n")
            
            # Check if we need to preserve structure despite verification results
            min_topics_required = 3
            min_verification_ratio = 0.4  # Lower threshold - only filter if less than 40% verified
            
            # Count verified topics
            verified_topics = len([n for n in all_nodes if n.get('type') == 'topic' and n.get('verified', False)])
            
            # If verification removed too much content, we need to preserve structure
            if verified_topics < min_topics_required or verification_percentage < min_verification_ratio * 100:
                logger.warning(f"Verification would remove too much content (only {verified_topics} topics verified). Using preservation mode.")
                
                # Mark important structural nodes as verified to preserve mindmap structure
                for node in all_nodes:
                    # Always keep root and topic nodes
                    if node.get('type') in ['root', 'topic']:
                        node['verified'] = True
                    # Keep subtopics with a high enough importance
                    elif node.get('type') == 'subtopic' and not node.get('verified', False):
                        # Keep subtopics if they have verified details or are needed for structure
                        has_verified_details = any(
                            n.get('verified', False) and n.get('type') == 'detail' and n.get('path') == node.get('path', []) + [node.get('text', '')]
                            for n in all_nodes
                        )
                        if has_verified_details:
                            node['verified'] = True
                
                # Recalculate statistics
                verification_stats['verified'] = len([n for n in all_nodes if n.get('verified', False)])
                verification_stats['not_verified'] = len(all_nodes) - verification_stats['verified']
                verification_percentage = (verification_stats['verified'] / verification_stats['total'] * 100) if verification_stats['total'] > 0 else 0
                
                logger.info("\n" + "="*80)
                logger.info(colored("ğŸ”„ UPDATED REALITY CHECK WITH STRUCTURE PRESERVATION", "yellow", attrs=['bold']))
                logger.info(f"Verified after preservation: {verification_stats['verified']} ({verification_percentage:.1f}%)")
                logger.info(f"Not verified after preservation: {verification_stats['not_verified']} ({100-verification_percentage:.1f}%)")
                logger.info("="*80 + "\n")
            
            # Rebuild mindmap with preserving structure
            def rebuild_mindmap(node):
                """Recursively rebuild mindmap keeping only verified nodes."""
                if not node:
                    return None
                    
                result = copy.deepcopy(node)
                result['subtopics'] = []
                
                # Process subtopics and keep only verified ones
                verified_subtopics = []
                for subtopic in node.get('subtopics', []):
                    if not subtopic.get('name'):
                        continue
                        
                    # Check if this subtopic is verified by comparing with stored nodes
                    subtopic_verified = False
                    subtopic_id = id(subtopic)
                    
                    for n in all_nodes:
                        # First try to match by direct object reference
                        if n.get('node_ref') is subtopic and n.get('verified', False):
                            subtopic_verified = True
                            break
                        # Fallback to matching by object ID if reference comparison fails
                        elif n.get('node_id') == subtopic_id and n.get('verified', False):
                            subtopic_verified = True
                            break
                        # Last resort: match by name and path
                        elif (n.get('type') in ['topic', 'subtopic'] and 
                            n.get('text') == subtopic.get('name') and 
                            n.get('verified', False)):
                            subtopic_verified = True
                            break
                    
                    if subtopic_verified:
                        rebuilt_subtopic = rebuild_mindmap(subtopic)
                        if rebuilt_subtopic:
                            verified_subtopics.append(rebuilt_subtopic)
                
                result['subtopics'] = verified_subtopics
                
                # Filter details to keep only verified ones
                if 'details' in result:
                    verified_details = []
                    for detail in result.get('details', []):
                        if not isinstance(detail, dict) or 'text' not in detail:
                            continue
                            
                        # Check if this detail is verified
                        detail_verified = False
                        detail_id = id(detail)
                        
                        for n in all_nodes:
                            # First try to match by direct object reference
                            if n.get('node_ref') is detail and n.get('verified', False):
                                detail_verified = True
                                break
                            # Fallback to matching by object ID
                            elif n.get('node_id') == detail_id and n.get('verified', False):
                                detail_verified = True
                                break
                            # Last resort: match by text content
                            elif n.get('type') == 'detail' and n.get('text') == detail.get('text') and n.get('verified', False):
                                detail_verified = True
                                break
                        
                        if detail_verified:
                            verified_details.append(detail)
                    
                    result['details'] = verified_details
                
                # Only return node if it has content
                if result.get('subtopics') or result.get('details'):
                    return result
                return None
            
            # Rebuild mindmap with only verified content
            verified_mindmap = {
                'central_theme': rebuild_mindmap(mindmap_data.get('central_theme', {}))
            }
            
            # Final safety check - if we have no content after verification, use original
            if not verified_mindmap.get('central_theme') or not verified_mindmap.get('central_theme', {}).get('subtopics'):
                logger.warning("After verification, no valid content remains - using original mindmap with warning")
                return mindmap_data
            
            # Calculate how much content was preserved
            original_count = len(all_nodes)
            verified_count = len([n for n in all_nodes if n.get('verified', False)])
            preservation_rate = (verified_count / original_count * 100) if original_count > 0 else 0
            
            logger.info(
                f"\n{colored('âœ… REALITY CHECK COMPLETE', 'green', attrs=['bold'])}\n"
                f"Preserved {verified_count}/{original_count} nodes ({preservation_rate:.1f}%)"
            )
            
            return verified_mindmap
        
        except Exception as e:
            # Better error handling with detailed logging
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error during verification: {str(e)}\n{error_details}")
            # Return the original mindmap in case of any errors
            return mindmap_data

    def _generate_mermaid_mindmap(self, concepts: Dict[str, Any]) -> str:
        """Generate complete Mermaid mindmap syntax from concepts.
        
        Args:
            concepts (Dict[str, Any]): The complete mindmap concept hierarchy
            
        Returns:
            str: Complete Mermaid mindmap syntax
        """
        mindmap_lines = ["mindmap"]
        
        # Start with root node - ignore any name/text for root, just use document emoji
        self._add_node_to_mindmap({'name': ''}, mindmap_lines, indent_level=1)
        
        # Add all main topics under root
        for topic in concepts.get('central_theme', {}).get('subtopics', []):
            self._add_node_to_mindmap(topic, mindmap_lines, indent_level=2)
        
        return "\n".join(mindmap_lines)

    def _convert_mindmap_to_markdown(self, mermaid_syntax: str) -> str:
        """Convert Mermaid mindmap syntax to properly formatted Markdown outline.
        
        Args:
            mermaid_syntax (str): The Mermaid mindmap syntax string
            
        Returns:
            str: Properly formatted Markdown outline
        """
        markdown_lines = []
        
        # Split into lines and process each (skip the 'mindmap' header)
        lines = mermaid_syntax.split('\n')[1:]
        
        for line in lines:
            # Skip empty lines
            if not line.strip():
                continue
                
            # Count indentation level (number of 4-space blocks)
            indent_level = len(re.match(r'^\s*', line).group()) // 4
            
            # Extract the content between node shapes
            content = line.strip()
            
            # Handle different node types based on indent level
            if indent_level == 1 and '((ğŸ“„))' in content:  # Root node
                continue  # Skip the document emoji node
                
            elif indent_level == 2:  # Main topics
                # Extract content between (( and ))
                node_text = re.search(r'\(\((.*?)\)\)', content)
                if node_text:
                    if markdown_lines:  # Add extra newline between main topics
                        markdown_lines.append("")
                    current_topic = node_text.group(1).strip()
                    markdown_lines.append(f"# {current_topic}")
                    markdown_lines.append("")  # Add blank line after topic
                    
            elif indent_level == 3:  # Subtopics
                # Extract content between ( and )
                node_text = re.search(r'\((.*?)\)', content)
                if node_text:
                    if markdown_lines and not markdown_lines[-1].startswith("#"):
                        markdown_lines.append("")
                    current_subtopic = node_text.group(1).strip()
                    markdown_lines.append(f"## {current_subtopic}")
                    markdown_lines.append("")  # Add blank line after subtopic
                    
            elif indent_level == 4:  # Details
                # Extract content between [ and ]
                node_text = re.search(r'\[(.*?)\]', content)
                if node_text:
                    detail_text = node_text.group(1).strip()
                    markdown_lines.append(detail_text)
                    markdown_lines.append("")  # Add blank line after each detail
        
        # Join lines with proper spacing
        markdown_text = "\n".join(markdown_lines)
        
        # Clean up any lingering Mermaid syntax artifacts
        markdown_text = re.sub(r'\\\(', '(', markdown_text)
        markdown_text = re.sub(r'\\\)', ')', markdown_text)
        markdown_text = re.sub(r'\\(?=[()])', '', markdown_text)
        
        # Clean up multiple consecutive blank lines
        markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
        
        return markdown_text.strip()
    
    async def generate_mindmap_simple(self, document_content: str, request_id: str) -> str:
        """Generate a mindmap using a single prompt approach for faster generation.
        
        This is a simplified version that uses one LLM call to generate the entire mindmap structure,
        trading some quality for speed.
        
        Args:
            document_content (str): The document content to analyze
            request_id (str): Unique identifier for request tracking
            
        Returns:
            str: Complete Mermaid mindmap syntax
            
        Raises:
            MindMapGenerationError: If mindmap generation fails
        """
        try:
            logger.info("Starting simple mindmap generation process...", extra={"request_id": request_id})
            
            # Initialize tracking
            self._llm_calls = {
                'topics': 0,
                'subtopics': 0,
                'details': 0,
                'simple': 0
            }
            
            # Create simple prompt for direct mindmap generation
            simple_prompt = f"""You are an expert at identifying unique, distinct main topics within content.

            Analyze this document focusing on main conceptual themes and relationships.

            Identify major themes that:
            - Represent complete, independent ideas
            - Form logical groupings of related concepts
            - Support the document's main purpose
            - Connect to other important themes

            Consider:
            1. What are the fundamental ideas being presented?
            2. How do these ideas relate to each other?
            3. What are the key areas of focus?
            4. How is the information structured?

            Avoid topics that are:
            - Too specific (individual examples)
            - Too broad (entire subject areas)
            - Isolated facts without context
            - Purely formatting elements

            Additional requirements:
            1. Each topic must be truly distinct from others - avoid overlapping concepts
            2. Combine similar themes into single, well-defined topics
            3. Ensure topics are specific enough to be meaningful but general enough to support subtopics
            4. Aim for 4-8 most significant topics that capture the key distinct areas
            5. Focus on conceptual separation - each topic should represent a unique aspect or dimension
            6. Avoid topics that are too similar or could be subtopics of each other
            7. Prioritize broader topics that can encompass multiple subtopics
            8. Eliminate redundancy - each topic should cover a distinct area with no overlap

            IMPORTANT: 
            1. DO NOT include specific statistics, percentages, or numerical data unless explicitly stated in the source text
            2. DO NOT refer to modern studies, surveys, or analyses that aren't mentioned in the document
            3. DO NOT make up correlation coefficients, growth rates, or other numerical relationships
            4. Keep your content strictly based on what's in the document, not general knowledge about the topic
            5. Use general descriptions rather than specific numbers if the document doesn't provide exact figures

            Current content:
            {document_content[:4000]}

            IMPORTANT: Respond with ONLY a JSON array of strings representing the main distinct topics.
            Example format: ["First Distinct Topic", "Second Distinct Topic", "Third Distinct Topic"]

            Format: Return a JSON array of primary themes or concept areas."""

            # Make single LLM call
            logger.info("Making single LLM call for simplified mindmap generation...", extra={"request_id": request_id})
            
            response = await self.optimizer.generate_completion(
                simple_prompt,
                max_tokens=3000,
                request_id=request_id,
                task="simple_mindmap_generation"
            )
            
            self._llm_calls['simple'] = 1
            
            if not response:
                raise MindMapGenerationError("No response from LLM for simple mindmap generation")

            # Parse the response
            try:
                # Use the simpler _parse_llm_response method that handles arrays better
                logger.debug(f"Raw simple generation response: {response}", 
                            extra={"request_id": request_id})
                
                parsed_response = self._parse_llm_response(response, "array")
                
                # Validate structure - should be array of strings
                if not isinstance(parsed_response, list):
                    logger.warning(f"Expected list, got {type(parsed_response)}")
                    raise ValueError("Expected array of topic strings")
                
                # Convert to internal format with simplified structure
                concepts = {
                    'central_theme': {
                        'name': 'Document Mindmap',
                        'subtopics': []
                    }
                }
                
                # Process each topic string
                for topic_name in parsed_response:
                    if isinstance(topic_name, str) and topic_name.strip():
                        cleaned_name = re.sub(r'[`*_#]', '', topic_name)
                        cleaned_name = ' '.join(cleaned_name.split())
                        
                        # Select appropriate emoji for topic
                        emoji = await self._select_emoji(cleaned_name, 'topic')
                        
                        # Create simplified topic structure
                        topic = {
                            'name': cleaned_name,
                            'emoji': emoji,
                            'importance': 'high',
                            'subtopics': [{
                                'name': f'{cleaned_name} - å…³é”®è¦ç‚¹',
                                'emoji': 'ğŸ“‹',
                                'importance': 'medium',
                                'details': [
                                    {
                                        'text': f'å…³äº{cleaned_name}çš„é‡è¦ä¿¡æ¯',
                                        'importance': 'medium'
                                    },
                                    {
                                        'text': f'{cleaned_name}çš„æ ¸å¿ƒæ¦‚å¿µ',
                                        'importance': 'medium'
                                    }
                                ]
                            }]
                        }
                        
                        concepts['central_theme']['subtopics'].append(topic)
                
                # If no topics were extracted, create a fallback
                if not concepts['central_theme']['subtopics']:
                    logger.warning("No topics extracted, creating fallback structure")
                    concepts['central_theme']['subtopics'] = [{
                        'name': 'Document Summary',
                        'emoji': 'ğŸ“„',
                        'importance': 'high',
                        'subtopics': [{
                            'name': 'Key Content',
                            'emoji': 'ğŸ“‹',
                            'importance': 'medium',
                            'details': [
                                {'text': 'Document analysis completed', 'importance': 'medium'},
                                {'text': 'Content processed successfully', 'importance': 'medium'}
                            ]
                        }]
                    }]
                
                # Generate Mermaid syntax
                mermaid_syntax = self._generate_mermaid_mindmap(concepts)
                
                logger.info(
                    f"Simple mindmap generation completed successfully. "
                    f"Topics: {len(concepts['central_theme']['subtopics'])}, "
                    f"LLM calls: {self._llm_calls['simple']}, "
                    f"Output length: {len(mermaid_syntax)} characters",
                    extra={"request_id": request_id}
                )
                
                return mermaid_syntax
                
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse LLM response as JSON: {str(e)}")
                # Fallback: try to extract basic structure from text
                return await self._fallback_simple_generation(response, request_id)
                
        except Exception as e:
            logger.error(f"Error in simple mindmap generation: {str(e)}", extra={"request_id": request_id})
            raise MindMapGenerationError(f"Failed to generate simple mindmap: {str(e)}")

    async def _fallback_simple_generation(self, response: str, request_id: str) -> str:
        """Fallback method when JSON parsing fails - extract basic structure from text."""
        try:
            logger.info("Using fallback simple generation method...", extra={"request_id": request_id})
            
            # Create basic structure from response text
            lines = response.strip().split('\n')
            topics = []
            
            current_topic = None
            current_subtopic = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # Look for topic indicators
                if any(marker in line.lower() for marker in ['topic:', 'main:', 'ä¸»é¢˜:', '##']):
                    if current_topic:
                        topics.append(current_topic)
                    current_topic = {
                        'name': line.split(':', 1)[-1].strip() if ':' in line else line.replace('#', '').strip(),
                        'emoji': 'ğŸ“„',
                        'importance': 'high',
                        'subtopics': []
                    }
                    current_subtopic = None
                    
                # Look for subtopic indicators
                elif any(marker in line.lower() for marker in ['subtopic:', 'sub:', 'å­ä¸»é¢˜:', '###']):
                    if current_topic:
                        if current_subtopic:
                            current_topic['subtopics'].append(current_subtopic)
                        current_subtopic = {
                            'name': line.split(':', 1)[-1].strip() if ':' in line else line.replace('#', '').strip(),
                            'emoji': 'ğŸ“‹',
                            'importance': 'medium',
                            'details': []
                        }
                        
                # Look for detail indicators
                elif any(marker in line for marker in ['-', 'â€¢', '*', '1.', '2.', '3.']):
                    if current_subtopic:
                        detail_text = line.lstrip('-â€¢*0123456789. ').strip()
                        if detail_text:
                            current_subtopic['details'].append({
                                'text': detail_text,
                                'importance': 'medium'
                            })
            
            # Add final items
            if current_subtopic and current_topic:
                current_topic['subtopics'].append(current_subtopic)
            if current_topic:
                topics.append(current_topic)
            
            # If no topics found, create a basic structure
            if not topics:
                topics = [{
                    'name': 'Document Summary',
                    'emoji': 'ğŸ“„',
                    'importance': 'high',
                    'subtopics': [{
                        'name': 'Key Points',
                        'emoji': 'ğŸ“‹',
                        'importance': 'medium',
                        'details': [
                            {'text': 'Content analysis completed', 'importance': 'medium'},
                            {'text': 'Basic structure extracted', 'importance': 'medium'}
                        ]
                    }]
                }]
            
            # Create concepts structure
            concepts = {
                'central_theme': {
                    'name': 'Document Mindmap',
                    'subtopics': topics
                }
            }
            
            # Generate Mermaid syntax
            mermaid_syntax = self._generate_mermaid_mindmap(concepts)
            
            logger.info(
                f"Fallback simple generation completed. Topics: {len(topics)}",
                extra={"request_id": request_id}
            )
            
            return mermaid_syntax
            
        except Exception as e:
            logger.error(f"Fallback simple generation failed: {str(e)}")
            # Return minimal mindmap
            return """mindmap
    ((ğŸ“„))
        ((ğŸ“‹ Document Analysis))
            (Error in Processing)
                [Simple generation failed]
                [Please try again]"""

def generate_mermaid_html(mermaid_code):
    # Remove leading/trailing triple backticks if present
    mermaid_code = mermaid_code.strip()
    if mermaid_code.startswith('```') and mermaid_code.endswith('```'):
        mermaid_code = mermaid_code[3:-3].strip()
    # Create the data object to be encoded
    data = {
        "code": mermaid_code,
        "mermaid": {"theme": "default"}
    }
    json_string = json.dumps(data)
    compressed_data = zlib.compress(json_string.encode('utf-8'), level=9)
    base64_string = base64.urlsafe_b64encode(compressed_data).decode('utf-8').rstrip('=')
    edit_url = f'https://mermaid.live/edit#pako:{base64_string}'
    # Now generate the HTML template
    html_template = f'''<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Mermaid Mindmap</title>
  <!-- Tailwind CSS -->
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <!-- Mermaid JS -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.0/dist/mermaid.min.js"></script>
  <style>
    body {{
      margin: 0;
      padding: 0;
    }}
    #mermaid {{
      width: 100%;
      height: calc(100vh - 64px); /* Adjust height considering header */
      overflow: auto;
    }}
  </style>
</head>
<body class="bg-gray-100">
  <div class="flex items-center justify-between p-4 bg-white shadow">
    <h1 class="text-xl font-bold">Mermaid Mindmap</h1>
    <a href="{edit_url}" target="_blank" id="editButton" class="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600">Edit in Mermaid Live Editor</a>
  </div>
  <div id="mermaid" class="p-4">
    <pre class="mermaid">
{mermaid_code}
    </pre>
  </div>
  <script>
    mermaid.initialize({{
      startOnLoad: true,
      securityLevel: 'loose',
      theme: 'default',
      mindmap: {{
        useMaxWidth: true
      }},
      themeConfig: {{
        controlBar: true
      }}
    }});
  </script>
</body>
</html>'''
    return html_template

async def generate_document_mindmap(document_id: str, request_id: str) -> Tuple[str, str]:
    """Generate both Mermaid mindmap and Markdown outline for a document.
    
    Args:
        document_id (str): The ID of the document to process
        request_id (str): Unique identifier for request tracking
        
    Returns:
        Tuple[str, str]: (mindmap_file_path, markdown_file_path)
    """
    try:
        generator = MindMapGenerator()
        db = await initialize_db()
        document = await db.get_document_by_id(document_id)
        if not document:
            logger.error(f"Document not found: {document_id}", extra={"request_id": request_id})
            return "", ""

        # Define file paths for both formats
        mindmap_file_path = f"generated_mindmaps/{document['sanitized_filename']}_mermaid_mindmap__{Config.API_PROVIDER.lower()}.txt"
        mindmap_html_file_path = f"generated_mindmaps/{document['sanitized_filename']}_mindmap__{Config.API_PROVIDER.lower()}.html"
        markdown_file_path = f"generated_mindmaps/{document['sanitized_filename']}_mindmap_outline__{Config.API_PROVIDER.lower()}.md"
        
        # Check if both files already exist
        if os.path.exists(mindmap_file_path) and os.path.exists(markdown_file_path):
            logger.info(f"Both mindmap and markdown already exist for document {document_id}. Reusing existing files.", extra={"request_id": request_id})
            return mindmap_file_path, markdown_file_path

        optimized_text = await db.get_optimized_text(document_id, request_id)
        if not optimized_text:
            logger.error(f"Optimized text not found for document: {document_id}", extra={"request_id": request_id})
            return "", ""

        # Generate mindmap
        mermaid_syntax = await generator.generate_mindmap(optimized_text, request_id)
        
        # Convert to HTML
        mermaid_html = generate_mermaid_html(mermaid_syntax)
        
        # Convert to markdown
        markdown_outline = generator._convert_mindmap_to_markdown(mermaid_syntax)

        # Save all 3 formats
        os.makedirs(os.path.dirname(mindmap_file_path), exist_ok=True)
        
        async with aiofiles.open(mindmap_file_path, 'w', encoding='utf-8') as f:
            await f.write(mermaid_syntax)
            
        async with aiofiles.open(mindmap_html_file_path, 'w', encoding='utf-8') as f:
            await f.write(mermaid_html)
            
        async with aiofiles.open(markdown_file_path, 'w', encoding='utf-8') as f:
            await f.write(markdown_outline)

        logger.info(f"Mindmap and associated html/markdown generated successfully for document {document_id}", extra={"request_id": request_id})
        return mindmap_file_path, mindmap_html_file_path, markdown_file_path
        
    except Exception as e:
        logger.error(f"Error generating mindmap and associated html/markdown for document {document_id}: {str(e)}", extra={"request_id": request_id})
        return "", ""

async def process_text_file(filepath: str):
    """Process a single text file and generate mindmap outputs."""
    logger = get_logger()
    try:
        # Read the input file
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        # Store content in our stub database
        MinimalDatabaseStub.store_text(content)
        # Generate a unique document ID based on content hash
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = os.path.splitext(os.path.basename(filepath))[0]
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        # Initialize the mindmap generator
        generator = MindMapGenerator()
        # Generate the mindmap
        mindmap = await generator.generate_mindmap(content, request_id=document_id)
        # Generate HTML
        html = generate_mermaid_html(mindmap)
        # Generate markdown outline
        markdown_outline = generator._convert_mindmap_to_markdown(mindmap)
        # Create output directory if it doesn't exist
        os.makedirs("mindmap_outputs", exist_ok=True)
        # Save outputs with simple names based on input file
        output_files = {
            f"mindmap_outputs/{base_filename}_mindmap__{Config.API_PROVIDER.lower()}.txt": mindmap,
            f"mindmap_outputs/{base_filename}_mindmap__{Config.API_PROVIDER.lower()}.html": html,
            f"mindmap_outputs/{base_filename}_mindmap_outline__{Config.API_PROVIDER.lower()}.md": markdown_outline
        }
        # Save all outputs
        for filename, content in output_files.items():
            with open(filename, "w", encoding="utf-8") as f:
                f.write(content)
                logger.info(f"Saved {filename}")
        
        logger.info("Mindmap generation completed successfully!")
        return output_files
    except Exception as e:
        logger.error(f"Error processing file: {str(e)}")
        raise
    
async def main():
    input_file = "sample_input_document_as_markdown__durnovo_memo.md"  # <-- Change this to your input file path
    # input_file = "sample_input_document_as_markdown__small.md"
    try:
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"Input file not found: {input_file}")
            
        # Process the file
        logger.info(f"Generating mindmap for {input_file}")
        output_files = await process_text_file(input_file)
        
        # Print summary
        print("\nMindmap Generation Complete!")
        print("Generated files:")
        for filename in output_files:
            print(f"- {filename}")
            
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        raise
if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="README.md">
# ğŸ§  æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨

> åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ–‡æ¡£åˆ†æä¸æ€ç»´å¯¼å›¾ç”Ÿæˆç³»ç»Ÿ

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†ææ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆç»“æ„åŒ–çš„äº¤äº’å¼æ€ç»´å¯¼å›¾ã€‚ç³»ç»Ÿé›†æˆäº†å¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å‰åç«¯åˆ†ç¦»æ¶æ„ï¼Œæä¾›ä¼˜ç§€çš„ç”¨æˆ·ä½“éªŒã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸ¤– **å¤šAIæ¨¡å‹æ”¯æŒ**ï¼šDeepSeekã€OpenAI GPTã€Claudeã€Geminiç­‰ä¸»æµæ¨¡å‹
- ğŸ“„ **å¤šæ ¼å¼æ”¯æŒ**ï¼šMarkdown (.md)ã€æ–‡æœ¬ (.txt) æ–‡ä»¶ï¼Œå¯æ‰©å±•PDFæ”¯æŒ  
- ğŸ¨ **äº¤äº’å¼å¯è§†åŒ–**ï¼šåŸºäºMermaid.jsçš„é«˜è´¨é‡æ€ç»´å¯¼å›¾
- âš¡ **å¼‚æ­¥å¤„ç†**ï¼šæ–‡æ¡£ä¸Šä¼ åç«‹å³æ˜¾ç¤ºï¼Œæ€ç»´å¯¼å›¾å¼‚æ­¥ç”Ÿæˆ
- ğŸ’» **ç°ä»£åŒ–ç•Œé¢**ï¼šReact + Tailwind CSSå“åº”å¼è®¾è®¡
- ğŸ“± **ç§»åŠ¨ç«¯é€‚é…**ï¼šæ”¯æŒç§»åŠ¨è®¾å¤‡è®¿é—®
- ğŸ”„ **å®æ—¶åŒæ­¥**ï¼šæ–‡æ¡£é˜…è¯»ä¸æ€ç»´å¯¼å›¾è”åŠ¨é«˜äº®
- ğŸ“Š **è®ºè¯ç»“æ„åˆ†æ**ï¼šä¸“é—¨çš„å­¦æœ¯æ–‡æ¡£è®ºè¯é€»è¾‘åˆ†æ
- ğŸ’¾ **å¤šç§å¯¼å‡º**ï¼šæ”¯æŒMarkdownã€Mermaidä»£ç ã€HTMLç­‰æ ¼å¼
- ğŸ”— **åœ¨çº¿ç¼–è¾‘**ï¼šé›†æˆMermaid Live Editor

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "å‰ç«¯ç•Œé¢"
        A[React App] --> B[æ–‡æ¡£ä¸Šä¼ é¡µé¢]
        A --> C[æ–‡æ¡£é˜…è¯»å™¨] 
        A --> D[æ€ç»´å¯¼å›¾æ˜¾ç¤º]
    end
    
    subgraph "åç«¯API"
        F[FastAPIæœåŠ¡] --> G[æ–‡æ¡£è§£æå™¨]
        F --> H[æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨]
        F --> I[è®ºè¯ç»“æ„åˆ†æå™¨]
    end
    
    subgraph "AIæœåŠ¡å±‚"
        J[AIæ¨¡å‹ç®¡ç†å™¨] --> K[DeepSeek]
        J --> L[OpenAI/ç¡…åŸºæµåŠ¨]
        J --> M[Claude]
        J --> N[Gemini]
    end
    
    subgraph "æ•°æ®å¤„ç†"
        O[æ–‡æ¡£ç»“æ„åˆ†æ] --> P[æ®µè½çº§æ˜ å°„]
        O --> Q[è¯­ä¹‰å—æå–]
        O --> R[å†…å®¹å»é‡]
    end
    
    A --> F
    F --> J
    G --> O
    H --> O
    I --> O
```

### æŠ€æœ¯æ ˆè¯¦è§£

#### åç«¯æŠ€æœ¯æ ˆ
- **FastAPI**: ç°ä»£å¼‚æ­¥Webæ¡†æ¶ï¼Œæä¾›é«˜æ€§èƒ½APIæœåŠ¡
- **Python 3.8+**: æ ¸å¿ƒå¼€å‘è¯­è¨€
- **å¤šAIæ¨¡å‹é›†æˆ**: ç»Ÿä¸€çš„AIæ¥å£æŠ½è±¡å±‚
- **å¼‚æ­¥å¤„ç†**: æ”¯æŒå¹¶å‘è¯·æ±‚å’Œé•¿æ—¶é—´è¿è¡Œçš„AIä»»åŠ¡
- **æ–‡æ¡£è§£æ**: åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„Markdownç»“æ„åˆ†æ
- **æ™ºèƒ½å»é‡**: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å†…å®¹å»é‡ç®—æ³•

#### å‰ç«¯æŠ€æœ¯æ ˆ
- **React 18**: ç°ä»£å‰ç«¯æ¡†æ¶ï¼Œä½¿ç”¨Hooksæ¶æ„
- **Tailwind CSS**: åŸå­åŒ–CSSæ¡†æ¶ï¼Œå“åº”å¼è®¾è®¡
- **Mermaid.js**: å¼ºå¤§çš„å›¾è¡¨æ¸²æŸ“å¼•æ“
- **React Router**: å•é¡µåº”ç”¨è·¯ç”±ç®¡ç†
- **Axios**: HTTPå®¢æˆ·ç«¯ï¼Œå¤„ç†APIè°ƒç”¨
- **React Hot Toast**: ä¼˜é›…çš„é€šçŸ¥ç³»ç»Ÿ

#### AIé›†æˆç‰¹æ€§
- **å¤šæä¾›å•†æ”¯æŒ**: ç»Ÿä¸€æ¥å£æ”¯æŒå¤šç§AIæœåŠ¡
- **æˆæœ¬æ§åˆ¶**: ç²¾ç¡®çš„tokenä½¿ç”¨è¿½è¸ªå’Œæˆæœ¬è®¡ç®—
- **æ™ºèƒ½é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
- **é”™è¯¯æ¢å¤**: å¤šå±‚æ¬¡é”™è¯¯å¤„ç†å’Œä¼˜é›…é™çº§

## ğŸ“ é¡¹ç›®ç»“æ„è¯¦è§£

```
mindmap-generator/
â”œâ”€â”€ ğŸ“„ README.md                     # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ ğŸ“„ requirements-web.txt          # Pythonä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ ğŸ“„ .env.example                  # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ ğŸ“„ .gitignore                    # Gitå¿½ç•¥æ–‡ä»¶é…ç½®
â”‚
â”œâ”€â”€ ğŸš€ æ ¸å¿ƒåç«¯æ–‡ä»¶
â”‚   â”œâ”€â”€ ğŸ“„ mindmap_generator.py      # æ ¸å¿ƒæ€ç»´å¯¼å›¾ç”Ÿæˆå¼•æ“
â”‚   â”œâ”€â”€ ğŸ“„ web_backend.py            # FastAPIåç«¯APIæœåŠ¡
â”‚   â”œâ”€â”€ ğŸ“„ document_parser.py        # æ–‡æ¡£ç»“æ„è§£æå™¨
â”‚   â””â”€â”€ ğŸ“„ start_conda_web_app.py    # ä¸€é”®å¯åŠ¨è„šæœ¬
â”‚
â”œâ”€â”€ ğŸ¨ å‰ç«¯Reactåº”ç”¨
â”‚   â”œâ”€â”€ ğŸ“„ package.json              # Node.jsä¾èµ–é…ç½®
â”‚   â”œâ”€â”€ ğŸ“„ tailwind.config.js        # Tailwind CSSé…ç½®
â”‚   â”œâ”€â”€ ğŸ“ src/                      # æºä»£ç ç›®å½•
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ App.js                # ä¸»åº”ç”¨ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ App.css               # å…¨å±€æ ·å¼
â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/           # Reactç»„ä»¶
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ UploadPage.js           # æ–‡ä»¶ä¸Šä¼ é¡µé¢
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ViewerPageRefactored.js # æ–‡æ¡£æŸ¥çœ‹å™¨ï¼ˆé‡æ„ç‰ˆï¼‰
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ MermaidDiagram.js       # æ€ç»´å¯¼å›¾ç»„ä»¶
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ DocumentRenderer.js     # æ–‡æ¡£æ¸²æŸ“å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ PDFViewer.js            # PDFæŸ¥çœ‹å™¨

â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ TableOfContents.js      # ç›®å½•ç»„ä»¶
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ ThemeToggle.js          # æš—é»‘æ¨¡å¼åˆ‡æ¢
â”‚   â”‚   â”œâ”€â”€ ğŸ“ contexts/             # Reactä¸Šä¸‹æ–‡
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ ThemeContext.js         # ä¸»é¢˜ä¸Šä¸‹æ–‡
â”‚   â”‚   â””â”€â”€ ğŸ“ hooks/                # è‡ªå®šä¹‰React Hooks
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ useDocumentViewer.js     # æ–‡æ¡£æŸ¥çœ‹å™¨é€»è¾‘
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ useMindmapGeneration.js  # æ€ç»´å¯¼å›¾ç”Ÿæˆé€»è¾‘
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ usePanelResize.js        # é¢æ¿è°ƒæ•´é€»è¾‘

â”‚   â”‚       â””â”€â”€ ğŸ“„ useScrollDetection.js    # æ»šåŠ¨æ£€æµ‹ä¸è”åŠ¨é«˜äº®
â”‚   â””â”€â”€ ğŸ“ public/                   # é™æ€èµ„æº
â”‚       â”œâ”€â”€ ğŸ“„ index.html            # HTMLæ¨¡æ¿
â”‚       â”œâ”€â”€ ğŸ“„ favicon.ico           # ç½‘ç«™å›¾æ ‡
â”‚       â””â”€â”€ ğŸ“„ manifest.json         # PWAé…ç½®
â”‚
â”œâ”€â”€ ğŸ“ å­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ ğŸ“ uploads/                  # ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶å­˜å‚¨
â”‚   â”œâ”€â”€ ğŸ“ mindmap_outputs/          # æ€ç»´å¯¼å›¾è¾“å‡ºæ–‡ä»¶
â”‚   â”œâ”€â”€ ğŸ“ pdf_outputs/              # PDFè§£æç»“æœ
â”‚   â””â”€â”€ ğŸ“ api_responses/            # AI APIå“åº”æ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰
â”‚
â”œâ”€â”€ ğŸ“ æ–‡æ¡£å’Œç¤ºä¾‹
â”‚   â”œâ”€â”€ ğŸ“„ WEBåº”ç”¨ä½¿ç”¨è¯´æ˜.md         # Webåº”ç”¨ä½¿ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ ğŸ“„ Condaç¯å¢ƒä½¿ç”¨æŒ‡å—.md       # Condaç¯å¢ƒé…ç½®æŒ‡å—
â”‚   â”œâ”€â”€ ğŸ“„ æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨æŠ€æœ¯å®ç°è¯¦è§£.md # æŠ€æœ¯å®ç°è¯¦ç»†æ–‡æ¡£
â”‚   â”œâ”€â”€ ğŸ“„ markdown.md               # Markdownç¤ºä¾‹æ–‡æ¡£
â”‚   â”œâ”€â”€ ğŸ“„ sample_input_document_as_markdown__durnovo_memo.md
â”‚   â”œâ”€â”€ ğŸ“„ sample_input_document_as_markdown__small.md
â”‚   â””â”€â”€ ğŸ“ screenshots/              # é¡¹ç›®æˆªå›¾
â”‚       â”œâ”€â”€ ğŸ“„ illustration.webp           # åŠŸèƒ½æ¼”ç¤ºå›¾
â”‚       â”œâ”€â”€ ğŸ“„ mindmap_outline_md_example_durnovo.webp
â”‚       â”œâ”€â”€ ğŸ“„ mermaid_diagram_example_durnovo.webp
â”‚       â”œâ”€â”€ ğŸ“„ logging_output_during_run.webp
â”‚       â”œâ”€â”€ ğŸ“„ token_usage_report.webp
â”‚       â””â”€â”€ ğŸ“„ mindmap-architecture.svg
â”‚
â””â”€â”€ ğŸ“ å…¶ä»–æ–‡ä»¶
    â”œâ”€â”€ ğŸ“„ package.json              # æ ¹ç›®å½•Node.jsé…ç½®
    â”œâ”€â”€ ğŸ“„ package-lock.json         # ä¾èµ–é”å®šæ–‡ä»¶
    â””â”€â”€ ğŸ“ venv/                     # Pythonè™šæ‹Ÿç¯å¢ƒ
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **Node.js**: 16 æˆ–æ›´é«˜ç‰ˆæœ¬
- **Conda**: æ¨èä½¿ç”¨ï¼ˆå¯é€‰ï¼‰

### ğŸ¯ ä¸€é”®å¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd mindmap-generator

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„APIå¯†é’¥

# ä¸€é”®å¯åŠ¨ï¼ˆè‡ªåŠ¨å®‰è£…ä¾èµ–å¹¶å¯åŠ¨æœåŠ¡ï¼‰
python start_conda_web_app.py
```

å¯åŠ¨è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- âœ… æ£€æŸ¥å¹¶å®‰è£…Pythonä¾èµ–
- âœ… æ£€æŸ¥Node.jsç¯å¢ƒå¹¶å®‰è£…å‰ç«¯ä¾èµ–
- âœ… å¯åŠ¨åç«¯FastAPIæœåŠ¡ (ç«¯å£8000)
- âœ… å¯åŠ¨å‰ç«¯Reactå¼€å‘æœåŠ¡å™¨ (ç«¯å£3000)
- âœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨è®¿é—®åº”ç”¨

### âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®

åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®AIæœåŠ¡ï¼š

```env
# é€‰æ‹©AIæä¾›å•† (DEEPSEEK, OPENAI, CLAUDE, GEMINI)
API_PROVIDER=DEEPSEEK

# DeepSeek APIï¼ˆæ¨èï¼Œæˆæœ¬ä½å»‰ï¼‰
DEEPSEEK_API_KEY=your_deepseek_api_key

# OpenAI APIï¼ˆæ¨èä½¿ç”¨ç¡…åŸºæµåŠ¨ï¼‰
OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=https://api.siliconflow.cn/v1

# Claude API
ANTHROPIC_API_KEY=your_anthropic_api_key

# Gemini API
GEMINI_API_KEY=your_gemini_api_key
```

### ğŸ”§ æ‰‹åŠ¨å®‰è£…ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦æ‰‹åŠ¨å®‰è£…ï¼š

```bash
# 1. å®‰è£…Pythonä¾èµ–
pip install -r requirements-web.txt

# 2. å®‰è£…å‰ç«¯ä¾èµ–
cd frontend
npm install
cd ..

# 3. å¯åŠ¨åç«¯æœåŠ¡
python -m uvicorn web_backend:app --host 0.0.0.0 --port 8000 --reload

# 4. å¯åŠ¨å‰ç«¯æœåŠ¡ï¼ˆæ–°ç»ˆç«¯ï¼‰
cd frontend
npm start
```

### ğŸŒ è®¿é—®åº”ç”¨

- **å‰ç«¯ç•Œé¢**: http://localhost:3000
- **åç«¯API**: http://localhost:8000  
- **APIæ–‡æ¡£**: http://localhost:8000/docs

## ğŸ’¡ ä½¿ç”¨æŒ‡å—

### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£

1. è®¿é—®é¦–é¡µï¼Œç‚¹å‡»ä¸Šä¼ åŒºåŸŸæˆ–æ‹–æ‹½æ–‡ä»¶
2. æ”¯æŒ `.md` å’Œ `.txt` æ ¼å¼æ–‡ä»¶
3. æ–‡ä»¶ä¸Šä¼ åç«‹å³æ˜¾ç¤ºæ–‡æ¡£å†…å®¹

### ğŸ“– æ–‡æ¡£é˜…è¯»

- **å·¦ä¾§é¢æ¿**ï¼šæ–‡æ¡£å†…å®¹ï¼Œæ”¯æŒMarkdownæ¸²æŸ“
- **å³ä¾§é¢æ¿**ï¼šæ€ç»´å¯¼å›¾å’Œè®ºè¯ç»“æ„å›¾
- **ç›®å½•å¯¼èˆª**ï¼šå¯æŠ˜å çš„æ–‡æ¡£ç»“æ„ç›®å½•
- **æš—é»‘æ¨¡å¼**ï¼šæ”¯æŒæ˜æš—ä¸»é¢˜åˆ‡æ¢

### ğŸ§  æ€ç»´å¯¼å›¾ç”Ÿæˆ

1. æ–‡æ¡£ä¸Šä¼ åï¼Œç³»ç»Ÿè‡ªåŠ¨å¼€å§‹åˆ†æ
2. **è®ºè¯ç»“æ„åˆ†æ**ï¼šè¯†åˆ«æ–‡æ¡£çš„é€»è¾‘è®ºè¯æµç¨‹
3. **è¯­ä¹‰å—æ˜ å°„**ï¼šå°†æ–‡æ¡£æ®µè½æ˜ å°„åˆ°æ€ç»´å¯¼å›¾èŠ‚ç‚¹
4. **å®æ—¶é«˜äº®**ï¼šé˜…è¯»æ—¶è‡ªåŠ¨é«˜äº®å¯¹åº”çš„æ€ç»´å¯¼å›¾èŠ‚ç‚¹

### ğŸ”„ äº¤äº’åŠŸèƒ½

- **èŠ‚ç‚¹ç‚¹å‡»**ï¼šç‚¹å‡»æ€ç»´å¯¼å›¾èŠ‚ç‚¹è·³è½¬åˆ°å¯¹åº”æ–‡æ¡£æ®µè½
- **æ»šåŠ¨è”åŠ¨**ï¼šæ–‡æ¡£æ»šåŠ¨æ—¶è‡ªåŠ¨é«˜äº®å¯¹åº”æ€ç»´å¯¼å›¾èŠ‚ç‚¹
- **é¢æ¿è°ƒæ•´**ï¼šå¯æ‹–æ‹½è°ƒæ•´å·¦å³é¢æ¿å¤§å°
- **å¯¼å‡ºåŠŸèƒ½**ï¼šæ”¯æŒä¸‹è½½Mermaidä»£ç å’ŒMarkdownæ–‡æ¡£



## ğŸ”¬ æŠ€æœ¯å®ç°è¯¦è§£

### ğŸ§® AIæ¨¡å‹ç®¡ç†

ç³»ç»Ÿæ”¯æŒå¤šç§AIæä¾›å•†ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¥å£æŠ½è±¡ï¼š

```python
class DocumentOptimizer:
    async def generate_completion(self, prompt: str, max_tokens: int, 
                                task: str) -> Optional[str]:
        # æ ¹æ®API_PROVIDERé€‰æ‹©å¯¹åº”çš„AIæœåŠ¡
        if Config.API_PROVIDER == "DEEPSEEK":
            return await self._call_deepseek(prompt, max_tokens)
        elif Config.API_PROVIDER == "OPENAI":
            return await self._call_openai(prompt, max_tokens)
        # ... å…¶ä»–æä¾›å•†
```

**æ”¯æŒçš„AIæ¨¡å‹**ï¼š

| æä¾›å•† | æ¨¡å‹ | ç‰¹ç‚¹ | æˆæœ¬ |
|--------|------|------|------|
| DeepSeek | deepseek-chat | ä¸­æ–‡ä¼˜åŒ–ï¼Œæ¨ç†èƒ½åŠ›å¼º | æä½ |
| OpenAI | gpt-4o-mini | é«˜è´¨é‡è¾“å‡ºï¼Œç¨³å®šæ€§å¥½ | ä¸­ç­‰ |
| Claude | claude-3-5-haiku | å¿«é€Ÿå“åº”ï¼Œç†è§£èƒ½åŠ›å¼º | ä¸­ç­‰ |
| Gemini | gemini-2.0-flash-lite | Googleæœ€æ–°æ¨¡å‹ | ä½ |

### ğŸ“ æ–‡æ¡£ç»“æ„åˆ†æ

é‡‡ç”¨åŸºäºMarkdownæ ‡é¢˜çš„å±‚çº§è§£æç®—æ³•ï¼š

```python
class DocumentParser:
    def parse_document(self, markdown_text: str) -> DocumentNode:
        # 1. æå–æ‰€æœ‰æ ‡é¢˜
        headings = self._extract_headings(markdown_text)
        
        # 2. æ„å»ºå±‚çº§æ ‘ç»“æ„
        root = self._build_tree_structure(headings, markdown_text)
        
        # 3. åˆ†é…å†…å®¹å’Œè®¡ç®—èŒƒå›´
        self._post_process_tree(root, markdown_text)
        
        return root
```

**è§£æåŠŸèƒ½**ï¼š
- è‡ªåŠ¨è¯†åˆ«Markdownæ ‡é¢˜å±‚çº§
- æ„å»ºæ–‡æ¡£æ ‘å½¢ç»“æ„
- ç”Ÿæˆå¯å¯¼èˆªçš„ç›®å½•
- æ”¯æŒæ®µè½çº§ç²¾ç¡®å®šä½

### ğŸ”— è¯­ä¹‰æ˜ å°„ç³»ç»Ÿ

å®ç°æ–‡æ¡£æ®µè½ä¸æ€ç»´å¯¼å›¾èŠ‚ç‚¹çš„æ™ºèƒ½æ˜ å°„ï¼š

```python
def updateDynamicMapping(chunks, mermaidCode, nodeMapping):
    # AIåˆ†æç”Ÿæˆçš„è¯­ä¹‰å—æ˜ å°„
    # æ®µè½ID -> èŠ‚ç‚¹ID çš„æ˜ å°„å…³ç³»
    # æ”¯æŒä¸€å¯¹å¤šå’Œå¤šå¯¹ä¸€çš„å¤æ‚æ˜ å°„
```

**æ˜ å°„ç‰¹æ€§**ï¼š
- **æ®µè½çº§æ˜ å°„**ï¼šç²¾ç¡®åˆ°æ¯ä¸ªæ–‡æ¡£æ®µè½
- **è¯­ä¹‰åˆ†ç»„**ï¼šç›¸å…³æ®µè½ç»„åˆæˆé€»è¾‘èŠ‚ç‚¹
- **åŒå‘å¯¼èˆª**ï¼šæ–‡æ¡£â†”æ€ç»´å¯¼å›¾åŒå‘è·³è½¬
- **å®æ—¶é«˜äº®**ï¼šæ»šåŠ¨æ—¶è‡ªåŠ¨åŒæ­¥é«˜äº®

### ğŸ¨ æ€ç»´å¯¼å›¾æ¸²æŸ“

åŸºäºMermaid.jsçš„é«˜è´¨é‡å›¾è¡¨æ¸²æŸ“ï¼š

```javascript
// åŠ¨æ€ç”ŸæˆMermaidè¯­æ³•
const mermaidCode = `
graph TD
    A[å¼•è¨€] --> B{æ ¸å¿ƒè®ºç‚¹}
    B --> C[æ”¯æ’‘è¯æ®1]
    B --> D[æ”¯æ’‘è¯æ®2]
    C --> E[ç»“è®º]
    D --> E
`;

// äº¤äº’å¼æ¸²æŸ“
<MermaidDiagram 
  code={mermaidCode}
  onNodeClick={handleNodeClick}
  ref={mermaidDiagramRef}
/>
```

**æ¸²æŸ“ç‰¹æ€§**ï¼š
- **äº¤äº’å¼èŠ‚ç‚¹**ï¼šæ”¯æŒç‚¹å‡»è·³è½¬
- **åŠ¨æ€é«˜äº®**ï¼šå®æ—¶è§†è§‰åé¦ˆ
- **è‡ªé€‚åº”å¸ƒå±€**ï¼šå“åº”å¼è®¾è®¡
- **åœ¨çº¿ç¼–è¾‘**ï¼šé›†æˆMermaid Live Editor

### âš¡ å¼‚æ­¥å¤„ç†æ¶æ„

é‡‡ç”¨ç°ä»£å¼‚æ­¥å¤„ç†æ¨¡å¼ï¼š

```python
# åç«¯å¼‚æ­¥ä»»åŠ¡ç®¡ç†
async def generate_argument_structure_async(document_id: str, content: str):
    try:
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        document_status[document_id]['status_demo'] = 'generating'
        
        # AIåˆ†æå¤„ç†ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
        result = await analyzer.generate_argument_structure(content)
        
        # æ›´æ–°å®ŒæˆçŠ¶æ€
        document_status[document_id]['status_demo'] = 'completed'
        document_status[document_id]['mermaid_code_demo'] = result['mermaid_string']
        
    except Exception as e:
        document_status[document_id]['status_demo'] = 'error'
```

**å¼‚æ­¥ç‰¹æ€§**ï¼š
- **éé˜»å¡ä¸Šä¼ **ï¼šæ–‡æ¡£ä¸Šä¼ åç«‹å³å¯é˜…è¯»
- **åå°å¤„ç†**ï¼šAIåˆ†æåœ¨åå°å¼‚æ­¥è¿›è¡Œ  
- **å®æ—¶çŠ¶æ€**ï¼šå‰ç«¯è½®è¯¢è·å–å¤„ç†è¿›åº¦
- **ä¼˜é›…é™çº§**ï¼šå¤„ç†å¤±è´¥æ—¶çš„å‹å¥½æç¤º

## ğŸ› ï¸ APIæ¥å£æ–‡æ¡£

### ğŸ“¤ æ–‡æ¡£ä¸Šä¼ 
```http
POST /api/upload-document
Content-Type: multipart/form-data

{
  "file": "document.md"
}
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "document_id": "doc_abc123",
  "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ",
  "content": "æ–‡æ¡£å†…å®¹...",
  "parsed_content": "è§£æåçš„æ–‡æ¡£..."
}
```

### ğŸ§  ç”Ÿæˆè®ºè¯ç»“æ„
```http
POST /api/generate-argument-structure/{document_id}
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "status": "completed",
  "mermaid_code": "graph TD\n  A[å¼•è¨€] --> B[è®ºç‚¹]...",
  "node_mappings": {
    "A": {
      "paragraph_ids": ["para-1", "para-2"],
      "semantic_role": "å¼•è¨€",
      "text_snippet": "æ–‡æ¡£å¼€ç¯‡ä»‹ç»äº†..."
    }
  }
}
```

### ğŸ“Š æŸ¥è¯¢çŠ¶æ€
```http
GET /api/document-status/{document_id}
```

### ğŸ“– è·å–æ–‡æ¡£
```http
GET /api/document/{document_id}
```

### ğŸ—‚ï¸ è·å–æ–‡æ¡£ç»“æ„  
```http
GET /api/document-structure/{document_id}
```

### ğŸ“š è·å–ç›®å½•
```http
GET /api/document-toc/{document_id}
```

å®Œæ•´çš„APIæ–‡æ¡£è¯·è®¿é—®ï¼šhttp://localhost:8000/docs

## ğŸ”§ é«˜çº§é…ç½®

### AIæ¨¡å‹ä¼˜åŒ–

```env
# DeepSeeké…ç½®ï¼ˆæ¨èï¼‰
API_PROVIDER=DEEPSEEK
DEEPSEEK_API_KEY=your_key
DEEPSEEK_COMPLETION_MODEL=deepseek-chat  # æˆ– deepseek-reasoner

# OpenAIå…¼å®¹é…ç½®ï¼ˆä½¿ç”¨ç¡…åŸºæµåŠ¨ç­‰ä»£ç†ï¼‰
API_PROVIDER=OPENAI  
OPENAI_API_KEY=your_siliconflow_key
OPENAI_BASE_URL=https://api.siliconflow.cn/v1
OPENAI_COMPLETION_MODEL=gpt-4o-mini-2024-07-18
```

### æ€§èƒ½è°ƒä¼˜

```python
# å¹¶å‘å¤„ç†é…ç½®
MAX_CONCURRENT_REQUESTS = 5
REQUEST_TIMEOUT = 60
RETRY_ATTEMPTS = 3

# ç¼“å­˜é…ç½®  
ENABLE_EMOJI_CACHE = True
CACHE_DIRECTORY = "./cache"

# å†…å®¹é™åˆ¶
MAX_CONTENT_LENGTH = 100000  # å­—ç¬¦
MAX_TOPICS = 8
MAX_SUBTOPICS_PER_TOPIC = 6
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. å¯åŠ¨å¤±è´¥**
```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version  # åº”è¯¥ >= 3.8

# æ£€æŸ¥Node.jsç‰ˆæœ¬
node --version   # åº”è¯¥ >= 16

# é‡æ–°å®‰è£…ä¾èµ–
pip install -r requirements-web.txt --force-reinstall
```

**2. AI APIè°ƒç”¨å¤±è´¥**
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $API_PROVIDER
echo $DEEPSEEK_API_KEY

# æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I https://api.deepseek.com

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—
tail -f api_responses/*.txt
```

**3. å‰ç«¯ç¼–è¯‘é”™è¯¯**
```bash
# æ¸…ç†ç¼“å­˜é‡æ–°å®‰è£…
cd frontend
rm -rf node_modules package-lock.json
npm install

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -ano | findstr :3000  # Windows
lsof -ti:3000 | xargs kill    # macOS/Linux
```

**4. æ€ç»´å¯¼å›¾ä¸æ˜¾ç¤º**
- æ£€æŸ¥æµè§ˆå™¨æ§åˆ¶å°é”™è¯¯
- ç¡®è®¤Mermaidä»£ç æ ¼å¼æ­£ç¡®
- æ£€æŸ¥ç½‘ç»œè¯·æ±‚æ˜¯å¦æˆåŠŸ

### è°ƒè¯•æ¨¡å¼

å¼€å¯è¯¦ç»†æ—¥å¿—ï¼š

```python
# åœ¨ web_backend.py ä¸­
import logging
logging.basicConfig(level=logging.DEBUG)
```

æŸ¥çœ‹AI APIå“åº”ï¼š
```bash
# æŸ¥çœ‹æœ€æ–°çš„APIè°ƒç”¨æ—¥å¿—
ls -la api_responses/
cat api_responses/latest_response.txt
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# 1. Forkä»“åº“å¹¶å…‹éš†
git clone https://github.com/your-username/mindmap-generator.git
cd mindmap-generator

# 2. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/your-feature-name

# 3. è®¾ç½®å¼€å‘ç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements-web.txt

# 4. å‰ç«¯å¼€å‘ç¯å¢ƒ
cd frontend  
npm install
npm start
```

### ä»£ç è§„èŒƒ

- **Python**: éµå¾ªPEP 8è§„èŒƒ
- **JavaScript**: ä½¿ç”¨ES6+è¯­æ³•ï¼Œéµå¾ªAirbnbé£æ ¼
- **æäº¤ä¿¡æ¯**: ä½¿ç”¨çº¦å®šå¼æäº¤æ ¼å¼

### æµ‹è¯•æµç¨‹

```bash
# è¿è¡Œåç«¯æµ‹è¯•
python -m pytest tests/

# è¿è¡Œå‰ç«¯æµ‹è¯•  
cd frontend
npm test

# ç«¯åˆ°ç«¯æµ‹è¯•
npm run test:e2e
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [MITè®¸å¯è¯](LICENSE) å¼€æºã€‚

## ğŸ™ è‡´è°¢

- [Mermaid.js](https://mermaid.js.org/) - å¼ºå¤§çš„å›¾è¡¨æ¸²æŸ“å¼•æ“
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£Python Webæ¡†æ¶
- [React](https://react.dev/) - ä¼˜ç§€çš„å‰ç«¯æ¡†æ¶
- [Tailwind CSS](https://tailwindcss.com/) - å®ç”¨çš„CSSæ¡†æ¶
- [DeepSeek](https://www.deepseek.com/) - é«˜æ€§èƒ½AIæ¨¡å‹æœåŠ¡

## ğŸ“ æ”¯æŒä¸åé¦ˆ

- ğŸ“§ Email: [your-email@example.com]
- ğŸ› Issues: [GitHub Issues](https://github.com/your-username/mindmap-generator/issues)
- ğŸ’¬ è®¨è®º: [GitHub Discussions](https://github.com/your-username/mindmap-generator/discussions)

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼
</file>

<file path="frontend/package.json">
{
  "name": "mindmap-generator-frontend",
  "version": "1.0.0",
  "private": true,
  "dependencies": {
    "@dnd-kit/core": "^6.3.1",
    "@dnd-kit/modifiers": "^9.0.0",
    "@dnd-kit/sortable": "^10.0.0",
    "@dnd-kit/utilities": "^3.2.2",
    "@tailwindcss/typography": "^0.5.9",
    "@testing-library/jest-dom": "^5.16.4",
    "@testing-library/react": "^13.3.0",
    "@testing-library/user-event": "^13.5.0",
    "autoprefixer": "^10.4.14",
    "axios": "^1.4.0",
    "dagre": "^0.8.5",
    "lucide-react": "^0.263.1",
    "mermaid": "^10.2.3",
    "postcss": "^8.4.24",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-dropzone": "^14.2.3",
    "react-hot-toast": "^2.4.1",
    "react-markdown": "^8.0.7",
    "react-router-dom": "^6.3.0",
    "react-scripts": "5.0.1",
    "react-toastify": "^11.0.5",
    "reactflow": "^11.11.4",
    "tailwindcss": "^3.3.0"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  },
  "proxy": "http://localhost:8000"
}
</file>

<file path="frontend/src/App.js">
import React from 'react';
import { BrowserRouter as Router, Routes, Route, useLocation } from 'react-router-dom';
import { Toaster } from 'react-hot-toast';
import { ThemeProvider } from './contexts/ThemeContext';
import ThemeToggle from './components/ThemeToggle';
import UploadPage from './components/UploadPage';
import ViewerPageRefactored from './components/ViewerPageRefactored';
import './App.css';

function AppContent() {
  const location = useLocation();
  const isViewerPage = location.pathname.startsWith('/viewer/');

  return (
    <div className={isViewerPage ? "h-full bg-gray-50 dark:bg-gray-900" : "min-h-screen bg-gray-50 dark:bg-gray-900"}>
      {!isViewerPage && (
        <header className="bg-white dark:bg-gray-800 shadow-sm border-b border-gray-200 dark:border-gray-700">
          <div className="max-w-7xl mx-auto px-4 py-4 flex items-center justify-between">
            <h1 className="text-2xl font-bold text-gray-900 dark:text-white flex items-center">
              <svg className="w-8 h-8 mr-3 text-blue-600 dark:text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z" />
              </svg>
              æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨
            </h1>
            <ThemeToggle />
          </div>
        </header>
      )}

      <main className={isViewerPage ? "h-full" : "max-w-7xl mx-auto"}>
        <Routes>
          <Route path="/" element={<UploadPage />} />
          <Route path="/viewer/:documentId" element={<ViewerPageRefactored />} />
        </Routes>
      </main>
    </div>
  );
}

function App() {
  return (
    <ThemeProvider>
      <div className="App h-full">
        <Router>
          <AppContent />
        </Router>
        <Toaster 
          position="top-right"
          toastOptions={{
            duration: 4000,
            style: {
              background: 'var(--toast-bg)',
              color: 'var(--toast-color)',
            },
            className: 'dark:bg-gray-800 dark:text-white bg-gray-800 text-white',
          }}
        />
      </div>
    </ThemeProvider>
  );
}

export default App;
</file>

<file path="frontend/src/components/EditableNode.css">
.editable-node {
  width: 200px;
  min-height: 50px;
  padding: 10px;
  border: 1px solid #ddd;
  border-radius: 8px;
  background-color: white;
  text-align: center;
  cursor: pointer;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
  position: relative;
  display: flex;
  flex-direction: column;
  justify-content: center;
  transition: all 0.3s ease; /* æ·»åŠ è¿‡æ¸¡åŠ¨ç”» */
}

.editable-node-content {
  font-size: 14px;
  line-height: 1.4;
  word-break: break-word;
  min-height: 20px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.editable-node-textarea {
  border: none;
  outline: none;
  background: transparent;
  resize: none;
  width: 100%;
  font-size: 14px;
  line-height: 1.4;
  text-align: center;
  font-family: inherit;
  min-height: 40px;
  cursor: text;
  overflow: hidden; /* é˜²æ­¢å‡ºç°æ»šåŠ¨æ¡ */
  padding: 0;
  margin: 0;
  box-sizing: border-box;
}

/* é«˜äº®æ˜¾ç¤ºç¼–è¾‘çŠ¶æ€ */
.editable-node:has(.editable-node-textarea:focus) {
  border-color: #4299e1;
  box-shadow: 0 0 0 2px rgba(66, 153, 225, 0.2);
}

/* å ä½ç¬¦æ ·å¼ */
.editable-node-textarea::placeholder {
  color: #a0aec0;
  opacity: 0.7;
}

/* èŠ‚ç‚¹é«˜äº®æ ·å¼ - éç ´åæ€§é«˜äº® */
.react-flow__node.highlighted-node .editable-node {
  border-color: #f59e0b !important;
  background-color: rgba(245, 158, 11, 0.1) !important;
  box-shadow: 0 0 0 3px rgba(245, 158, 11, 0.2), 0 4px 12px rgba(245, 158, 11, 0.3) !important;
  transform: scale(1.02);
}

/* æ·±è‰²æ¨¡å¼ä¸‹çš„èŠ‚ç‚¹é«˜äº® */
:root.dark .react-flow__node.highlighted-node .editable-node {
  border-color: #fbbf24 !important;
  background-color: rgba(251, 191, 36, 0.15) !important;
  box-shadow: 0 0 0 3px rgba(251, 191, 36, 0.25), 0 4px 12px rgba(251, 191, 36, 0.35) !important;
}

/* ç¡®ä¿ç¼–è¾‘çŠ¶æ€ä¸‹çš„èŠ‚ç‚¹é«˜äº®ä¸ä¼šå½±å“ç¼–è¾‘åŠŸèƒ½ */
.react-flow__node.highlighted-node .editable-node:has(.editable-node-textarea:focus) {
  border-color: #4299e1 !important;
  box-shadow: 0 0 0 2px rgba(66, 153, 225, 0.2) !important;
  background-color: white !important;
  transform: scale(1);
}

/* React FlowèŠ‚ç‚¹å®¹å™¨çš„é«˜äº®æ ·å¼ */
.react-flow__node.highlighted-node {
  z-index: 1000 !important;
}

/* é˜²æ­¢é«˜äº®åŠ¨ç”»å½±å“ç¼–è¾‘åŠŸèƒ½ */
.react-flow__node .editable-node-textarea:focus {
  position: relative;
  z-index: 1001;
}

/* å·¥å…·æ æ ·å¼ */
.node-toolbar {
  position: absolute;
  top: -50px;
  left: 50%;
  transform: translateX(-50%);
  background: white;
  border: 1px solid #e2e8f0;
  border-radius: 8px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  padding: 6px;
  display: flex;
  gap: 4px;
  z-index: 1000;
  animation: fadeInUp 0.2s ease-out;
}

/* å·¥å…·æ æŒ‰é’®æ ·å¼ */
.toolbar-button {
  background: transparent;
  border: none;
  width: 32px;
  height: 32px;
  border-radius: 6px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  color: #64748b;
  transition: all 0.2s ease;
  padding: 0;
}

.toolbar-button:hover {
  background-color: #f1f5f9;
  color: #334155;
  transform: scale(1.05);
}

.toolbar-button:active {
  transform: scale(0.95);
}

/* å·¥å…·æ æ˜¾ç¤ºåŠ¨ç”» */
@keyframes fadeInUp {
  from {
    opacity: 0;
    transform: translateX(-50%) translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateX(-50%) translateY(0);
  }
}

/* æ·±è‰²æ¨¡å¼ä¸‹çš„å·¥å…·æ æ ·å¼ */
:root.dark .node-toolbar {
  background: #1e293b;
  border-color: #334155;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
}

:root.dark .toolbar-button {
  color: #94a3b8;
}

:root.dark .toolbar-button:hover {
  background-color: #334155;
  color: #e2e8f0;
}

/* å·¥å…·æ æ›´å¤šé€‰é¡¹å®¹å™¨ */
.toolbar-more-container {
  position: relative;
}

/* å·¥å…·æ ä¸‹æ‹‰èœå• */
.toolbar-more-menu {
  position: absolute;
  top: 100%;
  right: 0;
  margin-top: 4px;
  background: white;
  border: 1px solid #e2e8f0;
  border-radius: 6px;
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.12);
  min-width: 120px;
  z-index: 1001;
  animation: fadeInDown 0.15s ease-out;
}

/* å·¥å…·æ èœå•é¡¹ */
.toolbar-menu-item {
  width: 100%;
  padding: 8px 12px;
  border: none;
  background: transparent;
  display: flex;
  align-items: center;
  gap: 8px;
  cursor: pointer;
  font-size: 13px;
  color: #374151;
  transition: background-color 0.2s ease;
  border-radius: 4px;
  margin: 2px;
}

.toolbar-menu-item:hover {
  background-color: #f1f5f9;
}

.toolbar-menu-item:first-child {
  margin-top: 4px;
}

.toolbar-menu-item:last-child {
  margin-bottom: 4px;
}

/* åˆ é™¤æŒ‰é’®ç‰¹æ®Šæ ·å¼ */
.toolbar-menu-item.delete-item:hover {
  background-color: #fef2f2;
  color: #dc2626;
}

/* ä¸‹æ‹‰èœå•æ˜¾ç¤ºåŠ¨ç”» */
@keyframes fadeInDown {
  from {
    opacity: 0;
    transform: translateY(-8px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* æ·±è‰²æ¨¡å¼ä¸‹çš„ä¸‹æ‹‰èœå•æ ·å¼ */
:root.dark .toolbar-more-menu {
  background: #1e293b;
  border-color: #334155;
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.4);
}

:root.dark .toolbar-menu-item {
  color: #e2e8f0;
}

:root.dark .toolbar-menu-item:hover {
  background-color: #374151;
}

:root.dark .toolbar-menu-item.delete-item:hover {
  background-color: #450a0a;
  color: #f87171;
}
</file>

<file path="frontend/src/components/EditableNode.js">
import React, { useState, useRef, useEffect } from 'react';
import { Handle, Position } from 'reactflow';
import { CornerDownRight, GitBranchPlus, MoreHorizontal, Trash2 } from 'lucide-react';
import './EditableNode.css'; // å¼•å…¥CSSæ–‡ä»¶

// å·¥å…·æ ç»„ä»¶
const Toolbar = ({ nodeId, onAddChildNode, onAddSiblingNode, onDeleteNode }) => {
  const [showMoreMenu, setShowMoreMenu] = useState(false);
  const moreButtonRef = useRef(null);
  
  // å¤„ç†ç‚¹å‡»å¤–éƒ¨å…³é—­èœå•
  useEffect(() => {
    const handleClickOutside = (event) => {
      if (moreButtonRef.current && !moreButtonRef.current.contains(event.target)) {
        setShowMoreMenu(false);
      }
    };
    
    if (showMoreMenu) {
      document.addEventListener('mousedown', handleClickOutside);
      return () => document.removeEventListener('mousedown', handleClickOutside);
    }
  }, [showMoreMenu]);

  return (
    <div className="node-toolbar">
      <button 
        className="toolbar-button" 
        title="æ·»åŠ åŒçº§èŠ‚ç‚¹"
        onClick={(e) => {
          e.stopPropagation();
          if (onAddSiblingNode) {
            onAddSiblingNode(nodeId);
          }
        }}
      >
        <CornerDownRight size={16} />
      </button>
      
      <button 
        className="toolbar-button" 
        title="æ·»åŠ å­èŠ‚ç‚¹"
        onClick={(e) => {
          e.stopPropagation();
          if (onAddChildNode) {
            onAddChildNode(nodeId);
          }
        }}
      >
        <GitBranchPlus size={16} />
      </button>
      
      <div className="toolbar-more-container" ref={moreButtonRef}>
        <button 
          className="toolbar-button" 
          title="æ›´å¤š"
          onClick={(e) => {
            e.stopPropagation();
            setShowMoreMenu(!showMoreMenu);
          }}
        >
          <MoreHorizontal size={16} />
        </button>
        
        {showMoreMenu && (
          <div className="toolbar-more-menu">
            <button 
              className="toolbar-menu-item delete-item"
              onClick={(e) => {
                e.stopPropagation();
                if (onDeleteNode && window.confirm('ç¡®å®šè¦åˆ é™¤è¿™ä¸ªèŠ‚ç‚¹å—ï¼Ÿ')) {
                  onDeleteNode(nodeId);
                }
                setShowMoreMenu(false);
              }}
            >
              <Trash2 size={14} />
              <span>åˆ é™¤èŠ‚ç‚¹</span>
            </button>
          </div>
        )}
      </div>
    </div>
  );
};

const EditableNode = ({ data, id }) => {
  const [isEditing, setIsEditing] = useState(false);
  const [isHovering, setIsHovering] = useState(false);
  const [editingContent, setEditingContent] = useState(''); // ä¿å­˜ç¼–è¾‘ä¸­çš„å†…å®¹
  const textareaRef = useRef(null);
  const wasEditingRef = useRef(false); // è·Ÿè¸ªä¹‹å‰æ˜¯å¦åœ¨ç¼–è¾‘çŠ¶æ€

  // ä¿æŠ¤ç¼–è¾‘çŠ¶æ€ï¼šå½“ç»„ä»¶é‡æ–°æ¸²æŸ“æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤ç¼–è¾‘çŠ¶æ€
  useEffect(() => {
    // å¦‚æœä¹‹å‰åœ¨ç¼–è¾‘ä½†ç°åœ¨ä¸åœ¨ç¼–è¾‘ï¼Œå¯èƒ½æ˜¯è¢«æ„å¤–é‡ç½®äº†
    if (wasEditingRef.current && !isEditing && editingContent) {
      console.log('ğŸ›¡ï¸ [ç¼–è¾‘ä¿æŠ¤] æ£€æµ‹åˆ°ç¼–è¾‘çŠ¶æ€å¯èƒ½è¢«é‡ç½®ï¼Œå°è¯•æ¢å¤:', id);
      setIsEditing(true);
      // å»¶è¿Ÿæ¢å¤ç„¦ç‚¹ï¼Œç¡®ä¿DOMå·²æ›´æ–°
      setTimeout(() => {
        if (textareaRef.current) {
          textareaRef.current.focus();
          textareaRef.current.value = editingContent;
          adjustTextareaHeight(textareaRef.current);
          console.log('ğŸ›¡ï¸ [ç¼–è¾‘ä¿æŠ¤] ç¼–è¾‘çŠ¶æ€å·²æ¢å¤:', id);
        }
      }, 0);
    }
    
    // æ›´æ–°ç¼–è¾‘çŠ¶æ€è·Ÿè¸ª
    wasEditingRef.current = isEditing;
  }, [isEditing, editingContent, id]);

  // å¤„ç†åŒå‡»äº‹ä»¶ï¼Œè¿›å…¥ç¼–è¾‘æ¨¡å¼
  const handleDoubleClick = () => {
    console.log('ğŸ“ [ç¼–è¾‘å¼€å§‹] èŠ‚ç‚¹è¿›å…¥ç¼–è¾‘æ¨¡å¼:', id);
    setEditingContent(data.label || '');
    setIsEditing(true);
  };

  // å¤„ç†å¤±ç„¦äº‹ä»¶ï¼Œä¿å­˜å¹¶é€€å‡ºç¼–è¾‘æ¨¡å¼
  const handleBlur = (event) => {
    const newLabel = event.target.value.trim();
    console.log('ğŸ“ [ç¼–è¾‘ç»“æŸ] èŠ‚ç‚¹ç¼–è¾‘å®Œæˆ:', id, 'æ–°å†…å®¹:', newLabel);
    
    if (newLabel !== data.label && data.onLabelChange) {
      data.onLabelChange(id, newLabel);
    }
    
    // æ¸…ç†ç¼–è¾‘çŠ¶æ€
    setEditingContent('');
    setIsEditing(false);
    wasEditingRef.current = false;
  };

  // å¤„ç†é”®ç›˜äº‹ä»¶
  const handleKeyDown = (event) => {
    if (event.key === 'Enter' && !event.shiftKey) {
      event.preventDefault();
      const newLabel = event.target.value.trim();
      console.log('ğŸ“ [ç¼–è¾‘å®Œæˆ-å›è½¦] èŠ‚ç‚¹ç¼–è¾‘å®Œæˆ:', id, 'æ–°å†…å®¹:', newLabel);
      
      if (newLabel !== data.label && data.onLabelChange) {
        data.onLabelChange(id, newLabel);
      }
      
      // æ¸…ç†ç¼–è¾‘çŠ¶æ€
      setEditingContent('');
      setIsEditing(false);
      wasEditingRef.current = false;
    }
    // æŒ‰ Escape é”®å–æ¶ˆç¼–è¾‘
    if (event.key === 'Escape') {
      console.log('ğŸ“ [ç¼–è¾‘å–æ¶ˆ] ç”¨æˆ·å–æ¶ˆç¼–è¾‘:', id);
      // æ¸…ç†ç¼–è¾‘çŠ¶æ€
      setEditingContent('');
      setIsEditing(false);
      wasEditingRef.current = false;
    }
  };

  // å¤„ç†è¾“å…¥äº‹ä»¶ï¼Œä¿å­˜ç¼–è¾‘å†…å®¹å¹¶è‡ªåŠ¨è°ƒæ•´é«˜åº¦
  const handleInput = (event) => {
    const currentContent = event.target.value;
    setEditingContent(currentContent); // å®æ—¶ä¿å­˜ç¼–è¾‘å†…å®¹
    adjustTextareaHeight(event.target);
  };

  // è°ƒæ•´æ–‡æœ¬åŒºåŸŸé«˜åº¦çš„è¾…åŠ©å‡½æ•°
  const adjustTextareaHeight = (textarea) => {
    if (!textarea) return;
    
    // å…ˆé‡ç½®é«˜åº¦ï¼Œå†æ ¹æ®å†…å®¹è®¾ç½®æ–°é«˜åº¦
    textarea.style.height = 'auto';
    textarea.style.height = `${textarea.scrollHeight}px`;
  };

  // å½“è¿›å…¥ç¼–è¾‘æ¨¡å¼æ—¶ï¼Œè®¾ç½®åˆå§‹é«˜åº¦
  useEffect(() => {
    if (isEditing && textareaRef.current) {
      // ç¡®ä¿æ–‡æœ¬åŒºåŸŸè·å¾—ç„¦ç‚¹
      textareaRef.current.focus();
      
      // è®¾ç½®åˆå§‹é«˜åº¦
      setTimeout(() => {
        adjustTextareaHeight(textareaRef.current);
      }, 0);
    }
  }, [isEditing]);

  // å¤„ç†é¼ æ ‡è¿›å…¥äº‹ä»¶
  const handleMouseEnter = () => {
    setIsHovering(true);
  };

  // å¤„ç†é¼ æ ‡ç¦»å¼€äº‹ä»¶
  const handleMouseLeave = () => {
    setIsHovering(false);
  };

  return (
    <div 
      className="editable-node" 
      data-node-id={id}
      onMouseEnter={handleMouseEnter}
      onMouseLeave={handleMouseLeave}
    >
      {/* æ·»åŠ ä¸€ä¸ªç”¨äºæ¥æ”¶è¿çº¿çš„Handleåœ¨é¡¶éƒ¨ */}
      <Handle 
        type="target" 
        position={Position.Top} 
        style={{ background: '#555', width: 8, height: 8 }} 
      />
      
      {!isEditing ? (
        <div 
          onDoubleClick={handleDoubleClick}
          className="editable-node-content"
          title="åŒå‡»ç¼–è¾‘"
        >
          {data.label || 'æœªå‘½åèŠ‚ç‚¹'}
        </div>
      ) : (
        <textarea
          ref={textareaRef}
          defaultValue={editingContent || data.label || ''}
          autoFocus
          onBlur={handleBlur}
          onKeyDown={handleKeyDown}
          onInput={handleInput}
          className="editable-node-textarea"
          placeholder="è¾“å…¥èŠ‚ç‚¹å†…å®¹..."
        />
      )}
      
      {/* æ¡ä»¶æ¸²æŸ“å·¥å…·æ  */}
      {isHovering && (
        <Toolbar 
          nodeId={id} 
          onAddChildNode={data.onAddChildNode}
          onAddSiblingNode={data.onAddSiblingNode}
          onDeleteNode={data.onDeleteNode}
        />
      )}
      
      {/* æ·»åŠ ä¸€ä¸ªç”¨äºå‘å‡ºè¿çº¿çš„Handleåœ¨åº•éƒ¨ */}
      <Handle 
        type="source" 
        position={Position.Bottom} 
        style={{ background: '#555', width: 8, height: 8 }} 
      />
    </div>
  );
};

export default EditableNode;
</file>

<file path="start_conda_web_app.py">
#!/usr/bin/env python3
"""
Condaç¯å¢ƒä¸“ç”¨å¯åŠ¨è„šæœ¬
ä¸ºåœ¨Condaç¯å¢ƒä¸­è¿è¡Œæ€ç»´å¯¼å›¾ç”Ÿæˆå™¨Webåº”ç”¨è€Œä¼˜åŒ–
"""

import subprocess
import sys
import os
import time
import tempfile
import shutil
from pathlib import Path
import threading
import webbrowser
from urllib.parse import urlparse
import urllib.request
import urllib.error

def print_status(message, status="INFO"):
    """æ‰“å°å¸¦çŠ¶æ€çš„æ¶ˆæ¯"""
    colors = {
        "INFO": "\033[96m",      # é’è‰²
        "SUCCESS": "\033[92m",   # ç»¿è‰²  
        "WARNING": "\033[93m",   # é»„è‰²
        "ERROR": "\033[91m",     # çº¢è‰²
        "ENDC": "\033[0m"        # ç»“æŸé¢œè‰²
    }
    print(f"{colors.get(status, '')}{status}: {message}{colors['ENDC']}")

def check_conda_env():
    """æ£€æŸ¥æ˜¯å¦åœ¨Condaç¯å¢ƒä¸­"""
    conda_env = os.environ.get('CONDA_DEFAULT_ENV')
    if conda_env:
        print_status(f"æ£€æµ‹åˆ°Condaç¯å¢ƒ: {conda_env}", "SUCCESS")
        return True
    else:
        print_status("æœªæ£€æµ‹åˆ°Condaç¯å¢ƒï¼Œä½†ç»§ç»­è¿è¡Œ", "WARNING")
        return False

def install_requirements():
    """å®‰è£…é¡¹ç›®ä¾èµ–"""
    print_status("æ£€æŸ¥å¹¶å®‰è£…é¡¹ç›®ä¾èµ–...", "INFO")
    
    # ç¡®ä¿æœ‰requirements-web.txtæ–‡ä»¶
    if not Path("requirements-web.txt").exists():
        print_status("requirements-web.txt ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤ä¾èµ–æ–‡ä»¶...", "WARNING")
        default_requirements = [
            "fastapi>=0.104.0",
            "uvicorn[standard]>=0.24.0",
            "python-multipart>=0.0.6",
            "aiofiles>=23.2.1",
            "aiolimiter>=1.1.0",
            "openai>=1.3.0",
            "anthropic>=0.7.7",
            "google-generativeai>=0.3.0",
            "fuzzywuzzy>=0.18.0",
            "python-Levenshtein>=0.21.1",
            "sqlmodel>=0.0.14",
            "tiktoken>=0.5.1",
            "transformers>=4.35.0",
            "numpy>=1.24.0",
            "psutil>=5.9.6",
            "spacy>=3.7.2",
            "async-timeout>=4.0.3",
            "python-decouple>=3.8",
            "aiosqlite>=0.19.0",
            "sqlalchemy>=2.0.23",
            "termcolor>=2.3.0"
        ]
        
        with open("requirements-web.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(default_requirements))
        
        print_status("å·²åˆ›å»º requirements-web.txt", "SUCCESS")
    
    try:
        # æ£€æŸ¥å…³é”®åŒ…æ˜¯å¦å·²å®‰è£…
        import fastapi
        import google.generativeai
        print_status("æ ¸å¿ƒä¾èµ–å·²å®‰è£…", "SUCCESS")
    except ImportError as e:
        print_status(f"ç¼ºå°‘å…³é”®ä¾èµ–: {e}", "WARNING")
        print_status("æ­£åœ¨å®‰è£…ä¾èµ–åŒ…...", "INFO")
        
        try:
            subprocess.run([
                sys.executable, "-m", "pip", "install", 
                "-r", "requirements-web.txt", 
                "--upgrade", "--no-warn-script-location"
            ], check=True, capture_output=True, text=True)
            print_status("ä¾èµ–å®‰è£…å®Œæˆ", "SUCCESS")
        except subprocess.CalledProcessError as e:
            print_status(f"ä¾èµ–å®‰è£…å¤±è´¥: {e}", "ERROR")
            print_status("å°è¯•å•ç‹¬å®‰è£…å…³é”®åŒ…...", "INFO")
            
            # å°è¯•å®‰è£…å…³é”®åŒ…
            key_packages = [
                "fastapi>=0.104.0",
                "uvicorn[standard]>=0.24.0", 
                "google-generativeai>=0.3.0",
                "openai>=1.3.0",
                "python-multipart>=0.0.6"
            ]
            
            for package in key_packages:
                try:
                    subprocess.run([
                        sys.executable, "-m", "pip", "install", package, "--no-warn-script-location"
                    ], check=True, capture_output=True, text=True)
                    print_status(f"å®‰è£…æˆåŠŸ: {package}", "SUCCESS")
                except subprocess.CalledProcessError:
                    print_status(f"å®‰è£…å¤±è´¥: {package}", "ERROR")

def check_environment_variables():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®"""
    print_status("æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®...", "INFO")
    
    # æ£€æŸ¥ .env æ–‡ä»¶
    if Path(".env").exists():
        print_status("æ‰¾åˆ° .env æ–‡ä»¶", "SUCCESS")
        
        # è¯»å–å¹¶æ£€æŸ¥å…³é”®é…ç½®
        try:
            with open(".env", "r", encoding="utf-8") as f:
                env_content = f.read()
                
            if "API_PROVIDER" in env_content:
                print_status("API_PROVIDER å·²é…ç½®", "SUCCESS")
            else:
                print_status("API_PROVIDER æœªé…ç½®ï¼Œå»ºè®®è®¾ç½®", "WARNING")
                
            if any(key in env_content for key in ["DEEPSEEK_API_KEY", "OPENAI_API_KEY", "ANTHROPIC_API_KEY"]):
                print_status("APIå¯†é’¥å·²é…ç½®", "SUCCESS")
            else:
                print_status("å»ºè®®é…ç½®APIå¯†é’¥ä»¥è·å¾—æœ€ä½³ä½“éªŒ", "WARNING")
                
        except Exception as e:
            print_status(f"è¯»å– .env æ–‡ä»¶å¤±è´¥: {e}", "ERROR")
    else:
        print_status(".env æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®", "WARNING")

def start_backend():
    """å¯åŠ¨åç«¯æœåŠ¡"""
    print_status("å¯åŠ¨åç«¯APIæœåŠ¡...", "INFO")
    
    try:
        # æ£€æŸ¥web_backend.pyæ˜¯å¦å­˜åœ¨
        if not Path("web_backend.py").exists():
            print_status("web_backend.py ä¸å­˜åœ¨!", "ERROR")
            return None
            
        print_status("åç«¯å°†æ˜¾ç¤ºè¯¦ç»†çš„å¤„ç†æ—¥å¿—ï¼ŒåŒ…æ‹¬æ€ç»´å¯¼å›¾ç”Ÿæˆè¿‡ç¨‹", "INFO")
        print_status("=" * 50, "INFO")
        
        # å¯åŠ¨FastAPIæœåŠ¡å™¨ - ä¸é‡å®šå‘è¾“å‡ºï¼Œä¿ç•™consoleæ—¥å¿—
        backend_process = subprocess.Popen([
            sys.executable, "-m", "uvicorn", 
            "web_backend:app",
            "--host", "0.0.0.0",
            "--port", "8000",
            "--reload",
            "--log-level", "info"
        ])  # ç§»é™¤äº†stdoutå’Œstderrçš„é‡å®šå‘
        
        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        time.sleep(3)
        
        if backend_process.poll() is None:
            print_status("åç«¯æœåŠ¡å¯åŠ¨æˆåŠŸ (http://localhost:8000)", "SUCCESS")
            print_status("æ€ç»´å¯¼å›¾ç”Ÿæˆæ—¥å¿—å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º", "INFO")
            print_status("=" * 50, "INFO")
            return backend_process
        else:
            print_status("åç«¯æœåŠ¡å¯åŠ¨å¤±è´¥", "ERROR")
            return None
            
    except Exception as e:
        print_status(f"å¯åŠ¨åç«¯æœåŠ¡æ—¶å‡ºé”™: {e}", "ERROR")
        return None

def find_npm():
    """æŸ¥æ‰¾npmå¯æ‰§è¡Œæ–‡ä»¶"""
    # å¸¸è§çš„npmè·¯å¾„
    npm_paths = [
        "npm",  # ç³»ç»ŸPATHä¸­
        "npm.cmd",  # Windows
        "npm.ps1",  # PowerShell
        r"D:\nodejs\npm.cmd",  # å¸¸è§å®‰è£…è·¯å¾„
        r"D:\nodejs\npm.ps1",
    ]
    
    for npm_path in npm_paths:
        try:
            result = subprocess.run([npm_path, "--version"], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                print_status(f"æ‰¾åˆ°npm: {npm_path} (ç‰ˆæœ¬: {result.stdout.strip()})", "SUCCESS")
                return npm_path
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
            continue
    
    return None

def install_nodejs_in_conda():
    """åœ¨condaç¯å¢ƒä¸­å®‰è£…Node.js"""
    print_status("æ­£åœ¨å°è¯•å®‰è£…Node.jsåˆ°condaç¯å¢ƒ...", "INFO")
    try:
        subprocess.run([
            "conda", "install", "-y", "nodejs", "npm", "-c", "conda-forge"
        ], check=True, capture_output=True, text=True)
        print_status("Node.jså®‰è£…æˆåŠŸ", "SUCCESS")
        return True
    except subprocess.CalledProcessError as e:
        print_status(f"condaå®‰è£…Node.jså¤±è´¥: {e}", "ERROR")
        return False

def start_frontend():
    """å¯åŠ¨å‰ç«¯æœåŠ¡"""
    print_status("å¯åŠ¨å‰ç«¯ReactæœåŠ¡...", "INFO")
    
    frontend_dir = Path("frontend")
    if not frontend_dir.exists():
        print_status("frontend ç›®å½•ä¸å­˜åœ¨!", "ERROR")
        return None
    
    # æŸ¥æ‰¾npm
    npm_command = find_npm()
    
    if not npm_command:
        print_status("æœªæ‰¾åˆ°npmï¼Œå°è¯•å®‰è£…Node.js...", "WARNING")
        if install_nodejs_in_conda():
            npm_command = find_npm()
        
        if not npm_command:
            print_status("æ— æ³•æ‰¾åˆ°æˆ–å®‰è£…npmï¼Œè·³è¿‡å‰ç«¯å¯åŠ¨", "ERROR")
            print_status("è¯·æ‰‹åŠ¨å®‰è£…Node.js: https://nodejs.org/", "INFO")
            return None
    
    try:
        # æ£€æŸ¥node_modulesæ˜¯å¦å­˜åœ¨
        if not (frontend_dir / "node_modules").exists():
            print_status("å®‰è£…å‰ç«¯ä¾èµ–...", "INFO")
            result = subprocess.run([npm_command, "install"], 
                                  cwd=frontend_dir, check=True, 
                                  capture_output=True, text=True, timeout=300)
            print_status("å‰ç«¯ä¾èµ–å®‰è£…å®Œæˆ", "SUCCESS")
        
        # å¯åŠ¨Reactå¼€å‘æœåŠ¡å™¨ - ä¸é‡å®šå‘è¾“å‡ºï¼Œæ˜¾ç¤ºç¼–è¯‘ä¿¡æ¯
        print_status(f"ä½¿ç”¨ {npm_command} å¯åŠ¨å‰ç«¯æœåŠ¡...", "INFO")
        print_status("å‰ç«¯ç¼–è¯‘ä¿¡æ¯:", "INFO")
        print_status("-" * 30, "INFO")
        
        frontend_process = subprocess.Popen([
            npm_command, "start"
        ], cwd=frontend_dir)  # ç§»é™¤stdoutå’Œstderré‡å®šå‘
        
        # ç­‰å¾…å‰ç«¯æœåŠ¡å¯åŠ¨ - å¢åŠ ç­‰å¾…æ—¶é—´
        print_status("ç­‰å¾…å‰ç«¯ç¼–è¯‘å®Œæˆ...", "INFO")
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦çœŸçš„å¯è®¿é—®
        max_attempts = 30  # æœ€å¤šç­‰å¾…30æ¬¡ï¼Œæ¯æ¬¡2ç§’
        for attempt in range(max_attempts):
            if frontend_process.poll() is not None:
                print_status("å‰ç«¯è¿›ç¨‹æ„å¤–é€€å‡º", "ERROR")
                return None
                
            try:
                # å°è¯•è®¿é—®å‰ç«¯æœåŠ¡
                response = urllib.request.urlopen("http://localhost:3000", timeout=2)
                if response.status == 200:
                    print_status("å‰ç«¯æœåŠ¡å¯åŠ¨æˆåŠŸ (http://localhost:3000)", "SUCCESS")
                    return frontend_process
            except (urllib.error.URLError, urllib.error.HTTPError, OSError):
                # æœåŠ¡è¿˜æ²¡å‡†å¤‡å¥½ï¼Œç»§ç»­ç­‰å¾…
                time.sleep(2)
                if attempt % 5 == 0:  # æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print_status(f"æ­£åœ¨ç­‰å¾…å‰ç«¯å¯åŠ¨... ({attempt*2}s)", "INFO")
        
        print_status("å‰ç«¯æœåŠ¡å¯åŠ¨è¶…æ—¶ï¼Œä½†è¿›ç¨‹ä»åœ¨è¿è¡Œ", "WARNING")
        print_status("è¯·æ‰‹åŠ¨æ£€æŸ¥ http://localhost:3000 æ˜¯å¦å¯è®¿é—®", "INFO")
        return frontend_process
            
    except subprocess.CalledProcessError as e:
        print_status(f"å¯åŠ¨å‰ç«¯æœåŠ¡æ—¶å‡ºé”™: {e}", "ERROR")
        if hasattr(e, 'stdout') and e.stdout:
            print_status(f"è¾“å‡º: {e.stdout}", "INFO")
        if hasattr(e, 'stderr') and e.stderr:
            print_status(f"é”™è¯¯: {e.stderr}", "ERROR")
        return None
    except subprocess.TimeoutExpired:
        print_status("npm installè¶…æ—¶ï¼Œå¯èƒ½ç½‘ç»œè¾ƒæ…¢", "ERROR")
        return None
    except Exception as e:
        print_status(f"å¯åŠ¨å‰ç«¯æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", "ERROR")
        return None

def open_browser():
    """å»¶è¿Ÿæ‰“å¼€æµè§ˆå™¨"""
    def delayed_open():
        time.sleep(15)  # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨ï¼Œå¢åŠ åˆ°15ç§’
        try:
            webbrowser.open("http://localhost:3000")
            print_status("å·²æ‰“å¼€æµè§ˆå™¨", "SUCCESS")
        except Exception as e:
            print_status(f"è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨å¤±è´¥: {e}", "WARNING")
            print_status("è¯·æ‰‹åŠ¨è®¿é—®: http://localhost:3000", "INFO")
    
    threading.Thread(target=delayed_open, daemon=True).start()

def main():
    """ä¸»å‡½æ•°"""
    print_status("=== æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ Conda ç¯å¢ƒå¯åŠ¨å™¨ ===", "INFO")
    print_status("ä¼˜åŒ–çš„ä¸Šä¼ ä½“éªŒï¼šå…ˆæ˜¾ç¤ºæ–‡æ¡£ï¼Œå†ç”Ÿæˆæ€ç»´å¯¼å›¾", "INFO")
    print_status("âœ¨ æ–°åŠŸèƒ½ï¼šä¿ç•™è¯¦ç»†çš„æ§åˆ¶å°æ—¥å¿—è¾“å‡º", "INFO")
    print("")
    
    # æ£€æŸ¥Condaç¯å¢ƒ
    check_conda_env()
    
    # å®‰è£…ä¾èµ–
    install_requirements()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    check_environment_variables()
    
    print_status("å¯åŠ¨Webåº”ç”¨æœåŠ¡...", "INFO")
    print("")
    
    # å¯åŠ¨åç«¯
    backend_process = start_backend()
    if not backend_process:
        print_status("åç«¯å¯åŠ¨å¤±è´¥ï¼Œé€€å‡º", "ERROR")
        return
    
    # å¯åŠ¨å‰ç«¯
    frontend_process = start_frontend()
    if not frontend_process:
        print_status("å‰ç«¯å¯åŠ¨å¤±è´¥ï¼Œä½†åç«¯ä»åœ¨è¿è¡Œ", "WARNING")
        print_status("ä½ å¯ä»¥è®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£", "INFO")
    else:
        # æ‰“å¼€æµè§ˆå™¨
        open_browser()
    
    print("")
    print_status("=== æœåŠ¡è¿è¡Œä¸­ ===", "SUCCESS")
    print_status("å‰ç«¯åœ°å€: http://localhost:3000", "INFO")
    print_status("åç«¯åœ°å€: http://localhost:8000", "INFO")
    print_status("APIæ–‡æ¡£: http://localhost:8000/docs", "INFO")
    print("")
    print_status("ğŸ“‹ åŠŸèƒ½ç‰¹è‰²:", "INFO")
    print_status("  â€¢ ä¸Šä¼ æ–‡ä»¶åç«‹å³æ˜¾ç¤ºæ–‡æ¡£å†…å®¹", "INFO")
    print_status("  â€¢ æ€ç»´å¯¼å›¾å¼‚æ­¥ç”Ÿæˆï¼Œå®æ—¶çŠ¶æ€æ›´æ–°", "INFO")
    print_status("  â€¢ è¯¦ç»†çš„æ§åˆ¶å°æ—¥å¿—ï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æ§", "INFO")
    print_status("  â€¢ æ”¯æŒDeepSeekã€OpenAIã€Claudeã€Geminiç­‰å¤šç§AIæ¨¡å‹", "INFO")
    print("")
    print_status("ğŸ’¡ ä½¿ç”¨æç¤º:", "INFO")
    print_status("  â€¢ ä¸Šä¼ .mdæˆ–.txtæ–‡ä»¶åç«‹å³å¯ä»¥é˜…è¯»å†…å®¹", "INFO")
    print_status("  â€¢ æ€ç»´å¯¼å›¾ç”Ÿæˆè¿‡ç¨‹ä¼šåœ¨æ§åˆ¶å°æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—", "INFO")
    print_status("  â€¢ ç”Ÿæˆå®Œæˆåå¯ä¸‹è½½æ€ç»´å¯¼å›¾å’ŒMermaidä»£ç ", "INFO")
    print("")
    print_status("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡", "WARNING")
    
    try:
        # ç­‰å¾…ç”¨æˆ·ä¸­æ–­
        if frontend_process:
            frontend_process.wait()
        else:
            backend_process.wait()
    except KeyboardInterrupt:
        print_status("\næ­£åœ¨åœæ­¢æœåŠ¡...", "INFO")
        
        if frontend_process:
            frontend_process.terminate()
            print_status("å‰ç«¯æœåŠ¡å·²åœæ­¢", "SUCCESS")
            
        if backend_process:
            backend_process.terminate()
            print_status("åç«¯æœåŠ¡å·²åœæ­¢", "SUCCESS")
        
        print_status("æ‰€æœ‰æœåŠ¡å·²åœæ­¢", "SUCCESS")

if __name__ == "__main__":
    main()
</file>

<file path="frontend/src/components/DocumentRenderer.js">
import React, { useMemo, useState, useCallback } from 'react';
import ReactMarkdown from 'react-markdown';
import { DndContext, closestCenter, KeyboardSensor, PointerSensor, useSensor, useSensors } from '@dnd-kit/core';
import { SortableContext, verticalListSortingStrategy, arrayMove } from '@dnd-kit/sortable';
import { restrictToVerticalAxis } from '@dnd-kit/modifiers';
import LogicalDivider from './LogicalDivider';
import SortableParagraph from './SortableParagraph';
import SortableDivider from './SortableDivider';
import './DocumentRenderer.css';

// è§£ææ®µè½å’Œåˆ†å‰²çº¿çš„æ•°æ®ç»“æ„
const parseContentWithDividers = (content, onContentBlockRef, nodeMapping = null) => {
  if (!content) return [];
  
  // åˆ›å»ºæ®µè½IDåˆ°èŠ‚ç‚¹IDçš„æ˜ å°„
  const paragraphToNodeMap = {};
  if (nodeMapping) {
    Object.entries(nodeMapping).forEach(([nodeId, nodeData]) => {
      if (nodeData.paragraph_ids && Array.isArray(nodeData.paragraph_ids)) {
        nodeData.paragraph_ids.forEach(paragraphId => {
          paragraphToNodeMap[paragraphId] = nodeId;
        });
      }
    });
  }
  
  console.log('ğŸ“ [é€»è¾‘åˆ†å‰²] æ®µè½åˆ°èŠ‚ç‚¹çš„æ˜ å°„:', paragraphToNodeMap);
  
  // æŒ‰æ®µè½åˆ†å‰²å†…å®¹ï¼Œä¿ç•™æ®µè½IDæ ‡è®°
  const paragraphs = content.split(/(\[para-\d+\])/g).filter(part => part.trim());
  console.log('ğŸ“ [æ®µè½è§£æ] æ€»æ®µè½æ•°é‡:', paragraphs.length, 'å‰5ä¸ªéƒ¨åˆ†:', paragraphs.slice(0, 5));
  
  const items = [];
  let currentParagraphId = null;
  let currentContent = '';
  let currentNodeId = null;
  
  // æ ¹æ®è¯­ä¹‰è§’è‰²è®¾ç½®é¢œè‰²
  const getColorByRole = (role) => {
    if (!role) return 'gray';
    const roleColors = {
      'å¼•è¨€': 'blue',
      'æ ¸å¿ƒè®ºç‚¹': 'purple',
      'æ”¯æ’‘è¯æ®': 'green',
      'åé©³': 'red',
      'ç»“è®º': 'yellow',
      'å†å²æ¡ˆä¾‹': 'blue',
      'ç†è®ºæ‹“å±•': 'purple'
    };
    return roleColors[role] || 'gray';
  };
  
  paragraphs.forEach((part, partIndex) => {
    const paraIdMatch = part.match(/\[para-(\d+)\]/);
    
    if (paraIdMatch) {
      // å¦‚æœæœ‰ä¹‹å‰çš„å†…å®¹ï¼Œå…ˆå¤„ç†å®ƒ
      if (currentContent.trim() && currentParagraphId) {
        console.log(`ğŸ“ [æ®µè½è§£æ] åˆ›å»ºæ®µè½æ•°æ®: ${currentParagraphId}, å†…å®¹é•¿åº¦: ${currentContent.trim().length}`);
        
        items.push({
          id: currentParagraphId,
          type: 'paragraph',
          paragraphId: currentParagraphId,
          content: currentContent.trim(),
          nodeId: currentNodeId,
          onContentBlockRef
        });
      }
      
      // è®¾ç½®æ–°çš„æ®µè½ID
      const newParagraphId = `para-${paraIdMatch[1]}`;
      const newNodeId = paragraphToNodeMap[newParagraphId];
      
      // æ£€æŸ¥èŠ‚ç‚¹å˜åŒ–ï¼Œå¦‚æœèŠ‚ç‚¹å‘ç”Ÿå˜åŒ–ä¸”ä¸æ˜¯ç¬¬ä¸€ä¸ªæ®µè½ï¼Œåˆ™æ’å…¥åˆ†å‰²çº¿
      if (nodeMapping && newNodeId && currentNodeId && newNodeId !== currentNodeId) {
        const nodeInfo = nodeMapping[newNodeId];
        if (nodeInfo) {
          console.log(`ğŸ“ [é€»è¾‘åˆ†å‰²] æ£€æµ‹åˆ°èŠ‚ç‚¹å˜åŒ–: ${currentNodeId} -> ${newNodeId}`);
          
          // æ’å…¥é€»è¾‘åˆ†å‰²çº¿
          items.push({
            id: `divider-${newNodeId}`,
            type: 'divider',
            nodeId: newNodeId,
            nodeInfo: {
              title: nodeInfo.text_snippet || nodeInfo.semantic_role || newNodeId,
              id: newNodeId,
              color: getColorByRole(nodeInfo.semantic_role)
            }
          });
        }
      }
      
      currentParagraphId = newParagraphId;
      currentNodeId = newNodeId;
      currentContent = '';
      
      console.log(`ğŸ“ [æ®µè½è§£æ] å‘ç°æ®µè½æ ‡è®°: ${currentParagraphId}, èŠ‚ç‚¹ID: ${currentNodeId}`);
    } else {
      // ç´¯ç§¯å†…å®¹
      currentContent += part;
      console.log(`ğŸ“ [å†…å®¹ç´¯ç§¯] å½“å‰æ®µè½: ${currentParagraphId}, ç´¯ç§¯é•¿åº¦: ${currentContent.length}, æ–°å¢: ${part.substring(0, 30)}...`);
    }
  });
  
  // å¤„ç†æœ€åä¸€ä¸ªæ®µè½
  if (currentContent.trim() && currentParagraphId) {
    console.log(`ğŸ“ [æ®µè½è§£æ-æœ€å] åˆ›å»ºæœ€åæ®µè½æ•°æ®: ${currentParagraphId}, å†…å®¹é•¿åº¦: ${currentContent.trim().length}`);
    
    items.push({
      id: currentParagraphId,
      type: 'paragraph',
      paragraphId: currentParagraphId,
      content: currentContent.trim(),
      nodeId: currentNodeId,
      onContentBlockRef
    });
  }
  
  console.log(`ğŸ“ [è§£ææ€»ç»“] æ€»å…±åˆ›å»ºäº† ${items.length} ä¸ªé¡¹ç›®`);
  
  // å¥å£®æ€§æ£€æŸ¥ï¼šè¿‡æ»¤æ‰å¯èƒ½çš„æ— æ•ˆå…ƒç´ 
  const validItems = items.filter(item => {
    if (!item) {
      console.warn('ğŸ“ [è§£ææ€»ç»“] âš ï¸ å‘ç°ç©ºçš„ item');
      return false;
    }
    if (!item.id) {
      console.warn('ğŸ“ [è§£ææ€»ç»“] âš ï¸ å‘ç°æ²¡æœ‰ id çš„ item:', item);
      return false;
    }
    if (!item.type) {
      console.warn('ğŸ“ [è§£ææ€»ç»“] âš ï¸ å‘ç°æ²¡æœ‰ type çš„ item:', item);
      return false;
    }
    return true;
  });
  
  if (validItems.length !== items.length) {
    console.warn(`ğŸ“ [è§£ææ€»ç»“] âš ï¸ è¿‡æ»¤æ‰äº† ${items.length - validItems.length} ä¸ªæ— æ•ˆé¡¹ç›®`);
  }
  
  return validItems;
};

// æ¸²æŸ“æ®µè½ç»„ä»¶
const renderParagraphComponent = (item) => {
  const { paragraphId, content, onContentBlockRef } = item;
  
  // ğŸ”§ å›ºå®šå½“å‰æ®µè½IDï¼Œé¿å…é—­åŒ…é™·é˜±
  const paragraphIdToRegister = paragraphId;
  const contentPreview = content.substring(0, 50) + '...';
  
  return (
    <div 
      key={`${paragraphId}-content`}
      id={paragraphId}
      data-para-id={paragraphId}
      className="paragraph-block mb-3 p-2 rounded transition-all duration-200"
      ref={(el) => {
        console.log('ğŸ“ [æ®µè½æ³¨å†Œ] æ³¨å†Œæ®µè½å¼•ç”¨:', paragraphIdToRegister, 'å…ƒç´ :', !!el, 'å†…å®¹é¢„è§ˆ:', contentPreview);
        if (el) {
          console.log('ğŸ“ [æ®µè½æ³¨å†Œ-DOM] å…ƒç´ DOMä¿¡æ¯:', {
            id: el.id,
            dataParaId: el.getAttribute('data-para-id'),
            className: el.className,
            offsetTop: el.offsetTop,
            clientHeight: el.clientHeight
          });
        } else {
          console.log('ğŸ“ [æ®µè½æ³¨å†Œ-DOM] å…ƒç´ ä¸ºnullï¼Œæ®µè½:', paragraphIdToRegister);
        }
        onContentBlockRef(el, paragraphIdToRegister);
      }}
    >
      <ReactMarkdown
        components={{
          h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
          h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
          h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
          h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
          h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
          h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
          p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
          ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
          ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
          li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
          blockquote: ({node, ...props}) => (
            <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
          ),
          code: ({node, inline, ...props}) => 
            inline 
              ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
              : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
          pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
        }}
      >
        {content}
      </ReactMarkdown>
    </div>
  );
};

// ç‹¬ç«‹çš„æ®µè½æ¸²æŸ“å‡½æ•°ï¼Œé¿å…React Hookè§„åˆ™é—®é¢˜ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
const renderParagraphsWithIds = (content, onContentBlockRef, nodeMapping = null) => {
  if (!content) return null;
  
  // ä¸ºäº†å‘åå…¼å®¹ï¼Œè¿”å›è§£æåçš„é¡¹ç›®æ¸²æŸ“ä¸ºJSX
  const items = parseContentWithDividers(content, onContentBlockRef, nodeMapping);
  
  return items.map((item) => {
    if (item.type === 'paragraph') {
      return renderParagraphComponent(item);
    } else if (item.type === 'divider') {
      return (
        <LogicalDivider 
          key={item.id}
          nodeInfo={item.nodeInfo}
        />
      );
    }
    return null;
  });
};

// å¯æ’åºçš„å†…å®¹æ¸²æŸ“ç»„ä»¶
const SortableContentRenderer = ({ content, onContentBlockRef, nodeMapping = null, onNodeMappingUpdate, onOrderChange }) => {
  const [items, setItems] = useState([]);
  
  // ä½¿ç”¨ useMemo ç¼“å­˜è§£æç»“æœ
  const parsedItems = useMemo(() => {
    const result = parseContentWithDividers(content, onContentBlockRef, nodeMapping);
    // å¥å£®æ€§æ£€æŸ¥ï¼šç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„æ•°ç»„ï¼Œä¸”æ¯ä¸ªå…ƒç´ éƒ½æœ‰ id å±æ€§
    if (!Array.isArray(result)) {
      console.warn('ğŸ“ [è§£æå†…å®¹] âš ï¸ parseContentWithDividers è¿”å›äº†éæ•°ç»„ç»“æœ:', result);
      return [];
    }
    
    const validResult = result.filter(item => item && item.id);
    if (validResult.length !== result.length) {
      console.warn('ğŸ“ [è§£æå†…å®¹] âš ï¸ è§£æç»“æœåŒ…å«æ— æ•ˆå…ƒç´ ï¼Œå·²è¿‡æ»¤:', {
        åŸå§‹é•¿åº¦: result.length,
        æœ‰æ•ˆé•¿åº¦: validResult.length,
        æ— æ•ˆå…ƒç´ : result.filter(item => !item || !item.id)
      });
    }
    
    return validResult;
  }, [content, onContentBlockRef, nodeMapping]);
  
  // åˆå§‹åŒ– items çŠ¶æ€
  React.useEffect(() => {
    console.log('ğŸ“ [çŠ¶æ€æ›´æ–°] æ›´æ–° items çŠ¶æ€ï¼Œæ–°é•¿åº¦:', parsedItems.length);
    setItems(parsedItems);
  }, [parsedItems]);
  
  // ä¼ æ„Ÿå™¨é…ç½®
  const sensors = useSensors(
    useSensor(PointerSensor),
    useSensor(KeyboardSensor)
  );
  
  // æ‹–æ‹½ç»“æŸå¤„ç†å‡½æ•° - é‡æ„ä¸ºåªè´Ÿè´£è®¡ç®—æ–°é¡ºåº
  const handleDragEnd = useCallback((event) => {
    const { active, over } = event;
    
    // å¥å£®æ€§æ£€æŸ¥ï¼šç¡®ä¿ active å’Œ over å¯¹è±¡åŠå…¶ id å±æ€§å­˜åœ¨
    if (!active || !over || !active.id || !over.id) {
      console.warn('ğŸ“ [æ‹–æ‹½æ’åº] âš ï¸ æ‹–æ‹½äº‹ä»¶å¯¹è±¡ä¸å®Œæ•´:', { active, over });
      return;
    }
    
    if (active.id !== over.id) {
      // å¥å£®æ€§æ£€æŸ¥ï¼šç¡®ä¿ items æ•°ç»„å­˜åœ¨ä¸”ä¸ä¸ºç©º
      if (!items || items.length === 0) {
        console.warn('ğŸ“ [æ‹–æ‹½æ’åº] âš ï¸ items æ•°ç»„ä¸ºç©ºæˆ–ä¸å­˜åœ¨');
        return;
      }
      
      // å¥å£®æ€§æ£€æŸ¥ï¼šè¿‡æ»¤æ‰å¯èƒ½çš„ null/undefined å…ƒç´ ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªå…ƒç´ éƒ½æœ‰ id å±æ€§
      const validItems = items.filter(item => item && item.id);
      if (validItems.length !== items.length) {
        console.warn('ğŸ“ [æ‹–æ‹½æ’åº] âš ï¸ items æ•°ç»„åŒ…å«æ— æ•ˆå…ƒç´ ï¼Œå·²è¿‡æ»¤:', {
          åŸå§‹é•¿åº¦: items.length,
          æœ‰æ•ˆé•¿åº¦: validItems.length,
          æ— æ•ˆå…ƒç´ : items.filter(item => !item || !item.id)
        });
      }
      
      const oldIndex = validItems.findIndex((item) => item.id === active.id);
      const newIndex = validItems.findIndex((item) => item.id === over.id);
      
      // ç¡®ä¿æ‰¾åˆ°äº†æœ‰æ•ˆçš„ç´¢å¼•
      if (oldIndex === -1 || newIndex === -1) {
        console.warn('ğŸ“ [æ‹–æ‹½æ’åº] âš ï¸ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ‹–æ‹½ç´¢å¼•:', {
          activeId: active.id,
          overId: over.id,
          oldIndex,
          newIndex,
          validItemIds: validItems.map(item => item.id)
        });
        return;
      }
      
      console.log('ğŸ“ [æ‹–æ‹½æ’åº] ç§»åŠ¨é¡¹ç›®:', {
        activeId: active.id,
        overId: over.id,
        oldIndex,
        newIndex
      });
      
      const newItems = arrayMove(validItems, oldIndex, newIndex);
      console.log('ğŸ“ [æ‹–æ‹½æ’åº] è®¡ç®—å‡ºæ–°çš„é¡¹ç›®é¡ºåºï¼Œé•¿åº¦:', newItems.length);
      
      // å…ˆæ›´æ–°æœ¬åœ°çŠ¶æ€ï¼Œç¡®ä¿UIç«‹å³å“åº”
      setItems(newItems);
      
      // è°ƒç”¨çˆ¶ç»„ä»¶ä¼ å…¥çš„å›è°ƒå‡½æ•°ï¼Œä¼ é€’æ–°çš„é¡¹ç›®é¡ºåº
      if (onOrderChange) {
        console.log('ğŸ“ [æ‹–æ‹½æ’åº] è°ƒç”¨ onOrderChange å›è°ƒå‡½æ•°');
        onOrderChange(newItems);
      } else {
        console.warn('ğŸ“ [æ‹–æ‹½æ’åº] âš ï¸ onOrderChange å›è°ƒå‡½æ•°æœªæä¾›');
      }
    }
  }, [items, onOrderChange]);
  
  // è·å–æ‰€æœ‰é¡¹ç›®çš„IDï¼ˆåŒ…æ‹¬æ®µè½å’Œåˆ†å‰²çº¿ï¼‰
  const sortableItemIds = useMemo(() => {
    // å¥å£®æ€§æ£€æŸ¥ï¼šç¡®ä¿ items æ˜¯æœ‰æ•ˆæ•°ç»„ä¸”å…ƒç´ æœ‰ id å±æ€§
    if (!Array.isArray(items)) {
      console.warn('ğŸ“ [sortableItemIds] âš ï¸ items ä¸æ˜¯æ•°ç»„:', items);
      return [];
    }
    
    const validIds = items
      .filter(item => item && item.id)
      .map(item => item.id);
    
    console.log('ğŸ“ [sortableItemIds] ç”Ÿæˆçš„IDåˆ—è¡¨é•¿åº¦:', validIds.length);
    return validIds;
  }, [items]);
  
  return (
    <DndContext
      sensors={sensors}
      collisionDetection={closestCenter}
      onDragEnd={handleDragEnd}
      modifiers={[restrictToVerticalAxis]}
    >
      <SortableContext items={sortableItemIds} strategy={verticalListSortingStrategy}>
        <div className="sortable-content">
          {items
            .filter(item => item && item.id && item.type) // è¿‡æ»¤æ— æ•ˆé¡¹ç›®
            .map((item) => {
            if (item.type === 'paragraph') {
              // æ®µè½è¢«SortableParagraphåŒ…è£…ï¼Œä½†ç”¨æˆ·ä¸èƒ½ç›´æ¥æ‹–æ‹½
              return (
                <SortableParagraph
                  key={item.id}
                  id={item.id}
                  className="mb-4"
                >
                  {renderParagraphComponent(item)}
                </SortableParagraph>
              );
            } else if (item.type === 'divider') {
              // åªæœ‰åˆ†å‰²çº¿å¯æ‹–æ‹½
              return (
                <SortableDivider
                  key={item.id}
                  id={item.id}
                  nodeInfo={item.nodeInfo}
                  className="mb-4"
                />
              );
            }
            return null;
          })}
        </div>
      </SortableContext>
    </DndContext>
  );
};

// ç»“æ„åŒ–Markdownæ¸²æŸ“å™¨ç»„ä»¶
const StructuredMarkdownRenderer = ({ content, chunks, onSectionRef }) => {
  if (!chunks || chunks.length === 0) {
    // å›é€€åˆ°åŸå§‹çš„ReactMarkdownæ¸²æŸ“
    return (
      <div className="prose prose-sm max-w-none">
        <ReactMarkdown
          components={{
            h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
            h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
            h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
            h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
            ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
            ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
            li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
            blockquote: ({node, ...props}) => (
              <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
            ),
            code: ({node, inline, ...props}) => 
              inline 
                ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
            pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
          }}
        >
          {content}
        </ReactMarkdown>
      </div>
    );
  }

  // æŒ‰ç»“æ„åŒ–æ–¹å¼æ¸²æŸ“
  return (
    <div className="prose prose-sm max-w-none">
      {chunks.map((chunk, index) => (
        <div
          key={chunk.chunk_id}
          ref={(el) => onSectionRef(el, chunk.chunk_id)}
          data-chunk-index={index}
          data-chunk-id={chunk.chunk_id}
          className="mb-6 chunk-section transition-all duration-200 ease-in-out border-l-4 border-transparent hover:border-gray-200 dark:hover:border-gray-600"
        >
          {/* æ¸²æŸ“æ ‡é¢˜ */}
          {chunk.heading && (
            <div className="mb-3">
              <ReactMarkdown
                components={{
                  h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
                  h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
                  h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
                  h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                }}
              >
                {chunk.heading}
              </ReactMarkdown>
            </div>
          )}
          
          {/* æ¸²æŸ“å†…å®¹ */}
          {chunk.content && (
            <div className="chunk-content">
              <ReactMarkdown
                components={{
                  p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
                  ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
                  ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
                  li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
                  blockquote: ({node, ...props}) => (
                    <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
                  ),
                  code: ({node, inline, ...props}) => 
                    inline 
                      ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                      : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
                  pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
                  // é˜²æ­¢åµŒå¥—æ ‡é¢˜
                  h1: ({node, ...props}) => <p className="font-bold text-lg text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h2: ({node, ...props}) => <p className="font-bold text-base text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h3: ({node, ...props}) => <p className="font-semibold text-base text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h4: ({node, ...props}) => <p className="font-semibold text-sm text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h5: ({node, ...props}) => <p className="font-semibold text-sm text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                  h6: ({node, ...props}) => <p className="font-semibold text-sm text-gray-800 dark:text-gray-200 mb-2" {...props} />,
                }}
              >
                {chunk.content}
              </ReactMarkdown>
            </div>
          )}
        </div>
      ))}
    </div>
  );
};

// æ¼”ç¤ºæ¨¡å¼æ¸²æŸ“å™¨ç»„ä»¶ - æ”¯æŒæ¼”ç¤ºæ¨¡å¼å’ŒçœŸå®æ–‡æ¡£
const DemoModeRenderer = ({ content, onContentBlockRef, isRealDocument = false, chunks = [], nodeMapping = null, onNodeMappingUpdate, onOrderChange }) => {
  
  console.log('ğŸ“„ [DemoModeRenderer] æ¸²æŸ“å™¨è°ƒç”¨å‚æ•°:');
  console.log('  - contentå­˜åœ¨:', !!content);
  console.log('  - contenté•¿åº¦:', content?.length || 0);
  console.log('  - isRealDocument:', isRealDocument);
  console.log('  - chunksæ•°é‡:', chunks?.length || 0);
  console.log('  - chunksè¯¦æƒ…:', chunks);
  console.log('  - nodeMappingå­˜åœ¨:', !!nodeMapping);
  console.log('  - onNodeMappingUpdateå­˜åœ¨:', !!onNodeMappingUpdate);
  console.log('  - onOrderChangeå­˜åœ¨:', !!onOrderChange);
  
  // æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«æ®µè½IDæ ‡è®°
  const hasParaIds = content && content.includes('[para-');
  console.log('ğŸ“„ [DemoModeRenderer] å†…å®¹åˆ†æ:', {
    hasParaIds,
    contentPreview: content?.substring(0, 200) + '...',
    contentSample: content?.substring(0, 500) // æ›´é•¿çš„å†…å®¹æ ·æœ¬
  });
  
  // å¼ºåˆ¶è°ƒè¯•ï¼šæ˜¾ç¤ºå†…å®¹ä¸­çš„æ®µè½IDåŒ¹é…
  if (content) {
    const paraMatches = content.match(/\[para-\d+\]/g);
    console.log('ğŸ“„ [DemoModeRenderer] æ‰¾åˆ°çš„æ®µè½IDæ ‡è®°:', paraMatches);
    console.log('ğŸ“„ [DemoModeRenderer] æ®µè½IDæ•°é‡:', paraMatches?.length || 0);
  }
  
  // ğŸ”§ ç¼“å­˜æ®µè½æ¸²æŸ“ç»“æœï¼Œé˜²æ­¢æ— é™é‡æ¸²æŸ“å¯¼è‡´çš„refæ³¨å†Œé—®é¢˜ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
  const renderedParagraphs = useMemo(() => {
    if (content && content.includes('[para-')) {
      console.log('ğŸ“„ [useMemoç¼“å­˜] é‡æ–°æ¸²æŸ“æ®µè½å†…å®¹ï¼Œå†…å®¹é•¿åº¦:', content.length);
      const result = renderParagraphsWithIds(content, onContentBlockRef, nodeMapping);
      console.log('ğŸ“„ [useMemoç¼“å­˜] æ®µè½æ¸²æŸ“å®Œæˆï¼Œåˆ›å»ºçš„å…ƒç´ æ•°é‡:', result?.length || 0);
      if (result && result.length > 0) {
        console.log('ğŸ“„ [useMemoç¼“å­˜] ç¬¬ä¸€ä¸ªå…ƒç´ key:', result[0]?.key);
        console.log('ğŸ“„ [useMemoç¼“å­˜] æœ€åä¸€ä¸ªå…ƒç´ key:', result[result.length - 1]?.key);
      }
      return result;
    }
    return null;
  }, [content, onContentBlockRef, nodeMapping]);
  
  console.log('ğŸ“„ [useMemoç¼“å­˜] æ®µè½æ¸²æŸ“ç»“æœç¼“å­˜çŠ¶æ€:', !!renderedParagraphs);
  
  // å¦‚æœå†…å®¹åŒ…å«æ®µè½IDæ ‡è®°ï¼Œä½¿ç”¨å¯æ’åºçš„å†…å®¹æ¸²æŸ“å™¨
  if (isRealDocument && hasParaIds) {
    console.log('ğŸ“„ [DemoModeRenderer] è¿›å…¥çœŸå®æ–‡æ¡£æ®µè½IDæ¨¡å¼ï¼Œä½¿ç”¨å¯æ’åºæ¸²æŸ“å™¨');
    
    return (
      <div className="prose prose-sm max-w-none">
        <SortableContentRenderer 
          content={content}
          onContentBlockRef={onContentBlockRef}
          nodeMapping={nodeMapping}
          onNodeMappingUpdate={onNodeMappingUpdate}
          onOrderChange={onOrderChange}
        />
      </div>
    );
  }
  
  // çœŸå®æ–‡æ¡£æ¨¡å¼ï¼šåŸºäºchunksçš„ç»“æ„åŒ–æ¸²æŸ“ï¼ˆæ²¡æœ‰æ®µè½IDæ—¶ï¼‰
  if (isRealDocument && chunks && chunks.length > 0) {
    console.log('ğŸ“„ [DemoModeRenderer] è¿›å…¥çœŸå®æ–‡æ¡£chunksæ¨¡å¼ï¼Œchunksæ•°é‡:', chunks.length);
    
    return (
      <div className="prose prose-sm max-w-none">
        {chunks.map((chunk, index) => {
          const blockId = `chunk-${index + 1}`;
          
          console.log(`ğŸ“„ [DemoModeRenderer] æ¸²æŸ“chunk ${index + 1}:`, {
            blockId,
            chunkId: chunk.chunk_id,
            title: chunk.title,
            contentLength: chunk.content?.length || 0
          });
          
          // ä½¿ç”¨å¸¸è§„çš„ ReactMarkdown æ¸²æŸ“ï¼ˆchunksæ¨¡å¼ä¸‹çš„å†…å®¹æ²¡æœ‰æ®µè½IDæ ‡è®°ï¼‰
          const renderChunkContent = (content) => {
            if (!content) return null;
            
            return (
              <ReactMarkdown
                components={{
                  h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
                  h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
                  h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
                  h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                  p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
                  ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
                  ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
                  li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
                  blockquote: ({node, ...props}) => (
                    <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
                  ),
                  code: ({node, inline, ...props}) => 
                    inline 
                      ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                      : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
                  pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
                }}
              >
                {content}
              </ReactMarkdown>
            );
          };
          
          return (
            <div 
              key={chunk.chunk_id}
              id={blockId}
              className="content-block mb-6 p-4 border-l-4 border-transparent transition-all duration-200"
              ref={(el) => onContentBlockRef(el, blockId)}
            >
              {/* æ¸²æŸ“æ ‡é¢˜ */}
              {chunk.heading && (
                <div className="mb-3">
                  <ReactMarkdown
                    components={{
                      h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
                      h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
                      h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
                      h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                      h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                      h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
                    }}
                  >
                    {chunk.heading}
                  </ReactMarkdown>
                </div>
              )}
              
              {/* æ¸²æŸ“å†…å®¹ */}
              {chunk.content && renderChunkContent(chunk.content)}
            </div>
          );
        })}
      </div>
    );
  }
  
  // ä¼ å…¥çœŸå®å†…å®¹ä½†æ²¡æœ‰chunksçš„æƒ…å†µï¼ˆå‘åå…¼å®¹ï¼‰
  if (content && !isRealDocument) {
    console.log('ğŸ“„ [DemoModeRenderer] è¿›å…¥å‘åå…¼å®¹æ¨¡å¼ï¼ˆcontentå­˜åœ¨ä½†éçœŸå®æ–‡æ¡£ï¼‰');
    
    // å¦‚æœå†…å®¹åŒ…å«æ®µè½IDï¼Œä½¿ç”¨å¯æ’åºçš„å†…å®¹æ¸²æŸ“å™¨
    if (content.includes('[para-')) {
      console.log('ğŸ“„ [å‘åå…¼å®¹] æ£€æµ‹åˆ°æ®µè½IDï¼Œä½¿ç”¨å¯æ’åºæ¸²æŸ“å™¨');
      
      return (
        <div className="prose prose-sm max-w-none">
          <SortableContentRenderer 
            content={content}
            onContentBlockRef={onContentBlockRef}
            nodeMapping={nodeMapping}
            onNodeMappingUpdate={onNodeMappingUpdate}
            onOrderChange={onOrderChange}
          />
        </div>
      );
    }
    
    // æ™®é€šå†…å®¹æ¸²æŸ“
    return (
      <div className="prose prose-sm max-w-none">
        <ReactMarkdown
          components={{
            h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
            h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
            h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
            h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
            p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
            ul: ({node, ...props}) => <ul className="mb-3 ml-4 list-disc" {...props} />,
            ol: ({node, ...props}) => <ol className="mb-3 ml-4 list-decimal" {...props} />,
            li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
            blockquote: ({node, ...props}) => (
              <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
            ),
            code: ({node, inline, ...props}) => 
              inline 
                ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
            pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
          }}
        >
          {content}
        </ReactMarkdown>
      </div>
    );
  }

  // æ¼”ç¤ºæ–‡æ¡£çš„å†…å®¹å—æ•°æ®ï¼ˆåªåœ¨çº¯ç¤ºä¾‹æ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
  console.log('ğŸ“„ [DemoModeRenderer] è¿›å…¥çº¯ç¤ºä¾‹æ¨¡å¼ï¼ˆä½¿ç”¨ç¡¬ç¼–ç å†…å®¹ï¼‰');
  const demoContentBlocks = [
    {
      id: "text-A-introduction",
      content: `ä¸ºä»€ä¹ˆä¸€ä½è¾©è¯å­¦å®¶åº”è¯¥å­¦ç€æ•°åˆ°å››ï¼Ÿ

ä¸‰å…ƒç»„/ä¸‰ä½ä¸€ä½“ä¸å…¶æº¢å‡º/è¿‡å‰©â€”â€”æ–°æ•™ã€é›…å„å®¾ä¸»ä¹‰â€¦â€¦ä¸å…¶ä»–"æ¶ˆå¤±çš„ä¸­ä»‹è€…"â€”"ä½ æ‰‹æŒ‡çš„ä¸€æ•²"â€¦â€¦â€” ä¸ºä»€ä¹ˆçœŸç†æ€»æ˜¯æ”¿æ²»æ€§çš„ï¼Ÿ`
    },
    {
      id: "text-B-fourth-party",
      content: `**Â·ä¸‰å…ƒç»„/ä¸‰ä½ä¸€ä½“ä¸å…¶æº¢å‡º/è¿‡å‰©**

ä¸€ä½é»‘æ ¼å°”æ´¾çš„è¾©è¯å­¦å®¶å¿…é¡»å­¦ç€æ•°åˆ°å¤šå°‘å‘¢ï¼Ÿå¤§å¤šæ•°é»‘æ ¼å°”çš„è§£é‡Šè€…ï¼Œæ›´ä¸ç”¨è¯´ä»–çš„æ‰¹è¯„è€…ï¼Œéƒ½è¯•å›¾ä¸€è‡´åœ°è¯´æœæˆ‘ä»¬ï¼Œæ­£ç¡®çš„ç­”æ¡ˆæ˜¯ï¼šåˆ°ä¸‰ï¼ˆè¾©è¯çš„ä¸‰å…ƒç»„[the dialectical triad]ï¼Œç­‰ç­‰ï¼‰ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜äº’ç›¸äº‰å¤ºè°èƒ½æ›´æœ‰è¯´æœåŠ›åœ°å”¤èµ·æˆ‘ä»¬å¯¹"ç¬¬å››æ–¹"çš„æ³¨æ„ã€‚è¿™ä¸ªç¬¬å››æ–¹å°±æ˜¯ä¸å¯è¾©è¯çš„æº¢å‡º/è¿‡å‰©ï¼Œæ˜¯æ­»äº¡ä¹‹å¤„æ‰€ï¼ˆâ€¦ ï¼‰ã€‚æ®è¯´å®ƒé€ƒç¦»è¾©è¯æ³•çš„æŒæ¡ï¼Œå°½ç®¡ï¼ˆæˆ–è€…æ›´å‡†ç¡®åœ°è¯´ï¼Œå› ä¸ºï¼‰å®ƒæ˜¯è¾©è¯æ³•è¿åŠ¨çš„å†…åœ¨å¯èƒ½æ€§æ¡ä»¶ï¼šåœ¨å…¶ç»“æœï¼ˆResultï¼‰ä¸­ä¸èƒ½è¢«æ‰¬å¼ƒ[aufgehoben]ã€ä¸èƒ½é‡æ–°è¢«æ”¶å…¥çš„ï¼ˆre-collectedï¼‰çº¯æ”¯å‡ºæ€§çš„å¦å®šæ€§ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¢è¡¥æ€§çš„è¦ç´ æ˜¯å¦‚ä½•å‡ºç°çš„ï¼šä¸€æ—¦æˆ‘ä»¬å°†ç›´æ¥ä¹‹ç‰©ï¼ˆtheimmediateï¼‰çš„å¦å®šï¼ˆnegationï¼‰æ·»åŠ ç»™ç›´æ¥ä¹‹ç‰©ï¼Œè¿™ä¸€å¦å®šï¼ˆnegationï¼‰å°±å›æº¯æ€§åœ°æ”¹å˜äº†ç›´æ¥æ€§ï¼ˆimmediacyï¼‰çš„æ„ä¹‰ï¼Œæ‰€ä»¥æˆ‘ä»¬è™½ç„¶å®é™…ä¸Šä»…æ‹¥æœ‰ä¸¤ä¸ªè¦ç´ å´å¿…é¡»æ•°åˆ°ä¸‰ã€‚æˆ–è€…ï¼Œå¦‚æœæˆ‘ä»¬è®¾æƒ³è¾©è¯è¿‡ç¨‹çš„å®Œæ•´å¾ªç¯ï¼Œè¿™é‡Œåªæœ‰ä¸‰ä¸ª"è‚¯å®šï¼ˆpositiveï¼‰"çš„ç¯èŠ‚ï¼ˆç›´æ¥æ€§ã€å…¶ä¸­ä»‹å’Œæœ€åå¯¹è¢«ä¸­ä»‹çš„ç›´æ¥æ€§çš„å¤å½’ï¼‰è¦å»æ•°â€”â€”æˆ‘ä»¬æ¼æ‰çš„æ˜¯çº¯ç²¹å·®å¼‚çš„é‚£éš¾ä»¥ç†è§£çš„å‰©ä½™ç‰©ï¼ˆsurplusï¼‰ï¼Œå®ƒè™½ä½¿å¾—æ•´ä¸ªè¿‡ç¨‹å¾—ä»¥è¿›è¡Œå´"ä»€ä¹ˆä¹Ÿä¸ç®—ï¼ˆcounts for nothingï¼‰"ï¼›æˆ‘ä»¬æ¼æ‰çš„æ˜¯è¿™ä¸€"å®ä½“çš„è™šç©º"ï¼Œï¼ˆå¦‚é»‘æ ¼å°”æ‰€è¨€ï¼‰å®ƒåŒæ—¶ä¹Ÿæ˜¯æ‰€æœ‰ä¸€åˆ‡ï¼ˆall and everythingï¼‰çš„"å®¹å™¨ï¼ˆreceptacle [Rezeptakulum]ï¼‰"ã€‚`
    },
    {
      id: "text-C-vanishing-mediator-core",
      content: `**Â·æ–°æ•™ã€é›…å„å®¾ä¸»ä¹‰â€¦â€¦**

ç„¶è€Œï¼Œåœ¨é‚£å¯¹"è¾©è¯æ–¹æ³•"è¿›è¡Œæƒ¹äººæ¼æ€’çš„æŠ½è±¡åæ˜ ï¼ˆabstract reflectionsï¼‰çš„æœ€ä½³ä¼ ç»Ÿä¸­ï¼Œè¿™ç§æ€è€ƒï¼ˆruminationsï¼‰æœ‰ç€ä¸€ç§çº¯ç²¹å½¢å¼çš„æœ¬æ€§ï¼›å®ƒä»¬æ‰€ç¼ºä¹çš„æ˜¯ä¸å…·ä½“å†å²å†…å®¹å†…åœ¨çš„ç›¸äº’è”ç³»ï¼ˆrelatednessï¼‰ã€‚ä¸€æ—¦æˆ‘ä»¬åˆ°è¾¾è¿™ç§å±‚æ¬¡ï¼Œç¬¬å››çš„å‰©ä½™ç‰©-ç¯èŠ‚ï¼ˆsurplus-momentï¼‰ä½œä¸ºç¬¬äºŒä¸ªç¯èŠ‚ï¼ˆåˆ†è£‚ã€æŠ½è±¡å¯¹ç«‹ï¼‰ä¸æœ€ç»ˆç»“æœ[Result]ï¼ˆå’Œè§£[reconciliation]ï¼‰ä¹‹é—´"æ¶ˆå¤±çš„ä¸­ä»‹è€…"è¿™ç§æƒ³æ³•ç«‹åˆ»è·å¾—äº†å…·ä½“çš„è½®å»“â€”â€”äººä»¬åªéœ€æƒ³æƒ³è©¹æ˜ä¿¡åœ¨å…¶è®ºé©¬å…‹æ–¯Â·éŸ¦ä¼¯çš„æ–‡ç« ï¼ˆè¿™ç¯‡æ–‡ç« æœ‰å…³éŸ¦ä¼¯å…³äºæ–°æ•™åœ¨èµ„æœ¬ä¸»ä¹‰å´›èµ·ä¸­çš„ä½œç”¨çš„ç†è®ºï¼‰ä¸­é˜æ˜"æ¶ˆå¤±çš„ä¸­ä»‹è€…"è¿™ä¸€æ¦‚å¿µçš„æ–¹å¼ã€‚`
    },
    {
      id: "text-D-mechanism",
      content: `**Â·â€¦â€¦ä¸å…¶ä»–æ¶ˆå¤±çš„ä¸­ä»‹è€…**

å½¢å¼å’Œå…¶æ¦‚å¿µå†…å®¹é—´çš„è£‚éš™ï¼Œä¹Ÿç»™æˆ‘ä»¬æä¾›äº†é€šå‘"æ¶ˆå¤±çš„ä¸­ä»‹è€…"çš„å¿…ç„¶æ€§çš„å…³é”®ï¼šä»å°å»ºä¸»ä¹‰åˆ°æ–°æ•™çš„è·¯å¾„ä¸ä»æ–°æ•™åˆ°å…·æœ‰å®—æ•™ç§äººåŒ–ç‰¹å¾çš„èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»çš„è·¯å¾„æ²¡æœ‰ç›¸åŒçš„ç‰¹å¾ã€‚ç¬¬ä¸€ä¸ªè·¯å¾„å…³ç³»åˆ°"å†…å®¹"ï¼ˆåœ¨ä¿æŒæˆ–è€…ç”šè‡³åŠ å¼ºå®—æ•™å½¢å¼çš„ä¼ªè£…ä¸‹ï¼Œå‘ç”Ÿäº†å…³é”®æ€§çš„å˜åŒ–â€”â€”ç»æµæ´»åŠ¨ä¸­ç¦æ¬²å¼è´ªå¾—[asceticacquisitive]çš„æ€åº¦è¢«æ˜ç¡®è‚¯å®šä¸ºå±•ç¤ºæ©å…¸çš„åŠ¿åŠ›èŒƒå›´ï¼‰ï¼Œè€Œç¬¬äºŒä¸ªè·¯å¾„åˆ™æ˜¯ä¸€ä¸ªçº¯ç²¹å½¢å¼çš„è¡ŒåŠ¨ï¼Œä¸€ç§å½¢å¼çš„å˜åŒ–ï¼ˆä¸€æ—¦æ–°æ•™ä½œä¸ºç¦æ¬²å¼è´ªå¾—[ascetic-acquisitive]çš„æ€åº¦å¾—åˆ°å®ç°ï¼Œå®ƒå°±ä¼šä½œä¸ºå½¢å¼è€Œè„±è½ï¼‰ã€‚`
    },
    {
      id: "text-D1D2D3-mechanism-stages",
      content: `å› æ­¤ï¼Œ"æ¶ˆå¤±çš„ä¸­ä»‹è€…"ä¹‹æ‰€ä»¥å‡ºç°ï¼Œæ˜¯å› ä¸ºåœ¨ä¸€ä¸ªè¾©è¯çš„è¿‡ç¨‹ä¸­ï¼Œå½¢å¼åœç•™åœ¨å†…å®¹åé¢çš„æ–¹å¼ï¼šé¦–å…ˆï¼Œå…³é”®æ€§çš„è½¬å˜å‘ç”Ÿåœ¨æ—§å½¢å¼çš„é™åº¦å†…ï¼Œç”šè‡³å‘ˆç°å‡ºå…¶å¤å…´çš„ä¸»å¼ è¿™ä¸€å¤–è¡¨ï¼ˆå¯¹åŸºç£æ•™æ€§çš„æ™®éåŒ–ï¼Œå›åˆ°å…¶"çœŸæ­£çš„å†…å®¹"ï¼Œç­‰ç­‰ï¼‰ï¼›ç„¶åï¼Œä¸€æ—¦"ç²¾ç¥çš„æ— å£°ç¼–ç»‡ï¼ˆsilent weavingï¼‰"å®Œæˆå…¶å·¥ä½œï¼Œæ—§å½¢å¼å°±ä¼šè„±è½ã€‚è¿™ä¸€è¿‡ç¨‹çš„åŒé‡èŠ‚å¥ï¼ˆscansionï¼‰æ‰©å±•ä½¿æˆ‘ä»¬èƒ½å¤Ÿå…·ä½“åœ°æŒæ¡"å¦å®šä¹‹å¦å®šï¼ˆnegation of negationï¼‰"è¿™ä¸€é™ˆæ—§çš„å…¬å¼ï¼šç¬¬ä¸€ä¸ªå¦å®šåœ¨äºå®è´¨æ€§å†…å®¹ç¼“æ…¢ã€ç§˜å¯†ä¸”æ— å½¢çš„å˜åŒ–ï¼Œè€Œè‡ªç›¸çŸ›ç›¾çš„æ˜¯ï¼Œè¿™ç§å˜åŒ–å‘ç”Ÿåœ¨å…¶è‡ªèº«å½¢å¼çš„åä¹‰ä¸‹çš„ï¼›é‚£ä¹ˆï¼Œä¸€æ—¦å½¢å¼å¤±å»äº†å®ƒçš„å®è´¨æ€§æƒåˆ©ï¼ˆsubstantial rightï¼‰ï¼Œå®ƒå°±ä¼šè‡ªå·±æ‘”å¾—ç²‰ç¢â€”â€”å¦å®šçš„å½¢å¼è¢«å¦å®šäº†ï¼Œæˆ–è€…ç”¨é»‘æ ¼å°”çš„ç»å…¸å¯¹å­æ¥è¯´ï¼Œå‘ç”Ÿ"åœ¨å…¶è‡ªèº«ä¸­çš„"ï¼ˆin itselfï¼‰å˜åŒ–å˜æˆäº†"å¯¹äºå…¶è‡ªèº«çš„"ï¼ˆfor itselfï¼‰ã€æˆ–ï¼Œ"è‡ªåœ¨"å‘ç”Ÿçš„å˜åŒ–å˜æˆäº†"è‡ªä¸ºçš„"â€”â€”è¯‘æ³¨ã€‘ã€‚`
    },
    {
      id: "text-E1-protestantism",
      content: `è¿™ç§è¾©è¯çš„å¿…ç„¶æ€§ä½äºä½•å¤„å‘¢ï¼Ÿæ¢è¨€ä¹‹ï¼šå…·ä½“æ¥è¯´ï¼Œæ–°æ•™æ˜¯æ€æ ·ä¸ºèµ„æœ¬ä¸»ä¹‰çš„å‡ºç°åˆ›é€ æ¡ä»¶çš„ï¼Ÿå¹¶éå¦‚äººä»¬ä¼šæœŸå¾…çš„é‚£æ ·ï¼Œé€šè¿‡é™åˆ¶å®—æ•™æ„è¯†å½¢æ€çš„å½±å“èŒƒå›´æˆ–é€šè¿‡åŠ¨æ‘‡å…¶åœ¨ä¸­ä¸–çºªç¤¾ä¼šæ— å¤„ä¸åœ¨çš„ç‰¹å¾ï¼Œè€Œæ˜¯ç›¸åé€šè¿‡å°†å…¶æ„ä¹‰ï¼ˆrelevanceï¼‰æ™®éåŒ–ï¼šè·¯å¾·åå¯¹ç”¨ä¸€é“é¸¿æ²Ÿå°†ä¿®é“é™¢ï¼ˆcloistersï¼‰ä¸ç¤¼æ‹œï¼ˆchurchï¼‰ä½œä¸ºä¸€ç§ç‹¬ç«‹çš„åˆ¶åº¦ï¼ˆinstitutionï¼‰åŒç¤¾ä¼šçš„å…¶ä»–éƒ¨åˆ†éš”ç»å¼€æ¥ï¼Œå› ä¸ºä»–å¸Œæœ›åŸºç£æ•™çš„æ€åº¦èƒ½å¤Ÿæ¸—é€å¹¶å†³å®šæˆ‘ä»¬æ•´ä¸ªçš„ä¸–ä¿—æ—¥å¸¸ç”Ÿæ´»ã€‚

å½“ç„¶ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å¯¹æ–°æ•™çš„å¹»è§‰ä¿æŒä¸€ç§åè®½çš„è·ç¦»ï¼Œå¹¶æŒ‡å‡ºæ–°æ•™åŠªåŠ›åºŸé™¤å®—æ•™ä¸æ—¥å¸¸ç”Ÿæ´»ä¹‹é—´å·®è·çš„æœ€ç»ˆç»“æœæ˜¯å¦‚ä½•å°†å®—æ•™è´¬ä½ä¸ºä¸€ç§"æ²»ç–—æ€§ï¼ˆtherapeuticï¼‰"çš„æ‰‹æ®µï¼›æ›´å›°éš¾çš„åˆ™æ˜¯è¦å»æ„æƒ³æ–°æ•™ä½œä¸ºä¸­ä¸–çºªç¤¾å›¢ä¸»ä¹‰å’Œèµ„æœ¬ä¸»ä¹‰ä¸ªäººä¸»ä¹‰é—´"æ¶ˆå¤±çš„ä¸­ä»‹è€…ï¼ˆvanishing mediatorï¼‰"çš„å¿…ç„¶æ€§ã€‚æ¢å¥è¯è¯´ï¼Œä¸å¯å¿½è§†çš„ä¸€ç‚¹æ˜¯ï¼Œå¦‚æœï¼Œäººä»¬ä¸å¯èƒ½ç›´æ¥åœ°ï¼Œä¹Ÿå°±æ˜¯ç¼ºå°‘æ–°æ•™ä½œä¸º "æ¶ˆå¤±çš„ä¸­ä»‹è€…"çš„è°ƒè§£ï¼ˆintercessionï¼‰è€Œä»ä¸­ä¸–çºªçš„"å°é—­"ç¤¾ä¼šè¿›å…¥èµ„äº§é˜¶çº§ç¤¾ä¼šï¼šæ­£æ˜¯æ–°æ•™é€šè¿‡å…¶å¯¹åŸºç£æ•™æ€§ï¼ˆChristianityï¼‰çš„æ™®éåŒ–ï¼Œä¸ºå…¶æ’¤å›åˆ°ç§å¯†é¢†åŸŸé¢„å¤‡äº†åŸºç¡€ã€‚`
    },
    {
      id: "text-E2-jacobinism",
      content: `åœ¨æ”¿æ²»é¢†åŸŸï¼Œé›…å„å®¾ä¸»ä¹‰æ‰®æ¼”äº†åŒæ ·çš„è§’è‰²ï¼Œå®ƒç”šè‡³å¯ä»¥è¢«å®šä¹‰ä¸º"æ”¿æ²»çš„æ–°æ•™"ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¹Ÿå¾ˆå®¹æ˜“ä¿æŒä¸€ç§åè®½çš„è·ç¦»ï¼Œå¹¶æŒ‡å‡ºé›…å„å®¾ä¸»ä¹‰å¦‚ä½•å¿…ç„¶ä¼šé€šè¿‡å°†ç¤¾ä¼šæ•´ä½“ç²—æš´åœ°ç¼©å‡ä¸ºæŠ½è±¡çš„å¹³ç­‰åŸåˆ™è€Œåœ¨ææ€–ä¸»ä¹‰ä¸­ç»“æŸï¼Œå› ä¸ºè¿™ç§ç¼©å‡å—åˆ°äº†åˆ†æ”¯çš„ï¼ˆramifiedï¼‰å…·ä½“å…³ç³»ä¹‹ç½‘çš„æŠµåˆ¶ï¼ˆè§é»‘æ ¼å°”åœ¨ã€Šç²¾ç¥ç°è±¡å­¦ã€‹ä¸­å¯¹é›…å„å®¾ä¸»ä¹‰çš„ç»å…¸æ‰¹è¯„ï¼‰ã€‚æ›´éš¾åšåˆ°çš„æ˜¯ï¼Œè¦è¯æ˜ä¸ºä»€ä¹ˆä¸å¯èƒ½ä»æ—§åˆ¶åº¦ç›´æ¥è¿›å…¥è‡ªæˆ‘æœ¬ä½çš„èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»â€”â€”ä¸ºä»€ä¹ˆï¼Œæ­£æ˜¯å› ä¸ºä»–ä»¬è™šå¹»åœ°å°†ç¤¾ä¼šæ•´ä½“è¿˜åŸä¸ºæ°‘ä¸»æ”¿æ²»æ–¹æ¡ˆï¼Œé›…å„å®¾ä¸»ä¹‰æ˜¯ä¸€ä¸ªå¿…è¦çš„"æ¶ˆå¤±çš„ä¸­ä»‹è€…"ï¼ˆé»‘æ ¼å°”æ‰¹è¯„å¾—å®é™…è¦ç‚¹å¹¶ä¸åœ¨äºè¯´é›…å„å®¾ä¸»ä¹‰æ–¹æ¡ˆæœ‰ä¹Œæ‰˜é‚¦ï¼ææ€–ä¸»ä¹‰ç‰¹å¾è¿™æ ·çš„è€ç”Ÿå¸¸è°ˆä¸­ï¼Œè€Œæ˜¯åœ¨äºæ­¤ï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨é›…å„å®¾ä¸»ä¹‰ä¸­å‘ç°ç°ä»£"ææƒä¸»ä¹‰"çš„æ ¹æºå’Œç¬¬ä¸€ä¸ªå½¢å¼æ˜¯å¾ˆå®¹æ˜“çš„ï¼›è€Œè¦å®Œå…¨æ‰¿è®¤å’Œé‡‡çº³æ²¡æœ‰é›…å„å®¾ä¸»ä¹‰çš„"æº¢å‡º/è¿‡å‰©"å°±ä¸ä¼šæœ‰"å¸¸æ€çš„"å¤šå…ƒæ°‘ä¸»è¿™æ ·ä¸€ä¸ªäº‹å®åˆ™è¦æ›´åŠ å›°éš¾å¹¶ä»¤äººä¸å®‰ã€‚`
    },
    {
      id: "text-E3-other-examples",
      content: `æˆ‘ä»¬åº”è¯¥è¿›ä¸€æ­¥å¤æ‚åŒ–è¿™å‰¯å›¾æ™¯ï¼šä»”ç»†è§‚å¯Ÿå¯ä»¥å‘ç°ï¼Œåœ¨ä»å°å»ºæ”¿æ²»ç»“æ„åˆ°èµ„äº§é˜¶çº§æ”¿æ²»ç»“æ„çš„è¿‡ç¨‹ä¸­ï¼Œå­˜åœ¨ç€ä¸¤ä¸ª"æ¶ˆå¤±çš„ä¸­ä»‹è€…"ï¼šç»å¯¹å›ä¸»åˆ¶å’Œé›…å„å®¾ä¸»ä¹‰ã€‚ç¬¬ä¸€ä¸ªæ˜¯æœ‰å…³ä¸€ä¸ªæ‚–è®ºå¼å¦¥åçš„æ ‡å¿—ä¸ä½“ç°ï¼ˆembodimentï¼‰ï¼šè¿™ç§æ”¿æ²»å½¢å¼ä½¿å´›èµ·çš„èµ„äº§é˜¶çº§èƒ½å¤Ÿé€šè¿‡æ‰“ç ´å°å»ºä¸»ä¹‰ã€å…¶è¡Œä¼šå’Œç¤¾å›¢ï¼ˆcorporationsï¼‰çš„ç»æµåŠ›é‡æ¥åŠ å¼ºå…¶ç»æµéœ¸æƒâ€”â€”å½“ç„¶ï¼Œå®ƒçš„è‡ªç›¸çŸ›ç›¾ä¹‹å¤„åœ¨äºï¼Œå°å»ºä¸»ä¹‰æ­£æ˜¯é€šè¿‡å°†è‡ªå·±çš„æœ€é«˜ç‚¹ï¼ˆcrowning pointï¼‰ç»å¯¹åŒ–â€”â€”å°†ç»å¯¹æƒåŠ›èµ‹äºˆå›ä¸»â€”â€”æ¥"è‡ªæ˜åŸå¢“"çš„ï¼›å› æ­¤ï¼Œç»å¯¹å›ä¸»åˆ¶çš„ç»“æœæ˜¯æ”¿æ²»ç§©åºä¸ç»æµåŸºç¡€ç›¸"åˆ†ç¦»"ã€‚åŒæ ·çš„"è„±èŠ‚ï¼ˆdisconnectionï¼‰"ä¹Ÿæ˜¯é›…å„å®¾ä¸»ä¹‰çš„ç‰¹å¾ï¼šæŠŠé›…å„å®¾ä¸»ä¹‰è§„å®šä¸ºä¸€ç§æ¿€è¿›æ„è¯†å½¢æ€å·²ç»æ˜¯é™ˆè¯æ»¥è°ƒäº†ï¼Œå®ƒ"ä»å­—é¢ä¸Š"æ¥å—äº†èµ„äº§é˜¶çº§çš„æ”¿æ²»çº²é¢†ï¼ˆå¹³ç­‰ã€è‡ªç”±ã€åšçˆ±[brotherhood]ï¼‰ï¼Œå¹¶åŠªåŠ›å®ç°å®ƒï¼Œè€Œä¸è€ƒè™‘åŒå…¬æ°‘ç¤¾ä¼šçš„å…·ä½“è¡”æ¥ã€‚

ä¸¤è€…éƒ½ä¸ºä»–ä»¬çš„å¹»æƒ³ä»˜å‡ºäº†æ²‰é‡çš„ä»£ä»·ï¼šä¸“åˆ¶å›ä¸»å¾ˆæ™šæ‰æ³¨æ„åˆ°ï¼Œç¤¾ä¼šç§°èµä»–æ˜¯ä¸‡èƒ½çš„ï¼Œåªæ˜¯ä¸ºäº†è®©ä¸€ä¸ªé˜¶çº§æ¨ç¿»å¦ä¸€ä¸ªé˜¶çº§ï¼›é›…å„å®¾æ´¾ä¸€æ—¦å®Œæˆäº†æ‘§æ¯æ—§åˆ¶åº¦çš„æœºå™¨çš„å·¥ä½œï¼Œä¹Ÿå°±å˜å¾—å¤šä½™äº†ã€‚ä¸¤è€…éƒ½è¢«å…³äºæ”¿æ²»é¢†åŸŸè‡ªä¸»æ€§ï¼ˆautonomyï¼‰çš„å¹»æƒ³æ‰€è¿·æƒ‘ï¼Œéƒ½ç›¸ä¿¡è‡ªå·±çš„æ”¿æ²»ä½¿å‘½ï¼šä¸€ä¸ªç›¸ä¿¡çš‡æƒçš„ä¸å¯è´¨ç–‘æ€§ï¼Œå¦ä¸€ä¸ªç›¸ä¿¡å…¶æ”¿æ²»æ–¹æ¡ˆçš„æ°å½“æ€§ï¼ˆpertinenceï¼‰ã€‚åœ¨å¦ä¸€ä¸ªå±‚é¢ä¸Šï¼Œæˆ‘ä»¬ä¸æ˜¯ä¹Ÿå¯ä»¥è¿™æ ·è¯´æ³•è¥¿æ–¯ä¸»ä¹‰å’Œå…±äº§ä¸»ä¹‰ï¼Œå³"å®é™…ç°å­˜çš„ç¤¾ä¼šä¸»ä¹‰ï¼ˆactually existing socialismï¼‰"å—ï¼Ÿæ³•è¥¿æ–¯ä¸»ä¹‰éš¾é“ä¸æ˜¯ä¸€ç§èµ„æœ¬ä¸»ä¹‰å›ºæœ‰çš„è‡ªæˆ‘å¦å®šï¼Œä¸æ˜¯è¯•å›¾é€šè¿‡ä¸€ç§ä½¿ç»æµä»å±äºæ„è¯†å½¢æ€-æ”¿æ²»ï¼ˆideological-politicalï¼‰é¢†åŸŸçš„æ„è¯†å½¢æ€æ¥"æ”¹å˜ä¸€äº›ä¸œè¥¿ï¼Œä»¥ä¾¿æ²¡æœ‰çœŸæ­£çš„æ”¹å˜"å—ï¼Ÿåˆ—å®ä¸»ä¹‰çš„"å®é™…å­˜åœ¨çš„ç¤¾ä¼šä¸»ä¹‰"éš¾é“ä¸æ˜¯ä¸€ç§"ç¤¾ä¼šä¸»ä¹‰çš„é›…å„å®¾ä¸»ä¹‰"ï¼Œä¸æ˜¯è¯•å›¾ä½¿æ•´ä¸ªç¤¾ä¼šç»æµç”Ÿæ´»ä»å±äºç¤¾ä¼šä¸»ä¹‰å›½å®¶çš„ç›´æ¥æ”¿æ²»è°ƒèŠ‚å—ï¼Ÿå®ƒä¸¤è€…éƒ½æ˜¯"æ¶ˆå¤±çš„ä¸­ä»‹è€…"ï¼Œä½†è¿›å…¥äº†ä»€ä¹ˆå‘¢ï¼Ÿé€šå¸¸çš„çŠ¬å„’å¼ç­”æ¡ˆ"ä»èµ„æœ¬ä¸»ä¹‰å›åˆ°èµ„æœ¬ä¸»ä¹‰"ä¼¼ä¹æœ‰ç‚¹å¤ªå®¹æ˜“äº†â€¦â€¦`
    },
    {
      id: "text-F-mediator-illusion",
      content: `ä¹Ÿå°±æ˜¯è¯´ï¼Œæ–°æ•™å’Œé›…å„å®¾ä¸»ä¹‰æ‰€é™·å…¥çš„å¹»è§‰ï¼Œæ¯”ä¹çœ‹ä¹‹ä¸‹è¦å¤æ‚å¾—å¤šï¼šå®ƒå¹¶ä¸ç®€å•åœ°åœ¨äºä»–ä»¬å¯¹åŸºç£æ•™æˆ–å¹³ç­‰ä¸»ä¹‰æ°‘ä¸»æ–¹æ¡ˆï¼ˆegalitarian-democratic projectï¼‰çš„é‚£æœ´ç´ é“å¾·ä¸»ä¹‰å¼çš„æ™®éåŒ–ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¹¶ä¸ç®€å•åœ°åœ¨äºä»–ä»¬å¿½ç•¥äº†æŠµåˆ¶è¿™ç§ç›´æ¥æ™®éåŒ–çš„ç¤¾ä¼šå…³ç³»çš„å…·ä½“è´¢å¯Œï¼ˆconcrete wealth of social relationsï¼‰ã€‚ä»–ä»¬çš„å¹»è§‰è¦æ¿€è¿›å¾—å¤šï¼šå®ƒåŒæ‰€æœ‰åœ¨å†å²ä¸Šç›¸å…³çš„æœ‰å…³æ”¿æ²»ä¹Œæ‰˜é‚¦çš„å¹»è§‰å…·æœ‰ç›¸åŒçš„æœ¬æ€§ã€‚é©¬å…‹æ€åœ¨è°ˆåˆ°æŸæ‹‰å›¾çš„å›½å®¶ï¼ˆStateï¼‰æ—¶æè¯·æˆ‘ä»¬æ³¨æ„è¿™ç§å¹»è§‰ï¼Œä»–è¯´ï¼ŒæŸæ‹‰å›¾æ²¡æœ‰çœ‹åˆ°ä»–äº‹å®ä¸Šæ‰€æè¿°çš„ä¸æ˜¯ä¸€ä¸ªå°šæœªå®ç°çš„ç†æƒ³ï¼ˆidealï¼‰ï¼Œè€Œæ˜¯ç°å­˜å¸Œè…Šå›½å®¶çš„åŸºæœ¬ç»“æ„ã€‚æ¢å¥è¯è¯´ï¼Œä¹Œæ‰˜é‚¦ï¼ˆutopiasï¼‰ä¹‹æ‰€ä»¥æ˜¯"ä¹Œæ‰˜é‚¦çš„"ï¼Œä¸æ˜¯å› ä¸ºå®ƒä»¬æç»˜äº†ä¸€ä¸ª"ä¸å¯èƒ½çš„ç†æƒ³ï¼ˆIdealï¼‰"ï¼Œä¸€ä¸ªä¸å±äºè¿™ä¸ªä¸–ç•Œçš„æ¢¦æƒ³ï¼Œè€Œæ˜¯å› ä¸ºå®ƒä»¬æ²¡æœ‰è®¤å‡ºå®ƒä»¬çš„ç†æƒ³å›½ï¼ˆideal stateï¼‰åœ¨å…¶åŸºæœ¬å†…å®¹æ–¹é¢ï¼ˆé»‘æ ¼å°”ä¼šè¯´ï¼Œ"åœ¨å…¶æ¦‚å¿µæ–¹é¢"ï¼‰å¦‚ä½•å·²ç„¶å®ç°äº†ã€‚

å½“ç¤¾ä¼šç°å®è¢«æ„é€ æˆä¸€ä¸ª"æ–°æ•™ä¸–ç•Œ"çš„æ—¶å€™ï¼Œæ–°æ•™å°±å˜å¾—å¤šä½™ï¼Œå¯ä»¥ä½œä¸ºä¸€ä¸ªä¸­ä»‹æ¶ˆå¤±äº†ï¼šèµ„æœ¬ä¸»ä¹‰å…¬æ°‘ç¤¾ä¼šçš„æ¦‚å¿µç»“æ„ï¼ˆnotional structureï¼‰æ˜¯ä¸€ä¸ªç”±"è´ªå¾—çš„ç¦æ¬²ä¸»ä¹‰"ï¼ˆ"ä½ æ‹¥æœ‰çš„è¶Šå¤šï¼Œä½ å°±è¶Šè¦æ”¾å¼ƒæ¶ˆè´¹"ï¼‰è¿™ä¸ªæ‚–è®ºæ‰€å®šä¹‰çš„åŸå­åŒ–ä¸ªäººçš„ä¸–ç•Œâ€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œç¼ºå°‘æ–°æ•™ä¹‹ç§¯æå®—æ•™å½¢å¼è€Œåªæœ‰æ–°æ•™ä¹‹å†…å®¹çš„ç»“æ„ã€‚é›…å„å®¾ä¸»ä¹‰ä¹Ÿæ˜¯å¦‚æ­¤ï¼šé›…å„å®¾æ´¾æ‰€å¿½è§†çš„äº‹å®æ˜¯ï¼Œä»–ä»¬åŠªåŠ›è¿½æ±‚çš„ç†æƒ³åœ¨å…¶æ¦‚å¿µç»“æ„ä¸­åœ¨"è‚®è„çš„"è´ªå¾—æ´»åŠ¨ï¼ˆacquisitive activityï¼‰ä¸­å·²ç„¶å®ç°ï¼Œè€Œè¿™ç§æ´»åŠ¨åœ¨ä»–ä»¬çœ‹æ¥æ˜¯å¯¹å…¶å´‡é«˜ç†æƒ³çš„èƒŒå›ã€‚åº¸ä¿—çš„ã€åˆ©å·±ä¸»ä¹‰çš„èµ„äº§é˜¶çº§æ—¥å¸¸ç”Ÿæ´»æ˜¯è‡ªç”±ã€å¹³ç­‰å’Œåšçˆ±çš„ç°å®æ€§ï¼ˆactualityï¼‰ï¼šè‡ªç”±è´¸æ˜“çš„è‡ªç”±ï¼Œæ³•å¾‹é¢å‰çš„å½¢å¼å¹³ç­‰ï¼Œç­‰ç­‰ã€‚`
    },
    {
      id: "text-G-beautiful-soul-analogy", 
      content: `"æ¶ˆå¤±çš„ä¸­ä»‹è€…"â€”â€”æ–°æ•™å¾’ã€é›…å„å®¾ä¸»ä¹‰â€”â€”æ‰€ç‰¹æœ‰çš„å¹»è§‰æ­£æ˜¯é»‘æ ¼å°”å¼çš„"ç¾ä¸½çµé­‚"çš„å¹»è§‰ï¼šä»–ä»¬æ‹’ç»åœ¨ä»–ä»¬æ‰€å“€å¹çš„è…è´¥ç°å®ä¸­æ‰¿è®¤ä»–ä»¬è‡ªå·±çš„è¡Œä¸ºçš„æœ€ç»ˆç»“æœâ€”â€”å¦‚æ‹‰åº·æ‰€è¯´ï¼Œä»–ä»¬è‡ªå·±çš„ä¿¡æ¯ä»¥å…¶çœŸå®è€Œé¢ å€’çš„å½¢å¼å‡ºç°ã€‚è€Œä½œä¸ºæ–°æ•™å’Œé›…å„å®¾ä¸»ä¹‰çš„"æ¸…é†’çš„" ç»§æ‰¿è€…ï¼Œæˆ‘ä»¬çš„å¹»è§‰ä¹Ÿä¸å°‘ï¼šæˆ‘ä»¬æŠŠé‚£äº›"æ¶ˆå¤±çš„ä¸­ä»‹è€…"è§†ä¸ºåå¸¸ï¼ˆaberrationsï¼‰æˆ–æº¢å‡º/è¿‡å‰©ï¼Œæ²¡èƒ½æ³¨æ„åˆ°æˆ‘ä»¬ä½•ä»¥åªæ˜¯"æ²¡æœ‰é›…å„å®¾å½¢å¼çš„é›…å„å®¾æ´¾"ä¸"æ²¡æœ‰æ–°æ•™å½¢å¼çš„æ–°æ•™å¾’"ã€‚`
    },
    {
      id: "text-H-mediator-event-subject",
      content: `**Â·ä½ æ‰‹æŒ‡çš„ä¸€æ•²â€¦â€¦**

"æ¶ˆå¤±çš„ä¸­ä»‹è€…"å®é™…ä¸Šä»…æ˜¾ç°ä¸ºä¸€ä¸ªä¸­ä»‹è€…ï¼Œä¸€ä¸ªä»‹äºä¸¤ç§"å¸¸æ€"äº‹ç‰©çŠ¶æ€ä¹‹é—´çš„ä¸­é—´å½¢è±¡ï¼ˆfigureï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§è§£è¯»æ˜¯å”¯ä¸€å¯èƒ½çš„è§£è¯»å—ï¼Ÿç”±"åé©¬å…‹æ€ä¸»ä¹‰"æ”¿æ²»ç†è®ºï¼ˆClaude Lefort, Ernesto Laclauï¼‰æ‰€é˜è¿°çš„æ¦‚å¿µè£…ç½®å…è®¸å¦ä¸€ç§è§£è¯»ï¼Œè€Œè¿™ç§è§£è¯»ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è§†è§’ã€‚åœ¨è¿™ä¸ªé¢†åŸŸä¸­ï¼Œ"æ¶ˆå¤±çš„ä¸­ä»‹è€…"è¿™ä¸€ç¯èŠ‚è¢«é˜¿å…°Â·å·´è¿ªæ¬§å®šä¹‰ä¸º"äº‹ä»¶"ï¼ˆå®ƒæœ‰å…³å·²ç¡®ç«‹çš„ç»“æ„ï¼‰çš„ç¯èŠ‚ï¼šå…¶çœŸç›¸åœ¨å…¶ä¸­å‡ºç°çš„ç¯èŠ‚ã€æœ‰å…³"å¼€æ”¾æ€§ï¼ˆopennessï¼‰"çš„ç¯èŠ‚â€”â€”ä¸€æ—¦"äº‹ä»¶"çš„çˆ†å‘è¢«åˆ¶åº¦åŒ–ä¸ºä¸€ç§æ–°çš„è‚¯å®šæ€§ï¼ˆpositivityï¼‰ï¼Œå®ƒå°±ä¼šæ¶ˆå¤±ï¼Œæˆ–è€…æ›´ç¡®åˆ‡åœ°è¯´ï¼Œåœ¨å­—é¢ä¸Šä¸å¯è§äº†ã€‚`
    },
    {
      id: "text-H1-subject-definition",
      content: `è¿™ä¸€æœ‰å…³å¼€æ”¾æ€§ï¼ˆopennessï¼‰çš„"ä¸å¯èƒ½çš„"ç¯èŠ‚æ„æˆäº†ä¸»ä½“æ€§çš„ç¯èŠ‚ï¼š"ä¸»ä½“"æ˜¯ä¸€ä¸ªåç§°ï¼ŒæŒ‡çš„æ˜¯é‚£ä¸ªè¢«å¬å”¤çš„ã€çªç„¶é—´è´Ÿæœ‰è´£ä»»çš„æ·±ä¸å¯æµ‹ï¼ˆunfathomableï¼‰çš„ Xï¼Œå®ƒåœ¨è¿™æ ·ä¸€ä¸ªæœ‰å…³ä¸ç¡®å®šæ€§ï¼ˆundecidabilityï¼‰çš„æ—¶åˆ»è¢«æŠ›å…¥ä¸€ä¸ªè´£ä»»çš„ä½ç½®ï¼Œè¢«æŠ›å…¥è¿™å…³äºå†³å®šï¼ˆdecisionï¼‰çš„ç´§æ€¥äº‹æ€ä¹‹ä¸­ã€‚è¿™å°±æ˜¯æˆ‘ä»¬è§£è¯»é»‘æ ¼å°”çš„è¿™ä¸€å‘½é¢˜â€”â€”"çœŸç†ï¼ˆTrueï¼‰ä¸ä»…è¦è¢«ç†è§£ä¸ºå®ä½“ï¼Œè€Œä¸”åŒæ ·è¦è¢«ç†è§£ä¸ºä¸»ä½“"â€”â€”ä¸å¾—ä¸é‡‡å–çš„æ–¹å¼ï¼šä¸ä»…è¦è¢«ç†è§£ä¸ºä¸€ä¸ªå—æŸç§éšè—çš„ç†æ€§å¿…ç„¶æ€§æ”¯é…çš„å®¢è§‚è¿‡ç¨‹ï¼ˆå³ä½¿è¿™ç§å¿…ç„¶æ€§å…·æœ‰é»‘æ ¼å°”å¼"ç†æ€§çš„ç‹¡è®¡"çš„ï¼‰ï¼Œè€Œä¸”è¦è¢«ç†è§£ä¸ºä¸€ä¸ªè¢«æœ‰å…³å¼€æ”¾æ€§ï¼ä¸ç¡®å®šæ€§ï¼ˆundecidabilityï¼‰çš„ç¯èŠ‚æ‰€æ‰“æ–­å¹¶å®¡è§†ï¼ˆscanï¼‰çš„è¿‡ç¨‹â€”â€”ä¸»ä½“çš„ä¸å¯è¿˜åŸçš„å¶ç„¶è¡Œä¸ºå»ºç«‹äº†ä¸€ä¸ªæ–°çš„å¿…ç„¶æ€§ã€‚`
    },
    {
      id: "text-H2-action-retroactive",
      content: `æ ¹æ®ä¸€ä¸ªè‘—åçš„æ„è§ï¼ˆdoxaï¼‰ï¼Œè¾©è¯æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿç©¿é€è¯¸å¶ç„¶æ€§çš„è¡¨é¢æˆå‰§ï¼Œè¾¾è‡³åœ¨ä¸»ä½“èƒŒå"æ“çºµç€è¡¨æ¼”"çš„æ ¹æœ¬çš„ï¼ˆunderlyingï¼‰ç†æ€§å¿…ç„¶æ€§ã€‚ä¸€ä¸ªæ°å½“çš„é»‘æ ¼å°”å¼çš„è¾©è¯è¿åŠ¨å‡ ä¹æ˜¯è¿™ä¸€ç¨‹åºçš„å®Œå…¨é¢ å€’ï¼šå®ƒé©±æ•£äº†å¯¹"å®¢è§‚å†å²è¿›ç¨‹"çš„è¿·ä¿¡å¹¶è®©æˆ‘ä»¬çœ‹åˆ°å®ƒçš„èµ·æºï¼šå†å²ä¸Šçš„å¿…ç„¶æ€§å‡ºç°çš„æ–¹å¼â€”â€”å®ƒæ˜¯ä¸€ç§å®è¯åŒ–ï¼ˆpositivizationï¼‰ã€ä¸»ä½“åœ¨ä¸€ä¸ªå¼€æ”¾çš„ã€ä¸ç¡®å®šçš„æƒ…åŠ¿ä¸‹çš„æ ¹æœ¬å¶ç„¶å†³å®šçš„ä¸€ä¸ª"å‡ç»“ï¼ˆcoagulationï¼‰"ã€‚æ ¹æ®å®šä¹‰ï¼Œ"è¾©è¯çš„å¿…ç„¶æ€§"æ€»æ˜¯äº‹åçš„ï¼ˆaprÃ¨s coupï¼‰å¿…ç„¶æ€§ï¼šä¸€ä¸ªé€‚å½“çš„è¾©è¯è§£è¯»è´¨ç–‘å¯¹"å®é™…ä¸Šå‘ç”Ÿçš„äº‹æƒ…"çš„è‡ªæˆ‘è¯æ˜ï¼Œå¹¶å°†å…¶ä¸æ²¡æœ‰å‘ç”Ÿçš„äº‹æƒ…å¯¹ç«‹èµ·æ¥â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒè®¤ä¸ºæ²¡æœ‰å‘ç”Ÿçš„äº‹æƒ…ï¼ˆä¸€ç³»åˆ—é”™è¿‡çš„æœºä¼šã€ä¸€ç³»åˆ—"æ›¿ä»£æ€§å†å²"ï¼‰æ˜¯"å®é™…ä¸Šå‘ç”Ÿçš„äº‹æƒ…"çš„æ„æˆéƒ¨åˆ†ã€‚

è¿™ä¸ªè¡ŒåŠ¨å› è€Œæ˜¯"è¿°è¡Œæ€§"çš„ï¼Œåœ¨è¶…å‡ºäº†ï¼ˆexceedsï¼‰"è¨€è¯­è¡Œä¸º"çš„æ„ä¹‰ä¸Šï¼šå…¶è¿°è¡Œæ€§æ˜¯"å›æº¯æ€§çš„"ï¼šå®ƒé‡æ–°å®šä¹‰äº†å…¶è¯¸é¢„è®¾çš„ç½‘ç»œã€‚è¡ŒåŠ¨çš„å›æº¯è¿°è¡Œæ€§è¿™ä¸€"æº¢å‡º/è¿‡å‰©"ä¹Ÿå¯ä»¥å€ŸåŠ©é»‘æ ¼å°”å…³äºæ³•å¾‹ä¸å…¶é€¾è¶Šï¼ˆtransgressionï¼‰ã€çŠ¯ç½ªçš„è¾©è¯æ³•å¾—åˆ°é˜é‡Š...`
    },
    {
      id: "text-H3-positing-presuppositions",
      content: `æ­£æ˜¯é¢å¯¹è¿™æ ·çš„èƒŒæ™¯ï¼Œæˆ‘ä»¬æ‰å¿…é¡»ç†è§£é»‘æ ¼å°”æœ‰å…³"è®¾å®šé¢„è®¾ï¼ˆpositing ofpresuppositionsï¼‰"çš„è®ºé¢˜ï¼šè¿™ç§å›æº¯æ€§çš„è®¾å®šæ°æ°æ˜¯å¿…ç„¶æ€§ä»å¶ç„¶æ€§ä¸­å‡ºç°çš„æ–¹å¼ã€‚ä¸»ä½“"è®¾å®šå…¶é¢„è®¾"çš„ç¯èŠ‚ï¼Œæ­£æ˜¯ä»–ä½œä¸ºä¸»ä½“è¢«æŠ¹å»çš„ç¯èŠ‚ï¼Œä»–ä½œä¸ºä¸­ä»‹è€…æ¶ˆå¤±çš„ç¯èŠ‚ï¼šå½“ä¸»ä½“çš„å†³å®šè¡Œä¸ºï¼ˆact of decisionï¼‰å˜æˆå®ƒçš„åé¢æ—¶çš„é‚£ä¸ªç»“æŸçš„ç¯èŠ‚ï¼›å»ºç«‹ä¸€ä¸ªæ–°çš„è±¡å¾ç½‘ç»œï¼Œè€Œå†å²å€ŸåŠ©è¿™ä¸€ç½‘ç»œå†æ¬¡è·å¾—äº†çº¿æ€§æ¼”è¿›çš„è‡ªæˆ‘è¯æ˜ã€‚è®©æˆ‘ä»¬å›åˆ°åæœˆé©å‘½ï¼šå…¶"é¢„è®¾"åœ¨å®ƒçš„èƒœåˆ©å’Œæ–°æ”¿æƒçš„å·©å›ºä¹‹åã€å½¢åŠ¿çš„å¼€æ”¾æ€§å†æ¬¡ä¸§å¤±ä¹‹æ—¶æ‰è¢«"è®¾å®š"â€”â€”ä»¥"å®¢è§‚è§‚å¯Ÿè€…"çš„èº«ä»½å™è¿°äº‹ä»¶çš„çº¿æ€§å‘å±•ï¼ˆç¡®å®šè‹ç»´åŸƒæ”¿æƒå¦‚ä½•åœ¨å…¶æœ€è–„å¼±çš„ç¯èŠ‚æ‰“ç ´å¸å›½ä¸»ä¹‰é“¾æ¡å¹¶ä»è€Œå¼€å¯ä¸–ç•Œå†å²çš„æ–°çºªå…ƒï¼Œç­‰ç­‰ï¼‰åœ¨è¿™ä¸ªæ—¶å€™æ‰åˆä¸€æ¬¡å¾—ä»¥å¯èƒ½ã€‚åœ¨æ­¤ä¸¥æ ¼çš„æ„ä¹‰ä¸Šï¼Œä¸»ä½“æ˜¯ä¸€ä¸ª"æ¶ˆå¤±çš„ä¸­ä»‹è€…"ï¼šå®ƒçš„è¡Œä¸ºé€šè¿‡å˜å¾—ä¸å¯è§è€ŒæˆåŠŸâ€”â€”é€šè¿‡åœ¨ä¸€ä¸ªæ–°çš„è±¡å¾ç½‘ç»œä¸­"å®è¯åŒ–ï¼ˆpositivizingï¼‰"è‡ªå·±ï¼Œå®ƒå°†è‡ªå·±å®šä½åœ¨æ­¤ç½‘ç»œä¸­å¹¶åœ¨å…¶ä¸­å°†è‡ªå·±è§£é‡Šä¸ºå†å²è¿›ç¨‹çš„ç»“æœï¼Œä»è€Œå°†è‡ªå·±é™ä¸ºå…¶è‡ªèº«è¡Œä¸ºæ‰€äº§ç”Ÿçš„æ•´ä½“ä¸­çš„ä¸€ä¸ªå•çº¯çš„ç¯èŠ‚ã€‚`
    },
    {
      id: "text-I-truth-political-intro",
      content: `**Â·ä¸ºä»€ä¹ˆçœŸç†æ€»æ˜¯æ”¿æ²»æ€§ï¼ˆpoliticalï¼‰çš„ï¼Ÿ**

è¡ŒåŠ¨çš„æ¦‚å¿µç›´æ¥ç›¸å…³äºç¤¾ä¼šå’Œæ”¿æ²»ï¼ˆSocial and Politicalï¼‰ä¹‹é—´çš„å…³ç³»â€”â€”ç›¸å…³äº"æ”¿æ²»æ€§ï¼ˆthe Politicalï¼‰"å’Œ"æ”¿æ²»ï¼ˆpoliticsï¼‰"ä¹‹é—´çš„åŒºåˆ«ï¼Œæ­£å¦‚ Lefortå’ŒLaclauæ‰€é˜è¿°çš„é‚£æ ·...`
    },
    {
      id: "text-I1-politics-vs-thepolitical", 
      content: `"æ”¿æ²»"æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç¤¾ä¼šç»¼åˆä½“ï¼ˆseparate socialcomplexï¼‰ã€ä¸€ä¸ªä¸å…¶ä»–å­ç³»ç»Ÿï¼ˆç»æµã€æ–‡åŒ–å½¢å¼ï¼‰ç›¸äº’ä½œç”¨çš„ã€è¢«è‚¯å®šè§„å®šçš„ï¼ˆpositively determinedï¼‰ç¤¾ä¼šå…³ç³»çš„å­ç³»ç»Ÿï¼Œè€Œ"æ”¿æ²»æ€§"[le Politique]åˆ™æ˜¯æœ‰å…³å¼€æ”¾æ€§çš„ã€ä¸ç¡®å®šçš„ç¯èŠ‚ï¼ˆæ­¤æ—¶ï¼Œç¤¾ä¼šçš„ç»“æ„æ€§åŸåˆ™ã€ç¤¾ä¼šå¥‘çº¦çš„åŸºæœ¬å½¢å¼è¢«è´¨ç–‘ï¼‰â€”â€”ç®€è€Œè¨€ä¹‹ï¼Œå°±æ˜¯é€šè¿‡å»ºç«‹"æ–°å’Œè°"çš„è¡ŒåŠ¨æ¥å…‹æœå…¨çƒå±æœºçš„ç¯èŠ‚ã€‚`
    },
    {
      id: "text-I2-thepolitical-explanation",
      content: `å› æ­¤ï¼Œ"æ”¿æ²»æ€§"çš„ç»´åº¦å¾—åˆ°äº†åŒé‡çš„åˆ»ç”»ï¼šå®ƒæ˜¯ç¤¾ä¼šæ•´ä½“çš„ä¸€ä¸ªç¯èŠ‚ï¼Œæ˜¯å…¶å­ç³»ç»Ÿä¸­çš„ä¸€ä¸ªï¼Œå¹¶ä¸”ä¹Ÿæ˜¯æ•´ä½“ä¹‹å‘½è¿åœ¨å…¶ä¸­è¢«å†³å®šâ€”â€”æ–°çš„å¥‘çº¦åœ¨å…¶ä¸­è¢«è®¾è®¡å¹¶ç¼”ç»“â€”â€”çš„åœ°å¸¦ã€‚`
    },
    {
      id: "text-I3-origin-of-order-political",
      content: `åœ¨ç¤¾ä¼šç†è®ºä¸­ï¼Œäººä»¬é€šå¸¸è®¤ä¸ºæ”¿æ²»ç»´åº¦ç›¸å¯¹äºç¤¾ä¼šï¼ˆthe Socialï¼‰æœ¬èº«è€Œè¨€æ˜¯æ¬¡è¦çš„ã€‚åœ¨å®è¯ä¸»ä¹‰ç¤¾ä¼šå­¦ä¸­ï¼Œæ”¿æ²»æ˜¯ç¤¾ä¼šç»„ç»‡ç”¨ä»¥ç»„ç»‡å…¶è‡ªæˆ‘è°ƒèŠ‚çš„ä¸€ä¸ªå­ç³»ç»Ÿï¼›åœ¨ç»å…¸é©¬å…‹æ€ä¸»ä¹‰ä¸­ï¼Œæ”¿æ²»æ˜¯ç¤¾ä¼šé˜¶çº§åˆ†åŒ–æ‰€å¯¼è‡´çš„å¼‚åŒ–æ™®éæ€§ï¼ˆalienatedUniversalityï¼‰çš„ç‹¬ç«‹é¢†åŸŸï¼ˆå…¶åŸºæœ¬å«ä¹‰æ˜¯ï¼Œæ— é˜¶çº§ç¤¾ä¼šå°†æ„å‘³ç€ä½œä¸ºä¸€ä¸ªç‹¬ç«‹é¢†åŸŸçš„æ”¿æ²»æ€§ï¼ˆthe Politicalï¼‰çš„ç»ˆç»“ï¼‰ï¼›ç”šè‡³åœ¨ä¸€äº›"æ–°ç¤¾ä¼šè¿åŠ¨"çš„æ„è¯†å½¢æ€ä¸­ï¼Œæ”¿æ²»æ€§ï¼ˆthe Politicalï¼‰è¢«åˆ’å®šä¸ºå›½å®¶æƒåŠ›çš„é¢†åŸŸï¼Œå…¬æ°‘ç¤¾ä¼šå¿…é¡»ç»„ç»‡å…¶è‡ªå«è°ƒèŠ‚æœºåˆ¶åå¯¹å®ƒã€‚é’ˆå¯¹è¿™äº›æ¦‚å¿µï¼Œäººä»¬å¯ä»¥å†’é™©æå‡ºè¿™æ ·çš„å‡è®¾ï¼šç¤¾ä¼šçš„èµ·æºæ€»æ˜¯"æ”¿æ²»æ€§çš„ï¼ˆpoliticalï¼‰"â€”â€”ä¸€ä¸ªç§¯æï¼ˆpositivelyï¼‰ç°å­˜çš„ç¤¾ä¼šä½“ç³»åªä¸è¿‡æ˜¯ä¸€ç§å½¢å¼ï¼Œåœ¨è¿™ç§å½¢å¼ä¸­ï¼Œä¸€ä¸ªå½»åº•å¶ç„¶ä¹‹å†³å®šçš„å¦å®šæ€§è·å¾—äº†ï¼ˆassumesï¼‰ç§¯æçš„ï¼ˆpositiveï¼‰ã€æœ‰è§„å®šçš„ï¼ˆdeterminateï¼‰å®å­˜ã€‚`
    },
    {
      id: "text-J-conclusion-subject-as-mediator",
      content: `ç°åœ¨æˆ‘ä»¬å¯ä»¥å›åˆ°è‡­åæ˜­è‘—çš„é»‘æ ¼å°”ä¸‰å…ƒç»„ï¼šä¸»ä½“æ˜¯è¿™ä¸ª"æ¶ˆå¤±çš„ä¸­ä»‹è€…"ã€è¿™ä¸ªç¬¬å››ç¯èŠ‚ï¼Œå¯ä»¥è¯´ï¼Œå®ƒé¢å¸ƒäº†è‡ªå·±çš„æ¶ˆå¤±ï¼›å®ƒçš„æ¶ˆå¤±æ­£æ˜¯è¡¡é‡å…¶"æˆåŠŸ"çš„æ ‡å‡†ä¹Ÿæ˜¯è‡ªæˆ‘å…³è”çš„å¦å®šæ€§çš„è™šç©ºï¼Œä¸€æ—¦æˆ‘ä»¬ä»å…¶ç»“æœ"å›å¤´"çœ‹è¿™ä¸ªè¿‡ç¨‹ï¼Œå®ƒå°±å˜å¾—ä¸å¯è§äº†ã€‚`
    },
    {
      id: "text-K-truth-contingency-trauma",
      content: `å¯¹é»‘æ ¼å°”ä¸‰å…ƒç»„ä¸­è¿™ä¸€æº¢å‡ºçš„ç¬¬å››ç¯èŠ‚çš„è€ƒå¯Ÿï¼Œä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ ¼é›·é©¬æ–¯çš„"ç¬¦å·å­¦çŸ©é˜µ"çš„èƒŒæ™¯ä¸‹è§£è¯»å®ƒï¼š

å¿…ç„¶æ€§ï¼ˆnecessityï¼‰å’Œä¸å¯èƒ½æ€§ï¼ˆimpossibilityï¼‰çš„å¯¹ç«‹æœ¬èº«æº¶è§£è¿›å…¥å¯èƒ½æ€§ï¼ˆpossibilityï¼‰çš„é¢†åŸŸï¼ˆå¯ä»¥è¯´ï¼Œå¯èƒ½æ€§æ˜¯å¯¹å¿…ç„¶æ€§çš„"å¦å®šä¹‹å¦å®š"ï¼‰â€”â€”éšä¹‹æ¶ˆå¤±çš„æ˜¯ç¬¬å››ä¸ªæœ¯è¯­ï¼Œå³ç»ä¸å¯èƒ½ç­‰åŒäºå¯èƒ½ï¼ˆPossibleï¼‰çš„å¶ç„¶ï¼ˆtheContingentï¼‰ã€‚åœ¨å¶ç„¶æ€§ï¼ˆcontingencyï¼‰ä¸­æ€»å­˜åœ¨æŸäº›"ä¸å®åœ¨ç•Œé­é‡"çš„ä¸œè¥¿ï¼ŒæŸäº›å‰æ‰€æœªé—»çš„å®ä½“çš„çŒ›ç„¶å‡ºç°ï¼Œå®ƒè¿æŠ—äº†äººä»¬å¯¹"å¯èƒ½"æ‰€æŒçš„æ—¢å®šåœºåŸŸçš„é™åº¦ï¼Œè€Œ"å¯èƒ½"å¯ä»¥è¯´æ˜¯ä¸€ç§"æ¸©å’Œçš„"ã€å¹³å’Œçš„å¶ç„¶æ€§ï¼Œä¸€ç§è¢«æ‹”æ‰äº†åˆºçš„å¶ç„¶æ€§ã€‚`
    },
    {
      id: "text-K1-analogy-greimas-lacan",
      content: `ä¾‹å¦‚ï¼Œåœ¨ç²¾ç¥åˆ†æä¸­ï¼ŒçœŸç†å±äºå¶ç„¶æ€§çš„ç§©åºï¼šæˆ‘ä»¬åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­è¿‡ç€æ— èŠçš„ç”Ÿæ´»ï¼Œæ·±é™·äºç»“æ„å®ƒçš„æ™®éçš„è°è¨€ï¼ˆuniversal Lieï¼‰ä¹‹ä¸­ï¼Œè€Œçªç„¶é—´ï¼Œä¸€äº›å®Œå…¨å¶ç„¶çš„é­é‡â€”â€”æœ‹å‹çš„ä¸€å¥é—²è¯ï¼Œæˆ‘ä»¬ç›®ç¹çš„ä¸€ä»¶äº‹æ•…â€”â€”å”¤èµ·äº†å…³äºè¢«å‹æŠ‘çš„æ—§åˆ›ä¼¤çš„è®°å¿†ï¼Œæ‰“ç ´äº†æˆ‘ä»¬çš„è‡ªæˆ‘æ¬ºéª—ã€‚ç²¾ç¥åˆ†æåœ¨è¿™é‡Œæ˜¯å½»åº•åæŸæ‹‰å›¾çš„ï¼šæ™®éæ€§æ˜¯æœ€å“è¶Šçš„è™šå‡æ€§ï¼ˆFalsity par excellenceï¼‰çš„é¢†åŸŸï¼Œè€ŒçœŸç†åˆ™æ˜¯ä½œä¸ºä¸€ç§ç‰¹æ®Šçš„å¶ç„¶é­é‡å‡ºç°çš„ï¼Œè¿™ç§é­é‡ä½¿å…¶"è¢«å‹æŠ‘"çš„ä¸œè¥¿å˜å¾—å¯è§ã€‚åœ¨"å¯èƒ½æ€§"ä¸­æ‰€å¤±å»çš„ç»´åº¦æ­£æ˜¯è¿™ç§æœ‰å…³çœŸç†ä¹‹å‡ºç°çš„åˆ›ä¼¤æ€§çš„ã€æ— ä¿è¯çš„ï¼ˆunwarrantedï¼‰ç‰¹æ€§ï¼šå½“ä¸€ä¸ªçœŸç†å˜å¾—"å¯èƒ½"æ—¶ï¼Œå®ƒå¤±å»äº†"äº‹ä»¶"çš„ç‰¹æ€§ï¼Œå®ƒå˜æˆäº†ä¸€ä¸ªå•çº¯çš„æœ‰å…³äº‹å®çš„ï¼ˆfactualï¼‰å‡†ç¡®æ€§ï¼Œä»è€Œæˆä¸ºç»Ÿæ²»æ€§çš„æ™®éè°è¨€çš„ç»„æˆéƒ¨åˆ†ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ‹‰åº·çš„ç²¾ç¥åˆ†æä¸ç½—è’‚é‚£ç§å¤šå…ƒå®ç”¨ä¸»ä¹‰çš„"è‡ªç”±ä¸»ä¹‰"æœ‰å¤šè¿œã€‚æ‹‰åº·çš„æœ€åä¸€è¯¾ä¸æ˜¯çœŸç†ï¼ˆtruthsï¼‰çš„ç›¸å¯¹æ€§å’Œå¤šå…ƒæ€§ï¼Œè€Œæ˜¯åšç¡¬çš„ã€åˆ›ä¼¤æ€§çš„äº‹å®ï¼Œå³åœ¨æ¯ä¸€ä¸ªå…·ä½“çš„æ˜Ÿä¸›ä¸­ï¼ŒçœŸç†ï¼ˆtruthï¼‰å¿…ç„¶ä¼šä»¥æŸç§å¶ç„¶çš„ç»†èŠ‚å‡ºç°ã€‚æ¢å¥è¯è¯´ï¼Œå°½ç®¡çœŸç†æ˜¯ä¾èµ–äºè¯­å¢ƒçš„â€”â€”å°½ç®¡ä¸€èˆ¬æ„ä¹‰ä¸Šçš„çœŸç†å¹¶ä¸å­˜åœ¨ï¼Œæœ‰çš„æ€»æ˜¯æŸç§æƒ…å†µçš„çœŸç†â€”â€”ä½†åœ¨æ¯ä¸€ä¸ªå¤šå…ƒåœºåŸŸä¸­éƒ½ä¾ç„¶æœ‰ä¸€ä¸ªé˜æ˜å…¶çœŸç†å¹¶ä¸”æœ¬èº«ä¸èƒ½è¢«ç›¸å¯¹åŒ–çš„ç‰¹æ®Šçš„ç‚¹ï¼›åœ¨è¿™ä¸ªç¡®åˆ‡çš„æ„ä¹‰ä¸Šï¼ŒçœŸç†æ€»æ˜¯ä¸€ã€‚å¦‚æœæˆ‘ä»¬æŠŠ"æœ¬ä½“è®º"çŸ©é˜µæ¢æˆ"ä¹‰åŠ¡è®º"çŸ©é˜µï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçš„ç›®æ ‡å°±ä¼šæ›´æ¸…æ¥šï¼š

æˆ‘ä»¬ç”šè‡³ç¼ºä¹ä¸€ä¸ªåˆé€‚çš„æœ¯è¯­æ¥å½¢å®¹è¿™ä¸ªXï¼Œæ¥å½¢å®¹è¿™"ä¸æ˜¯å‘½ä»¤çš„ï¼ˆnotprescribedï¼‰"ã€"å®¹è®¸çš„ï¼ˆfacultativeï¼‰"ï¼Œä½†åˆä¸æ˜¯ç®€å•çš„"å…è®¸çš„ï¼ˆpermittedï¼‰"ä¸œè¥¿çš„å¥‡æ€ªçŠ¶æ€â€”â€”ä¾‹å¦‚ï¼Œåœ¨ç²¾ç¥åˆ†æç–—æ³•ä¸­å‡ºç°äº†ä¸€äº›è¿„ä»Šä¸ºæ­¢è¢«ç¦æ­¢çš„çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯¹ç¦ä»¤è¿›è¡Œäº†å˜²å¼„ï¼Œæš´éœ²äº†å…¶éšè—æœºåˆ¶ï¼Œä½†å¹¶æ²¡æœ‰å› æ­¤è€Œå˜æˆä¸€ç§ä¸­æ€§çš„"å…è®¸ï¼ˆpermissivenessï¼‰"ã€‚ä¸¤è€…ä¹‹é—´çš„åŒºåˆ«æ¶‰åŠåˆ°å¯¹æ™®éç§©åºçš„ä¸åŒå…³ç³»ï¼š"å…è®¸ï¼ˆpermissivenessï¼‰"æ˜¯ç”±å®ƒä¿è¯çš„ï¼ˆwarrantedï¼‰ï¼Œè€Œè¿™ç§ä¿è¯åœ¨"ä½ å¯ä»¥ï¼ˆmayï¼‰â€¦â€¦"çš„æƒ…å†µä¸‹æ˜¯ç¼ºä¹çš„ï¼Œæ‹‰åº·ç§°è¿™ç§æƒ…å†µä¸ºscilicetï¼šä½ å¯ä»¥çŸ¥é“ï¼ˆå…³äºä½ çš„æ¬²æœ›çš„çœŸç›¸ï¼‰â€”â€”å¦‚æœä½ ä¸ºè‡ªå·±æ‰¿æ‹…é£é™©ã€‚è¿™ä¸ªscilicet ä¹Ÿè®¸æ˜¯æ‰¹åˆ¤æ€§æ€ç»´çš„æœ€ç»ˆè¿½ç´¢ã€‚`
    }
  ];

  return (
    <div className="prose prose-sm max-w-none">
      {demoContentBlocks.map((block) => (
        <div 
          key={block.id}
          id={block.id}
          className="content-block mb-6 p-4"
          ref={(el) => {
            console.log('ğŸ“ [ç¤ºä¾‹æ–‡æ¡£] æ³¨å†Œç¤ºä¾‹æ®µè½å¼•ç”¨:', block.id, !!el);
            onContentBlockRef(el, block.id);
          }}
        >
          <ReactMarkdown
            components={{
              h1: ({node, ...props}) => <h1 className="text-2xl font-bold mb-3 text-gray-900 dark:text-white border-b border-gray-200 dark:border-gray-700 pb-2" {...props} />,
              h2: ({node, ...props}) => <h2 className="text-xl font-semibold mb-2 text-gray-800 dark:text-gray-200 mt-4" {...props} />,
              h3: ({node, ...props}) => <h3 className="text-lg font-medium mb-2 text-gray-700 dark:text-gray-300 mt-3" {...props} />,
              h4: ({node, ...props}) => <h4 className="text-base font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
              h5: ({node, ...props}) => <h5 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
              h6: ({node, ...props}) => <h6 className="text-sm font-medium mb-2 text-gray-700 dark:text-gray-300 mt-2" {...props} />,
              p: ({node, ...props}) => <p className="mb-3 text-gray-600 dark:text-gray-300 leading-relaxed text-sm" {...props} />,
              ul: ({node, ...props}) => <ul className="list-disc list-inside mb-3 space-y-1" {...props} />,
              ol: ({node, ...props}) => <ol className="list-decimal list-inside mb-3 space-y-1" {...props} />,
              li: ({node, ...props}) => <li className="mb-1 text-gray-600 dark:text-gray-300 text-sm" {...props} />,
              blockquote: ({node, ...props}) => (
                <blockquote className="border-l-4 border-blue-500 dark:border-blue-400 pl-3 py-2 mb-3 bg-blue-50 dark:bg-blue-900/20 text-gray-700 dark:text-gray-300 italic text-sm" {...props} />
              ),
              code: ({node, inline, ...props}) => 
                inline 
                  ? <code className="bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-xs font-mono text-red-600 dark:text-red-400" {...props} />
                  : <code className="block bg-gray-900 dark:bg-gray-800 text-green-400 dark:text-green-300 p-3 rounded-lg overflow-x-auto text-xs font-mono" {...props} />,
              pre: ({node, ...props}) => <pre className="mb-3 overflow-x-auto" {...props} />,
            }}
          >
            {block.content}
          </ReactMarkdown>
        </div>
      ))}
    </div>
  );
};

export { StructuredMarkdownRenderer, DemoModeRenderer };
</file>

<file path="frontend/src/App.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

/* è‡ªå®šä¹‰æ ·å¼ */
.App {
  text-align: left;
}

/* Markdownå†…å®¹æ ·å¼ä¼˜åŒ– */
.prose {
  max-width: none !important;
  font-size: 16px !important;
}

.prose h1 {
  @apply text-4xl font-bold text-gray-900 dark:text-gray-100 mb-4 pb-2 border-b border-gray-200 dark:border-gray-700;
}

.prose h2 {
  @apply text-3xl font-semibold text-gray-800 dark:text-gray-200 mb-3 mt-8;
}

.prose h3 {
  @apply text-2xl font-medium text-gray-700 dark:text-gray-300 mb-2 mt-6;
}

.prose p {
  @apply text-gray-600 dark:text-gray-300 mb-4 leading-relaxed;
  font-size: 16px !important;
  line-height: 1.7 !important;
}

.prose ul {
  @apply list-disc list-inside mb-4 space-y-1 text-gray-600 dark:text-gray-300;
  font-size: 16px !important;
}

.prose ol {
  @apply list-decimal list-inside mb-4 space-y-1 text-gray-600 dark:text-gray-300;
  font-size: 16px !important;
}

.prose blockquote {
  @apply border-l-4 border-blue-500 dark:border-blue-400 pl-4 italic text-gray-700 dark:text-gray-300 mb-4 bg-blue-50 dark:bg-blue-900/20 py-2;
  font-size: 16px !important;
}

.prose code {
  @apply bg-gray-100 dark:bg-gray-700 px-1 py-0.5 rounded text-sm font-mono text-gray-800 dark:text-gray-200;
}

.prose pre {
  @apply bg-gray-100 dark:bg-gray-800 p-4 rounded-lg overflow-x-auto mb-4;
}

.prose pre code {
  @apply bg-transparent px-0 py-0;
}

/* Mermaidå›¾è¡¨æ ·å¼ */
.mermaid {
  @apply w-full h-auto;
}

/* æš—æ¨¡å¼ä¸‹çš„Mermaidå›¾è¡¨æ ·å¼ */
:root.dark .mermaid {
  filter: brightness(0.8) contrast(1.2);
}

/* MermaidèŠ‚ç‚¹é«˜äº®æ ·å¼ - åªé«˜äº®è¾¹æ¡†ï¼Œä¸æ”¹å˜å¡«å……è‰² */
.mermaid svg g.mermaid-highlighted-node > rect,
.mermaid svg g.mermaid-highlighted-node > circle,
.mermaid svg g.mermaid-highlighted-node > ellipse,
.mermaid svg g.mermaid-highlighted-node > polygon,
.mermaid svg .mermaid-highlighted-node rect,
.mermaid svg .mermaid-highlighted-node circle,
.mermaid svg .mermaid-highlighted-node ellipse,
.mermaid svg .mermaid-highlighted-node polygon {
  stroke: #FB923C !important;
  stroke-width: 3px !important;
  filter: drop-shadow(0 0 8px rgba(251, 146, 60, 0.6)) drop-shadow(0 0 16px rgba(251, 146, 60, 0.3)) !important;
  /* ä¸æ”¹å˜å¡«å……è‰²ï¼Œä¿æŒåŸæœ‰é¢œè‰² */
}

/* æš—æ¨¡å¼ä¸‹çš„Mermaidé«˜äº®æ ·å¼ */
:root.dark .mermaid svg g.mermaid-highlighted-node > rect,
:root.dark .mermaid svg g.mermaid-highlighted-node > circle,
:root.dark .mermaid svg g.mermaid-highlighted-node > ellipse,
:root.dark .mermaid svg g.mermaid-highlighted-node > polygon,
:root.dark .mermaid svg .mermaid-highlighted-node rect,
:root.dark .mermaid svg .mermaid-highlighted-node circle,
:root.dark .mermaid svg .mermaid-highlighted-node ellipse,
:root.dark .mermaid svg .mermaid-highlighted-node polygon {
  stroke: #FED7AA !important;
  stroke-width: 3px !important;
  filter: drop-shadow(0 0 8px rgba(253, 215, 170, 0.8)) drop-shadow(0 0 16px rgba(253, 215, 170, 0.4)) !important;
}

/* ç¡®ä¿è¿æ¥çº¿ä¸å—é«˜äº®å½±å“ */
.mermaid svg g.mermaid-highlighted-node > path,
.mermaid svg g.mermaid-highlighted-node > line,
.mermaid svg .mermaid-highlighted-node path,
.mermaid svg .mermaid-highlighted-node line {
  stroke: #9ca3af !important;
  stroke-width: 1.5px !important;
  fill: none !important;
  filter: none !important;
}

/* æš—æ¨¡å¼ä¸‹çš„è¿æ¥çº¿ */
:root.dark .mermaid svg g.mermaid-highlighted-node > path,
:root.dark .mermaid svg g.mermaid-highlighted-node > line,
:root.dark .mermaid svg .mermaid-highlighted-node path,
:root.dark .mermaid svg .mermaid-highlighted-node line {
  stroke: #6b7280 !important;
}

/* React FlowèŠ‚ç‚¹é«˜äº®æ ·å¼ */
.react-flow__node.highlighted-node,
.react-flow__node.mermaid-highlighted-node {
  border: 3px solid #FB923C !important;
  box-shadow: 0 0 15px rgba(251, 146, 60, 0.6), 0 0 30px rgba(251, 146, 60, 0.3) !important;
  z-index: 1000 !important;
  background-color: rgba(251, 146, 60, 0.05) !important;
  transition: all 0.2s ease-in-out !important;
}

/* æš—æ¨¡å¼ä¸‹çš„React FlowèŠ‚ç‚¹é«˜äº® */
:root.dark .react-flow__node.highlighted-node,
:root.dark .react-flow__node.mermaid-highlighted-node {
  border: 3px solid #FED7AA !important;
  box-shadow: 0 0 15px rgba(253, 215, 170, 0.8), 0 0 30px rgba(253, 215, 170, 0.4) !important;
  background-color: rgba(253, 215, 170, 0.08) !important;
}

/* æ´»è·ƒæ–‡æœ¬å—æ ·å¼ - é™æ­¢çŠ¶æ€ï¼Œæ— åŠ¨ç”» */
.content-block {
  border-left: 4px solid transparent;
}

.content-block.active {
  border-left-color: #FB923C !important;
  /* ä¸è®¾ç½®ä»»ä½•å…¶ä»–æ ·å¼ï¼Œç¡®ä¿åªæœ‰è¾¹æ¡†é¢œè‰²æ”¹å˜ */
}

/* æš—æ¨¡å¼ä¸‹çš„æ´»è·ƒæ–‡æœ¬å— */
:root.dark .content-block.active {
  border-left-color: #FED7AA !important;
}

/* ç¡®ä¿æ–‡æœ¬å†…å®¹ä¸å—MermaidèŠ‚ç‚¹æ ·å¼å½±å“ */
.content-block,
.content-block *,
.prose,
.prose * {
  filter: none !important;
}

/* å¼ºåˆ¶ç¡®ä¿æ´»è·ƒå†…å®¹å—å†…çš„æ‰€æœ‰å…ƒç´ éƒ½ä¸å—filterå½±å“ */
.content-block.active,
.content-block.active *,
.content-block.active p,
.content-block.active h1,
.content-block.active h2,
.content-block.active h3,
.content-block.active h4,
.content-block.active h5,
.content-block.active h6,
.content-block.active span,
.content-block.active div {
  color: inherit !important;
  background-color: transparent !important;
  background: none !important;
  filter: none !important;
}

/* é˜²æ­¢é€‰æ‹©çŠ¶æ€å½±å“æ˜¾ç¤º */
.content-block.active::selection,
.content-block.active *::selection {
  background-color: rgba(59, 130, 246, 0.2) !important;
  color: inherit !important;
}

/* ç« èŠ‚é«˜äº®æ ·å¼ - é™æ­¢çŠ¶æ€ï¼Œæ— åŠ¨ç”» */
.section-highlighted {
  border-left: 4px solid #FB923C;
  background-color: rgba(251, 146, 60, 0.05) !important;
  transform: translateX(2px);
}

/* æš—æ¨¡å¼ä¸‹çš„ç« èŠ‚é«˜äº® */
:root.dark .section-highlighted {
  border-left-color: #FED7AA;
  background-color: rgba(253, 215, 170, 0.1) !important;
}

/* æ™®é€šæ¨¡å¼ä¸‹çš„chunk-sectioné«˜äº®æ•ˆæœ */
.chunk-section.section-highlighted {
  border-left-color: #FB923C !important;
  background-color: rgba(251, 146, 60, 0.05) !important;
  transform: translateX(2px);
  box-shadow: 0 2px 8px rgba(251, 146, 60, 0.1);
}

:root.dark .chunk-section.section-highlighted {
  border-left-color: #FED7AA !important;
  background-color: rgba(253, 215, 170, 0.1) !important;
  box-shadow: 0 2px 8px rgba(253, 215, 170, 0.15);
}

/* chunk-sectionçš„åŸºç¡€æ ·å¼ */
.chunk-section {
  padding: 12px 16px;
  border-radius: 6px;
  margin-bottom: 16px;
}

.chunk-section:hover {
  background-color: rgba(249, 250, 251, 0.5);
}

:root.dark .chunk-section:hover {
  background-color: rgba(31, 41, 55, 0.5);
}

/* æ»šåŠ¨æ¡æ ·å¼ */
::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}

::-webkit-scrollbar-track {
  @apply bg-gray-100 dark:bg-gray-800;
}

::-webkit-scrollbar-thumb {
  @apply bg-gray-300 dark:bg-gray-600 rounded;
}

::-webkit-scrollbar-thumb:hover {
  @apply bg-gray-400 dark:bg-gray-500;
}

/* æ‹–æ‹½ä¸Šä¼ åŒºåŸŸæ ·å¼ */
.upload-area {
  transition: all 0.2s ease-in-out;
}

.upload-area:hover {
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
}

/* æš—æ¨¡å¼ä¸‹çš„æ‹–æ‹½ä¸Šä¼ åŒºåŸŸ */
:root.dark .upload-area:hover {
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
}

/* å“åº”å¼è°ƒæ•´ */
@media (max-width: 768px) {
  .prose {
    font-size: 15px !important;
  }
  
  .prose h1 {
    @apply text-3xl;
  }
  
  .prose h2 {
    @apply text-2xl;
  }
  
  .prose h3 {
    @apply text-xl;
  }
  
  .prose p {
    font-size: 15px !important;
  }
}

/* åŠ è½½åŠ¨ç”» */
@keyframes pulse {
  0%, 100% {
    opacity: 1;
  }
  50% {
    opacity: 0.5;
  }
}

.pulse {
  animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite;
}

/* æ¸å…¥åŠ¨ç”» */
@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.fade-in {
  animation: fadeIn 0.3s ease-out;
}

/* å·¥å…·æç¤ºæ ·å¼ */
.tooltip {
  @apply invisible absolute z-10 py-2 px-3 text-sm font-medium text-white bg-gray-900 dark:bg-gray-700 rounded-lg shadow-sm opacity-0 transition-opacity duration-300;
}

.tooltip-trigger:hover .tooltip {
  @apply visible opacity-100;
}

/* æš—æ¨¡å¼ä¸‹çš„Toasté€šçŸ¥æ ·å¼ */
:root {
  --toast-bg: #363636;
  --toast-color: #fff;
}

:root.dark {
  --toast-bg: #374151;
  --toast-color: #f9fafb;
}

/* ç§»é™¤ä¸éœ€è¦çš„æ ·å¼ */
/* .content-block-highlighted å’Œå…¶ä»–èƒŒæ™¯é«˜äº®æ ·å¼å·²åˆ é™¤ */

/* 
é«˜äº®æ ·å¼è¯´æ˜ï¼š
- äº®æ¨¡å¼ï¼šä½¿ç”¨#FB923C (æ©™è‰²-400)ä½œä¸ºé«˜äº®è‰²
- æš—æ¨¡å¼ï¼šä½¿ç”¨#FED7AA (æ©™è‰²-200)ä½œä¸ºé«˜äº®è‰²ï¼Œæ›´é€‚åˆæš—è‰²èƒŒæ™¯
- MermaidèŠ‚ç‚¹ï¼šä»…é«˜äº®è¾¹æ¡†å’Œæ·»åŠ å‘å…‰æ•ˆæœï¼Œä¸æ”¹å˜å¡«å……è‰²
- æ–‡æœ¬å—ï¼šä»…å·¦ä¾§è¾¹æ¡†é«˜äº®ï¼Œä¸æ”¹å˜èƒŒæ™¯å’Œæ–‡å­—é¢œè‰²
*/

/* è¯­ä¹‰å—é«˜äº®æ ·å¼ - åŸºäºAIç”Ÿæˆçš„è¯­ä¹‰åˆ†æ - åªæœ‰å·¦è¾¹æ¡†ï¼Œä¸æ”¹å˜æ ¼å¼ */
.semantic-block-highlighted {
  border-left: 4px solid #FB923C !important;
}

/* æš—æ¨¡å¼ä¸‹çš„è¯­ä¹‰å—é«˜äº® */
:root.dark .semantic-block-highlighted {
  border-left-color: #FED7AA !important;
}

/* æ®µè½çº§è¯­ä¹‰é«˜äº®æ ·å¼ - å›ºå®šè¾¹æ¡†ï¼Œä¸ç§»åŠ¨æ–‡æœ¬ */
.semantic-paragraph-highlighted {
  border-left: 4px solid #FB923C !important;
  /* ç¡®ä¿åŸå§‹å¸ƒå±€ä¸å˜ï¼Œé¿å…æ–‡æœ¬ç§»åŠ¨ */
  margin-left: 0 !important;
  padding-left: 16px; /* å¢åŠ å†…è¾¹è·è€Œä¸æ˜¯å¤–è¾¹è· */
  box-sizing: border-box;
}

/* æš—æ¨¡å¼ä¸‹çš„æ®µè½çº§é«˜äº® */
:root.dark .semantic-paragraph-highlighted {
  border-left-color: #FED7AA !important;
}

/* æ®µè½å—çš„åŸºç¡€æ ·å¼ - ç§»é™¤hoveræ•ˆæœ */
.paragraph-block, .content-block {
  /* ç§»é™¤æ‰€æœ‰hoveræ•ˆæœï¼Œç¡®ä¿æ²¡æœ‰äº¤äº’å¼æ ·å¼ */
  border-left: 4px solid transparent; /* é¢„ç•™è¾¹æ¡†ç©ºé—´ï¼Œé¿å…é«˜äº®æ—¶å¸ƒå±€å˜åŒ– */
  padding-left: 16px;
  box-sizing: border-box;
  transition: border-left-color 0.2s ease; /* åªè¿‡æ¸¡è¾¹æ¡†é¢œè‰² */
}

/* ç¡®ä¿è¯­ä¹‰é«˜äº®æ ·å¼ä¼˜å…ˆçº§æ›´é«˜ */
.semantic-block-highlighted,
.semantic-paragraph-highlighted {
  position: relative;
  z-index: 10;
}

/* è¯­ä¹‰å—ç»„åˆé«˜äº® - å½“å¤šä¸ªæ®µè½å±äºåŒä¸€è¯­ä¹‰å—æ—¶ */
.semantic-group-highlighted {
  background: linear-gradient(to right, 
    rgba(59, 130, 246, 0.05), 
    rgba(59, 130, 246, 0.1), 
    rgba(59, 130, 246, 0.05)
  ) !important;
  border-left: 4px solid #3b82f6 !important;
  border-radius: 8px;
  padding: 12px 16px;
  margin: 8px 0;
  box-shadow: 0 4px 12px rgba(59, 130, 246, 0.1);
}

:root.dark .semantic-group-highlighted {
  background: linear-gradient(to right, 
    rgba(59, 130, 246, 0.08), 
    rgba(59, 130, 246, 0.15), 
    rgba(59, 130, 246, 0.08)
  ) !important;
  border-left-color: #60a5fa !important;
  box-shadow: 0 4px 12px rgba(96, 165, 250, 0.15);
}
</file>

<file path="frontend/src/components/FlowDiagram.js">
import React, { useEffect, useState, useCallback, useImperativeHandle, forwardRef, useMemo, useRef } from 'react';
import ReactFlow, {
  ReactFlowProvider,
  Background,
  Controls,
  MiniMap,
  useNodesState,
  useEdgesState,
  addEdge,
  ConnectionLineType,
  Panel,
  useReactFlow,
} from 'reactflow';
import 'reactflow/dist/style.css';

import { convertDataToReactFlow } from '../utils/dataConverter';
import { getLayoutedElements } from '../utils/layoutHelper';
import { updateNodeLabel, handleApiError } from '../utils/api';
import EditableNode from './EditableNode';

// æ³¨å†Œè‡ªå®šä¹‰èŠ‚ç‚¹ç±»å‹
const nodeTypes = {
  editableNode: EditableNode,
};

/**
 * React Flowå›¾è¡¨ç»„ä»¶ï¼Œå…¼å®¹MermaidDiagramæ¥å£
 * @param {Object} props - ç»„ä»¶å±æ€§
 * @param {string} props.code - Mermaidä»£ç å­—ç¬¦ä¸² (å‘åå…¼å®¹)
 * @param {Object} props.apiData - åŒ…å«mermaid_stringå’Œnode_mappingsçš„æ•°æ®
 * @param {string} props.highlightedNodeId - éœ€è¦é«˜äº®çš„èŠ‚ç‚¹ID
 * @param {Function} props.onNodeClick - èŠ‚ç‚¹ç‚¹å‡»å›è°ƒå‡½æ•°
 * @param {Function} props.onNodeLabelUpdate - èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°å›è°ƒå‡½æ•°
 * @param {Function} props.onAddChildNode - æ·»åŠ å­èŠ‚ç‚¹å›è°ƒå‡½æ•°
 * @param {Function} props.onAddSiblingNode - æ·»åŠ åŒçº§èŠ‚ç‚¹å›è°ƒå‡½æ•°
 * @param {Function} props.onDeleteNode - åˆ é™¤èŠ‚ç‚¹å›è°ƒå‡½æ•°
 * @param {Object} props.layoutOptions - å¸ƒå±€é€‰é¡¹
 * @param {string} props.className - CSSç±»å
 */
const FlowDiagramInner = ({ 
  code, 
  apiData,
  highlightedNodeId,
  onNodeClick, 
  onNodeLabelUpdate,
  onAddChildNode,
  onAddSiblingNode,
  onDeleteNode,
  layoutOptions = {}, 
  className = '',
  onReactFlowInstanceChange
}) => {
  const [nodes, setNodes, onNodesChange] = useNodesState([]);
  const [edges, setEdges, onEdgesChange] = useEdgesState([]);
  const [isLoading, setIsLoading] = useState(false);
  const [documentId, setDocumentId] = useState(null);

  // ä½¿ç”¨useRefæ¥ç¨³å®šåŒ–handleLabelChangeå‡½æ•°ï¼Œé¿å…ä¸å¿…è¦çš„é‡æ–°æ¸²æŸ“
  const handleLabelChangeRef = useRef(null);

  // æ ‡ç­¾æ›´æ–°çš„å›è°ƒå‡½æ•° - ä½¿ç”¨useCallbackä½†ä¸åŒ…å«åœ¨useEffectä¾èµ–ä¸­
  const handleLabelChange = useCallback(async (nodeId, newLabel) => {
    try {
      console.log('ğŸ”„ [FlowDiagram] æ›´æ–°èŠ‚ç‚¹æ ‡ç­¾:', nodeId, '->', newLabel);
      
      // æ›´æ–°æœ¬åœ°çŠ¶æ€
      setNodes((currentNodes) => 
        currentNodes.map(node => 
          node.id === nodeId 
            ? { ...node, data: { ...node.data, label: newLabel } }
            : node
        )
      );

      // è°ƒç”¨çˆ¶ç»„ä»¶çš„èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°å›è°ƒï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
      if (onNodeLabelUpdate) {
        console.log('ğŸ”„ [FlowDiagram] è°ƒç”¨çˆ¶ç»„ä»¶èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°å›è°ƒ');
        onNodeLabelUpdate(nodeId, newLabel);
      }

      // è°ƒç”¨åç«¯APIæŒä¹…åŒ–æ›´æ”¹
      if (documentId) {
        try {
          await updateNodeLabel(documentId, nodeId, newLabel);
          console.log('ğŸ“ [FlowDiagram] èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°æˆåŠŸ');
        } catch (apiError) {
          console.error('âŒ [FlowDiagram] APIè°ƒç”¨å¤±è´¥:', apiError);
          // å¯ä»¥é€‰æ‹©æ˜¾ç¤ºç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
          // alert(handleApiError(apiError));
        }
      }
    } catch (error) {
      console.error('âŒ [FlowDiagram] æ›´æ–°èŠ‚ç‚¹æ ‡ç­¾å¤±è´¥:', error);
      // å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é”™è¯¯æç¤º
    }
  }, [documentId, setNodes, onNodeLabelUpdate]); // ğŸ”‘ æ·»åŠ onNodeLabelUpdateåˆ°ä¾èµ–ä¸­

  // å°†handleLabelChangeå­˜å‚¨åˆ°refä¸­ï¼Œä¿æŒå¼•ç”¨ç¨³å®š
  useEffect(() => {
    handleLabelChangeRef.current = handleLabelChange;
  }, [handleLabelChange]);

  // å¤„ç†æ•°æ®å˜åŒ– - ç§»é™¤handleLabelChangeä¾èµ–ï¼Œä½¿ç”¨refæ¥é¿å…é‡æ–°æ¸²æŸ“
  useEffect(() => {
    // ä¼˜å…ˆä½¿ç”¨apiDataï¼Œå¦åˆ™ä½¿ç”¨codeè¿›è¡Œå‘åå…¼å®¹
    const dataToProcess = apiData || (code ? {
      mermaid_string: code,
      node_mappings: extractNodeMappingsFromMermaid(code)
    } : null);

    if (!dataToProcess) {
      setNodes([]);
      setEdges([]);
      return;
    }

    // ä» apiData ä¸­æå– document_idï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    if (apiData && apiData.document_id) {
      setDocumentId(apiData.document_id);
    }

    setIsLoading(true);

    try {
      // è½¬æ¢APIæ•°æ®ä¸ºReact Flowæ ¼å¼
      const { nodes: convertedNodes, edges: convertedEdges } = convertDataToReactFlow(dataToProcess);
      
      console.log('ğŸ”„ [FlowDiagram] æ•°æ®è½¬æ¢ - èŠ‚ç‚¹:', convertedNodes.length, 'è¾¹:', convertedEdges.length);
      
      if (convertedNodes.length === 0) {
        console.log('ğŸ”„ [FlowDiagram] æ²¡æœ‰è½¬æ¢å‡ºèŠ‚ç‚¹ï¼Œè®¾ç½®ç©ºæ•°ç»„');
        setNodes([]);
        setEdges([]);
        setIsLoading(false);
        return;
      }

      // ä¸ºèŠ‚ç‚¹æ·»åŠ  onLabelChange å›è°ƒå¹¶è®¾ç½®ä¸ºå¯ç¼–è¾‘ç±»å‹
      // ä½¿ç”¨refä¸­çš„å‡½æ•°é¿å…é‡æ–°åˆ›å»º
      const nodesWithCallback = convertedNodes.map(node => ({
        ...node,
        type: 'editableNode', // è®¾ç½®ä¸ºå¯ç¼–è¾‘èŠ‚ç‚¹ç±»å‹
        data: {
          ...node.data,
          onLabelChange: (...args) => handleLabelChangeRef.current?.(...args), // ä½¿ç”¨refä¸­çš„å‡½æ•°
          onAddChildNode: onAddChildNode,
          onAddSiblingNode: onAddSiblingNode,
          onDeleteNode: onDeleteNode
        }
      }));

      // åº”ç”¨è‡ªåŠ¨å¸ƒå±€
      const layoutOptionsToUse = {
        direction: layoutOptions.direction || 'TB',
        nodeSpacing: layoutOptions.nodeSpacing || 100,
        rankSpacing: layoutOptions.rankSpacing || 150,
        nodeWidth: layoutOptions.nodeWidth || 200,
        nodeHeight: layoutOptions.nodeHeight || 80,
        ...layoutOptions
      };
      
      const { nodes: layoutedNodes, edges: layoutedEdges } = getLayoutedElements(
        nodesWithCallback, 
        convertedEdges,
        layoutOptionsToUse
      );

      console.log('ğŸ”„ [FlowDiagram] å¸ƒå±€å®Œæˆ - èŠ‚ç‚¹æ•°é‡:', layoutedNodes.length);
      console.log('ğŸ”„ [å…³é”®] è®¾ç½®åˆ°çŠ¶æ€çš„èŠ‚ç‚¹ä½ç½®:', layoutedNodes.map(n => ({ id: n.id, position: n.position })));

      setNodes(layoutedNodes);
      setEdges(layoutedEdges);

    } catch (error) {
      console.error('å¤„ç†å›¾è¡¨æ•°æ®æ—¶å‡ºé”™:', error);
      setNodes([]);
      setEdges([]);
    } finally {
      setIsLoading(false);
    }
  }, [code, apiData, layoutOptions]); // ç§»é™¤handleLabelChangeä¾èµ–

  // éç ´åæ€§é«˜äº®å®ç° - ç›´æ¥æ“ä½œDOMè€Œä¸ä¿®æ”¹èŠ‚ç‚¹å¯¹è±¡
  const applyNodeHighlighting = useCallback((nodeIdToHighlight) => {
    console.log('ğŸ¯ [éç ´åæ€§é«˜äº®] å¼€å§‹åº”ç”¨èŠ‚ç‚¹é«˜äº®:', nodeIdToHighlight);
    
    // ğŸ”‘ ä¼˜åŒ–ï¼šä½¿ç”¨æ›´ç¨³å®šçš„æŸ¥æ‰¾æ–¹å¼ï¼Œé¿å…åœ¨æ‹–æ‹½æ—¶å¤±æ•ˆ
    const findNodeElement = (nodeId) => {
      // ç­–ç•¥1ï¼šç›´æ¥é€šè¿‡data-idå±æ€§æŸ¥æ‰¾
      let nodeElement = document.querySelector(`[data-id="${nodeId}"]`);
      if (nodeElement) {
        console.log('ğŸ¯ [èŠ‚ç‚¹æŸ¥æ‰¾] ç­–ç•¥1æˆåŠŸ - data-id:', nodeId);
        return nodeElement;
      }
      
      // ç­–ç•¥2ï¼šæŸ¥æ‰¾React FlowèŠ‚ç‚¹å®¹å™¨
      nodeElement = document.querySelector(`.react-flow__node[data-id="${nodeId}"]`);
      if (nodeElement) {
        console.log('ğŸ¯ [èŠ‚ç‚¹æŸ¥æ‰¾] ç­–ç•¥2æˆåŠŸ - react-flow__node:', nodeId);
        return nodeElement;
      }
      
      // ç­–ç•¥3ï¼šéå†æ‰€æœ‰React FlowèŠ‚ç‚¹
      const allNodes = document.querySelectorAll('.react-flow__node');
      for (const node of allNodes) {
        const dataId = node.getAttribute('data-id');
        if (dataId === nodeId) {
          console.log('ğŸ¯ [èŠ‚ç‚¹æŸ¥æ‰¾] ç­–ç•¥3æˆåŠŸ - éå†åŒ¹é…:', nodeId);
          return node;
        }
        
        // æ£€æŸ¥å­å…ƒç´ 
        const childMatch = node.querySelector(`[data-id="${nodeId}"]`);
        if (childMatch) {
          console.log('ğŸ¯ [èŠ‚ç‚¹æŸ¥æ‰¾] ç­–ç•¥3æˆåŠŸ - å­å…ƒç´ åŒ¹é…:', nodeId);
          return node;
        }
      }
      
      console.warn('ğŸ¯ [èŠ‚ç‚¹æŸ¥æ‰¾] æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥äº†:', nodeId);
      return null;
    };
    
    // ç§»é™¤æ‰€æœ‰ç°æœ‰é«˜äº®
    const allNodes = document.querySelectorAll('.react-flow__node');
    allNodes.forEach(nodeElement => {
      nodeElement.classList.remove('highlighted-node');
    });
    console.log('ğŸ¯ [éç ´åæ€§é«˜äº®] æ¸…é™¤äº†æ‰€æœ‰ç°æœ‰é«˜äº®');
    
    // å¦‚æœæœ‰æŒ‡å®šçš„èŠ‚ç‚¹IDï¼Œæ·»åŠ é«˜äº®
    if (nodeIdToHighlight) {
      const foundNode = findNodeElement(nodeIdToHighlight);
      
      if (foundNode) {
        foundNode.classList.add('highlighted-node');
        console.log('ğŸ¯ [éç ´åæ€§é«˜äº®] âœ… æˆåŠŸé«˜äº®èŠ‚ç‚¹:', nodeIdToHighlight);
        
        // ğŸ”‘ å»¶è¿Ÿæ£€æŸ¥é«˜äº®æ˜¯å¦è¿˜åœ¨ï¼Œå¦‚æœä¸åœ¨åˆ™é‡æ–°åº”ç”¨
        setTimeout(() => {
          const stillHighlighted = foundNode.classList.contains('highlighted-node');
          if (!stillHighlighted) {
            console.log('ğŸ¯ [é«˜äº®æ¢å¤] æ£€æµ‹åˆ°é«˜äº®ä¸¢å¤±ï¼Œé‡æ–°åº”ç”¨:', nodeIdToHighlight);
            foundNode.classList.add('highlighted-node');
          }
        }, 100);
        
        // ç¡®ä¿é«˜äº®çš„èŠ‚ç‚¹åœ¨è§†å£ä¸­å¯è§ï¼ˆå¯é€‰ï¼‰
        const nodeRect = foundNode.getBoundingClientRect();
        const viewportHeight = window.innerHeight;
        const viewportWidth = window.innerWidth;
        
        const isVisible = nodeRect.top >= 0 && 
                         nodeRect.left >= 0 && 
                         nodeRect.bottom <= viewportHeight && 
                         nodeRect.right <= viewportWidth;
        
        if (!isVisible) {
          console.log('ğŸ¯ [éç ´åæ€§é«˜äº®] èŠ‚ç‚¹ä¸åœ¨è§†å£ä¸­ï¼Œæ»šåŠ¨åˆ°å¯è§ä½ç½®');
          foundNode.scrollIntoView({ 
            behavior: 'smooth', 
            block: 'center',
            inline: 'center'
          });
        }
      } else {
        console.warn('ğŸ¯ [éç ´åæ€§é«˜äº®] âŒ æœªæ‰¾åˆ°èŠ‚ç‚¹å…ƒç´ :', nodeIdToHighlight);
        
        // è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        const allNodes = document.querySelectorAll('.react-flow__node');
        const nodeIds = Array.from(allNodes).map(node => ({
          dataId: node.getAttribute('data-id'),
          id: node.id,
          className: node.className
        }));
        console.log('ğŸ¯ [è°ƒè¯•] é¡µé¢ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„ä¿¡æ¯:', nodeIds);
      }
    } else {
      console.log('ğŸ¯ [éç ´åæ€§é«˜äº®] æ¸…é™¤æ‰€æœ‰é«˜äº®ï¼ˆnodeIdToHighlightä¸ºç©ºï¼‰');
    }
  }, []);

  // ç›‘å¬é«˜äº®èŠ‚ç‚¹å˜åŒ–ï¼Œä½¿ç”¨éç ´åæ€§æ–¹å¼åº”ç”¨é«˜äº®
  useEffect(() => {
    if (nodes.length > 0) {
      // å»¶è¿Ÿæ‰§è¡Œï¼Œç¡®ä¿DOMå·²ç»æ›´æ–°
      setTimeout(() => {
        applyNodeHighlighting(highlightedNodeId);
      }, 100);
    }
  }, [highlightedNodeId, nodes, applyNodeHighlighting]); // ğŸ”‘ ä¿®å¤ï¼šç›‘å¬æ•´ä¸ªnodesæ•°ç»„è€Œä¸åªæ˜¯length

  // ğŸ”‘ æ–°å¢ï¼šå¤„ç†ReactFlowèŠ‚ç‚¹å˜åŒ–äº‹ä»¶ï¼Œç¡®ä¿æ‹–æ‹½åé‡æ–°åº”ç”¨é«˜äº®
  const handleNodesChange = useCallback((changes) => {
    console.log('ğŸ¯ [ReactFlow] èŠ‚ç‚¹å˜åŒ–äº‹ä»¶:', changes);
    
    // è°ƒç”¨åŸå§‹çš„onNodesChangeå¤„ç†å‡½æ•°
    onNodesChange(changes);
    
    // ğŸ”‘ ä¼˜åŒ–ï¼šåªåœ¨ç‰¹å®šå˜åŒ–ç±»å‹ä¸”æœ‰é«˜äº®èŠ‚ç‚¹æ—¶æ‰å¤„ç†
    if (!highlightedNodeId) {
      console.log('ğŸ¯ [ReactFlow] æ— é«˜äº®èŠ‚ç‚¹ï¼Œè·³è¿‡é«˜äº®å¤„ç†');
      return;
    }
    
    // æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦é‡æ–°åº”ç”¨é«˜äº®çš„å˜åŒ–
    const needsHighlightReapply = changes.some(change => {
      const isRelevantChange = 
        change.type === 'position' ||     // ä½ç½®å˜åŒ–ï¼ˆæ‹–æ‹½ï¼‰
        change.type === 'dimensions' ||   // å°ºå¯¸å˜åŒ–
        change.type === 'select' ||       // é€‰æ‹©çŠ¶æ€å˜åŒ–
        change.type === 'replace';        // èŠ‚ç‚¹æ›¿æ¢
      
      // å¦‚æœæ˜¯æ‹–æ‹½ç»“æŸäº‹ä»¶ï¼Œä¹Ÿéœ€è¦é‡æ–°åº”ç”¨é«˜äº®
      const isDragEnd = change.type === 'position' && change.dragging === false;
      
      return isRelevantChange || isDragEnd;
    });
    
    if (needsHighlightReapply) {
      console.log('ğŸ¯ [ReactFlow] æ£€æµ‹åˆ°éœ€è¦é‡æ–°åº”ç”¨é«˜äº®çš„å˜åŒ–ï¼ŒèŠ‚ç‚¹:', highlightedNodeId);
      
      // ğŸ”‘ ä½¿ç”¨æ›´çŸ­çš„å»¶è¿Ÿï¼Œæé«˜å“åº”é€Ÿåº¦
      setTimeout(() => {
        // å†æ¬¡æ£€æŸ¥é«˜äº®èŠ‚ç‚¹IDæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        if (highlightedNodeId) {
          console.log('ğŸ¯ [ReactFlow] æ‰§è¡Œå»¶è¿Ÿé«˜äº®é‡æ–°åº”ç”¨:', highlightedNodeId);
          applyNodeHighlighting(highlightedNodeId);
        }
      }, 50); // å‡å°‘å»¶è¿Ÿï¼Œæé«˜å“åº”é€Ÿåº¦
    } else {
      console.log('ğŸ¯ [ReactFlow] å˜åŒ–ä¸éœ€è¦é‡æ–°åº”ç”¨é«˜äº®');
    }
  }, [onNodesChange, highlightedNodeId, applyNodeHighlighting]);

  // ğŸ”‘ æ–°å¢ï¼šå¤„ç†ReactFlowç”»å¸ƒç‚¹å‡»äº‹ä»¶ï¼Œé˜²æ­¢é«˜äº®æ„å¤–æ¸…é™¤
  const handlePaneClick = useCallback((event) => {
    console.log('ï¿½ï¿½ [ReactFlow] ç”»å¸ƒç‚¹å‡»äº‹ä»¶ï¼Œå½“å‰é«˜äº®èŠ‚ç‚¹:', highlightedNodeId);
    
    // ğŸ”‘ ä¿æŒç°æœ‰é«˜äº®çŠ¶æ€ï¼Œä¸æ‰§è¡Œä»»ä½•æ¸…é™¤æ“ä½œ
    // å¦‚æœéœ€è¦æ¸…é™¤é«˜äº®ï¼Œåº”è¯¥é€šè¿‡å¤–éƒ¨æ§åˆ¶highlightedNodeIdçš„å€¼
    // è¿™æ ·å¯ä»¥ç¡®ä¿é«˜äº®çŠ¶æ€çš„ç®¡ç†æ˜¯ç»Ÿä¸€å’Œå¯æ§çš„
    
    // å¯é€‰ï¼šåœ¨ç”»å¸ƒç‚¹å‡»åéªŒè¯é«˜äº®çŠ¶æ€æ˜¯å¦ä»ç„¶æ­£ç¡®
    if (highlightedNodeId) {
      setTimeout(() => {
        const highlightedElement = document.querySelector(`[data-id="${highlightedNodeId}"].highlighted-node`);
        if (!highlightedElement) {
          console.log('ğŸ¯ [ReactFlow] ç”»å¸ƒç‚¹å‡»åæ£€æµ‹åˆ°é«˜äº®ä¸¢å¤±ï¼Œé‡æ–°åº”ç”¨:', highlightedNodeId);
          applyNodeHighlighting(highlightedNodeId);
        } else {
          console.log('ğŸ¯ [ReactFlow] ç”»å¸ƒç‚¹å‡»åé«˜äº®çŠ¶æ€æ­£å¸¸');
        }
      }, 50);
    }
  }, [highlightedNodeId, applyNodeHighlighting]);

  // ä»Mermaidä»£ç ä¸­æå–èŠ‚ç‚¹æ˜ å°„
  const extractNodeMappingsFromMermaid = (mermaidCode) => {
    const nodeMappings = {};
    
    if (!mermaidCode) return nodeMappings;

    // åŒ¹é…èŠ‚ç‚¹å®šä¹‰ï¼Œå¦‚ A[æ–‡æœ¬], A(æ–‡æœ¬), A{æ–‡æœ¬}
    const nodeDefRegex = /([A-Za-z0-9_]+)[\[\(\{]([^\]\)\}]+)[\]\)\}]/g;
    let match;
    
    while ((match = nodeDefRegex.exec(mermaidCode)) !== null) {
      const [, nodeId, nodeText] = match;
      nodeMappings[nodeId] = {
        text_snippet: nodeText.trim(),
        paragraph_ids: []
      };
    }

    // å¦‚æœæ²¡æœ‰æ‰¾åˆ°èŠ‚ç‚¹å®šä¹‰ï¼Œä»è¿æ¥å…³ç³»ä¸­æå–èŠ‚ç‚¹ID
    if (Object.keys(nodeMappings).length === 0) {
      const connectionRegex = /([A-Za-z0-9_]+)\s*(-{1,2}>?|={1,2}>?)\s*([A-Za-z0-9_]+)/g;
      const nodeIds = new Set();
      
      while ((match = connectionRegex.exec(mermaidCode)) !== null) {
        const [, source, , target] = match;
        nodeIds.add(source);
        nodeIds.add(target);
      }
      
      // ä¸ºæ¯ä¸ªèŠ‚ç‚¹IDåˆ›å»ºåŸºæœ¬æ˜ å°„
      nodeIds.forEach(nodeId => {
        nodeMappings[nodeId] = {
          text_snippet: nodeId,
          paragraph_ids: []
        };
      });
    }

    return nodeMappings;
  };

  // å¤„ç†è¿æ¥
  const onConnect = useCallback(
    (params) => setEdges((eds) => addEdge(params, eds)),
    [setEdges],
  );

  // å¤„ç†èŠ‚ç‚¹ç‚¹å‡»
  const onNodeClickHandler = useCallback((event, node) => {
    console.log('FlowDiagramèŠ‚ç‚¹ç‚¹å‡»:', node.id, node);
    if (onNodeClick) {
      // è°ƒç”¨ä¸MermaidDiagramå…¼å®¹çš„å›è°ƒ
      // ä¼ é€’èŠ‚ç‚¹IDä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œäº‹ä»¶ä½œä¸ºç¬¬äºŒä¸ªå‚æ•°
      onNodeClick(node.id, event);
    }
  }, [onNodeClick]);

  // å¤„ç†ReactFlowå®ä¾‹åˆå§‹åŒ–
  const onInit = useCallback((reactFlowInstance) => {
    console.log('ğŸ”„ [FlowDiagram] ReactFlowå®ä¾‹åˆå§‹åŒ–');
    if (onReactFlowInstanceChange) {
      onReactFlowInstanceChange(reactFlowInstance);
    }
    
    // å»¶è¿Ÿé€‚åº”è§†å›¾ï¼Œç¡®ä¿èŠ‚ç‚¹å·²ç»æ¸²æŸ“
    setTimeout(() => {
      const allNodes = reactFlowInstance.getNodes();
      console.log('ğŸ”„ [å…³é”®] ReactFlowå®ä¾‹ä¸­çš„èŠ‚ç‚¹:', allNodes.map(n => ({ 
        id: n.id, 
        position: n.position,
        width: n.width,
        height: n.height
      })));
      
      if (allNodes.length > 0) {
        console.log('ğŸ”„ [FlowDiagram] æ‰§è¡ŒfitView');
        reactFlowInstance.fitView({ padding: 0.2, duration: 800 });
      }
    }, 500); // å¢åŠ å»¶è¿Ÿæ—¶é—´ï¼Œç¡®ä¿å¸ƒå±€å®Œæˆ
  }, [onReactFlowInstanceChange]);

  // è‡ªå®šä¹‰èŠ‚ç‚¹æ ·å¼ - åŸºç¡€æ ·å¼ï¼Œé«˜äº®æ ·å¼ç”±CSSå¤„ç†
  const nodeDefaults = {
    style: {
      background: '#ffffff',
      border: '2px solid #1a192b',
      borderRadius: '8px',
      fontSize: '12px',
      fontWeight: 500,
      padding: '10px',
      boxShadow: '0 2px 4px rgba(0,0,0,0.1)',
      minWidth: '150px',
      textAlign: 'center',
      width: 200,
      height: 80
    },
  };

  return (
    <div className={`w-full h-full ${className}`}>
      {isLoading ? (
        <div className="flex items-center justify-center h-full">
          <div className="text-center">
            <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
            <p className="text-sm text-gray-500">æ­£åœ¨åŠ è½½æµç¨‹å›¾...</p>
          </div>
        </div>
      ) : (
        <ReactFlow
          nodes={nodes}  // ç›´æ¥ä½¿ç”¨åŸå§‹èŠ‚ç‚¹ï¼Œä¸å†é€šè¿‡nodesWithHighlightClasså¤„ç†
          edges={edges}
          onNodesChange={handleNodesChange}
          onEdgesChange={onEdgesChange}
          onConnect={onConnect}
          onNodeClick={onNodeClickHandler}
          onPaneClick={handlePaneClick}
          nodeTypes={nodeTypes}
          connectionLineType={ConnectionLineType.SmoothStep}
          fitView
          fitViewOptions={{
            padding: 0.1,
            includeHiddenNodes: false,
          }}
          onInit={(instance) => {
            console.log('ğŸ”„ [FlowDiagram] ReactFlowå®ä¾‹åˆå§‹åŒ–å®Œæˆ');
            if (onReactFlowInstanceChange) {
              onReactFlowInstanceChange(instance);
            }
          }}
        >
          <Background variant="dots" gap={20} size={1} />
          <Controls />
          <MiniMap 
            nodeStrokeColor="#374151" 
            nodeColor="#f3f4f6" 
            nodeBorderRadius={8}
          />
        </ReactFlow>
      )}
    </div>
  );
};

const FlowDiagram = forwardRef(({ 
  code, 
  apiData, 
  highlightedNodeId, 
  onNodeClick, 
  onNodeLabelUpdate,
  onAddChildNode,
  onAddSiblingNode,
  onDeleteNode,
  layoutOptions = {}, 
  className = '' 
}, ref) => {
  const [reactFlowInstance, setReactFlowInstance] = useState(null);

  // å¤„ç†ReactFlowå®ä¾‹å˜åŒ–
  const handleReactFlowInstanceChange = useCallback((instance) => {
    setReactFlowInstance(instance);
  }, []);

  // æä¾›ä¸MermaidDiagramå…¼å®¹çš„refæ–¹æ³•
  useImperativeHandle(ref, () => ({
    // å…¼å®¹MermaidDiagramçš„ensureNodeVisibleæ–¹æ³•
    ensureNodeVisible: (nodeId) => {
      if (reactFlowInstance) {
        try {
          // è·å–èŠ‚ç‚¹å¹¶èšç„¦åˆ°å®ƒ
          const node = reactFlowInstance.getNode(nodeId);
          if (node) {
            // ä½¿ç”¨æ›´å¹³æ»‘çš„åŠ¨ç”»æ•ˆæœèšç„¦åˆ°èŠ‚ç‚¹
            reactFlowInstance.setCenter(
              node.position.x + (node.width || 200) / 2, 
              node.position.y + (node.height || 80) / 2, 
              { zoom: 1.2, duration: 800 }
            );
          }
        } catch (error) {
          console.warn('æ— æ³•èšç„¦åˆ°èŠ‚ç‚¹:', nodeId, error);
        }
      }
    },
    
    // æä¾›è·å–React Flowå®ä¾‹çš„æ–¹æ³•
    getReactFlowInstance: () => reactFlowInstance,
    
    // é‡æ–°å¸ƒå±€æ–¹æ³•
    fitView: () => {
      if (reactFlowInstance) {
        reactFlowInstance.fitView({ padding: 0.2, duration: 800 });
      }
    }
  }), [reactFlowInstance]);

  // å½“é«˜äº®èŠ‚ç‚¹å˜åŒ–æ—¶ï¼Œè‡ªåŠ¨èšç„¦åˆ°è¯¥èŠ‚ç‚¹
  useEffect(() => {
    if (highlightedNodeId && reactFlowInstance) {
      // å»¶è¿Ÿæ‰§è¡Œï¼Œç¡®ä¿èŠ‚ç‚¹å·²ç»æ›´æ–°
      setTimeout(() => {
        const node = reactFlowInstance.getNode(highlightedNodeId);
        if (node && node.position) {
          console.log('ğŸ¯ [è‡ªåŠ¨èšç„¦] èšç„¦åˆ°èŠ‚ç‚¹:', highlightedNodeId, 'ä½ç½®:', node.position);
          reactFlowInstance.setCenter(
            node.position.x + (node.width || 200) / 2, 
            node.position.y + (node.height || 80) / 2, 
            { zoom: 1.2, duration: 800 }
          );
        } else {
          console.warn('ğŸ¯ [è‡ªåŠ¨èšç„¦] æœªæ‰¾åˆ°èŠ‚ç‚¹æˆ–èŠ‚ç‚¹ä½ç½®æ— æ•ˆ:', highlightedNodeId, node);
        }
      }, 300); // å¢åŠ å»¶è¿Ÿæ—¶é—´ï¼Œç¡®ä¿é«˜äº®æ ·å¼æ›´æ–°å®Œæˆ
    }
  }, [highlightedNodeId, reactFlowInstance]);

      return (
      <ReactFlowProvider>
        <FlowDiagramInner 
          code={code}
          apiData={apiData}
          highlightedNodeId={highlightedNodeId}
          onNodeClick={onNodeClick}
          onNodeLabelUpdate={onNodeLabelUpdate}
          onAddChildNode={onAddChildNode}
          onAddSiblingNode={onAddSiblingNode}
          onDeleteNode={onDeleteNode}
          layoutOptions={layoutOptions}
          className={className}
          onReactFlowInstanceChange={handleReactFlowInstanceChange}
        />
      </ReactFlowProvider>
    );
});

FlowDiagram.displayName = 'FlowDiagram';

export default FlowDiagram;
</file>

<file path="frontend/src/components/MermaidDiagram.js">
import React, { useEffect, useRef, useState, useCallback, useImperativeHandle, forwardRef } from 'react';
import mermaid from 'mermaid';
import { AlertCircle, Copy, Check, ZoomIn, ZoomOut, RotateCcw, Move } from 'lucide-react';
import toast from 'react-hot-toast';

// ç¾åŒ–MermaidèŠ‚ç‚¹çš„CSSæ ·å¼ - ç²¾ç¡®æ‚¬åœç‰ˆæœ¬
const mermaidStyles = `
  /* åŸºç¡€èŠ‚ç‚¹æ ·å¼ */
  .mermaid rect,
  .mermaid polygon,
  .mermaid circle,
  .mermaid ellipse {
    fill: #ffffff !important;
    stroke: rgba(0, 0, 0, 0.2) !important;
    stroke-width: 1px !important;
    filter: drop-shadow(2px 2px 6px rgba(0, 0, 0, 0.15)) !important;
    transition: all 0.3s ease !important;
  }

  /* çŸ©å½¢åœ†è§’ */
  .mermaid rect {
    rx: 8 !important;
    ry: 8 !important;
  }

  /* æ‚¬åœæ•ˆæœ - åªä½œç”¨äºå½“å‰æ‚¬åœçš„èŠ‚ç‚¹ */
  .mermaid g:hover > rect,
  .mermaid g:hover > polygon,
  .mermaid g:hover > circle,
  .mermaid g:hover > ellipse {
    fill: #ffffff !important;
    stroke: #3b82f6 !important;
    stroke-width: 2px !important;
    filter: drop-shadow(2px 2px 12px rgba(59, 130, 246, 0.3)) !important;
    cursor: pointer !important;
  }

  /* ç¡®ä¿æ‚¬åœæ—¶ä¸å½±å“å…¶ä»–èŠ‚ç‚¹ */
  .mermaid g:not(:hover) rect,
  .mermaid g:not(:hover) polygon,
  .mermaid g:not(:hover) circle,
  .mermaid g:not(:hover) ellipse {
    fill: #ffffff !important;
    stroke: rgba(0, 0, 0, 0.2) !important;
    stroke-width: 1px !important;
    filter: drop-shadow(2px 2px 6px rgba(0, 0, 0, 0.15)) !important;
  }

  /* ç¡®ä¿æ–‡æœ¬å¯è§ */
  .mermaid text {
    font-family: "Microsoft YaHei", Arial, sans-serif !important;
    font-weight: 500 !important;
    fill: #374151 !important;
    pointer-events: none !important;
  }

  /* è¿æ¥çº¿æ ·å¼ */
  .mermaid path {
    stroke: #9ca3af !important;
    stroke-width: 1.5px !important;
    fill: none !important;
  }

  .mermaid marker {
    fill: #9ca3af !important;
  }

  /* èŠ‚ç‚¹ç‚¹å‡»æ ·å¼ */
  .mermaid g {
    cursor: pointer !important;
  }

  /* å¢å¼ºèŠ‚ç‚¹ç‚¹å‡»åŒºåŸŸ */
  .mermaid g > rect,
  .mermaid g > polygon,
  .mermaid g > circle,
  .mermaid g > ellipse {
    cursor: pointer !important;
  }
`;

// æ³¨å…¥æ ·å¼åˆ°é¡µé¢
const injectStyles = () => {
  const styleId = 'mermaid-custom-styles';
  
  // å…ˆç§»é™¤ä¹‹å‰çš„æ ·å¼
  const existingStyle = document.getElementById(styleId);
  if (existingStyle) {
    existingStyle.remove();
    console.log('ğŸ—‘ï¸ ç§»é™¤äº†ä¹‹å‰çš„æ ·å¼');
  }

  const styleSheet = document.createElement('style');
  styleSheet.id = styleId;
  styleSheet.type = 'text/css';
  styleSheet.textContent = mermaidStyles;
  document.head.appendChild(styleSheet);
  
  console.log('âœ¨ Mermaidè‡ªå®šä¹‰æ ·å¼å·²æ³¨å…¥');
  console.log('ğŸ“‹ æ ·å¼å†…å®¹é•¿åº¦:', mermaidStyles.length);
  
  // éªŒè¯æ ·å¼æ˜¯å¦å·²æ·»åŠ 
  setTimeout(() => {
    const appliedStyle = document.getElementById(styleId);
    if (appliedStyle) {
      console.log('âœ… æ ·å¼ç¡®è®¤å·²æ·»åŠ åˆ°DOM');
      console.log('ğŸ“„ æ ·å¼è¡¨å†…å®¹é¢„è§ˆ:', appliedStyle.textContent.substring(0, 100) + '...');
      
      // æ£€æŸ¥æ˜¯å¦æœ‰Mermaid SVGå…ƒç´ å­˜åœ¨
      const mermaidSvgs = document.querySelectorAll('.mermaid svg');
      console.log('ğŸ¨ æ‰¾åˆ°', mermaidSvgs.length, 'ä¸ªMermaid SVGå…ƒç´ ');
      
      if (mermaidSvgs.length > 0) {
        const firstSvg = mermaidSvgs[0];
        const rects = firstSvg.querySelectorAll('g rect');
        console.log('ğŸ“¦ ç¬¬ä¸€ä¸ªSVGä¸­æœ‰', rects.length, 'ä¸ªçŸ©å½¢èŠ‚ç‚¹');
        
        if (rects.length > 0) {
          const computedStyle = window.getComputedStyle(rects[0]);
          console.log('ğŸ¨ ç¬¬ä¸€ä¸ªçŸ©å½¢çš„è®¡ç®—æ ·å¼:');
          console.log('  - fill:', computedStyle.fill);
          console.log('  - stroke:', computedStyle.stroke);
          console.log('  - filter:', computedStyle.filter);
        }
      }
    } else {
      console.error('âŒ æ ·å¼æ·»åŠ å¤±è´¥');
    }
  }, 100);
};

const MermaidDiagram = forwardRef(({ code, onNodeClick }, ref) => {
  const [diagramId] = useState(() => `mermaid-${Math.random().toString(36).substr(2, 9)}`);
  const [error, setError] = useState(null);
  const [copied, setCopied] = useState(false);
  const [isRendering, setIsRendering] = useState(false);
  const [hasRendered, setHasRendered] = useState(false);
  const [scale, setScale] = useState(1);
  const [position, setPosition] = useState({ x: 0, y: 0 });
  const [isDragging, setIsDragging] = useState(false);
  const [dragStart, setDragStart] = useState({ x: 0, y: 0 });
  const [mermaidInitialized, setMermaidInitialized] = useState(false);
  const [domReady, setDomReady] = useState(false);
  const containerRef = useRef(null);
  const diagramRef = useRef(null);
  const copyTimeoutRef = useRef(null);
  const parentContainerRef = useRef(null);

  // ä½¿ç”¨useRefæ¥ä¿å­˜äº‹ä»¶å¤„ç†å‡½æ•°çš„å¼•ç”¨
  const handleMouseMoveRef = useRef(null);
  const handleMouseUpRef = useRef(null);

  // ä½¿ç”¨RAFä¼˜åŒ–çš„æ‹–æ‹½å¤„ç†
  const dragAnimationFrame = useRef(null);
  const pendingPosition = useRef(null);
  
  // é˜²æŠ–ç›¸å…³çŠ¶æ€
  const isAnimating = useRef(false);
  const lastMoveTime = useRef(0);
  const lastMovedNode = useRef(null);
  const moveDebounceTimer = useRef(null);

  // èŠ‚ç‚¹å…³ç³»ç¼“å­˜
  const nodeRelationsRef = useRef(null);

  // è§£æMermaidä»£ç æ„å»ºèŠ‚ç‚¹å…³ç³»çš„å‡½æ•°
  const parseMermaidCode = useCallback((mermaidCode) => {
    if (!mermaidCode) return { nodes: new Set(), edges: [], adjacencyList: new Map() };

    console.log('ğŸ” [ä»£ç è§£æ] å¼€å§‹è§£æMermaidä»£ç ');
    console.log('ğŸ” [ä»£ç è§£æ] ä»£ç é¢„è§ˆ:', mermaidCode.substring(0, 200) + '...');

    const nodes = new Set();
    const edges = [];
    const adjacencyList = new Map(); // nodeId -> [childNodeIds]

    try {
      // å°†ä»£ç æŒ‰è¡Œåˆ†å‰²å¹¶æ¸…ç†
      const lines = mermaidCode
        .split('\n')
        .map(line => line.trim())
        .filter(line => line && !line.startsWith('%%') && !line.startsWith('#'));

      // åŒ¹é…å„ç§Mermaidè¯­æ³•çš„æ­£åˆ™è¡¨è¾¾å¼
      const patterns = [
        // åŸºæœ¬ç®­å¤´è¿æ¥: A --> B, A->B
        /^([A-Za-z0-9_]+)\s*(-->|->)\s*([A-Za-z0-9_]+)/,
        // å¸¦æ ‡ç­¾çš„ç®­å¤´: A -->|label| B, A ->|label| B  
        /^([A-Za-z0-9_]+)\s*(-->|->)\s*\|[^|]*\|\s*([A-Za-z0-9_]+)/,
        // å®çº¿è¿æ¥: A --- B, A-B
        /^([A-Za-z0-9_]+)\s*(---|--|-)\s*([A-Za-z0-9_]+)/,
        // å¸¦æ ‡ç­¾çš„å®çº¿: A ---|label| B
        /^([A-Za-z0-9_]+)\s*(---|--|-)\s*\|[^|]*\|\s*([A-Za-z0-9_]+)/,
        // èŠ‚ç‚¹å®šä¹‰: A[label], A(label), A{label}
        /^([A-Za-z0-9_]+)[\[\(\{]([^\]\)\}]*)[\]\)\}]/,
        // å¤æ‚ç®­å¤´: A ==> B, A -.-> B
        /^([A-Za-z0-9_]+)\s*(==>|\.->|\.\.>)\s*([A-Za-z0-9_]+)/,
        // å¸¦æ ‡ç­¾çš„å¤æ‚ç®­å¤´: A ==>|label| B
        /^([A-Za-z0-9_]+)\s*(==>|\.->|\.\.>)\s*\|[^|]*\|\s*([A-Za-z0-9_]+)/,
        // å¤šè¿æ¥æ¨¡å¼: A --> B & C & D
        /^([A-Za-z0-9_]+)\s*(-->|->)\s*([A-Za-z0-9_]+(?:\s*&\s*[A-Za-z0-9_]+)*)/,
        // ä»å¤šä¸ªèŠ‚ç‚¹è¿æ¥: A & B & C --> D
        /^([A-Za-z0-9_]+(?:\s*&\s*[A-Za-z0-9_]+)*)\s*(-->|->)\s*([A-Za-z0-9_]+)/
      ];

      for (const line of lines) {
        // è·³è¿‡å›¾è¡¨ç±»å‹å®šä¹‰è¡Œå’Œå­å›¾å®šä¹‰
        if (line.includes('flowchart') || line.includes('graph') || line.includes('TD') || 
            line.includes('LR') || line.includes('TB') || line.includes('RL') ||
            line.includes('subgraph') || line === 'end') {
          continue;
        }

        // å°è¯•åŒ¹é…å„ç§æ¨¡å¼
        let matched = false;
        
        for (const pattern of patterns) {
          const match = line.match(pattern);
          if (match) {
            matched = true;
            
            // å¦‚æœæ˜¯è¿æ¥å…³ç³»ï¼ˆæœ‰ç®­å¤´æˆ–è¿çº¿ï¼‰
            if (match[2] && (match[2].includes('>') || match[2].includes('-'))) {
              const fromPart = match[1];
              const toPart = match[3];
              
              // å¤„ç†å¤šè¿æ¥æ¨¡å¼ (A --> B & C & D)
              if (toPart && toPart.includes('&')) {
                const toNodes = toPart.split('&').map(n => n.trim());
                toNodes.forEach(toNode => {
                  if (fromPart && toNode) {
                    nodes.add(fromPart);
                    nodes.add(toNode);
                    edges.push({ from: fromPart, to: toNode, type: match[2] });
                    
                    if (!adjacencyList.has(fromPart)) {
                      adjacencyList.set(fromPart, []);
                    }
                    if (!adjacencyList.get(fromPart).includes(toNode)) {
                      adjacencyList.get(fromPart).push(toNode);
                    }
                    
                    console.log('ğŸ” [ä»£ç è§£æ] å‘ç°è¾¹ (å¤šè¿æ¥):', fromPart, '->', toNode);
                  }
                });
              }
              // å¤„ç†ä»å¤šä¸ªèŠ‚ç‚¹è¿æ¥æ¨¡å¼ (A & B & C --> D)
              else if (fromPart && fromPart.includes('&')) {
                const fromNodes = fromPart.split('&').map(n => n.trim());
                fromNodes.forEach(fromNode => {
                  if (fromNode && toPart) {
                    nodes.add(fromNode);
                    nodes.add(toPart);
                    edges.push({ from: fromNode, to: toPart, type: match[2] });
                    
                    if (!adjacencyList.has(fromNode)) {
                      adjacencyList.set(fromNode, []);
                    }
                    if (!adjacencyList.get(fromNode).includes(toPart)) {
                      adjacencyList.get(fromNode).push(toPart);
                    }
                    
                    console.log('ğŸ” [ä»£ç è§£æ] å‘ç°è¾¹ (å¤šæºè¿æ¥):', fromNode, '->', toPart);
                  }
                });
              }
              // æ™®é€šå•å¯¹å•è¿æ¥
              else if (fromPart && toPart) {
                nodes.add(fromPart);
                nodes.add(toPart);
                edges.push({ from: fromPart, to: toPart, type: match[2] });
                
                // æ„å»ºé‚»æ¥è¡¨
                if (!adjacencyList.has(fromPart)) {
                  adjacencyList.set(fromPart, []);
                }
                if (!adjacencyList.get(fromPart).includes(toPart)) {
                  adjacencyList.get(fromPart).push(toPart);
                }
                
                console.log('ğŸ” [ä»£ç è§£æ] å‘ç°è¾¹:', fromPart, '->', toPart);
              }
            } else if (match[1]) {
              // å•çº¯çš„èŠ‚ç‚¹å®šä¹‰
              nodes.add(match[1]);
              console.log('ğŸ” [ä»£ç è§£æ] å‘ç°èŠ‚ç‚¹å®šä¹‰:', match[1]);
            }
            break;
          }
        }

        // å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ‡å‡†æ¨¡å¼ï¼Œå°è¯•æå–å¯èƒ½çš„èŠ‚ç‚¹ID
        if (!matched) {
          // æŸ¥æ‰¾å¯èƒ½çš„èŠ‚ç‚¹ID (å­—æ¯æ•°å­—ç»„åˆ)
          const possibleNodes = line.match(/\b[A-Za-z][A-Za-z0-9_]*\b/g);
          if (possibleNodes && possibleNodes.length > 0) {
            // è¿‡æ»¤æ‰å¸¸è§çš„å…³é”®è¯
            const keywords = ['flowchart', 'graph', 'TD', 'LR', 'TB', 'RL', 'subgraph', 'end', 'class', 'style'];
            possibleNodes.forEach(node => {
              if (!keywords.includes(node.toLowerCase()) && node.length <= 10) {
                nodes.add(node);
                console.log('ğŸ” [ä»£ç è§£æ] å¯èƒ½çš„èŠ‚ç‚¹:', node);
              }
            });
          }
        }
      }

      console.log('ğŸ” [ä»£ç è§£æ] è§£æå®Œæˆ');
      console.log('ğŸ” [ä»£ç è§£æ] å‘ç°èŠ‚ç‚¹:', Array.from(nodes));
      console.log('ğŸ” [ä»£ç è§£æ] å‘ç°è¾¹:', edges);
      console.log('ğŸ” [ä»£ç è§£æ] é‚»æ¥è¡¨:', Object.fromEntries(adjacencyList));

      return { nodes, edges, adjacencyList };

    } catch (error) {
      console.error('ğŸ” [ä»£ç è§£æ] è§£æMermaidä»£ç æ—¶å‡ºé”™:', error);
      return { nodes: new Set(), edges: [], adjacencyList: new Map() };
    }
  }, []);

  // è·å–èŠ‚ç‚¹å…³ç³»æ•°æ®
  const getNodeRelations = useCallback(() => {
    if (!nodeRelationsRef.current && code) {
      console.log('ğŸ” [èŠ‚ç‚¹å…³ç³»] æ„å»ºèŠ‚ç‚¹å…³ç³»ç¼“å­˜');
      nodeRelationsRef.current = parseMermaidCode(code);
    }
    return nodeRelationsRef.current || { nodes: new Set(), edges: [], adjacencyList: new Map() };
  }, [code, parseMermaidCode]);

  // æ¸…ç†èŠ‚ç‚¹å…³ç³»ç¼“å­˜å½“ä»£ç å˜åŒ–æ—¶
  useEffect(() => {
    nodeRelationsRef.current = null;
    console.log('ğŸ” [èŠ‚ç‚¹å…³ç³»] æ¸…ç†ç¼“å­˜ï¼Œä»£ç å·²å˜åŒ–');
  }, [code]);

  // åŸºäºä»£ç è§£ææŸ¥æ‰¾å­èŠ‚ç‚¹
  const findChildNodes = useCallback((nodeId) => {
    const relations = getNodeRelations();
    const children = relations.adjacencyList.get(nodeId) || [];
    
    console.log('ğŸ” [å­èŠ‚ç‚¹æŸ¥æ‰¾] èŠ‚ç‚¹', nodeId, 'çš„ç›´æ¥å­èŠ‚ç‚¹:', children);
    return children;
  }, [getNodeRelations]);

  // åŸºäºä»£ç è§£ææ„å»ºé€»è¾‘é“¾æ¡
  const findLogicalChain = useCallback((startNodeId) => {
    const relations = getNodeRelations();
    const visited = new Set();
    const chain = [];
    let currentNode = startNodeId;
    const maxNodes = 6; // æœ€å¤š6ä¸ªèŠ‚ç‚¹

    console.log('ğŸ”— [é€»è¾‘é“¾æ¡] å¼€å§‹æ„å»ºé“¾æ¡ï¼Œèµ·å§‹èŠ‚ç‚¹:', startNodeId);

    while (currentNode && !visited.has(currentNode) && chain.length < maxNodes) {
      visited.add(currentNode);
      chain.push(currentNode);
      
      console.log('ğŸ”— [é€»è¾‘é“¾æ¡] æ·»åŠ èŠ‚ç‚¹åˆ°é“¾æ¡:', currentNode);

      // è·å–å½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹
      const children = relations.adjacencyList.get(currentNode) || [];
      
      if (children.length === 0) {
        // æ²¡æœ‰å­èŠ‚ç‚¹ï¼Œé“¾æ¡ç»“æŸ
        console.log('ğŸ”— [é€»è¾‘é“¾æ¡] èŠ‚ç‚¹æ— å­èŠ‚ç‚¹ï¼Œé“¾æ¡ç»“æŸ');
        break;
      } else if (children.length === 1) {
        // åªæœ‰ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œç»§ç»­é“¾æ¡
        currentNode = children[0];
        console.log('ğŸ”— [é€»è¾‘é“¾æ¡] å•å­èŠ‚ç‚¹ï¼Œç»§ç»­é“¾æ¡:', currentNode);
      } else {
        // å¤šä¸ªå­èŠ‚ç‚¹ï¼Œæ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦ç»§ç»­
        console.log('ğŸ”— [é€»è¾‘é“¾æ¡] å¤šå­èŠ‚ç‚¹æƒ…å†µ:', children);
        
        // ç®€å•ç­–ç•¥ï¼šå¤šå­èŠ‚ç‚¹æ—¶åœæ­¢ï¼Œå› ä¸ºè¿™é€šå¸¸è¡¨ç¤ºåˆ†æ”¯
        console.log('ğŸ”— [é€»è¾‘é“¾æ¡] é‡åˆ°åˆ†æ”¯ï¼Œåœæ­¢é“¾æ¡æ„å»º');
        break;
      }

      // å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ„å¤–çš„æ— é™å¾ªç¯
      if (chain.length >= maxNodes) {
        console.log('ğŸ”— [é€»è¾‘é“¾æ¡] è¾¾åˆ°æœ€å¤§èŠ‚ç‚¹æ•°ï¼Œåœæ­¢æ„å»º');
        break;
      }
    }

    console.log('ğŸ”— [é€»è¾‘é“¾æ¡] æœ€ç»ˆé“¾æ¡:', chain);
    return chain;
  }, [getNodeRelations]);

  // åˆ›å»ºèŠ‚ç‚¹IDæ˜ å°„ï¼Œå°†SVGä¸­çš„èŠ‚ç‚¹IDæ˜ å°„åˆ°ä»£ç ä¸­çš„èŠ‚ç‚¹ID
  const createNodeIdMapping = useCallback(() => {
    if (!containerRef.current) return new Map();

    const mapping = new Map(); // SVGèŠ‚ç‚¹ID -> ä»£ç èŠ‚ç‚¹ID
    const relations = getNodeRelations();
    const codeNodeIds = Array.from(relations.nodes);

    try {
      const svg = containerRef.current.querySelector('svg');
      if (!svg) return mapping;

      // æŸ¥æ‰¾SVGä¸­çš„æ‰€æœ‰èŠ‚ç‚¹å…ƒç´ 
      const svgNodes = svg.querySelectorAll('g[class*="node"], g[data-id], g[id]');
      
      console.log('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] SVGèŠ‚ç‚¹æ•°é‡:', svgNodes.length);
      console.log('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] ä»£ç èŠ‚ç‚¹ID:', codeNodeIds);

      for (const svgNode of svgNodes) {
        // è·å–SVGèŠ‚ç‚¹çš„å„ç§å¯èƒ½ID
        const svgNodeId = svgNode.getAttribute('data-id') || 
                         svgNode.getAttribute('id') || 
                         svgNode.className.baseVal || '';

        // å°è¯•åŒ¹é…ä»£ç ä¸­çš„èŠ‚ç‚¹ID
        let matchedCodeNodeId = null;

        // 1. ç›´æ¥åŒ¹é…
        if (codeNodeIds.includes(svgNodeId)) {
          matchedCodeNodeId = svgNodeId;
        } else {
          // 2. ä»SVGèŠ‚ç‚¹IDä¸­æå–å¯èƒ½çš„ä»£ç èŠ‚ç‚¹ID
          const extractedIds = [];
          
          // ä»ç±»åä¸­æå– (å¦‚: "node-A1" -> "A1")
          if (svgNodeId.includes('node')) {
            const match = svgNodeId.match(/node-?([A-Za-z0-9_]+)/);
            if (match) {
              extractedIds.push(match[1]);
            }
          }
          
          // ä»IDä¸­æå– (å¦‚: "flowchart-A1-123" -> "A1")
          const idMatches = svgNodeId.match(/[A-Za-z][A-Za-z0-9_]*/g);
          if (idMatches) {
            extractedIds.push(...idMatches);
          }

          // å°è¯•åŒ¹é…æå–çš„ID
          for (const extractedId of extractedIds) {
            if (codeNodeIds.includes(extractedId)) {
              matchedCodeNodeId = extractedId;
              break;
            }
          }

          // 3. æ¨¡ç³ŠåŒ¹é… (å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥)
          if (!matchedCodeNodeId) {
            for (const codeNodeId of codeNodeIds) {
              if (svgNodeId.includes(codeNodeId) || codeNodeId.includes(svgNodeId)) {
                matchedCodeNodeId = codeNodeId;
                break;
              }
            }
          }
        }

        if (matchedCodeNodeId) {
          mapping.set(svgNodeId, matchedCodeNodeId);
          console.log('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] æ˜ å°„:', svgNodeId, '->', matchedCodeNodeId);
        } else {
          console.log('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] æœªåŒ¹é…:', svgNodeId);
        }
      }

      console.log('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] å®Œæˆï¼Œæ˜ å°„æ•°é‡:', mapping.size);
      return mapping;

    } catch (error) {
      console.error('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] åˆ›å»ºèŠ‚ç‚¹æ˜ å°„æ—¶å‡ºé”™:', error);
      return new Map();
    }
  }, [getNodeRelations]);

  // å°†SVGèŠ‚ç‚¹IDè½¬æ¢ä¸ºä»£ç èŠ‚ç‚¹ID
  const mapSvgNodeIdToCodeNodeId = useCallback((svgNodeId) => {
    const mapping = createNodeIdMapping();
    const codeNodeId = mapping.get(svgNodeId);
    
    if (codeNodeId) {
      console.log('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] SVGèŠ‚ç‚¹', svgNodeId, 'æ˜ å°„åˆ°ä»£ç èŠ‚ç‚¹', codeNodeId);
      return codeNodeId;
    }

    // å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œå°è¯•ç›´æ¥è¿”å›å¯èƒ½çš„èŠ‚ç‚¹ID
    console.log('ğŸ”— [èŠ‚ç‚¹æ˜ å°„] æœªæ‰¾åˆ°æ˜ å°„ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨:', svgNodeId);
    return svgNodeId;
  }, [createNodeIdMapping]);

  // è°ƒè¯•å‡½æ•°ï¼šæ˜¾ç¤ºè§£æçš„èŠ‚ç‚¹å…³ç³»
  const debugNodeRelations = useCallback(() => {
    const relations = getNodeRelations();
    console.log('ğŸ” [è°ƒè¯•ä¿¡æ¯] ===== èŠ‚ç‚¹å…³ç³»è°ƒè¯• =====');
    console.log('ğŸ” [è°ƒè¯•ä¿¡æ¯] å‘ç°çš„èŠ‚ç‚¹:', Array.from(relations.nodes));
    console.log('ğŸ” [è°ƒè¯•ä¿¡æ¯] å‘ç°çš„è¾¹:', relations.edges);
    console.log('ğŸ” [è°ƒè¯•ä¿¡æ¯] é‚»æ¥è¡¨:');
    
    relations.adjacencyList.forEach((children, parent) => {
      console.log(`ğŸ” [è°ƒè¯•ä¿¡æ¯]   ${parent} -> [${children.join(', ')}]`);
    });
    
    console.log('ğŸ” [è°ƒè¯•ä¿¡æ¯] ===========================');
    
    // æµ‹è¯•æ¯ä¸ªèŠ‚ç‚¹çš„é€»è¾‘é“¾æ¡
    relations.nodes.forEach(nodeId => {
      const chain = findLogicalChain(nodeId);
      console.log(`ğŸ” [è°ƒè¯•ä¿¡æ¯] èŠ‚ç‚¹ ${nodeId} çš„é€»è¾‘é“¾æ¡:`, chain);
    });
  }, [getNodeRelations, findLogicalChain]);

  // åœ¨ä»£ç å˜åŒ–æ—¶è¾“å‡ºè°ƒè¯•ä¿¡æ¯
  useEffect(() => {
    if (code && hasRendered) {
      // å»¶è¿Ÿæ‰§è¡Œï¼Œç¡®ä¿DOMå·²ç»æ¸²æŸ“å®Œæˆ
      setTimeout(() => {
        debugNodeRelations();
      }, 1000);
    }
  }, [code, hasRendered, debugNodeRelations]);

  // å®‰å…¨çš„çŠ¶æ€æ›´æ–°å‡½æ•°
  const safeSetState = useCallback((setter, value) => {
    try {
      setter(value);
    } catch (error) {
      console.warn('çŠ¶æ€æ›´æ–°å¤±è´¥:', error);
    }
  }, []);

  // å®‰å…¨çš„DOMæ“ä½œå‡½æ•°
  const safeDOMOperation = useCallback((operation) => {
    if (containerRef.current) {
      try {
        return operation();
      } catch (error) {
        console.warn('DOM operation failed:', error);
        return false;
      }
    }
    return false;
  }, []);

  // ç¼©æ”¾æ§åˆ¶å‡½æ•°
  const handleZoomIn = useCallback(() => {
    setScale(prev => Math.min(prev * 1.2, 3));
  }, []);

  const handleZoomOut = useCallback(() => {
    setScale(prev => Math.max(prev / 1.2, 0.3));
  }, []);

  const handleReset = useCallback(() => {
    setScale(1);
    setPosition({ x: 0, y: 0 });
  }, []);

  // é¼ æ ‡æ»šè½®ç¼©æ”¾
  const handleWheel = useCallback((e) => {
    e.preventDefault();
    const delta = e.deltaY > 0 ? 0.9 : 1.1;
    setScale(prev => Math.max(0.3, Math.min(3, prev * delta)));
  }, []);

  // æ‹–æ‹½å¼€å§‹
  const handleMouseDown = useCallback((e) => {
    // æ£€æŸ¥æ˜¯å¦ç‚¹å‡»çš„æ˜¯èŠ‚ç‚¹å…ƒç´ 
    const target = e.target;
    const isNodeElement = target.closest('g[class*="node"], g[data-id], g[id]');
    
    // å¦‚æœç‚¹å‡»çš„æ˜¯èŠ‚ç‚¹ï¼Œä¸å¯åŠ¨æ‹–æ‹½
    if (isNodeElement) {
      console.log('ğŸ–±ï¸ [æ‹–æ‹½å¤„ç†] ç‚¹å‡»çš„æ˜¯èŠ‚ç‚¹ï¼Œä¸å¯åŠ¨æ‹–æ‹½');
      return;
    }

    if (e.button === 0) { // å·¦é”®
      e.preventDefault(); // é˜²æ­¢é»˜è®¤æ‹–æ‹½è¡Œä¸º
      setIsDragging(true);
      setDragStart({
        x: e.clientX - position.x,
        y: e.clientY - position.y
      });
    }
  }, [position]);

  // åˆ›å»ºäº‹ä»¶å¤„ç†å‡½æ•°
  useEffect(() => {
    handleMouseMoveRef.current = (e) => {
      if (isDragging && e) {
        e.preventDefault && e.preventDefault();
        const newX = e.clientX - dragStart.x;
        const newY = e.clientY - dragStart.y;
        
        // å­˜å‚¨å¾…æ›´æ–°çš„ä½ç½®
        pendingPosition.current = { x: newX, y: newY };
        
        // å¦‚æœè¿˜æ²¡æœ‰å®‰æ’æ›´æ–°ï¼Œåˆ™å®‰æ’ä¸€ä¸ª
        if (!dragAnimationFrame.current) {
          dragAnimationFrame.current = requestAnimationFrame(() => {
            if (pendingPosition.current) {
              setPosition(pendingPosition.current);
              pendingPosition.current = null;
            }
            dragAnimationFrame.current = null;
          });
        }
      }
    };

    handleMouseUpRef.current = (e) => {
      e && e.preventDefault && e.preventDefault();
      setIsDragging(false);
      // æ¸…ç†å¾…å¤„ç†çš„åŠ¨ç”»å¸§
      if (dragAnimationFrame.current) {
        cancelAnimationFrame(dragAnimationFrame.current);
        dragAnimationFrame.current = null;
      }
      // å¦‚æœæœ‰å¾…å¤„ç†çš„ä½ç½®æ›´æ–°ï¼Œç«‹å³åº”ç”¨
      if (pendingPosition.current) {
        setPosition(pendingPosition.current);
        pendingPosition.current = null;
      }
    };
  }, [isDragging, dragStart]);

  // ç®¡ç†äº‹ä»¶ç›‘å¬å™¨
  useEffect(() => {
    // ä½¿ç”¨å±€éƒ¨å˜é‡å­˜å‚¨äº‹ä»¶å¤„ç†å‡½æ•°çš„å¼•ç”¨ï¼Œé¿å…é—­åŒ…é—®é¢˜
    let localHandleMouseMove = null;
    let localHandleMouseUp = null;
    
    const handleMouseMove = (e) => {
      if (handleMouseMoveRef.current) {
        handleMouseMoveRef.current(e);
      }
    };

    const handleMouseUp = () => {
      if (handleMouseUpRef.current) {
        handleMouseUpRef.current();
      }
    };

    if (isDragging) {
      // ä½¿ç”¨window.documentç¡®ä¿è·å–å…¨å±€documentå¯¹è±¡ï¼Œå¹¶æ£€æŸ¥addEventListeneræ–¹æ³•æ˜¯å¦å­˜åœ¨
      const globalDocument = window.document;
      if (globalDocument && typeof globalDocument.addEventListener === 'function') {
        localHandleMouseMove = handleMouseMove;
        localHandleMouseUp = handleMouseUp;
        
        globalDocument.addEventListener('mousemove', localHandleMouseMove, { passive: false });
        globalDocument.addEventListener('mouseup', localHandleMouseUp, { passive: false });
      }
    }

    // æ¸…ç†å‡½æ•° - æ·»åŠ å¤šé‡å®‰å…¨æ£€æŸ¥
    return () => {
      try {
        // ä½¿ç”¨window.documentç¡®ä¿è·å–å…¨å±€documentå¯¹è±¡
        const globalDocument = window.document;
        if (globalDocument && typeof globalDocument.removeEventListener === 'function') {
          if (localHandleMouseMove) {
            globalDocument.removeEventListener('mousemove', localHandleMouseMove);
          }
          if (localHandleMouseUp) {
            globalDocument.removeEventListener('mouseup', localHandleMouseUp);
          }
        }
      } catch (error) {
        // é™é»˜å¤„ç†æ¸…ç†é”™è¯¯ï¼Œé¿å…å½±å“åº”ç”¨è¿è¡Œ
        console.warn('æ¸…ç†äº‹ä»¶ç›‘å¬å™¨æ—¶å‡ºé”™:', error);
      }
    };
  }, [isDragging]);

  // æ£€æŸ¥DOMç¯å¢ƒæ˜¯å¦å®Œå…¨å¯ç”¨
  const checkDOMEnvironment = useCallback(() => {
    try {
      // åŸºæœ¬DOMæ£€æŸ¥
      if (typeof window === 'undefined' || typeof document === 'undefined') {
        console.warn('DOMç¯å¢ƒä¸å¯ç”¨');
        return false;
      }

      // æ£€æŸ¥å…³é”®çš„DOM API
      const requiredAPIs = [
        'createElementNS',
        'createElement',
        'querySelector',
        'querySelectorAll'
      ];

      for (const api of requiredAPIs) {
        if (!document[api]) {
          console.warn(`DOM API ${api} ä¸å¯ç”¨`);
          return false;
        }
      }

      // æ£€æŸ¥SVGæ”¯æŒ
      try {
        const testSvg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
        if (!testSvg) {
          console.warn('SVGåˆ›å»ºå¤±è´¥');
          return false;
        }
      } catch (e) {
        console.warn('SVGæ”¯æŒæ£€æŸ¥å¤±è´¥:', e);
        return false;
      }

      // æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
      if (document.readyState === 'loading') {
        console.warn('æ–‡æ¡£ä»åœ¨åŠ è½½ä¸­');
        return false;
      }

      console.log('DOMç¯å¢ƒæ£€æŸ¥é€šè¿‡');
      return true;
    } catch (error) {
      console.error('DOMç¯å¢ƒæ£€æŸ¥å¼‚å¸¸:', error);
      return false;
    }
  }, []);

  // åˆå§‹åŒ–DOMæ£€æŸ¥
  useEffect(() => {
    const initDOM = () => {
      if (checkDOMEnvironment()) {
        setDomReady(true);
        // æ³¨å…¥è‡ªå®šä¹‰CSSæ ·å¼
        try {
          injectStyles();
          console.log('Mermaidè‡ªå®šä¹‰æ ·å¼å·²æ³¨å…¥');
        } catch (error) {
          console.warn('æ³¨å…¥è‡ªå®šä¹‰æ ·å¼å¤±è´¥:', error);
        }
      } else {
        // å¦‚æœDOMè¿˜æ²¡å‡†å¤‡å¥½ï¼Œç¨åé‡è¯•
        const retryTimeout = setTimeout(() => {
          if (checkDOMEnvironment()) {
            setDomReady(true);
            // æ³¨å…¥è‡ªå®šä¹‰CSSæ ·å¼
            try {
              injectStyles();
              console.log('Mermaidè‡ªå®šä¹‰æ ·å¼å·²æ³¨å…¥');
            } catch (error) {
              console.warn('æ³¨å…¥è‡ªå®šä¹‰æ ·å¼å¤±è´¥:', error);
            }
          }
        }, 100);
        
        return () => clearTimeout(retryTimeout);
      }
    };

    // ç«‹å³æ£€æŸ¥
    initDOM();

    // ç›‘å¬DOMåŠ è½½å®Œæˆäº‹ä»¶
    const handleDOMContentLoaded = () => {
      setTimeout(() => {
        if (checkDOMEnvironment()) {
          setDomReady(true);
          // æ³¨å…¥è‡ªå®šä¹‰CSSæ ·å¼
          try {
            injectStyles();
            console.log('Mermaidè‡ªå®šä¹‰æ ·å¼å·²æ³¨å…¥');
          } catch (error) {
            console.warn('æ³¨å…¥è‡ªå®šä¹‰æ ·å¼å¤±è´¥:', error);
          }
        }
      }, 50);
    };

    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', handleDOMContentLoaded);
      return () => document.removeEventListener('DOMContentLoaded', handleDOMContentLoaded);
    }
  }, [checkDOMEnvironment]);

  // èŠ‚ç‚¹ç‚¹å‡»å¤„ç†å‡½æ•°
  const handleNodeClick = useCallback((nodeId) => {
    console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç‚¹å‡»] èŠ‚ç‚¹è¢«ç‚¹å‡»:', nodeId);
    
    if (onNodeClick && typeof onNodeClick === 'function') {
      console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç‚¹å‡»] è°ƒç”¨å›è°ƒå‡½æ•°');
      try {
        // å°†SVGèŠ‚ç‚¹IDè½¬æ¢ä¸ºä»£ç èŠ‚ç‚¹ID
        const codeNodeId = mapSvgNodeIdToCodeNodeId(nodeId);
        console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç‚¹å‡»] æ˜ å°„åçš„ä»£ç èŠ‚ç‚¹ID:', codeNodeId);
        onNodeClick(codeNodeId);
      } catch (error) {
        console.error('ğŸ–±ï¸ [èŠ‚ç‚¹ç‚¹å‡»] å›è°ƒå‡½æ•°æ‰§è¡Œå‡ºé”™:', error);
      }
    } else {
      console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç‚¹å‡»] æœªæä¾›å›è°ƒå‡½æ•°');
    }
  }, [onNodeClick, mapSvgNodeIdToCodeNodeId]);

  // è®¾ç½®èŠ‚ç‚¹ç‚¹å‡»äº‹ä»¶ç›‘å¬å™¨
  const setupNodeClickListeners = useCallback(() => {
    if (!containerRef.current) {
      console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] å®¹å™¨ä¸å­˜åœ¨ï¼Œè·³è¿‡è®¾ç½®');
      return;
    }

    try {
      const svg = containerRef.current.querySelector('svg');
      if (!svg) {
        console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] SVGå…ƒç´ ä¸å­˜åœ¨ï¼Œè·³è¿‡è®¾ç½®');
        return;
      }

      // æŸ¥æ‰¾æ‰€æœ‰èŠ‚ç‚¹å…ƒç´ 
      const nodeElements = svg.querySelectorAll('g[class*="node"], g[data-id], g[id]');
      console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] æ‰¾åˆ°èŠ‚ç‚¹å…ƒç´ æ•°é‡:', nodeElements.length);

      // ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ ç‚¹å‡»ç›‘å¬å™¨
      nodeElements.forEach((nodeElement, index) => {
        // ç§»é™¤ä¹‹å‰çš„ç›‘å¬å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        nodeElement.removeEventListener('click', nodeElement._nodeClickHandler);
        
        // è·å–èŠ‚ç‚¹ID
        const nodeId = nodeElement.getAttribute('data-id') || 
                      nodeElement.getAttribute('id') || 
                      nodeElement.className.baseVal || 
                      `node-${index}`;

        // åˆ›å»ºç‚¹å‡»å¤„ç†å‡½æ•°
        const clickHandler = (e) => {
          e.preventDefault();
          e.stopPropagation();
          console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] èŠ‚ç‚¹ç‚¹å‡»äº‹ä»¶è§¦å‘:', nodeId);
          handleNodeClick(nodeId);
        };

        // ä¿å­˜å¤„ç†å‡½æ•°å¼•ç”¨ä»¥ä¾¿åç»­ç§»é™¤
        nodeElement._nodeClickHandler = clickHandler;
        
        // æ·»åŠ ç‚¹å‡»ç›‘å¬å™¨
        nodeElement.addEventListener('click', clickHandler, { passive: false });
        
        console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] ä¸ºèŠ‚ç‚¹æ·»åŠ ç‚¹å‡»ç›‘å¬å™¨:', nodeId);
      });

      console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨è®¾ç½®å®Œæˆ');
    } catch (error) {
      console.error('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] è®¾ç½®èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨æ—¶å‡ºé”™:', error);
    }
  }, [handleNodeClick]);

  // æ¸…ç†èŠ‚ç‚¹ç‚¹å‡»äº‹ä»¶ç›‘å¬å™¨
  const cleanupNodeClickListeners = useCallback(() => {
    if (!containerRef.current) return;

    try {
      const svg = containerRef.current.querySelector('svg');
      if (!svg) return;

      const nodeElements = svg.querySelectorAll('g[class*="node"], g[data-id], g[id]');
      nodeElements.forEach(nodeElement => {
        if (nodeElement._nodeClickHandler) {
          nodeElement.removeEventListener('click', nodeElement._nodeClickHandler);
          delete nodeElement._nodeClickHandler;
        }
      });

      console.log('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨æ¸…ç†å®Œæˆ');
    } catch (error) {
      console.error('ğŸ–±ï¸ [èŠ‚ç‚¹ç›‘å¬å™¨] æ¸…ç†èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨æ—¶å‡ºé”™:', error);
    }
  }, []);

  // æ¸²æŸ“å›¾è¡¨
  const renderDiagram = useCallback(async () => {
    if (!code || isRendering || !domReady) {
      console.log('è·³è¿‡æ¸²æŸ“:', { hasCode: !!code, isRendering, domReady });
      return;
    }

    // æ£€æŸ¥å®¹å™¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å»¶è¿Ÿé‡è¯•
    if (!containerRef.current) {
      console.log('å®¹å™¨æœªæŒ‚è½½ï¼Œå»¶è¿Ÿé‡è¯•...');
      setTimeout(() => {
        if (containerRef.current) {
          renderDiagram();
        }
      }, 100);
      return;
    }

    console.log('å¼€å§‹æ¸²æŸ“Mermaidå›¾è¡¨...');
    console.log('ä»£ç é¢„è§ˆ:', code.substring(0, 100) + (code.length > 100 ? '...' : ''));

    // æ¸…ç†ä¹‹å‰çš„ç‚¹å‡»ç›‘å¬å™¨
    cleanupNodeClickListeners();

    safeSetState(setIsRendering, true);
    safeSetState(setError, null);
    safeSetState(setHasRendered, false);

    // è®¾ç½®è¶…æ—¶
    const timeoutId = setTimeout(() => {
      console.error('æ¸²æŸ“è¶…æ—¶ï¼Œå¼ºåˆ¶åœæ­¢');
      safeSetState(setIsRendering, false);
      safeSetState(setError, 'æ¸²æŸ“è¶…æ—¶ï¼Œè¯·é‡è¯•');
    }, 15000); // 15ç§’è¶…æ—¶

    try {
      // å†æ¬¡ç¡®è®¤DOMç¯å¢ƒ
      if (!checkDOMEnvironment()) {
        throw new Error('DOMç¯å¢ƒæ£€æŸ¥å¤±è´¥');
      }

      // åˆå§‹åŒ–Mermaidé…ç½®ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
      if (!mermaidInitialized) {
        console.log('åˆå§‹åŒ–Mermaidé…ç½®...');
        
        // ç¡®ä¿mermaidå¯¹è±¡å¯ç”¨
        if (!mermaid || !mermaid.initialize) {
          throw new Error('Mermaidåº“æœªæ­£ç¡®åŠ è½½');
        }

        // é‡ç½®MermaidçŠ¶æ€
        try {
          mermaid.mermaidAPI.reset && mermaid.mermaidAPI.reset();
        } catch (resetError) {
          console.warn('Mermaidé‡ç½®å¤±è´¥ï¼Œç»§ç»­åˆå§‹åŒ–:', resetError);
        }

        // é…ç½®Mermaid
        const config = {
          startOnLoad: false,
          theme: 'default',
          securityLevel: 'loose',
          fontFamily: '"Segoe UI", Tahoma, Geneva, Verdana, sans-serif',
          logLevel: 'error',
          flowchart: {
            useMaxWidth: false,
            htmlLabels: true,
            curve: 'basis'
          },
          mindmap: {
            useMaxWidth: false,
            padding: 20
          },
          // æ·»åŠ æ›´å¤šé…ç½®ä»¥ç¡®ä¿å…¼å®¹æ€§
          deterministicIds: false,
          suppressErrorRendering: false,
          // ç¡®ä¿æ­£ç¡®çš„DOMè®¿é—®
          htmlLabels: true,
          wrap: false
        };

        mermaid.initialize(config);
        
        // éªŒè¯åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        if (!mermaid.mermaidAPI) {
          throw new Error('Mermaid APIåˆå§‹åŒ–å¤±è´¥');
        }

        setMermaidInitialized(true);
        console.log('Mermaidåˆå§‹åŒ–å®Œæˆ');
      }

      // æ£€æŸ¥å®¹å™¨æ˜¯å¦å­˜åœ¨
      if (!containerRef.current) {
        throw new Error('å›¾è¡¨å®¹å™¨ä¸å­˜åœ¨');
      }

      // æ¸…ç©ºå®¹å™¨
      containerRef.current.innerHTML = '';
      console.log('å®¹å™¨å·²æ¸…ç©º');

      // æ£€æŸ¥è¯­æ³•ï¼ˆå¯é€‰ï¼Œå¦‚æœå¤±è´¥å°±è·³è¿‡ï¼‰
      try {
        console.log('æ£€æŸ¥è¯­æ³•...');
        await mermaid.parse(code);
        console.log('è¯­æ³•æ£€æŸ¥é€šè¿‡');
      } catch (parseError) {
        console.warn('è¯­æ³•æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥æ¸²æŸ“:', parseError.message);
      }

      // æ¸²æŸ“å›¾è¡¨
      console.log('å¼€å§‹æ¸²æŸ“å›¾è¡¨...');
      const renderResult = await mermaid.render(diagramId, code);
      console.log('æ¸²æŸ“å®Œæˆï¼Œç»“æœ:', renderResult ? 'æœ‰æ•°æ®' : 'æ— æ•°æ®');
      
      // æ£€æŸ¥ç»„ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
      if (!containerRef.current) {
        console.log('å®¹å™¨ä¸å­˜åœ¨ï¼Œåœæ­¢æ¸²æŸ“');
        return;
      }

      if (renderResult && renderResult.svg) {
        console.log('å¤„ç†SVGç»“æœ...');
        // ç›´æ¥æ’å…¥SVGï¼Œé¿å…å¤æ‚çš„DOMæ“ä½œ
        containerRef.current.innerHTML = renderResult.svg;
        
        const svgElement = containerRef.current.querySelector('svg');
        if (svgElement) {
          console.log('SVGå…ƒç´ æ‰¾åˆ°ï¼Œè®¾ç½®åŸºç¡€æ ·å¼...');
          // åªè®¾ç½®å¿…è¦çš„SVGåŸºç¡€æ ·å¼
          svgElement.style.maxWidth = 'none';
          svgElement.style.height = 'auto';
          svgElement.style.userSelect = 'none';
          svgElement.style.cursor = isDragging ? 'grabbing' : 'grab';
          svgElement.style.display = 'block';
          
          console.log('SVGåŸºç¡€æ ·å¼å·²è®¾ç½®ï¼Œå…¶ä½™æ ·å¼ç”±CSSæ§åˆ¶');
          
          // å¼ºåˆ¶é‡æ–°æ³¨å…¥æ ·å¼ï¼Œç¡®ä¿æ ·å¼åº”ç”¨åˆ°æ–°æ¸²æŸ“çš„SVG
          setTimeout(() => {
            console.log('ğŸ”„ å¼ºåˆ¶é‡æ–°æ³¨å…¥æ ·å¼');
            injectStyles();
          }, 50);
          
          // è®¾ç½®èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨
          setTimeout(() => {
            console.log('ğŸ–±ï¸ [æ¸²æŸ“å®Œæˆ] è®¾ç½®èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨');
            setupNodeClickListeners();
          }, 100);
          
          safeSetState(setHasRendered, true);
        } else {
          console.log('SVGå…ƒç´ æœªæ‰¾åˆ°');
          throw new Error('SVGå…ƒç´ æœªæ‰¾åˆ°');
        }
      } else {
        throw new Error('æ¸²æŸ“ç»“æœä¸ºç©º');
      }

    } catch (error) {
      console.error('ä¸»æ¸²æŸ“æ–¹æ³•å¤±è´¥:', error);
      
      // å¦‚æœæ ‡å‡†æ–¹æ³•å¤±è´¥ï¼Œå°è¯•fallbackæ–¹æ³•
      if (containerRef.current && mermaidInitialized) {
        try {
          console.log('å°è¯•fallbackæ¸²æŸ“æ–¹æ³•...');
          
          // æ£€æŸ¥mermaidAPIæ˜¯å¦å¯ç”¨
          if (!mermaid.mermaidAPI || !mermaid.mermaidAPI.render) {
            throw new Error('MermaidAPIä¸å¯ç”¨');
          }
          
          // ä½¿ç”¨å›è°ƒæ–¹å¼æ¸²æŸ“
          const fallbackPromise = new Promise((resolve, reject) => {
            const fallbackTimeout = setTimeout(() => {
              reject(new Error('Fallbackæ¸²æŸ“è¶…æ—¶'));
            }, 10000);

            try {
              mermaid.mermaidAPI.render(
                diagramId + '_fallback',
                code,
                (svg) => {
                  clearTimeout(fallbackTimeout);
                  if (containerRef.current && svg) {
                    console.log('Fallbackæ¸²æŸ“æˆåŠŸ');
                    containerRef.current.innerHTML = svg;
                    
                    const svgElement = containerRef.current.querySelector('svg');
                    if (svgElement) {
                      svgElement.style.maxWidth = 'none';
                      svgElement.style.height = 'auto';
                      svgElement.style.userSelect = 'none';
                      svgElement.style.cursor = isDragging ? 'grabbing' : 'grab';
                      
                      console.log('Fallback: SVGåŸºç¡€æ ·å¼å·²è®¾ç½®');
                      
                      // å¼ºåˆ¶é‡æ–°æ³¨å…¥æ ·å¼ï¼Œç¡®ä¿æ ·å¼åº”ç”¨åˆ°æ–°æ¸²æŸ“çš„SVG
                      setTimeout(() => {
                        console.log('ğŸ”„ Fallback: å¼ºåˆ¶é‡æ–°æ³¨å…¥æ ·å¼');
                        injectStyles();
                      }, 50);

                      // Fallbackæ¸²æŸ“åä¹Ÿè®¾ç½®èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨
                      setTimeout(() => {
                        console.log('ğŸ–±ï¸ [Fallbackæ¸²æŸ“å®Œæˆ] è®¾ç½®èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨');
                        setupNodeClickListeners();
                      }, 100);
                    }
                    
                    safeSetState(setHasRendered, true);
                    resolve(svg);
                  } else {
                    reject(new Error('Fallbackç»“æœä¸ºç©º'));
                  }
                },
                containerRef.current
              );
            } catch (apiError) {
              clearTimeout(fallbackTimeout);
              reject(apiError);
            }
          });

          await fallbackPromise;
        } catch (fallbackError) {
          console.error('Fallbackæ¸²æŸ“ä¹Ÿå¤±è´¥:', fallbackError);
          safeSetState(setError, fallbackError.message || error.message || 'å›¾è¡¨æ¸²æŸ“å¤±è´¥');
        }
      } else {
        safeSetState(setError, error.message || 'å›¾è¡¨æ¸²æŸ“å¤±è´¥');
      }
    } finally {
      clearTimeout(timeoutId);
      console.log('æ¸²æŸ“æµç¨‹ç»“æŸ');
      safeSetState(setIsRendering, false);
    }
  }, [code, isRendering, domReady, diagramId, safeSetState, isDragging, mermaidInitialized, checkDOMEnvironment, cleanupNodeClickListeners, setupNodeClickListeners]);

  // ç›‘å¬ä»£ç å˜åŒ–é‡æ–°æ¸²æŸ“
  useEffect(() => {
    if (code && !isRendering && domReady) {
      // å»¶è¿Ÿæ‰§è¡Œï¼Œç¡®ä¿DOMå·²å‡†å¤‡å¥½
      const timeoutId = setTimeout(() => {
        renderDiagram();
      }, 100); // å‡å°‘å»¶è¿Ÿæ—¶é—´ï¼Œå› ä¸ºå·²ç»æœ‰äº†DOMå‡†å¤‡æ£€æŸ¥
      
      return () => clearTimeout(timeoutId);
    }
  }, [code, domReady, renderDiagram]);

  // å¤åˆ¶ä»£ç åŠŸèƒ½
  const handleCopyCode = useCallback(async () => {
    try {
      await navigator.clipboard.writeText(code);
      safeSetState(setCopied, true);
      toast.success('Mermaidä»£ç å·²å¤åˆ¶åˆ°å‰ªè´´æ¿');
      
      // æ¸…ç†ä¹‹å‰çš„timeout
      if (copyTimeoutRef.current) {
        clearTimeout(copyTimeoutRef.current);
      }
      
      // è®¾ç½®æ–°çš„timeout
      copyTimeoutRef.current = setTimeout(() => safeSetState(setCopied, false), 2000);
    } catch (error) {
      console.error('Failed to copy:', error);
      toast.error('å¤åˆ¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶');
    }
  }, [code, safeSetState]);

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†å®šæ—¶å™¨
  useEffect(() => {
    return () => {
      if (copyTimeoutRef.current) {
        clearTimeout(copyTimeoutRef.current);
      }
      // æ¸…ç†é˜²æŠ–å®šæ—¶å™¨
      if (moveDebounceTimer.current) {
        clearTimeout(moveDebounceTimer.current);
      }
      // æ¸…ç†èŠ‚ç‚¹ç‚¹å‡»ç›‘å¬å™¨
      cleanupNodeClickListeners();
      // é‡ç½®åŠ¨ç”»çŠ¶æ€
      isAnimating.current = false;
    };
  }, [cleanupNodeClickListeners]);

  // è®¡ç®—åŒ…å«èŠ‚ç‚¹åŠå…¶å­èŠ‚ç‚¹çš„æœ€ä¼˜è§†å›¾ä½ç½®
  const calculateOptimalViewForNodes = useCallback((nodeIds) => {
    if (!containerRef.current || !parentContainerRef.current || nodeIds.length === 0) {
      return null;
    }

    try {
      const containerBounds = parentContainerRef.current.getBoundingClientRect();
      const nodes = [];

      // æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„ä½ç½®ä¿¡æ¯
      for (const nodeId of nodeIds) {
        const selectors = [
          `[data-id="${nodeId}"]`,
          `#${nodeId}`,
          `.node-${nodeId}`,
          `[id*="${nodeId}"]`,
          `[class*="${nodeId}"]`
        ];
        
        let targetNode = null;
        for (const selector of selectors) {
          const foundNodes = containerRef.current.querySelectorAll(selector);
          if (foundNodes.length > 0) {
            targetNode = foundNodes[0];
            break;
          }
        }

        if (targetNode) {
          const nodeBounds = targetNode.getBoundingClientRect();
          nodes.push({
            id: nodeId,
            bounds: nodeBounds,
            relativeLeft: nodeBounds.left - containerBounds.left,
            relativeRight: nodeBounds.right - containerBounds.left,
            relativeTop: nodeBounds.top - containerBounds.top,
            relativeBottom: nodeBounds.bottom - containerBounds.top
          });
        }
      }

      if (nodes.length === 0) {
        return null;
      }

      // è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„è¾¹ç•Œæ¡†
      const minLeft = Math.min(...nodes.map(n => n.relativeLeft));
      const maxRight = Math.max(...nodes.map(n => n.relativeRight));
      const minTop = Math.min(...nodes.map(n => n.relativeTop));
      const maxBottom = Math.max(...nodes.map(n => n.relativeBottom));

      const groupWidth = maxRight - minLeft;
      const groupHeight = maxBottom - minTop;

      console.log('ğŸ¯ [èŠ‚ç‚¹ç»„è§†å›¾] èŠ‚ç‚¹ç»„è¾¹ç•Œ:', { minLeft, maxRight, minTop, maxBottom });
      console.log('ğŸ¯ [èŠ‚ç‚¹ç»„è§†å›¾] èŠ‚ç‚¹ç»„å°ºå¯¸:', { groupWidth, groupHeight });

      // è®¾ç½®è¾¹è·
      const margin = 60;
      const containerWidth = containerBounds.width;
      const containerHeight = containerBounds.height;

      // æ£€æŸ¥æ˜¯å¦å·²ç»å®Œå…¨å¯è§
      const isGroupFullyVisible = (
        minLeft >= margin &&
        maxRight <= containerWidth - margin &&
        minTop >= margin &&
        maxBottom <= containerHeight - margin
      );

      if (isGroupFullyVisible) {
        console.log('ğŸ¯ [èŠ‚ç‚¹ç»„è§†å›¾] èŠ‚ç‚¹ç»„å·²å®Œå…¨å¯è§');
        return null;
      }

      // è®¡ç®—éœ€è¦çš„ç§»åŠ¨è·ç¦»
      let deltaX = 0;
      let deltaY = 0;

      // æ°´å¹³æ–¹å‘è°ƒæ•´
      if (minLeft < margin) {
        deltaX = margin - minLeft;
      } else if (maxRight > containerWidth - margin) {
        deltaX = (containerWidth - margin) - maxRight;
      }

      // å‚ç›´æ–¹å‘è°ƒæ•´
      if (minTop < margin) {
        deltaY = margin - minTop;
      } else if (maxBottom > containerHeight - margin) {
        deltaY = (containerHeight - margin) - maxBottom;
      }

      console.log('ğŸ¯ [èŠ‚ç‚¹ç»„è§†å›¾] è®¡ç®—çš„ç§»åŠ¨è·ç¦»:', { deltaX, deltaY });

      return { deltaX, deltaY };

    } catch (error) {
      console.error('ğŸ¯ [èŠ‚ç‚¹ç»„è§†å›¾] è®¡ç®—æœ€ä¼˜è§†å›¾æ—¶å‡ºé”™:', error);
      return null;
    }
  }, []);

  // å®é™…æ‰§è¡ŒèŠ‚ç‚¹ç§»åŠ¨çš„å‡½æ•°
  const performNodeMove = useCallback((nodeId) => {
    if (!containerRef.current || !parentContainerRef.current) {
      return;
    }

    try {
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] å¼€å§‹ç¡®ä¿èŠ‚ç‚¹å¯è§:', nodeId);
      
      // å°†SVGèŠ‚ç‚¹IDè½¬æ¢ä¸ºä»£ç èŠ‚ç‚¹ID
      const codeNodeId = mapSvgNodeIdToCodeNodeId(nodeId);
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] æ˜ å°„åçš„ä»£ç èŠ‚ç‚¹ID:', codeNodeId);
      
      // åŸºäºä»£ç è§£ææŸ¥æ‰¾å­èŠ‚ç‚¹
      const childNodes = findChildNodes(codeNodeId);
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] å‘ç°ç›´æ¥å­èŠ‚ç‚¹:', childNodes);

      // æŸ¥æ‰¾å®Œæ•´çš„é€»è¾‘é“¾æ¡
      const logicalChain = findLogicalChain(codeNodeId);
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] å®Œæ•´é€»è¾‘é“¾æ¡:', logicalChain);

      // ç¡®å®šéœ€è¦ç¡®ä¿å¯è§çš„èŠ‚ç‚¹åˆ—è¡¨
      let nodesToShow = [codeNodeId];
      
      // æ™ºèƒ½å†³ç­–ï¼šåŒ…å«é€»è¾‘é“¾æ¡ä¸­çš„èŠ‚ç‚¹
      if (childNodes.length > 0) {
        // ç®€åŒ–é€»è¾‘ï¼šåªåŒ…å«æœ‰é™çš„é“¾æ¡èŠ‚ç‚¹
        if (logicalChain.length <= 4) { // å‡å°‘åˆ°æœ€å¤š4ä¸ªèŠ‚ç‚¹
          nodesToShow = logicalChain;
          console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] åŒ…å«é€»è¾‘é“¾æ¡:', logicalChain.length, 'ä¸ªèŠ‚ç‚¹');
        } else {
          // å¦‚æœé“¾æ¡å¤ªé•¿ï¼ŒåªåŒ…å«å‰3ä¸ªèŠ‚ç‚¹
          nodesToShow = logicalChain.slice(0, 3);
          console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] é“¾æ¡è¿‡é•¿ï¼ŒåªåŒ…å«å‰3ä¸ªèŠ‚ç‚¹');
        }
      } else {
        console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] æ— å­èŠ‚ç‚¹ï¼Œåªæ˜¾ç¤ºä¸»èŠ‚ç‚¹');
      }

      // åˆ›å»ºèŠ‚ç‚¹æ˜ å°„ä»¥ä¾¿åœ¨DOMä¸­æŸ¥æ‰¾å¯¹åº”çš„SVGèŠ‚ç‚¹
      const nodeMapping = createNodeIdMapping();
      const reversedMapping = new Map(); // ä»£ç èŠ‚ç‚¹ID -> SVGèŠ‚ç‚¹ID
      nodeMapping.forEach((codeId, svgId) => {
        reversedMapping.set(codeId, svgId);
      });

      // å°†ä»£ç èŠ‚ç‚¹IDè½¬æ¢å›SVGèŠ‚ç‚¹IDè¿›è¡ŒDOMæ“ä½œ
      const svgNodesToShow = nodesToShow.map(codeId => {
        const svgId = reversedMapping.get(codeId);
        if (svgId) {
          console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] ä»£ç èŠ‚ç‚¹', codeId, 'æ˜ å°„åˆ°SVGèŠ‚ç‚¹', svgId);
          return svgId;
        }
        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ä»£ç èŠ‚ç‚¹ID
        console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] æœªæ‰¾åˆ°æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨ä»£ç èŠ‚ç‚¹ID:', codeId);
        return codeId;
      });

      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] æœ€ç»ˆéœ€è¦æ˜¾ç¤ºçš„SVGèŠ‚ç‚¹:', svgNodesToShow);

      // è®¡ç®—æœ€ä¼˜è§†å›¾ä½ç½®
      const optimalView = calculateOptimalViewForNodes(svgNodesToShow);
      
      if (!optimalView) {
        console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] èŠ‚ç‚¹å·²åœ¨æœ€ä¼˜ä½ç½®');
        return;
      }

      const { deltaX, deltaY } = optimalView;

      // å¦‚æœç§»åŠ¨è·ç¦»å¾ˆå°ï¼Œå°±ä¸ç§»åŠ¨äº†
      if (Math.abs(deltaX) < 5 && Math.abs(deltaY) < 5) {
        console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] ç§»åŠ¨è·ç¦»æå°ï¼Œæ— éœ€è°ƒæ•´');
        return;
      }

      // è®¡ç®—ç›®æ ‡ä½ç½®
      const targetX = position.x + deltaX;
      const targetY = position.y + deltaY;

      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] è®¡ç®—ç»“æœ - å½“å‰ä½ç½®:', position);
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] è®¡ç®—ç»“æœ - ç›®æ ‡ä½ç½®:', { targetX, targetY });
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] è®¡ç®—ç»“æœ - ç§»åŠ¨è·ç¦»:', { deltaX, deltaY });

      // æ ‡è®°åŠ¨ç”»å¼€å§‹
      isAnimating.current = true;
      lastMoveTime.current = Date.now();
      lastMovedNode.current = nodeId;

      // ä½¿ç”¨æ›´å¹³æ»‘çš„åŠ¨ç”»
      const distance = Math.sqrt(deltaX * deltaX + deltaY * deltaY);
      const duration = Math.min(600, Math.max(300, distance * 1.5));
      const startTime = Date.now();
      const startPosition = { ...position };

      const animate = () => {
        const elapsed = Date.now() - startTime;
        const progress = Math.min(elapsed / duration, 1);
        
        // ä½¿ç”¨æ›´å¹³æ»‘çš„ç¼“åŠ¨å‡½æ•° (ease-out-quart)
        const easeOutQuart = (t) => 1 - Math.pow(1 - t, 4);
        const easeProgress = easeOutQuart(progress);

        const currentX = startPosition.x + (targetX - startPosition.x) * easeProgress;
        const currentY = startPosition.y + (targetY - startPosition.y) * easeProgress;

        setPosition({ x: currentX, y: currentY });

        if (progress < 1) {
          requestAnimationFrame(animate);
        } else {
          console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] åŠ¨ç”»å®Œæˆï¼Œæœ€ç»ˆä½ç½®:', { x: currentX, y: currentY });
          isAnimating.current = false; // æ ‡è®°åŠ¨ç”»ç»“æŸ
        }
      };

      requestAnimationFrame(animate);

    } catch (error) {
      console.error('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] ç¡®ä¿èŠ‚ç‚¹å¯è§æ—¶å‡ºé”™:', error);
      isAnimating.current = false; // å‡ºé”™æ—¶ä¹Ÿè¦é‡ç½®åŠ¨ç”»çŠ¶æ€
    }
  }, [scale, position, findChildNodes, findLogicalChain, calculateOptimalViewForNodes, mapSvgNodeIdToCodeNodeId, createNodeIdMapping]);

  // ç¡®ä¿èŠ‚ç‚¹å®Œæ•´æ˜¾ç¤ºåœ¨å¯è§†åŒºåŸŸå†…çš„å‡½æ•° - å¸¦é˜²æŠ–
  const ensureNodeVisible = useCallback((nodeId) => {
    if (!containerRef.current || !parentContainerRef.current) {
      console.warn('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] å®¹å™¨å¼•ç”¨ä¸å­˜åœ¨');
      return;
    }

    // é˜²æŠ–æ£€æŸ¥
    const now = Date.now();
    const timeSinceLastMove = now - lastMoveTime.current;
    const minInterval = 200; // æœ€å°ç§»åŠ¨é—´éš”
    const isSameNode = lastMovedNode.current === nodeId;

    // å¦‚æœæ­£åœ¨è¿›è¡ŒåŠ¨ç”»ï¼Œè·³è¿‡
    if (isAnimating.current) {
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] åŠ¨ç”»è¿›è¡Œä¸­ï¼Œè·³è¿‡ç§»åŠ¨');
      return;
    }

    // åªå¯¹åŒä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œä¸¥æ ¼çš„æ—¶é—´æ£€æŸ¥
    if (isSameNode && timeSinceLastMove < minInterval) {
      console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] åŒä¸€èŠ‚ç‚¹ç§»åŠ¨é—´éš”å¤ªçŸ­ï¼Œè·³è¿‡ç§»åŠ¨');
      return;
    }

    // æ¸…é™¤ä¹‹å‰çš„é˜²æŠ–å®šæ—¶å™¨
    if (moveDebounceTimer.current) {
      clearTimeout(moveDebounceTimer.current);
    }

    // å¦‚æœæ˜¯ä¸åŒèŠ‚ç‚¹ï¼Œç«‹å³æ‰§è¡Œï¼›å¦‚æœæ˜¯åŒä¸€èŠ‚ç‚¹ï¼Œç¨å¾®å»¶è¿Ÿ
    const debounceDelay = isSameNode ? 50 : 10;
    
    moveDebounceTimer.current = setTimeout(() => {
      performNodeMove(nodeId);
    }, debounceDelay);
  }, [performNodeMove]);

  // æš´éœ²æ–¹æ³•ç»™çˆ¶ç»„ä»¶
  useImperativeHandle(ref, () => ({
    ensureNodeVisible,
    handleNodeClick,
    zoomIn: handleZoomIn,
    zoomOut: handleZoomOut,
    reset: handleReset
  }), [ensureNodeVisible, handleNodeClick, handleZoomIn, handleZoomOut, handleReset]);

  if (!code) {
    return (
      <div className="flex items-center justify-center h-full bg-gray-50">
        <div className="text-center text-gray-500">
          <AlertCircle className="h-8 w-8 mx-auto mb-2" />
          <p>æš‚æ— æ€ç»´å¯¼å›¾æ•°æ®</p>
        </div>
      </div>
    );
  }

  // DOMç¯å¢ƒæœªå‡†å¤‡å¥½æ—¶æ˜¾ç¤ºåŠ è½½çŠ¶æ€
  if (!domReady) {
    return (
      <div className="flex items-center justify-center h-full bg-gray-50">
        <div className="text-center">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
          <p className="text-sm text-gray-600">æ­£åœ¨åˆå§‹åŒ–æ¸²æŸ“ç¯å¢ƒ...</p>
        </div>
      </div>
    );
  }

  return (
    <div ref={parentContainerRef} className="relative h-full bg-gray-50 overflow-hidden">
      {/* æ§åˆ¶å·¥å…·æ  */}
      <div className="absolute top-2 right-2 z-10 flex space-x-1 bg-white rounded-lg shadow-sm border p-1">
        <button
          onClick={handleZoomIn}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="æ”¾å¤§"
        >
          <ZoomIn className="w-4 h-4 text-gray-600" />
        </button>
        <button
          onClick={handleZoomOut}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="ç¼©å°"
        >
          <ZoomOut className="w-4 h-4 text-gray-600" />
        </button>
        <button
          onClick={handleReset}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="é‡ç½®è§†å›¾"
        >
          <RotateCcw className="w-4 h-4 text-gray-600" />
        </button>
        <div className="w-px bg-gray-300 mx-1"></div>
        <button
          onClick={handleCopyCode}
          className="p-1 hover:bg-gray-100 rounded transition-colors"
          title="å¤åˆ¶ä»£ç "
        >
          {copied ? (
            <Check className="w-4 h-4 text-green-600" />
          ) : (
            <Copy className="w-4 h-4 text-gray-600" />
          )}
        </button>
      </div>

      {/* ç¼©æ”¾æ¯”ä¾‹æ˜¾ç¤º */}
      <div className="absolute bottom-2 right-2 z-10 bg-white rounded px-2 py-1 shadow-sm border text-xs text-gray-600">
        {Math.round(scale * 100)}%
      </div>

      {/* æ‹–æ‹½æç¤º */}
      {!isDragging && hasRendered && (
        <div className="absolute bottom-2 left-2 z-10 bg-white rounded px-2 py-1 shadow-sm border text-xs text-gray-500 flex items-center">
          <Move className="w-3 h-3 mr-1" />
          æ‹–æ‹½ç§»åŠ¨ | æ»šè½®ç¼©æ”¾
        </div>
      )}

      {/* èŠ‚ç‚¹ç‚¹å‡»æç¤º */}
      {!isDragging && hasRendered && onNodeClick && (
        <div className="absolute bottom-8 left-2 z-10 bg-white rounded px-2 py-1 shadow-sm border text-xs text-gray-500 flex items-center">
          ğŸ–±ï¸ ç‚¹å‡»èŠ‚ç‚¹è·³è½¬åˆ°å¯¹åº”æ–‡æœ¬
        </div>
      )}

      {/* å›¾è¡¨å®¹å™¨ */}
      <div 
        className="w-full h-full flex items-center justify-center overflow-hidden"
        onWheel={handleWheel}
      >
        {isRendering && (
          <div className="absolute inset-0 flex items-center justify-center bg-gray-50 bg-opacity-75 z-20">
            <div className="text-center">
              <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
              <p className="text-sm text-gray-600">æ­£åœ¨æ¸²æŸ“å›¾è¡¨...</p>
            </div>
          </div>
        )}

        {error ? (
          <div className="text-center text-red-600 p-4">
            <AlertCircle className="h-8 w-8 mx-auto mb-2" />
            <p className="font-medium mb-1">æ¸²æŸ“å¤±è´¥</p>
            <p className="text-sm">{error}</p>
            <button
              onClick={renderDiagram}
              className="mt-2 px-3 py-1 bg-red-100 text-red-700 rounded text-sm hover:bg-red-200 transition-colors"
            >
              é‡è¯•
            </button>
          </div>
        ) : (
          <div
            ref={containerRef}
            className="mermaid"
            style={{
              transform: `translate(${position.x}px, ${position.y}px) scale(${scale})`,
              transformOrigin: 'center center',
              cursor: isDragging ? 'grabbing' : 'grab',
              willChange: 'transform', // æç¤ºæµè§ˆå™¨ä¼˜åŒ–transformæ€§èƒ½
              transition: isDragging ? 'none' : 'transform 0.1s ease-out' // æ‹–æ‹½æ—¶ç¦ç”¨transitionï¼Œåœæ­¢æ—¶å¯ç”¨
            }}
            onMouseDown={handleMouseDown}
          />
        )}
      </div>
    </div>
  );
});

MermaidDiagram.displayName = 'MermaidDiagram';

export default MermaidDiagram;
</file>

<file path="frontend/src/components/UploadPage.js">
import React, { useState } from 'react';
import { useDropzone } from 'react-dropzone';
import { useNavigate } from 'react-router-dom';
import toast from 'react-hot-toast';
import { Upload, FileText, AlertCircle, Eye } from 'lucide-react';
import axios from 'axios';

const UploadPage = () => {
  const [uploading, setUploading] = useState(false);
  const navigate = useNavigate();

  const onDrop = async (acceptedFiles) => {
    const file = acceptedFiles[0];
    
    if (!file) {
      toast.error('è¯·é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶');
      return;
    }

    // éªŒè¯æ–‡ä»¶ç±»å‹
    if (!file.name.endsWith('.md') && !file.name.endsWith('.txt') && !file.name.endsWith('.pdf')) {
      toast.error('åªæ”¯æŒ .mdã€.txt å’Œ .pdf æ–‡ä»¶');
      return;
    }

    // éªŒè¯æ–‡ä»¶å¤§å° (10MBé™åˆ¶)
    if (file.size > 10 * 1024 * 1024) {
      toast.error('æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡ 10MB');
      return;
    }

    setUploading(true);
    
    try {
      const formData = new FormData();
      formData.append('file', file);

      // ä½¿ç”¨æ–°çš„æ–‡æ¡£ä¸Šä¼ API
      const response = await axios.post('http://localhost:8000/api/upload-document', formData, {
        headers: {
          'Content-Type': 'multipart/form-data',
        },
      });

      if (response.data.success) {
        const documentId = response.data.document_id;
        toast.success('æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå°†ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾...');
        
        // ç›´æ¥è·³è½¬åˆ°æŸ¥çœ‹é¡µé¢ï¼Œä½¿ç”¨è®ºè¯ç»“æ„åˆ†æ
        navigate(`/viewer/${documentId}`);
      } else {
        toast.error('ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡è¯•');
      }
    } catch (error) {
      console.error('Upload error:', error);
      const errorMessage = error.response?.data?.detail || 'ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥';
      toast.error(errorMessage);
    } finally {
      setUploading(false);
    }
  };

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: {
      'text/markdown': ['.md'],
      'text/plain': ['.txt'],
      'application/pdf': ['.pdf'],
    },
    multiple: false,
  });

  return (
    <div className="min-h-screen flex items-center justify-center p-4">
      <div className="max-w-4xl w-full">
        <div className="text-center mb-8">
          <h2 className="text-3xl font-bold text-gray-900 dark:text-white mb-4">
            æ–‡æ¡£è®ºè¯ç»“æ„åˆ†æ
          </h2>
          <p className="text-lg text-gray-600 dark:text-gray-300">
            ä¸Šä¼ æ–‡æ¡£æ–‡ä»¶ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾ï¼Œæˆ–æŸ¥çœ‹é¢„è®¾ç¤ºä¾‹
          </p>
        </div>

        <div className="space-y-6">
          <div className="text-center">
            <h3 className="text-lg font-semibold text-gray-900 dark:text-white mb-4">é€‰æ‹©ä½¿ç”¨æ–¹å¼</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300 mb-6">
              æ‚¨å¯ä»¥ä¸Šä¼ è‡ªå·±çš„æ–‡ä»¶è¿›è¡Œè®ºè¯ç»“æ„åˆ†æï¼Œæˆ–ç›´æ¥æŸ¥çœ‹é¢„è®¾çš„æµç¨‹å›¾ç¤ºä¾‹
            </p>
          </div>
          
          <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
            {/* ä¸Šä¼ æ–‡ä»¶é€‰é¡¹ */}
            <div
              {...getRootProps()}
              className={`
                border-2 border-dashed rounded-lg p-8 text-center cursor-pointer transition-all duration-200
                ${isDragActive 
                  ? 'border-blue-400 bg-blue-50 dark:bg-blue-900/20' 
                  : 'border-gray-300 dark:border-gray-600 hover:border-blue-400 dark:hover:border-blue-400 hover:bg-gray-50 dark:hover:bg-gray-800'
                }
                ${uploading ? 'pointer-events-none opacity-50' : ''}
              `}
            >
              <input {...getInputProps()} />
              
              <div className="flex flex-col items-center">
                {uploading ? (
                  <>
                    <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mb-3"></div>
                    <p className="text-sm font-medium text-gray-700 dark:text-gray-300">æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...</p>
                    <p className="text-xs text-gray-500 dark:text-gray-400 mt-1">
                      å°†åˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„
                    </p>
                  </>
                ) : (
                  <>
                    <Upload className="h-8 w-8 text-blue-400 mb-3" />
                    <h4 className="font-medium text-gray-900 dark:text-white mb-2">ä¸Šä¼ æ‚¨çš„æ–‡ä»¶</h4>
                    {isDragActive ? (
                      <p className="text-sm text-blue-600 dark:text-blue-400">
                        æ¾å¼€é¼ æ ‡ä¸Šä¼ æ–‡ä»¶
                      </p>
                    ) : (
                      <>
                        <p className="text-sm text-gray-600 dark:text-gray-300 mb-1">
                          æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„ï¼Œæˆ–ç‚¹å‡»é€‰æ‹©
                        </p>
                        <p className="text-xs text-gray-500 dark:text-gray-400">
                          æ”¯æŒ .mdã€.txtã€.pdf æ–‡ä»¶
                        </p>
                      </>
                    )}
                  </>
                )}
              </div>
            </div>

            {/* æŸ¥çœ‹é¢„è®¾ç¤ºä¾‹é€‰é¡¹ */}
            <div className="border-2 border-dashed border-gray-300 dark:border-gray-600 rounded-lg p-8 text-center">
              <div className="flex flex-col items-center">
                <Eye className="h-8 w-8 text-orange-400 mb-3" />
                <h4 className="font-medium text-gray-900 dark:text-white mb-2">æŸ¥çœ‹é¢„è®¾ç¤ºä¾‹</h4>
                <p className="text-sm text-gray-600 dark:text-gray-300 mb-4">
                  æ— éœ€ä¸Šä¼ æ–‡ä»¶ï¼Œç›´æ¥ä½“éªŒè®ºè¯ç»“æ„åˆ†ææ•ˆæœ
                </p>
                <button
                  onClick={() => {
                    toast.success('è¿›å…¥ç¤ºä¾‹æ¨¡å¼ï¼Œå°†æ˜¾ç¤ºé¢„è®¾çš„è®ºè¯ç»“æ„æµç¨‹å›¾...');
                    // åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„æ–‡æ¡£IDç”¨äºæ¼”ç¤º
                    const demoDocId = 'demo-' + Date.now();
                    navigate(`/viewer/${demoDocId}`, { 
                      state: { selectedMode: 'demo' }
                    });
                  }}
                  className="inline-flex items-center px-4 py-2 bg-orange-600 hover:bg-orange-700 dark:bg-orange-500 dark:hover:bg-orange-600 text-white text-sm font-medium rounded-lg transition-colors"
                >
                  <Eye className="h-4 w-4 mr-2" />
                  æŸ¥çœ‹ç¤ºä¾‹
                </button>
              </div>
            </div>
          </div>
          
          <div className="text-center">
            <p className="text-xs text-gray-500 dark:text-gray-400">
              ç³»ç»Ÿå°†è‡ªåŠ¨ä¸ºæ¯ä¸ªæ®µè½åˆ†é…IDå·ï¼Œå¹¶ç”Ÿæˆè®ºè¯ç»“æ„çš„å¯è§†åŒ–æµç¨‹å›¾
            </p>
          </div>
        </div>

        <div className="mt-8 grid grid-cols-1 md:grid-cols-3 gap-6">
          <div className="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-sm border border-gray-200 dark:border-gray-700">
            <FileText className="h-8 w-8 text-green-600 dark:text-green-400 mb-3" />
            <h3 className="font-semibold text-gray-900 dark:text-white mb-2">å³æ—¶é˜…è¯»</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300">
              ä¸Šä¼ åç«‹å³æ˜¾ç¤ºæ–‡æ¡£å†…å®¹ï¼Œæ¯ä¸ªæ®µè½è‡ªåŠ¨åˆ†é…IDå·
            </p>
          </div>
          
          <div className="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-sm border border-gray-200 dark:border-gray-700">
            <AlertCircle className="h-8 w-8 text-blue-600 dark:text-blue-400 mb-3" />
            <h3 className="font-semibold text-gray-900 dark:text-white mb-2">AI è®ºè¯åˆ†æ</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300">
              æ™ºèƒ½æå–æ–‡æ¡£çš„æ ¸å¿ƒè®ºè¯ç»“æ„ï¼Œç”Ÿæˆå¯è§†åŒ–æµç¨‹å›¾
            </p>
          </div>

          <div className="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-sm border border-gray-200 dark:border-gray-700">
            <Eye className="h-8 w-8 text-orange-600 dark:text-orange-400 mb-3" />
            <h3 className="font-semibold text-gray-900 dark:text-white mb-2">ç¤ºä¾‹ä½“éªŒ</h3>
            <p className="text-sm text-gray-600 dark:text-gray-300">
              æ— éœ€ä¸Šä¼ æ–‡ä»¶å³å¯ä½“éªŒå®Œæ•´çš„è®ºè¯ç»“æ„åˆ†æåŠŸèƒ½
            </p>
          </div>
        </div>

        <div className="mt-8 text-center">
          <p className="text-sm text-gray-500 dark:text-gray-400">
            ç³»ç»Ÿå°†åˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„ï¼Œä¸ºæ¯ä¸ªæ®µè½åˆ†é…IDï¼Œå¹¶ç”ŸæˆåŒ…å«èŠ‚ç‚¹æ˜ å°„çš„æµç¨‹å›¾
          </p>
        </div>
      </div>
    </div>
  );
};

export default UploadPage;
</file>

<file path="frontend/src/hooks/useScrollDetection.js">
import { useState, useEffect, useCallback, useRef } from 'react';

// ç®€æ˜“èŠ‚æµå‡½æ•°å®ç°
const throttle = (func, limit) => {
  let inThrottle;
  return function() {
    const args = arguments;
    const context = this;
    if (!inThrottle) {
      func.apply(context, args);
      inThrottle = true;
      setTimeout(() => inThrottle = false, limit);
    }
  }
};

// æ–‡æœ¬å—åˆ°èŠ‚ç‚¹çš„æ˜ å°„å…³ç³» - æ¼”ç¤ºæ¨¡å¼ä½¿ç”¨
const textToNodeMap = {
  "text-A-introduction": "A",
  "text-B-fourth-party": "B", 
  "text-C-vanishing-mediator-core": "C",
  "text-D-mechanism": "D",
  "text-D1D2D3-mechanism-stages": "D", // åŒ…å«äº†D1, D2, D3çš„é€»è¾‘
  "text-E-examples-intro": "E", // Eçš„å¼•è¨€éƒ¨åˆ†
  "text-E1-protestantism": "E1",
  "text-E2-jacobinism": "E2", 
  "text-E3-other-examples": "E3",
  "text-F-mediator-illusion": "F",
  "text-G-beautiful-soul-analogy": "G",
  "text-H-mediator-event-subject": "H",
  "text-H1-subject-definition": "H1",
  "text-H2-action-retroactive": "H2",
  "text-H3-positing-presuppositions": "H3",
  "text-I-truth-political-intro": "I",
  "text-I1-politics-vs-thepolitical": "I1",
  "text-I2-thepolitical-explanation": "I2",
  "text-I3-origin-of-order-political": "I3",
  "text-J-conclusion-subject-as-mediator": "J", // Jçš„æ ¸å¿ƒè®ºç‚¹
  "text-J1-subject-fourth-element": "J1", // J1ä¸Jå†…å®¹ç´§å¯†ï¼Œå¯å…±ç”¨æˆ–ç»†åˆ†
  "text-K-truth-contingency-trauma": "K", // Kçš„å¼•è¨€å’Œæ ¸å¿ƒæ€æƒ³
  "text-K1-analogy-greimas-lacan": "K1", // åŒ…å«ä¸¤ä¸ªçŸ©é˜µç±»æ¯”å’Œç²¾ç¥åˆ†æçš„é˜è¿°
  "text-K2-truth-revelation": "K2" // K2çš„æ ¸å¿ƒæ€æƒ³åœ¨K1ä¸­å…³äºç²¾ç¥åˆ†æçš„éƒ¨åˆ†å·²é˜æ˜
};

// èŠ‚ç‚¹åˆ°æ–‡æœ¬å—çš„æ˜ å°„å…³ç³» - ç”¨äºèŠ‚ç‚¹ç‚¹å‡»è·³è½¬
const nodeToTextMap = {
  "A": "text-A-introduction",
  "B": "text-B-fourth-party", 
  "C": "text-C-vanishing-mediator-core",
  "D": "text-D-mechanism",
  "D1": "text-D1D2D3-mechanism-stages", // D1, D2, D3éƒ½æ˜ å°„åˆ°åŒä¸€ä¸ªæ–‡æœ¬å—
  "D2": "text-D1D2D3-mechanism-stages",
  "D3": "text-D1D2D3-mechanism-stages",
  "E": "text-E-examples-intro",
  "E1": "text-E1-protestantism",
  "E2": "text-E2-jacobinism", 
  "E3": "text-E3-other-examples",
  "F": "text-F-mediator-illusion",
  "G": "text-G-beautiful-soul-analogy",
  "H": "text-H-mediator-event-subject",
  "H1": "text-H1-subject-definition",
  "H2": "text-H2-action-retroactive",
  "H3": "text-H3-positing-presuppositions",
  "I": "text-I-truth-political-intro",
  "I1": "text-I1-politics-vs-thepolitical",
  "I2": "text-I2-thepolitical-explanation",
  "I3": "text-I3-origin-of-order-political",
  "J": "text-J-conclusion-subject-as-mediator",
  "J1": "text-J1-subject-fourth-element",
  "K": "text-K-truth-contingency-trauma",
  "K1": "text-K1-analogy-greimas-lacan",
  "K2": "text-K2-truth-revelation"
};

export const useScrollDetection = (containerRef, documentId, currentMindmapMode, mermaidDiagramRef) => {
  // å½“å‰æ´»åŠ¨æ®µè½å’ŒèŠ‚ç‚¹IDçš„çŠ¶æ€
  const [activeContentBlockId, setActiveContentBlockId] = useState(null);
  const [activeChunkId, setActiveChunkId] = useState(null);

  // å­˜å‚¨æ®µè½æ˜ å°„å…³ç³»
  const [dynamicTextToNodeMap, setDynamicTextToNodeMap] = useState({});
  const [dynamicNodeToTextMap, setDynamicNodeToTextMap] = useState({});

  // å­˜å‚¨é™æ€æ˜ å°„å…³ç³»
  const [textToNodeMap, setTextToNodeMap] = useState({});
  const [nodeToTextMap, setNodeToTextMap] = useState({});

  // å­˜å‚¨å†…å®¹å—/æ®µè½çš„å¼•ç”¨
  const contentBlockRefs = useRef(new Map());
  const sectionRefs = useRef(new Map());

  // å­˜å‚¨å†…å®¹å—
  const [contentChunks, setContentChunks] = useState([]);

  // å­˜å‚¨ä¹‹å‰çš„æ´»åŠ¨èŠ‚ç‚¹ï¼Œç”¨äºä¼˜åŒ–é«˜äº®æ€§èƒ½
  const [previousActiveNode, setPreviousActiveNode] = useState(null);
  
  // é™æ€æ˜ å°„å…³ç³» - ç”¨äºç¤ºä¾‹æ–‡æ¡£çš„å›ºå®šæ˜ å°„
  const staticNodeToTextMap = {
    'A': 'text-block-0',
    'B': 'text-block-1',
    'C': 'text-block-2',
    'D': 'text-block-3',
    'E': 'text-block-4',
    'F': 'text-block-5',
    'G': 'text-block-6',
    'H': 'text-block-7',
    'I': 'text-block-8',
    'J': 'text-block-9',
  };

  // åˆå§‹åŒ–é™æ€æ˜ å°„
  useEffect(() => {
    // åˆ›å»ºåå‘æ˜ å°„ textToNodeMap
    const reverseMapping = {};
    Object.keys(staticNodeToTextMap).forEach(nodeId => {
      const textId = staticNodeToTextMap[nodeId];
      reverseMapping[textId] = nodeId;
    });
    
    setTextToNodeMap(reverseMapping);
    setNodeToTextMap(staticNodeToTextMap);
    
    console.log('ğŸ“Š [é™æ€æ˜ å°„] åˆå§‹åŒ–å®Œæˆ');
    console.log('ğŸ“Š [é™æ€æ˜ å°„] èŠ‚ç‚¹åˆ°æ–‡æœ¬æ˜ å°„:', staticNodeToTextMap);
    console.log('ğŸ“Š [é™æ€æ˜ å°„] æ–‡æœ¬åˆ°èŠ‚ç‚¹æ˜ å°„:', reverseMapping);
  }, []);

  // onSectionRef å›è°ƒå®ç° - ä»…ç”¨äºç›®å½•å¯¼èˆªï¼Œä¸å½±å“æ®µè½é«˜äº®
  const handleSectionRef = useCallback((element, chunkId) => {
    if (element) {
      sectionRefs.current.set(chunkId, element);
      console.log('ğŸ“ [ç« èŠ‚å¼•ç”¨] è®¾ç½®ç« èŠ‚å¼•ç”¨ (ä»…ç”¨äºç›®å½•):', chunkId, 'æ€»æ•°:', sectionRefs.current.size);
    } else {
      sectionRefs.current.delete(chunkId);
      console.log('ğŸ“ [ç« èŠ‚å¼•ç”¨] ç§»é™¤ç« èŠ‚å¼•ç”¨:', chunkId, 'å‰©ä½™:', sectionRefs.current.size);
    }
  }, []);

  // å†…å®¹å—å¼•ç”¨å›è°ƒ - ç”¨äºæ®µè½çº§æ£€æµ‹å’Œé«˜äº®
  const handleContentBlockRef = useCallback((element, blockId) => {
    if (element) {
      contentBlockRefs.current.set(blockId, element);
      console.log('ğŸ“ [æ®µè½å¼•ç”¨] è®¾ç½®æ®µè½å¼•ç”¨:', blockId, 'æ€»æ•°:', contentBlockRefs.current.size);
      
      // è®¾ç½®åˆå§‹æ£€æµ‹ï¼Œç¡®ä¿é¡µé¢åŠ è½½åç«‹å³æ£€æµ‹å½“å‰é˜…è¯»çš„æ®µè½
      setTimeout(() => {
        if (contentBlockRefs.current.size > 0) {
          console.log('ğŸ“ [æ®µè½å¼•ç”¨] è§¦å‘æ®µè½æ£€æµ‹ï¼Œå› ä¸ºæœ‰æ–°æ®µè½è¢«æ³¨å†Œ');
          // æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡æ»šåŠ¨æ£€æµ‹äº‹ä»¶
          const event = new Event('scroll');
          const scrollContainer = containerRef.current?.querySelector('.overflow-y-auto');
          if (scrollContainer) {
            scrollContainer.dispatchEvent(event);
          } else {
            window.dispatchEvent(event);
          }
        }
      }, 200);
    } else {
      contentBlockRefs.current.delete(blockId);
      console.log('ğŸ“ [æ®µè½å¼•ç”¨] ç§»é™¤æ®µè½å¼•ç”¨:', blockId, 'å‰©ä½™:', contentBlockRefs.current.size);
    }
  }, []);

  // é«˜äº®MermaidèŠ‚ç‚¹
  const highlightMermaidNode = useCallback((nodeId) => {
    // ç¡®ä¿åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ä¸”documentå¯ç”¨
    if (typeof window === 'undefined' || !window.document || typeof window.document.querySelectorAll !== 'function') {
      console.warn('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] DOMç¯å¢ƒä¸å¯ç”¨ï¼Œè·³è¿‡èŠ‚ç‚¹é«˜äº®');
      return;
    }

    try {
      console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] å¼€å§‹é«˜äº®èŠ‚ç‚¹:', nodeId);
      
      // å®šä¹‰é«˜äº®åº”ç”¨å‡½æ•°
      const applyHighlighting = () => {
        // ğŸ”‘ å…³é”®ä¿®å¤ï¼šåªæœ‰å½“éœ€è¦åˆ‡æ¢åˆ°ä¸åŒèŠ‚ç‚¹æ—¶ï¼Œæ‰ç§»é™¤ä¹‹å‰çš„é«˜äº®
        if (previousActiveNode && previousActiveNode !== nodeId) {
          console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] ç§»é™¤ä¹‹å‰çš„é«˜äº®ï¼ˆä¸åŒèŠ‚ç‚¹ï¼‰:', previousActiveNode, 'â†’', nodeId);
          const prevSelectors = [
            `[data-id="${previousActiveNode}"]`,
            `#${previousActiveNode}`,
            `.node-${previousActiveNode}`,
            `[id*="${previousActiveNode}"]`
          ];
          
          let foundPrev = false;
          prevSelectors.forEach(selector => {
            const prevNodes = window.document.querySelectorAll(selector);
            if (prevNodes.length > 0) {
              foundPrev = true;
              console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] æ‰¾åˆ°ä¹‹å‰çš„èŠ‚ç‚¹:', selector, prevNodes.length);
            }
            prevNodes.forEach(node => {
              if (node && node.classList) {
                node.classList.remove('mermaid-highlighted-node');
              }
            });
          });
          
          if (!foundPrev) {
            console.warn('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] æœªæ‰¾åˆ°ä¹‹å‰çš„èŠ‚ç‚¹:', previousActiveNode);
          }
        } else if (previousActiveNode === nodeId) {
          console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] ç‚¹å‡»åŒä¸€èŠ‚ç‚¹ï¼Œä¿æŒç°æœ‰é«˜äº®:', nodeId);
        }

        // æ·»åŠ æ–°çš„é«˜äº®ï¼ˆå³ä½¿æ˜¯åŒä¸€èŠ‚ç‚¹ä¹Ÿç¡®ä¿é«˜äº®å­˜åœ¨ï¼‰
        if (nodeId) {
          console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] ç¡®ä¿èŠ‚ç‚¹é«˜äº®:', nodeId);
          
          // ä½¿ç”¨ç²¾ç®€çš„é€‰æ‹©å™¨åˆ—è¡¨ï¼Œé¿å…é‡å¤æ“ä½œ
          const selectors = [
            `[data-id="${nodeId}"]`,
            `#${nodeId}`,
            `[id*="${nodeId}"]`,
            `g[data-id="${nodeId}"]`
          ];
          
          console.log('ğŸ¯ [èŠ‚ç‚¹æœç´¢] å°è¯•çš„é€‰æ‹©å™¨åˆ—è¡¨:', selectors);
          
          let foundCurrent = false;
          let foundElements = [];
          
          selectors.forEach((selector, index) => {
            try {
              const currentNodes = window.document.querySelectorAll(selector);
              if (currentNodes.length > 0) {
                foundCurrent = true;
                foundElements.push(...currentNodes);
                console.log(`ğŸ¯ [èŠ‚ç‚¹é«˜äº®] é€‰æ‹©å™¨ ${index + 1} æˆåŠŸåŒ¹é…: ${selector} (æ‰¾åˆ° ${currentNodes.length} ä¸ªå…ƒç´ )`);
                currentNodes.forEach((node, nodeIndex) => {
                  if (node && node.classList) {
                    // ç¡®ä¿æ·»åŠ é«˜äº®class
                    if (!node.classList.contains('mermaid-highlighted-node')) {
                      node.classList.add('mermaid-highlighted-node');
                      console.log(`ğŸ¯ [èŠ‚ç‚¹é«˜äº®] æ–°å¢é«˜äº®èŠ‚ç‚¹ ${nodeIndex + 1}:`, {
                        tagName: node.tagName,
                        id: node.id,
                        dataId: node.getAttribute('data-id'),
                        className: node.className,
                        selector: selector
                      });
                    } else {
                      console.log(`ğŸ¯ [èŠ‚ç‚¹é«˜äº®] èŠ‚ç‚¹å·²é«˜äº® ${nodeIndex + 1}:`, selector);
                    }
                  }
                });
              } else {
                console.log(`ğŸ¯ [èŠ‚ç‚¹æœç´¢] é€‰æ‹©å™¨ ${index + 1} æ— åŒ¹é…: ${selector}`);
              }
            } catch (error) {
              console.warn(`ğŸ¯ [èŠ‚ç‚¹æœç´¢] é€‰æ‹©å™¨ ${index + 1} æ‰§è¡Œå‡ºé”™: ${selector}`, error);
            }
          });
          
          if (!foundCurrent) {
            console.warn('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°èŠ‚ç‚¹:', nodeId);
            
            // è¾“å‡ºè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            console.log('ğŸ” [è°ƒè¯•åˆ†æ] å¼€å§‹åˆ†æé¡µé¢ä¸­çš„æ‰€æœ‰å¯èƒ½èŠ‚ç‚¹...');
            
            // æŸ¥æ‰¾æ‰€æœ‰Mermaidç›¸å…³å…ƒç´ 
            const allMermaidElements = window.document.querySelectorAll('[class*="node"], [data-id], [id], g, .mermaid *');
            console.log('ğŸ” [è°ƒè¯•åˆ†æ] é¡µé¢ä¸­æ‰€æœ‰å¯èƒ½çš„Mermaidå…ƒç´ æ•°é‡:', allMermaidElements.length);
            
            // ç­›é€‰å‡ºå¯èƒ½ä¸ç›®æ ‡èŠ‚ç‚¹ç›¸å…³çš„å…ƒç´ 
            const relevantElements = Array.from(allMermaidElements).filter(el => {
              const id = el.id || '';
              const dataId = el.getAttribute('data-id') || '';
              const className = el.className || '';
              
              return id.includes(nodeId) || 
                     dataId.includes(nodeId) || 
                     className.includes(nodeId) ||
                     // æ£€æŸ¥æ˜¯å¦åŒ…å«èŠ‚ç‚¹IDçš„ä»»ä½•éƒ¨åˆ†
                     (nodeId.length > 1 && (id.includes(nodeId.substring(0, nodeId.length-1)) || 
                                           dataId.includes(nodeId.substring(0, nodeId.length-1))));
            });
            
            console.log(`ğŸ” [è°ƒè¯•åˆ†æ] ä¸èŠ‚ç‚¹ "${nodeId}" ç›¸å…³çš„å…ƒç´  (${relevantElements.length} ä¸ª):`, 
              relevantElements.map(el => ({
                tagName: el.tagName,
                id: el.id,
                dataId: el.getAttribute('data-id'),
                className: el.className.substring(0, 100),
                textContent: el.textContent?.substring(0, 50)
              }))
            );
            
            // ç‰¹åˆ«æ£€æŸ¥æ˜¯å¦æœ‰ç±»ä¼¼çš„èŠ‚ç‚¹ID
            const allDataIds = Array.from(allMermaidElements)
              .map(el => el.getAttribute('data-id'))
              .filter(Boolean);
            const allIds = Array.from(allMermaidElements)
              .map(el => el.id)
              .filter(Boolean);
            
            console.log('ğŸ” [è°ƒè¯•åˆ†æ] æ‰€æœ‰data-idå€¼:', [...new Set(allDataIds)]);
            console.log('ğŸ” [è°ƒè¯•åˆ†æ] æ‰€æœ‰idå€¼:', [...new Set(allIds)]);
            
            // æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„ID
            const similarIds = [...new Set([...allDataIds, ...allIds])].filter(id => 
              id.toLowerCase().includes(nodeId.toLowerCase()) ||
              nodeId.toLowerCase().includes(id.toLowerCase())
            );
            console.log(`ğŸ” [è°ƒè¯•åˆ†æ] ä¸ "${nodeId}" ç›¸ä¼¼çš„ID:`, similarIds);
            
          } else {
            console.log(`ğŸ¯ [èŠ‚ç‚¹é«˜äº®] æˆåŠŸæ‰¾åˆ°å¹¶ç¡®ä¿é«˜äº® ${foundElements.length} ä¸ªå…ƒç´ `);
          }
          
          console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] é«˜äº®èŠ‚ç‚¹å®Œæˆ:', nodeId);
          
          // è‡ªåŠ¨ç¡®ä¿é«˜äº®çš„èŠ‚ç‚¹å¯è§
          if (foundCurrent && mermaidDiagramRef && mermaidDiagramRef.current) {
            console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] å°è¯•ç¡®ä¿èŠ‚ç‚¹å¯è§:', nodeId);
            // å»¶è¿Ÿä¸€ç‚¹æ—¶é—´ç¡®ä¿é«˜äº®æ ·å¼å·²åº”ç”¨
            setTimeout(() => {
              try {
                mermaidDiagramRef.current.ensureNodeVisible(nodeId);
                console.log('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] æˆåŠŸè°ƒç”¨ensureNodeVisibleæ–¹æ³•');
              } catch (error) {
                console.error('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] è°ƒç”¨ensureNodeVisibleå¤±è´¥:', error);
              }
            }, 100); // å‡å°‘å»¶è¿Ÿæ—¶é—´ï¼Œè®©å“åº”æ›´å¿«
          } else {
            console.warn('ğŸ¯ [èŠ‚ç‚¹å¯è§æ€§] æ— æ³•ç¡®ä¿èŠ‚ç‚¹å¯è§ï¼ŒåŸå› :', {
              foundCurrent,
              hasMermaidRef: !!mermaidDiagramRef,
              hasCurrentRef: !!(mermaidDiagramRef && mermaidDiagramRef.current)
            });
          }
        }
      };

      // ç«‹å³å°è¯•åº”ç”¨é«˜äº®
      applyHighlighting();

      // ğŸ”‘ å‡å°‘é‡è¯•æ¬¡æ•°åˆ°1æ¬¡ï¼Œé¿å…è¿‡åº¦æ“ä½œ
      const retryTimeouts = [100];
      retryTimeouts.forEach(delay => {
        setTimeout(() => {
          console.log(`ğŸ¯ [èŠ‚ç‚¹é«˜äº®] å»¶è¿Ÿ${delay}msé‡è¯•é«˜äº®:`, nodeId);
          applyHighlighting();
        }, delay);
      });

      // è®¾ç½®MutationObserverç›‘å¬Mermaidå›¾è¡¨å˜åŒ–
      if (nodeId) {
        const mermaidContainer = window.document.querySelector('.mermaid, [data-processed-by-mermaid]');
        if (mermaidContainer) {
          console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] è®¾ç½®MutationObserverç›‘å¬å›¾è¡¨å˜åŒ–');
          
          // æ¸…é™¤ä¹‹å‰çš„è§‚å¯Ÿè€…
          if (window.mermaidMutationObserver) {
            window.mermaidMutationObserver.disconnect();
          }
          
          window.mermaidMutationObserver = new MutationObserver((mutations) => {
            let shouldReapply = false;
            mutations.forEach(mutation => {
              if (mutation.type === 'childList' || mutation.type === 'attributes') {
                shouldReapply = true;
              }
            });
            
            if (shouldReapply) {
              console.log('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] æ£€æµ‹åˆ°Mermaidå›¾è¡¨å˜åŒ–ï¼Œé‡æ–°åº”ç”¨é«˜äº®');
              setTimeout(() => {
                applyHighlighting();
              }, 100);
            }
          });
          
          window.mermaidMutationObserver.observe(mermaidContainer, {
            childList: true,
            subtree: true,
            attributes: true,
            attributeFilter: ['class', 'style']
          });
        }
      }
      
      setPreviousActiveNode(nodeId);
    } catch (error) {
      console.error('ğŸ¯ [èŠ‚ç‚¹é«˜äº®] é«˜äº®èŠ‚ç‚¹æ—¶å‡ºé”™:', error);
    }
  }, [previousActiveNode, mermaidDiagramRef]);

  // é«˜äº®æ®µè½å†…å®¹å—
  const highlightParagraph = useCallback((blockId) => {
    // ç¡®ä¿åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ä¸”documentå¯ç”¨
    if (typeof window === 'undefined' || !window.document || typeof window.document.querySelectorAll !== 'function') {
      console.warn('ğŸ¯ [æ®µè½é«˜äº®] DOMç¯å¢ƒä¸å¯ç”¨ï¼Œè·³è¿‡æ®µè½é«˜äº®');
      return;
    }

    try {
      console.log('ğŸ¯ [æ®µè½é«˜äº®] å¼€å§‹é«˜äº®æ®µè½:', blockId);
      
      // ğŸ”‘ å…³é”®ä¿®å¤ï¼šæ™ºèƒ½é«˜äº®é€»è¾‘
      const allElements = window.document.querySelectorAll('.paragraph-block, .content-block, [id^="para-"], [data-para-id], [id^="text-"], [id^="chunk-"]');
      console.log('ğŸ¯ [æ®µè½é«˜äº®] æ‰¾åˆ°æ‰€æœ‰æ®µè½å…ƒç´ æ•°é‡:', allElements.length);
      
      // é¦–å…ˆæ£€æŸ¥ç›®æ ‡æ®µè½çš„å½“å‰çŠ¶æ€
      const targetElement = contentBlockRefs.current.get(blockId);
      const targetCurrentlyHighlighted = targetElement?.classList?.contains('semantic-paragraph-highlighted');
      console.log('ğŸ¯ [æ®µè½é«˜äº®] ç›®æ ‡æ®µè½å½“å‰é«˜äº®çŠ¶æ€:', blockId, 'â†’', targetCurrentlyHighlighted);
      
      // ç§»é™¤å…¶ä»–æ®µè½çš„é«˜äº®ï¼Œä½†ä¿æŠ¤ç›®æ ‡æ®µè½
      let removedCount = 0;
      allElements.forEach(element => {
        const elementId = element.id || element.getAttribute('data-para-id');
        if (element && element.classList && elementId !== blockId) {
          if (element.classList.contains('semantic-paragraph-highlighted')) {
            element.classList.remove('semantic-paragraph-highlighted');
            removedCount++;
            console.log('ğŸ¯ [æ®µè½é«˜äº®] ç§»é™¤å…¶ä»–æ®µè½çš„é«˜äº®:', elementId);
          }
        }
      });
      console.log('ğŸ¯ [æ®µè½é«˜äº®] æ€»å…±ç§»é™¤äº†', removedCount, 'ä¸ªæ®µè½çš„é«˜äº®');

      // ç¡®ä¿ç›®æ ‡æ®µè½è¢«é«˜äº®
      if (blockId && targetElement) {
        if (!targetElement.classList.contains('semantic-paragraph-highlighted')) {
          targetElement.classList.add('semantic-paragraph-highlighted');
          console.log('ğŸ¯ [æ®µè½é«˜äº®] âœ… æˆåŠŸé«˜äº®ç›®æ ‡æ®µè½:', blockId);
        } else {
          console.log('ğŸ¯ [æ®µè½é«˜äº®] âœ… ç›®æ ‡æ®µè½å·²é«˜äº®ï¼ŒçŠ¶æ€ä¿æŒ:', blockId);
        }
        
        // éªŒè¯é«˜äº®çŠ¶æ€
        const finalHighlighted = targetElement.classList.contains('semantic-paragraph-highlighted');
        console.log('ğŸ¯ [æ®µè½é«˜äº®] æœ€ç»ˆéªŒè¯ - ç›®æ ‡æ®µè½é«˜äº®çŠ¶æ€:', blockId, 'â†’', finalHighlighted);
        
        // ç¡®ä¿æ®µè½å¯è§ï¼ˆæ»šåŠ¨åˆ°è§†å›¾ä¸­ï¼‰
        const rect = targetElement.getBoundingClientRect();
        const viewportHeight = window.innerHeight;
        const isVisible = rect.top >= 0 && rect.bottom <= viewportHeight;
        
        if (!isVisible) {
          console.log('ğŸ¯ [æ®µè½é«˜äº®] æ®µè½ä¸å®Œå…¨å¯è§ï¼Œæ»šåŠ¨åˆ°è§†å›¾ä¸­');
          targetElement.scrollIntoView({ 
            behavior: 'smooth', 
            block: 'center' 
          });
        }
      } else {
        console.warn('ğŸ¯ [æ®µè½é«˜äº®] âŒ æœªæ‰¾åˆ°ç›®æ ‡æ®µè½å…ƒç´ :', blockId);
        console.warn('ğŸ¯ [æ®µè½é«˜äº®] contentBlockRefsä¸­çš„æ‰€æœ‰é”®:', Array.from(contentBlockRefs.current.keys()));
      }
    } catch (error) {
      console.error('ğŸ¯ [æ®µè½é«˜äº®] é«˜äº®æ®µè½æ—¶å‡ºé”™:', error);
    }
  }, []);

  // æ®µè½æ£€æµ‹å‡½æ•° - ä¸“é—¨ç”¨äºæ£€æµ‹å½“å‰é˜…è¯»çš„æ®µè½
  const determineActiveParagraph = useCallback(() => {
    const viewportHeight = window.innerHeight;
    const anchorY = viewportHeight * 0.4; // è§†å£é¡¶éƒ¨å‘ä¸‹40%ä½œä¸ºé˜…è¯»é”šç‚¹

    console.log('ğŸ“– [æ®µè½æ£€æµ‹] å¼€å§‹æ£€æµ‹å½“å‰é˜…è¯»æ®µè½ï¼Œé”šç‚¹Y:', anchorY, 'æ®µè½æ•°é‡:', contentBlockRefs.current.size);
    console.log('ğŸ“– [æ®µè½æ£€æµ‹] å½“å‰çŠ¶æ€ - åŠ¨æ€æ˜ å°„æ•°é‡:', Object.keys(dynamicTextToNodeMap).length);
    console.log('ğŸ“– [æ®µè½æ£€æµ‹] å½“å‰çŠ¶æ€ - é™æ€æ˜ å°„æ•°é‡:', Object.keys(textToNodeMap).length);

    let currentActiveParagraphId = null;
    let bestDistance = Infinity;

    // éå†æ‰€æœ‰æ®µè½å—ï¼Œæ‰¾åˆ°æœ€æ¥è¿‘é˜…è¯»é”šç‚¹çš„æ®µè½
    contentBlockRefs.current.forEach((element, blockId) => {
      const rect = element.getBoundingClientRect();
      
      // è®¡ç®—æ®µè½ä¸­å¿ƒç‚¹åˆ°é˜…è¯»é”šç‚¹çš„è·ç¦»
      const paragraphCenter = rect.top + rect.height / 2;
      const distance = Math.abs(paragraphCenter - anchorY);
      
      console.log(`ğŸ“– [æ®µè½æ£€æµ‹] æ®µè½ ${blockId}: top=${rect.top.toFixed(1)}, center=${paragraphCenter.toFixed(1)}, bottom=${rect.bottom.toFixed(1)}, distance=${distance.toFixed(1)}`);
      
      // ç¡®ä¿æ®µè½åœ¨è§†å£ä¸­ä¸”è·ç¦»é˜…è¯»é”šç‚¹æœ€è¿‘
      if (rect.top < viewportHeight && rect.bottom > 0 && distance < bestDistance) {
        currentActiveParagraphId = blockId;
        bestDistance = distance;
        console.log(`ğŸ“– [æ®µè½æ£€æµ‹] æ®µè½ ${blockId} æˆä¸ºæœ€ä½³å€™é€‰ï¼Œè·ç¦»=${distance.toFixed(1)}`);
      }
    });

    console.log(`ğŸ“– [æ®µè½æ£€æµ‹] æœ€ç»ˆç¡®å®šæ´»åŠ¨æ®µè½: ${currentActiveParagraphId}`);

    // æ›´æ–°æ´»åŠ¨æ®µè½çŠ¶æ€
    setActiveContentBlockId(prevId => {
      // ğŸ”‘ å…³é”®ä¿®å¤ï¼šæ— è®ºæ®µè½æ˜¯å¦å˜æ›´ï¼Œéƒ½è¦ç¡®ä¿é«˜äº®çŠ¶æ€æ­£ç¡®
      if (currentActiveParagraphId) {
        if (prevId !== currentActiveParagraphId) {
          console.log("ğŸ“– [æ®µè½æ£€æµ‹] æ´»åŠ¨æ®µè½å˜æ›´:", prevId, "â†’", currentActiveParagraphId);
        } else {
          console.log("ğŸ“– [æ®µè½æ£€æµ‹] æ®µè½æœªå˜æ›´ï¼Œä½†ç¡®ä¿é«˜äº®çŠ¶æ€æ­£ç¡®:", currentActiveParagraphId);
        }
        
        // ğŸ”‘ æ— è®ºæ®µè½æ˜¯å¦å˜æ›´ï¼Œéƒ½è¦ç¡®ä¿é«˜äº®æ­£ç¡®
        highlightParagraph(currentActiveParagraphId);
        
        // ä¼˜å…ˆä½¿ç”¨åŠ¨æ€æ˜ å°„ï¼Œåªæœ‰åœ¨åŠ¨æ€æ˜ å°„ä¸ºç©ºæ—¶æ‰ä½¿ç”¨é™æ€æ˜ å°„
        const hasDynamicMapping = Object.keys(dynamicTextToNodeMap).length > 0;
        const currentTextToNodeMap = hasDynamicMapping ? dynamicTextToNodeMap : textToNodeMap;
        const nodeId = currentTextToNodeMap[currentActiveParagraphId];
        
        console.log('ğŸ” [èŠ‚ç‚¹æ˜ å°„æ£€æŸ¥] æ®µè½ID:', currentActiveParagraphId);
        console.log('ğŸ” [èŠ‚ç‚¹æ˜ å°„æ£€æŸ¥] åŠ¨æ€æ˜ å°„æ•°é‡:', Object.keys(dynamicTextToNodeMap).length);
        console.log('ğŸ” [èŠ‚ç‚¹æ˜ å°„æ£€æŸ¥] é™æ€æ˜ å°„æ•°é‡:', Object.keys(textToNodeMap).length);
        console.log('ğŸ” [èŠ‚ç‚¹æ˜ å°„æ£€æŸ¥] ä½¿ç”¨æ˜ å°„ç±»å‹:', hasDynamicMapping ? 'åŠ¨æ€æ˜ å°„' : 'é™æ€æ˜ å°„');
        console.log('ğŸ” [èŠ‚ç‚¹æ˜ å°„æ£€æŸ¥] æ˜ å°„è¡¨å‰5ä¸ªé”®:', Object.keys(currentTextToNodeMap).slice(0, 5));
        console.log('ğŸ” [èŠ‚ç‚¹æ˜ å°„æ£€æŸ¥] æ‰¾åˆ°èŠ‚ç‚¹ID:', nodeId);
        
        if (nodeId) {
          console.log('ğŸ“– [æ®µè½æ£€æµ‹] âœ… æ‰¾åˆ°å¯¹åº”èŠ‚ç‚¹ï¼Œå¼€å§‹é«˜äº®:', nodeId);
          highlightMermaidNode(nodeId);
        } else {
          console.warn('ğŸ“– [æ®µè½æ£€æµ‹] âŒ æœªæ‰¾åˆ°æ®µè½å¯¹åº”çš„èŠ‚ç‚¹æ˜ å°„:', currentActiveParagraphId);
          
          // è¯¦ç»†è°ƒè¯•ä¿¡æ¯
          if (hasDynamicMapping) {
            console.log('ğŸ” [è°ƒè¯•] åŠ¨æ€æ˜ å°„è¯¦æƒ…:', dynamicTextToNodeMap);
            // æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç±»ä¼¼çš„é”®
            const similarKeys = Object.keys(dynamicTextToNodeMap).filter(key => 
              key.includes(currentActiveParagraphId.replace('para-', '')) || 
              currentActiveParagraphId.includes(key.replace('para-', ''))
            );
            console.log('ğŸ” [è°ƒè¯•] ç›¸ä¼¼çš„é”®:', similarKeys);
          } else {
            console.log('ğŸ” [è°ƒè¯•] é™æ€æ˜ å°„è¯¦æƒ…:', Object.keys(textToNodeMap));
          }
          
          // å¦‚æœæ˜¯ä¸Šä¼ æ¨¡å¼ä¸”æ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œè¿™æ˜¯ä¸€ä¸ªé—®é¢˜
          if (currentActiveParagraphId.startsWith('para-') && !hasDynamicMapping) {
            console.error('âŒ [ä¸¥é‡é”™è¯¯] ä¸Šä¼ æ–‡æ¡£ä½¿ç”¨äº†é™æ€æ˜ å°„ï¼åŠ¨æ€æ˜ å°„åº”è¯¥å·²ç»åˆ›å»º');
          }
        }
      }
      
      return currentActiveParagraphId;
    });
  }, [highlightParagraph, highlightMermaidNode, textToNodeMap, dynamicTextToNodeMap]);

  // ç­‰å¾…Mermaidå›¾è¡¨æ¸²æŸ“å®Œæˆçš„æ£€æŸ¥å‡½æ•° - ç§»åˆ°é¡¶å±‚ä½œç”¨åŸŸ
  const waitForMermaidRender = useCallback(() => {
    return new Promise((resolve) => {
      const checkMermaid = () => {
        const mermaidElement = window.document?.querySelector('.mermaid, [data-processed-by-mermaid]');
        const svgElement = mermaidElement?.querySelector('svg');
        
        if (svgElement && svgElement.children.length > 0) {
          console.log('ğŸ¨ [Mermaidæ£€æŸ¥] Mermaidå›¾è¡¨å·²æ¸²æŸ“å®Œæˆ');
          resolve(true);
        } else {
          console.log('ğŸ¨ [Mermaidæ£€æŸ¥] ç­‰å¾…Mermaidå›¾è¡¨æ¸²æŸ“...');
          setTimeout(checkMermaid, 200);
        }
      };
      
      checkMermaid();
      
      // è¶…æ—¶ä¿æŠ¤
      setTimeout(() => {
        console.log('ğŸ¨ [Mermaidæ£€æŸ¥] Mermaidå›¾è¡¨æ¸²æŸ“æ£€æŸ¥è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ');
        resolve(false);
      }, 5000);
    });
  }, []);

  // åˆå§‹åŒ–æ£€æµ‹å‡½æ•° - ç»Ÿä¸€ä½¿ç”¨å†…å®¹å—æ£€æµ‹
  const initializeDetection = useCallback(async () => {
    console.log('ğŸ¨ [ç»Ÿä¸€æ¨¡å¼] å¼€å§‹åˆå§‹åŒ–å†…å®¹å—æ£€æµ‹ï¼Œæ–‡æ¡£ID:', documentId);
    
    // ç«‹å³å¯åŠ¨æ®µè½æ£€æµ‹ï¼Œä¸ç­‰å¾…æ€ç»´å¯¼å›¾æ¸²æŸ“
    setTimeout(() => {
      console.log('ğŸ¨ [ç»Ÿä¸€æ¨¡å¼] æ‰§è¡Œåˆå§‹å†…å®¹å—æ£€æµ‹');
      determineActiveParagraph();
    }, 300);
    
    // å¦‚æœå­˜åœ¨æ€ç»´å¯¼å›¾ï¼Œé¢å¤–ç­‰å¾…æ¸²æŸ“å®Œæˆåå†æ¬¡æ£€æµ‹
    const mermaidElement = window.document?.querySelector('.mermaid, [data-processed-by-mermaid]');
    if (mermaidElement) {
      console.log('ğŸ¨ [æ€ç»´å¯¼å›¾æ£€æµ‹] å‘ç°æ€ç»´å¯¼å›¾ï¼Œç­‰å¾…æ¸²æŸ“å®Œæˆ');
      await waitForMermaidRender();
      setTimeout(() => {
        console.log('ğŸ¨ [æ€ç»´å¯¼å›¾æ£€æµ‹] æ€ç»´å¯¼å›¾æ¸²æŸ“å®Œæˆï¼Œé‡æ–°æ‰§è¡Œæ®µè½æ£€æµ‹');
        determineActiveParagraph();
      }, 100);
    }
  }, [documentId, waitForMermaidRender, determineActiveParagraph]);

  // æ®µè½çº§æ»šåŠ¨æ£€æµ‹é€»è¾‘ - ä½¿ç”¨ç¨³å®šçš„å¼•ç”¨é¿å…é‡å¤æ‰§è¡Œ
  useEffect(() => {
    console.log('ğŸ”§ [æ®µè½æ»šåŠ¨æ£€æµ‹] useEffectè§¦å‘ï¼Œæ–‡æ¡£ID:', documentId);
    console.log('ğŸ”§ [æ®µè½æ»šåŠ¨æ£€æµ‹] å½“å‰åŠ¨æ€æ˜ å°„æ•°é‡:', Object.keys(dynamicTextToNodeMap).length);
    console.log('ğŸ”§ [æ®µè½æ»šåŠ¨æ£€æµ‹] å½“å‰é™æ€æ˜ å°„æ•°é‡:', Object.keys(textToNodeMap).length);
    
    // ğŸ”‘ ç®€åŒ–æ»šåŠ¨å¤„ç†ï¼šåªè°ƒç”¨ç»Ÿä¸€çš„æ£€æµ‹å‡½æ•°ï¼Œé¿å…é‡å¤é€»è¾‘
    const throttledHandler = throttle(() => {
      if (contentBlockRefs.current.size > 0) {
        console.log('ğŸ“œ [æ»šåŠ¨äº‹ä»¶] è§¦å‘æ®µè½æ£€æµ‹ï¼Œå½“å‰æ®µè½æ•°é‡:', contentBlockRefs.current.size);
        // ç›´æ¥è°ƒç”¨ç»Ÿä¸€çš„æ®µè½æ£€æµ‹å‡½æ•°ï¼Œé¿å…é‡å¤å®ç°
        determineActiveParagraph();
      } else {
        console.log('ğŸ“œ [æ»šåŠ¨äº‹ä»¶] æ²¡æœ‰å¯ç”¨çš„æ®µè½è¿›è¡Œæ£€æµ‹');
      }
    }, 200);

    // æŸ¥æ‰¾æ»šåŠ¨å®¹å™¨
    let scrollContainer = null;
    
    const findScrollContainer = () => {
      if (containerRef.current) {
        const selectors = [
          '.overflow-y-auto',
          '[style*="overflow-y: auto"]',
          '[style*="overflow: auto"]',
          '.h-full.overflow-hidden.flex.flex-col > div:last-child',
        ];
        
        for (const selector of selectors) {
          scrollContainer = containerRef.current.querySelector(selector);
          if (scrollContainer) {
            console.log('ğŸ“œ [æ»šåŠ¨æ£€æµ‹] æ‰¾åˆ°æ»šåŠ¨å®¹å™¨ï¼Œé€‰æ‹©å™¨:', selector);
            return scrollContainer;
          }
        }
        
        // é€šè¿‡æ ·å¼æ£€æµ‹
        const allElements = containerRef.current.querySelectorAll('*');
        for (const el of allElements) {
          const style = window.getComputedStyle(el);
          if (style.overflowY === 'auto' || style.overflowY === 'scroll' || 
              style.overflow === 'auto' || style.overflow === 'scroll') {
            scrollContainer = el;
            console.log('ğŸ“œ [æ»šåŠ¨æ£€æµ‹] é€šè¿‡æ ·å¼æ£€æµ‹æ‰¾åˆ°æ»šåŠ¨å®¹å™¨:', el.className);
            return scrollContainer;
          }
        }
      }
      return null;
    };

    // å»¶è¿Ÿè®¾ç½®ç›‘å¬å™¨ï¼Œç¡®ä¿DOMå·²ç»æ¸²æŸ“
    const setupScrollListener = () => {
      scrollContainer = findScrollContainer();
      
      if (scrollContainer) {
        console.log('ğŸ“œ [æ»šåŠ¨æ£€æµ‹] æ·»åŠ æ»šåŠ¨ç›‘å¬åˆ°å®¹å™¨');
        scrollContainer.addEventListener('scroll', throttledHandler, { passive: true });
      } else {
        console.log('ğŸ“œ [æ»šåŠ¨æ£€æµ‹] æœªæ‰¾åˆ°æ»šåŠ¨å®¹å™¨ï¼Œä½¿ç”¨windowæ»šåŠ¨ç›‘å¬');
        window.addEventListener('scroll', throttledHandler, { passive: true });
      }
      
      window.addEventListener('resize', throttledHandler, { passive: true });
    };

    // å»¶è¿Ÿè®¾ç½®ï¼Œç¡®ä¿DOMå®Œå…¨åŠ è½½
    const timer = setTimeout(setupScrollListener, 300);

    return () => {
      console.log('ğŸ”§ [æ®µè½æ»šåŠ¨æ£€æµ‹] æ¸…ç†äº‹ä»¶ç›‘å¬å™¨');
      clearTimeout(timer);
      if (scrollContainer) {
        scrollContainer.removeEventListener('scroll', throttledHandler);
      } else {
        window.removeEventListener('scroll', throttledHandler);
      }
      window.removeEventListener('resize', throttledHandler);
    };
  }, [documentId, determineActiveParagraph]); // åªä¾èµ–determineActiveParagraphå‡½æ•°

  // ç»Ÿä¸€çš„åˆå§‹åŒ–æ£€æµ‹ - åœ¨å†…å®¹åŠ è½½å®Œæˆåå¯åŠ¨
  useEffect(() => {
    console.log('ğŸ¨ [ç»Ÿä¸€åˆå§‹åŒ–] å¯åŠ¨åˆå§‹åŒ–æ£€æµ‹');
    
    // å¯åŠ¨åˆå§‹åŒ–æ£€æµ‹
    const timer = setTimeout(() => {
      initializeDetection();
    }, 200); // ç¨å¾®å»¶è¿Ÿç¡®ä¿DOMå·²å‡†å¤‡å¥½
    
    return () => {
      clearTimeout(timer);
    };
  }, [initializeDetection]); // åªä¾èµ–åˆå§‹åŒ–å‡½æ•°

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†MutationObserver
  useEffect(() => {
    return () => {
      if (typeof window !== 'undefined' && window.mermaidMutationObserver) {
        console.log('ğŸ§¹ [æ¸…ç†] æ–­å¼€MutationObserverè¿æ¥');
        window.mermaidMutationObserver.disconnect();
        window.mermaidMutationObserver = null;
      }
    };
  }, []);

  // ç›‘å¬åŠ¨æ€æ˜ å°„çŠ¶æ€å˜åŒ–ï¼Œç¡®ä¿çŠ¶æ€æ›´æ–°åé‡æ–°æ£€æµ‹
  useEffect(() => {
    const dynamicMappingCount = Object.keys(dynamicTextToNodeMap).length;
    console.log('ğŸ”„ [æ˜ å°„çŠ¶æ€ç›‘å¬] åŠ¨æ€æ˜ å°„çŠ¶æ€å˜åŒ–ï¼Œæ•°é‡:', dynamicMappingCount);
    
    if (dynamicMappingCount > 0) {
      console.log('ğŸ”„ [æ˜ å°„çŠ¶æ€ç›‘å¬] æ£€æµ‹åˆ°åŠ¨æ€æ˜ å°„å·²åˆ›å»ºï¼Œè§¦å‘æ®µè½é‡æ–°æ£€æµ‹');
      
      // å»¶è¿Ÿä¸€ç‚¹æ—¶é—´ç¡®ä¿çŠ¶æ€å®Œå…¨æ›´æ–°
      const timer = setTimeout(() => {
        console.log('ğŸ”„ [æ˜ å°„çŠ¶æ€ç›‘å¬] æ‰§è¡Œå»¶è¿Ÿæ®µè½æ£€æµ‹');
        determineActiveParagraph();
      }, 100);
      
      return () => clearTimeout(timer);
    }
  }, [dynamicTextToNodeMap, determineActiveParagraph]);

  const scrollToSection = (item) => {
    const element = sectionRefs.current.get(item.id);
    if (element) {
      element.scrollIntoView({ 
        behavior: 'smooth', 
        block: 'start' 
      });
    }
  };

  // æ ¹æ®èŠ‚ç‚¹IDæ»šåŠ¨åˆ°å¯¹åº”çš„è¯­ä¹‰å—ï¼ˆæ”¯æŒå¤šæ®µè½é«˜äº®ï¼‰
  const scrollToContentBlock = useCallback((nodeId) => {
    console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] å¼€å§‹æŸ¥æ‰¾èŠ‚ç‚¹å¯¹åº”çš„è¯­ä¹‰å—:', nodeId);
    console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] å½“å‰æ–‡æ¡£ID:', documentId);
    
    // ä¼˜å…ˆä½¿ç”¨åŠ¨æ€æ˜ å°„ï¼Œå›é€€åˆ°é™æ€æ˜ å°„
    const currentNodeToTextMap = Object.keys(dynamicNodeToTextMap).length > 0 ? dynamicNodeToTextMap : nodeToTextMap;
    const textBlockIds = currentNodeToTextMap[nodeId];
    
    console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] ä½¿ç”¨æ˜ å°„ç±»å‹:', Object.keys(dynamicNodeToTextMap).length > 0 ? 'semantic' : 'static');
    console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] åŠ¨æ€æ˜ å°„æ•°é‡:', Object.keys(dynamicNodeToTextMap).length);
    console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æ‰¾åˆ°çš„æ–‡æœ¬å—:', textBlockIds);
    
    if (!textBlockIds) {
      console.warn('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æœªæ‰¾åˆ°èŠ‚ç‚¹å¯¹åº”çš„æ–‡æœ¬å—æ˜ å°„:', nodeId);
      console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] å¯ç”¨çš„èŠ‚ç‚¹æ˜ å°„:', Object.keys(currentNodeToTextMap));
      console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] åŠ¨æ€æ˜ å°„è¯¦æƒ…:', dynamicNodeToTextMap);
      console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] é™æ€æ˜ å°„è¯¦æƒ…:', nodeToTextMap);
      return;
    }

    // å¤„ç†è¯­ä¹‰æ˜ å°„ï¼ˆæ•°ç»„ï¼‰æˆ–é™æ€æ˜ å°„ï¼ˆå­—ç¬¦ä¸²ï¼‰
    const targetIds = Array.isArray(textBlockIds) ? textBlockIds : [textBlockIds];
    console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] ç›®æ ‡æ®µè½/å—IDåˆ—è¡¨:', targetIds);

    // æŸ¥æ‰¾å¹¶é«˜äº®æ‰€æœ‰ç›¸å…³çš„æ®µè½
    const foundElements = [];
    let primaryElement = null;

    targetIds.forEach(blockId => {
      // é¦–å…ˆå°è¯•æŸ¥æ‰¾æ®µè½å…ƒç´ ï¼ˆpara-Xæ ¼å¼ï¼‰
      let element = null;
      
      if (blockId.startsWith('para-')) {
        // æŸ¥æ‰¾æ®µè½å…ƒç´ 
        element = window.document?.getElementById(blockId) || 
                 window.document?.querySelector(`[data-para-id="${blockId}"]`) ||
                 contentBlockRefs.current.get(blockId);
        
        if (element) {
          console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æ‰¾åˆ°æ®µè½å…ƒç´ :', blockId);
          foundElements.push({ element, id: blockId, type: 'paragraph' });
          
          // å°†ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ®µè½ä½œä¸ºä¸»è¦æ»šåŠ¨ç›®æ ‡
          if (!primaryElement) {
            primaryElement = element;
          }
        } else {
          console.warn('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æœªæ‰¾åˆ°æ®µè½å…ƒç´ :', blockId);
        }
      } else {
        // æŸ¥æ‰¾å†…å®¹å—å…ƒç´ ï¼ˆchunk-Xæ ¼å¼ï¼‰
        element = contentBlockRefs.current.get(blockId);
        
        if (element) {
          console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æ‰¾åˆ°å†…å®¹å—å…ƒç´ :', blockId);
          foundElements.push({ element, id: blockId, type: 'block' });
          
          if (!primaryElement) {
            primaryElement = element;
          }
        } else {
          console.warn('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æœªæ‰¾åˆ°å†…å®¹å—å…ƒç´ :', blockId);
        }
      }
    });

    if (foundElements.length > 0) {
      console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æ‰¾åˆ°', foundElements.length, 'ä¸ªç›¸å…³å…ƒç´ ');
      
      // æ»šåŠ¨åˆ°ä¸»è¦å…ƒç´  - å°†ç›®æ ‡æ®µè½æ”¾åœ¨è§†å£40%ä½ç½®
      if (primaryElement) {
        console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æ»šåŠ¨åˆ°ä¸»è¦å…ƒç´ ï¼ˆ40%ä½ç½®ï¼‰');
        
        // æŸ¥æ‰¾æ»šåŠ¨å®¹å™¨ï¼ˆä¼˜å…ˆæŸ¥æ‰¾.overflow-y-autoå®¹å™¨ï¼‰
        let scrollContainer = null;
        if (containerRef.current) {
          const selectors = [
            '.overflow-y-auto',
            '[style*="overflow-y: auto"]',
            '[style*="overflow: auto"]'
          ];
          
          for (const selector of selectors) {
            scrollContainer = containerRef.current.querySelector(selector);
            if (scrollContainer) {
              console.log('ğŸ“œ [æ»šåŠ¨å®¹å™¨] æ‰¾åˆ°æ»šåŠ¨å®¹å™¨ï¼Œé€‰æ‹©å™¨:', selector);
              break;
            }
          }
        }
        
        if (scrollContainer) {
          // ä½¿ç”¨å®¹å™¨æ»šåŠ¨
          const containerRect = scrollContainer.getBoundingClientRect();
          const elementRect = primaryElement.getBoundingClientRect();
          
          // è®¡ç®—å…ƒç´ ç›¸å¯¹äºæ»šåŠ¨å®¹å™¨çš„ä½ç½®
          const elementRelativeTop = elementRect.top - containerRect.top + scrollContainer.scrollTop;
          
          // è®¡ç®—å®¹å™¨é«˜åº¦çš„40%
          const containerHeight = scrollContainer.clientHeight;
          const targetOffset = containerHeight * 0.35;
          
          // è®¡ç®—æ»šåŠ¨ä½ç½®ï¼šå…ƒç´ é¡¶éƒ¨ - å®¹å™¨40%ä½ç½®
          const scrollTo = elementRelativeTop - targetOffset;
          
          console.log('ğŸ“œ [æ»šåŠ¨è®¡ç®—] ä½¿ç”¨å®¹å™¨æ»šåŠ¨ - å…ƒç´ ç›¸å¯¹ä½ç½®:', elementRelativeTop, 'å®¹å™¨40%åç§»:', targetOffset, 'ç›®æ ‡æ»šåŠ¨ä½ç½®:', scrollTo);
          
          // å¹³æ»‘æ»šåŠ¨åˆ°ç›®æ ‡ä½ç½®
          scrollContainer.scrollTo({
            top: Math.max(0, scrollTo), // ç¡®ä¿ä¸æ»šåŠ¨åˆ°è´Ÿæ•°ä½ç½®
            behavior: 'smooth'
          });
          
          // ğŸ”‘ æ»šåŠ¨å®Œæˆåç«‹å³è§¦å‘æ®µè½æ£€æµ‹ï¼Œç¡®ä¿é«˜äº®å¿«é€Ÿå“åº”
          setTimeout(() => {
            console.log('ğŸ“œ [æ»šåŠ¨å®Œæˆ] è§¦å‘æ®µè½æ£€æµ‹ä»¥æ›´æ–°é«˜äº®');
            determineActiveParagraph();
          }, 200); // å‡å°‘å»¶è¿Ÿï¼Œæé«˜å“åº”é€Ÿåº¦
          
        } else {
          // å›é€€åˆ°çª—å£æ»šåŠ¨
          console.log('ğŸ“œ [æ»šåŠ¨è®¡ç®—] æœªæ‰¾åˆ°å®¹å™¨ï¼Œä½¿ç”¨windowæ»šåŠ¨');
          const elementRect = primaryElement.getBoundingClientRect();
          const elementTop = elementRect.top + window.pageYOffset;
          
          // è®¡ç®—è§†å£é«˜åº¦çš„40%
          const viewportHeight = window.innerHeight;
          const targetOffset = viewportHeight * 0.35;
          
          // è®¡ç®—æ»šåŠ¨ä½ç½®ï¼šå…ƒç´ é¡¶éƒ¨ - è§†å£40%ä½ç½®
          const scrollTo = elementTop - targetOffset;
          
          console.log('ğŸ“œ [æ»šåŠ¨è®¡ç®—] å…ƒç´ é¡¶éƒ¨ä½ç½®:', elementTop, 'è§†å£40%åç§»:', targetOffset, 'ç›®æ ‡æ»šåŠ¨ä½ç½®:', scrollTo);
          
          // å¹³æ»‘æ»šåŠ¨åˆ°ç›®æ ‡ä½ç½®
          window.scrollTo({
            top: Math.max(0, scrollTo), // ç¡®ä¿ä¸æ»šåŠ¨åˆ°è´Ÿæ•°ä½ç½®
            behavior: 'smooth'
          });
          
          // ğŸ”‘ æ»šåŠ¨å®Œæˆåç«‹å³è§¦å‘æ®µè½æ£€æµ‹ï¼Œç¡®ä¿é«˜äº®å¿«é€Ÿå“åº”
          setTimeout(() => {
            console.log('ğŸ“œ [æ»šåŠ¨å®Œæˆ] è§¦å‘æ®µè½æ£€æµ‹ä»¥æ›´æ–°é«˜äº®');
            determineActiveParagraph();
          }, 200); // å‡å°‘å»¶è¿Ÿï¼Œæé«˜å“åº”é€Ÿåº¦
        }
      }
      
      console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æ»šåŠ¨å®Œæˆï¼Œæ®µè½æ£€æµ‹å°†è‡ªåŠ¨å¤„ç†é«˜äº®');
    } else {
      console.warn('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡å…ƒç´ ');
      console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] å¯ç”¨çš„å†…å®¹å—:', Array.from(contentBlockRefs.current.keys()));
      
      // è¾“å‡ºDOMä¸­æ‰€æœ‰å¯èƒ½çš„æ®µè½å…ƒç´ è¿›è¡Œè°ƒè¯•
      const allParaElements = window.document?.querySelectorAll('[id^="para-"], [data-para-id]');
      if (allParaElements && allParaElements.length > 0) {
        console.log('ğŸ“œ [è¯­ä¹‰å—æ»šåŠ¨] DOMä¸­çš„æ®µè½å…ƒç´ :', Array.from(allParaElements).map(el => el.id || el.getAttribute('data-para-id')));
      }
    }
  }, [documentId, dynamicNodeToTextMap, containerRef, determineActiveParagraph]);

  // è°ƒè¯•è¾…åŠ©å‡½æ•°
  const debugScrollDetection = useCallback(() => {
    console.log('ğŸ” [è°ƒè¯•ä¿¡æ¯] æ»šåŠ¨æ£€æµ‹çŠ¶æ€:');
    console.log('  - å½“å‰æ´»åŠ¨æ®µè½:', activeContentBlockId);
    console.log('  - å½“å‰æ´»åŠ¨ç« èŠ‚:', activeChunkId);
    console.log('  - æ®µè½å¼•ç”¨æ•°é‡:', contentBlockRefs.current.size);
    console.log('  - ç« èŠ‚å¼•ç”¨æ•°é‡:', sectionRefs.current.size);
    console.log('  - åŠ¨æ€æ˜ å°„æ•°é‡:', Object.keys(dynamicTextToNodeMap).length);
    console.log('  - é™æ€æ˜ å°„æ•°é‡:', Object.keys(textToNodeMap).length);
    console.log('  - æ–‡æ¡£ID:', documentId);
    console.log('  - æ€ç»´å¯¼å›¾æ¨¡å¼:', currentMindmapMode);
    console.log('  - æ‰€æœ‰æ®µè½ID:', Array.from(contentBlockRefs.current.keys()));
  }, [activeContentBlockId, activeChunkId, dynamicTextToNodeMap, textToNodeMap, documentId, currentMindmapMode]);

  // å°†è°ƒè¯•å‡½æ•°æš´éœ²åˆ°å…¨å±€windowå¯¹è±¡
  useEffect(() => {
    if (typeof window !== 'undefined') {
      window.debugScrollDetection = debugScrollDetection;
      console.log('ğŸ”§ [è°ƒè¯•å·¥å…·] debugScrollDetectionå‡½æ•°å·²æŒ‚è½½åˆ°windowå¯¹è±¡ï¼Œå¯åœ¨æ§åˆ¶å°ä¸­è°ƒç”¨ window.debugScrollDetection() æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯');
    }
    
    return () => {
      if (typeof window !== 'undefined') {
        delete window.debugScrollDetection;
      }
    };
  }, [debugScrollDetection]);

  // åŠ¨æ€æ˜ å°„æ›´æ–°å‡½æ•°
  const updateDynamicMapping = useCallback((textToNodeMapping, nodeToTextMapping) => {
    console.log('ğŸ“Š [åŠ¨æ€æ˜ å°„] æ›´æ–°åŠ¨æ€æ˜ å°„å…³ç³»');
    console.log('ğŸ“Š [åŠ¨æ€æ˜ å°„] æ–‡æœ¬åˆ°èŠ‚ç‚¹æ˜ å°„é¡¹æ•°:', Object.keys(textToNodeMapping).length);
    console.log('ğŸ“Š [åŠ¨æ€æ˜ å°„] èŠ‚ç‚¹åˆ°æ–‡æœ¬æ˜ å°„é¡¹æ•°:', Object.keys(nodeToTextMapping).length);
    
    setDynamicTextToNodeMap(textToNodeMapping);
    setDynamicNodeToTextMap(nodeToTextMapping);
    
    console.log('ğŸ“Š [åŠ¨æ€æ˜ å°„] ç¤ºä¾‹æ˜ å°„æ¡ç›®:');
    Object.keys(textToNodeMapping).slice(0, 3).forEach(textId => {
      console.log(`  ${textId} -> ${textToNodeMapping[textId]}`);
    });
  }, []);

  return {
    activeChunkId,
    activeContentBlockId,
    contentChunks,
    handleSectionRef,
    handleContentBlockRef,
    scrollToSection,
    scrollToContentBlock,
    highlightParagraph,
    highlightMermaidNode,
    updateDynamicMapping, // æš´éœ²åŠ¨æ€æ˜ å°„å‡½æ•°
    dynamicMapping: { textToNodeMap: dynamicTextToNodeMap, nodeToTextMap: dynamicNodeToTextMap }, // æš´éœ²åŠ¨æ€æ˜ å°„å…³ç³»
    nodeToTextMap, // æš´éœ²é™æ€æ˜ å°„å…³ç³»ä¾›å¤–éƒ¨ä½¿ç”¨
    textToNodeMap,  // æš´éœ²é™æ€æ˜ å°„å…³ç³»ä¾›å¤–éƒ¨ä½¿ç”¨
    debugScrollDetection, // æš´éœ²è°ƒè¯•å‡½æ•°
    setActiveContentBlockId, // ğŸ”‘ æš´éœ²çŠ¶æ€è®¾ç½®å‡½æ•°ä¾›å¤–éƒ¨ç›´æ¥è°ƒç”¨
  };
};
</file>

<file path="web_backend.py">
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import asyncio
import os
import hashlib
import tempfile
import re
from datetime import datetime
from pathlib import Path
import logging
import base64
from typing import List, Dict, Any
import json

# å¯¼å…¥ç°æœ‰çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨
from mindmap_generator import MindMapGenerator, MinimalDatabaseStub, get_logger, generate_mermaid_html, DocumentOptimizer

# å¯¼å…¥æ–‡æ¡£è§£æå™¨
from document_parser import DocumentParser

# å¯¼å…¥MinerUç›¸å…³æ¨¡å—
from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.config.enums import SupportedPdfParseMethod

app = FastAPI(title="Argument Structure Analyzer API", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Reactå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®æ—¥å¿—
logger = get_logger()

# åˆ›å»ºä¸Šä¼ ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# åˆ›å»ºPDFå¤„ç†ç›®å½•
PDF_OUTPUT_DIR = Path("pdf_outputs")
PDF_OUTPUT_DIR.mkdir(exist_ok=True)

# å­˜å‚¨æ–‡æ¡£çŠ¶æ€çš„å†…å­˜æ•°æ®åº“
document_status = {}



# å­˜å‚¨æ–‡æ¡£ç»“æ„çš„å†…å­˜æ•°æ®åº“
document_structures = {}

class ArgumentStructureAnalyzer:
    """è®ºè¯ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        self.generator = MindMapGenerator()
        self.document_parser = DocumentParser()
        # æ·»åŠ DocumentOptimizerå®ä¾‹ç”¨äºAIè°ƒç”¨
        self.optimizer = DocumentOptimizer()
    
    def add_paragraph_ids(self, text: str) -> str:
        """ä¸ºæ–‡æœ¬çš„æ¯ä¸ªæ®µè½æ·»åŠ IDå·"""
        try:
            # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
            paragraphs = text.split('\n\n')
            processed_paragraphs = []
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():  # åªå¤„ç†éç©ºæ®µè½
                    # ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ IDæ ‡è®°
                    para_id = f"para-{i+1}"
                    processed_paragraph = f"[{para_id}] {paragraph.strip()}"
                    processed_paragraphs.append(processed_paragraph)
                else:
                    processed_paragraphs.append(paragraph)
            
            return '\n\n'.join(processed_paragraphs)
            
        except Exception as e:
            print(f"âŒ [æ®µè½IDæ·»åŠ é”™è¯¯] {str(e)}")
            return text
    
    def split_text_into_chunks(self, text: str, document_id: str) -> List[Dict[str, Any]]:
        """å°†æ–‡æ¡£æŒ‰Markdownæ ‡é¢˜å±‚çº§åˆ†å—å¹¶åˆ†é…å”¯ä¸€æ ‡è¯†ç¬¦"""
        try:
            # ä½¿ç”¨æ–°çš„æ–‡æ¡£è§£æå™¨
            chunks = self.document_parser.parse_to_chunks(text, document_id)
            
            # åŒæ—¶ä¿å­˜æ–‡æ¡£ç»“æ„ç”¨äºç›®å½•ç”Ÿæˆ
            root = self.document_parser.parse_document(text, document_id)
            toc = self.document_parser.generate_toc(root)
            
            document_structures[document_id] = {
                'structure': root.to_dict(),
                'toc': toc,
                'chunks': chunks
            }
            
            print(f"ğŸ“„ [æ–‡æœ¬åˆ†å—] æ–‡æ¡£ {document_id} åˆ†ä¸º {len(chunks)} ä¸ªç»“æ„åŒ–å—")
            for i, chunk in enumerate(chunks[:3]):  # æ˜¾ç¤ºå‰3ä¸ªå—çš„ä¿¡æ¯
                print(f"   å— {i}: {chunk.get('title', 'æ— æ ‡é¢˜')} (çº§åˆ« {chunk.get('level', 0)})")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ [åˆ†å—é”™è¯¯] {str(e)}")
            return []
    
    async def generate_argument_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„"""
        try:
            # æ„å»ºåŸºäºæ®µè½çš„è®ºè¯ç»“æ„åˆ†æprompt
            prompt = f"""æˆ‘å¸Œæœ›ä½ æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åˆ†æå¸ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»æˆ‘æä¾›çš„ã€å·²ç»æŒ‰æ®µè½æ ‡è®°å¥½IDçš„æ–‡æœ¬ï¼Œå¹¶åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†æ¥åˆ†æå…¶è®ºè¯ç»“æ„ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š

ç¬¬ä¸€æ­¥ï¼šæ®µè½è§’è‰²è¯†åˆ«
- åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†ï¼ˆ[para-X]æ ‡è®°ï¼‰ï¼Œåˆ†ææ¯ä¸ªæ®µè½åœ¨è®ºè¯ä¸­çš„è§’è‰²
- ä¸è¦é‡æ–°åˆ’åˆ†æ®µè½ï¼Œè€Œæ˜¯åŸºäºç°æœ‰æ®µè½æ¥ç†è§£è®ºè¯é€»è¾‘
- è¯†åˆ«æ¯ä¸ªæ®µè½æ˜¯å¼•è¨€ã€è®ºç‚¹ã€è¯æ®ã€åé©³ã€ç»“è®ºç­‰å“ªç§ç±»å‹

ç¬¬äºŒæ­¥ï¼šæ„å»ºè®ºè¯ç»“æ„æµç¨‹å›¾
- åŸºäºæ®µè½çš„è®ºè¯è§’è‰²ï¼Œæ„å»ºé€»è¾‘æµç¨‹å›¾
- å°†å…·æœ‰ç›¸åŒæˆ–ç›¸å…³è®ºè¯åŠŸèƒ½çš„æ®µè½ç»„åˆæˆé€»è¾‘èŠ‚ç‚¹
- ç”¨ç®­å¤´è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»

ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€çš„ã€å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œä¸è¦åœ¨ JSON ä»£ç å—å‰åæ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡å­—ã€‚

è¿™ä¸ª JSON å¯¹è±¡å¿…é¡»åŒ…å«ä¸‰ä¸ªé¡¶çº§é”®ï¼š"mermaid_string"ã€"node_mappings" å’Œ "edges"ã€‚

mermaid_string:
- å€¼ä¸ºç¬¦åˆ Mermaid.js è¯­æ³•çš„æµç¨‹å›¾ï¼ˆgraph TDï¼‰
- å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ç»„ç›¸å…³çš„æ®µè½ï¼ˆåŸºäºè®ºè¯åŠŸèƒ½ï¼‰
- èŠ‚ç‚¹ ID ä½¿ç”¨ç®€çŸ­çš„å­—æ¯æˆ–å­—æ¯æ•°å­—ç»„åˆï¼ˆå¦‚ï¼šA, B, C1, D2ï¼‰
- èŠ‚ç‚¹æ ‡ç­¾åº”è¯¥ç®€æ´æ¦‚æ‹¬è¯¥ç»„æ®µè½çš„æ ¸å¿ƒè®ºè¯åŠŸèƒ½ï¼ˆä¸è¶…è¿‡20å­—ï¼‰
- ä½¿ç”¨ç®­å¤´ --> è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»
- å¯ä»¥ä½¿ç”¨ä¸åŒçš„èŠ‚ç‚¹å½¢çŠ¶æ¥åŒºåˆ†ä¸åŒç±»å‹çš„è®ºè¯åŠŸèƒ½ï¼š
  - [æ–¹æ‹¬å·] ç”¨äºä¸»è¦è®ºç‚¹
  - (åœ†æ‹¬å·) ç”¨äºæ”¯æ’‘è¯æ®
  - {{èŠ±æ‹¬å·}} ç”¨äºé€»è¾‘è½¬æŠ˜æˆ–å…³é”®åˆ¤æ–­

node_mappings:
- å€¼ä¸º JSON å¯¹è±¡ï¼Œé”®ä¸º Mermaid å›¾ä¸­çš„èŠ‚ç‚¹ ID
- æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å€¼åŒ…å«ï¼š
  - "text_snippet": è¯¥èŠ‚ç‚¹åŒ…å«æ®µè½çš„æ ¸å¿ƒå†…å®¹æ€»ç»“ï¼ˆ30-80å­—ï¼‰
  - "paragraph_ids": æ„æˆè¯¥èŠ‚ç‚¹çš„æ®µè½IDæ•°ç»„ï¼ˆå¦‚ ["para-2", "para-3"]ï¼‰
  - "semantic_role": è¯¥èŠ‚ç‚¹åœ¨è®ºè¯ä¸­çš„è§’è‰²ï¼ˆå¦‚ "å¼•è¨€"ã€"æ ¸å¿ƒè®ºç‚¹"ã€"æ”¯æ’‘è¯æ®"ã€"åé©³"ã€"ç»“è®º" ç­‰ï¼‰

edges:
- å€¼ä¸ºå¯¹è±¡æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡ä»£è¡¨ä¸€æ¡è¾¹
- æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä¸¤ä¸ªé”®ï¼š
  - "source": è¾¹çš„èµ·å§‹èŠ‚ç‚¹ID
  - "target": è¾¹çš„ç›®æ ‡èŠ‚ç‚¹ID
- è¿™äº›è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»ä¸€è‡´

å…³é”®è¦æ±‚ï¼š
1. æ‰€æœ‰èŠ‚ç‚¹ ID å¿…é¡»åœ¨ mermaid_string ä¸­å­˜åœ¨
2. paragraph_ids å¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŸæ–‡çš„æ®µè½æ ‡è®° [para-X]ï¼Œä¸å¯ä¿®æ”¹
3. åŸæ–‡çš„æ¯ä¸ªæ®µè½éƒ½åº”è¯¥è¢«åˆ†é…ç»™è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹
4. èŠ‚ç‚¹çš„åˆ’åˆ†åº”è¯¥åŸºäºæ®µè½çš„è®ºè¯åŠŸèƒ½ï¼Œç›¸å…³åŠŸèƒ½çš„æ®µè½å¯ä»¥ç»„åˆåœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸­
5. æµç¨‹å›¾åº”è¯¥æ¸…æ™°å±•ç°è®ºè¯çš„é€»è¾‘æ¨ç†è·¯å¾„
6. ä¿æŒæ®µè½çš„å®Œæ•´æ€§ï¼Œä¸è¦æ‹†åˆ†æˆ–é‡ç»„æ®µè½å†…å®¹
7. edges æ•°ç»„ä¸­çš„æ¯æ¡è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»å®Œå…¨ä¸€è‡´

ç°åœ¨ï¼Œè¯·åˆ†æä»¥ä¸‹å¸¦æœ‰æ®µè½IDçš„æ–‡æœ¬ï¼š

{text_with_ids}"""
            
            # ä½¿ç”¨DocumentOptimizerçš„generate_completionæ–¹æ³•
            response = await self.optimizer.generate_completion(
                prompt, 
                max_tokens=2000,
                task="åˆ†æè®ºè¯ç»“æ„"
            )
            
            if not response:
                print(f"âŒ [APIè°ƒç”¨å¤±è´¥] æœªæ”¶åˆ°AIå“åº”")
                return {"success": False, "error": "APIè°ƒç”¨å¤±è´¥ï¼Œæœªæ”¶åˆ°AIå“åº”"}
            
            # ä¿å­˜APIåŸå§‹å“åº”åˆ°æ–‡ä»¶
            try:
                from datetime import datetime
                import os
                
                # åˆ›å»ºapi_responsesæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                api_responses_dir = "api_responses"
                os.makedirs(api_responses_dir, exist_ok=True)
                
                # ç”Ÿæˆæ–‡ä»¶åï¼šæ—¶é—´æˆ³_è®ºè¯ç»“æ„åˆ†æ
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                response_filename = f"{timestamp}_argument_structure_analysis.txt"
                response_filepath = os.path.join(api_responses_dir, response_filename)
                
                # ä¿å­˜åŸå§‹å“åº”å’Œç›¸å…³ä¿¡æ¯
                with open(response_filepath, 'w', encoding='utf-8') as f:
                    f.write("=== APIè°ƒç”¨ä¿¡æ¯ ===\n")
                    f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ä»»åŠ¡: è®ºè¯ç»“æ„åˆ†æ\n")
                    f.write(f"æœ€å¤§tokens: 2000\n")
                    f.write(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦\n")
                    f.write(f"æ–‡æœ¬é•¿åº¦: {len(text_with_ids)} å­—ç¬¦\n")
                    f.write("\n=== å‘é€çš„Prompt ===\n")
                    f.write(prompt)
                    f.write("\n\n=== AIåŸå§‹å“åº” ===\n")
                    f.write(response)
                    f.write(f"\n\n=== å“åº”ç»“æŸ ===\n")
                
                print(f"ğŸ’¾ [APIå“åº”ä¿å­˜] å·²ä¿å­˜åˆ°: {response_filepath}")
                
            except Exception as save_error:
                print(f"âš ï¸ [å“åº”ä¿å­˜å¤±è´¥] {str(save_error)}")
            
            # è§£æJSONå“åº”
            try:
                # è¯¦ç»†è®°å½•åŸå§‹å“åº”
                print(f"ğŸ” [åŸå§‹AIå“åº”] é•¿åº¦: {len(response)} å­—ç¬¦")
                print(f"ğŸ” [åŸå§‹å“åº”å‰200å­—ç¬¦]: {response[:200]}")
                
                # æ›´å½»åº•çš„å“åº”æ¸…ç†
                clean_response = response.strip()
                
                # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                elif clean_response.startswith('```'):
                    clean_response = clean_response[3:]
                    
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                
                clean_response = clean_response.strip()
                
                # ç§»é™¤å¯èƒ½çš„è¯´æ˜æ–‡å­—ï¼Œåªä¿ç•™JSONéƒ¨åˆ†
                json_start = clean_response.find('{')
                json_end = clean_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    clean_response = clean_response[json_start:json_end+1]
                    print(f"ğŸ”§ [æå–JSON] æå–åˆ°JSONéƒ¨åˆ†ï¼Œé•¿åº¦: {len(clean_response)}")
                else:
                    print(f"âš ï¸ [JSONæå–å¤±è´¥] æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONç»“æ„")
                
                print(f"ğŸ” [æ¸…ç†åå“åº”å‰200å­—ç¬¦]: {clean_response[:200]}")
                
                structure_data = json.loads(clean_response)
                
                # éªŒè¯å¿…è¦çš„é”®
                if 'mermaid_string' not in structure_data or 'node_mappings' not in structure_data:
                    print(f"âŒ [æ•°æ®ç»“æ„é”™è¯¯] å“åº”é”®: {list(structure_data.keys())}")
                    return {"success": False, "error": "AIå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼šç¼ºå°‘å¿…è¦çš„é”®"}
                
                # éªŒè¯èŠ‚ç‚¹æ˜ å°„çš„ç»“æ„
                node_mappings = structure_data['node_mappings']
                valid_mappings = {}
                
                for node_id, mapping in node_mappings.items():
                    if isinstance(mapping, dict):
                        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨ï¼Œå¦‚æœç¼ºå°‘semantic_roleå°±æ·»åŠ é»˜è®¤å€¼
                        valid_mapping = {
                            "text_snippet": mapping.get("text_snippet", "è¯­ä¹‰å—å†…å®¹"),
                            "paragraph_ids": mapping.get("paragraph_ids", []),
                            "semantic_role": mapping.get("semantic_role", "è®ºè¯è¦ç´ ")
                        }
                        valid_mappings[node_id] = valid_mapping
                    else:
                        print(f"âš ï¸ [æ˜ å°„æ ¼å¼é”™è¯¯] èŠ‚ç‚¹ {node_id} çš„æ˜ å°„ä¸æ˜¯å­—å…¸æ ¼å¼")
                
                structure_data['node_mappings'] = valid_mappings
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«edgeså­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä»mermaid_stringä¸­æå–
                if 'edges' not in structure_data:
                    print("âš ï¸ [æ•°æ®ç»“æ„è­¦å‘Š] å“åº”ä¸­æ²¡æœ‰edgeså­—æ®µï¼Œå°†ä»mermaid_stringä¸­æå–")
                    # ä»mermaid_stringä¸­æå–è¾¹å…³ç³»
                    edges = []
                    mermaid_string = structure_data['mermaid_string']
                    # åŒ¹é…å½¢å¦‚ "A --> B" çš„è¾¹å®šä¹‰
                    edge_pattern = r'([A-Za-z0-9_]+)\s*-->\s*([A-Za-z0-9_]+)'
                    for match in re.finditer(edge_pattern, mermaid_string):
                        source, target = match.groups()
                        edges.append({"source": source, "target": target})
                    structure_data['edges'] = edges
                    print(f"ğŸ”§ [è‡ªåŠ¨æå–] ä»mermaid_stringä¸­æå–äº† {len(edges)} æ¡è¾¹")
                
                print(f"âœ… [è®ºè¯ç»“æ„åˆ†æ] æˆåŠŸç”ŸæˆåŒ…å« {len(structure_data['node_mappings'])} ä¸ªèŠ‚ç‚¹çš„æµç¨‹å›¾")
                
                # è¿”å›æˆåŠŸç»“æœ
                return {
                    "success": True,
                    "mermaid_code": structure_data['mermaid_string'],
                    "node_mappings": structure_data['node_mappings'],
                    "edges": structure_data['edges']
                }
                
            except json.JSONDecodeError as parse_error:
                print(f"âŒ [JSONè§£æé”™è¯¯] {str(parse_error)}")
                print(f"âŒ [å®Œæ•´åŸå§‹å“åº”]: {response}")
                print(f"âŒ [æ¸…ç†åå“åº”]: {clean_response}")
                return {"success": False, "error": f"JSONè§£æå¤±è´¥: {str(parse_error)}"}
                
        except Exception as e:
            print(f"âŒ [è®ºè¯ç»“æ„åˆ†æé”™è¯¯] {str(e)}")
            # æä¾›é™çº§ç­–ç•¥ - ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„
            try:
                fallback_structure = self.generate_fallback_structure(text_with_ids)
                print(f"ğŸ”„ [é™çº§ç­–ç•¥] ä½¿ç”¨åŸºæœ¬è®ºè¯ç»“æ„ï¼ŒåŒ…å« {len(fallback_structure['node_mappings'])} ä¸ªèŠ‚ç‚¹")
                return fallback_structure
            except Exception as fallback_error:
                print(f"âŒ [é™çº§ç­–ç•¥å¤±è´¥] {str(fallback_error)}")
                return {"success": False, "error": f"AIåˆ†æå¤±è´¥ä¸”é™çº§ç­–ç•¥ä¹Ÿå¤±è´¥: {str(e)}"}

    def generate_fallback_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„ä½œä¸ºé™çº§ç­–ç•¥"""
        import re
        
        # æå–æ‰€æœ‰æ®µè½ID
        para_ids = re.findall(r'\[para-(\d+)\]', text_with_ids)
        
        if not para_ids:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½IDï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬ç»“æ„
            return {
                "success": True,
                "mermaid_code": "graph TD\n    A[æ–‡æ¡£åˆ†æ] --> B[ä¸»è¦å†…å®¹]\n    B --> C[æ€»ç»“]",
                "node_mappings": {
                    "A": {
                        "text_snippet": "æ–‡æ¡£å¼€å§‹",
                        "paragraph_ids": ["para-1"],
                        "semantic_role": "å¼•è¨€"
                    },
                    "B": {
                        "text_snippet": "ä¸»è¦å†…å®¹",
                        "paragraph_ids": ["para-2"],
                        "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                    },
                    "C": {
                        "text_snippet": "æ–‡æ¡£ç»“è®º",
                        "paragraph_ids": ["para-3"],
                        "semantic_role": "ç»“è®º"
                    }
                }
            }
        
        # åŸºäºæ®µè½æ•°é‡ç”Ÿæˆç»“æ„
        total_paras = len(para_ids)
        
        if total_paras <= 3:
            # ç®€å•çº¿æ€§ç»“æ„
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[ä¸»ä½“]\n"
            mermaid_code += "    B --> C[ç»“è®º]"
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€éƒ¨åˆ†",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "æ–‡æ¡£ä¸»ä½“å†…å®¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:-1]] if total_paras > 2 else [f"para-{para_ids[1]}"] if total_paras > 1 else [],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"] if total_paras > 1 else [],
                    "semantic_role": "ç»“è®º"
                }
            }
        else:
            # å¤æ‚ç»“æ„ï¼šå¼•è¨€ -> å¤šä¸ªè®ºç‚¹ -> ç»“è®º
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[è®ºç‚¹1]\n"
            mermaid_code += "    A --> C[è®ºç‚¹2]\n"
            if total_paras > 5:
                mermaid_code += "    A --> D[è®ºç‚¹3]\n"
                mermaid_code += "    B --> E[ç»“è®º]\n"
                mermaid_code += "    C --> E\n"
                mermaid_code += "    D --> E"
            else:
                mermaid_code += "    B --> D[ç»“è®º]\n"
                mermaid_code += "    C --> D"
            
            # å°†æ®µè½åˆ†é…ç»™ä¸åŒèŠ‚ç‚¹
            para_per_section = max(1, total_paras // 4)
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "ç¬¬ä¸€ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:1+para_per_section]],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "ç¬¬äºŒä¸ªè®ºç‚¹", 
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+para_per_section:1+2*para_per_section]],
                    "semantic_role": "æ”¯æ’‘è¯æ®"
                }
            }
            
            if total_paras > 5:
                node_mappings["D"] = {
                    "text_snippet": "ç¬¬ä¸‰ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:-1]],
                    "semantic_role": "è¡¥å……è®ºè¯"
                }
                node_mappings["E"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"],
                    "semantic_role": "ç»“è®º"
                }
            else:
                node_mappings["D"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:]],
                    "semantic_role": "ç»“è®º"
                }
        
        return {
            "success": True,
            "mermaid_code": mermaid_code,
            "node_mappings": node_mappings
        }

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
argument_analyzer = ArgumentStructureAnalyzer()

async def process_pdf_to_markdown(pdf_file_path: str, document_id: str) -> str:
    """
    ä½¿ç”¨MinerUå¤„ç†PDFæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        pdf_file_path: PDFæ–‡ä»¶è·¯å¾„
        document_id: æ–‡æ¡£ID
        
    Returns:
        è½¬æ¢åçš„Markdownå†…å®¹
    """
    try:
        print(f"\nğŸ“„ [MinerU-PDFå¤„ç†] å¼€å§‹å¤„ç†PDFæ–‡ä»¶")
        print(f"    ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = PDF_OUTPUT_DIR / document_id
        image_dir = output_dir / "images"
        os.makedirs(image_dir, exist_ok=True)
        
        print(f"ğŸ“ [MinerU-ç›®å½•] åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ–¼ï¸  [MinerU-å›¾ç‰‡] å›¾ç‰‡ç›®å½•: {image_dir}")
        
        # åˆ›å»ºæ•°æ®è¯»å†™å™¨
        print("ğŸ”§ [MinerU-åˆå§‹åŒ–] åˆ›å»ºæ•°æ®è¯»å†™å™¨...")
        reader = FileBasedDataReader("")
        image_writer = FileBasedDataWriter(str(image_dir))
        md_writer = FileBasedDataWriter(str(output_dir))
        
        # è¯»å–PDFæ–‡ä»¶
        print("ğŸ“– [MinerU-è¯»å–] æ­£åœ¨è¯»å–PDFæ–‡ä»¶...")
        pdf_bytes = reader.read(pdf_file_path)
        print(f"ğŸ“Š [MinerU-æ•°æ®] PDFæ–‡ä»¶å¤§å°: {len(pdf_bytes)} å­—èŠ‚")
        
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        print("ğŸ—ï¸  [MinerU-æ•°æ®é›†] åˆ›å»ºPymuDocDatasetå®ä¾‹...")
        ds = PymuDocDataset(pdf_bytes)
        
        # åˆ†ç±»å¤„ç†æ¨¡å¼
        print("ğŸ” [MinerU-æ£€æµ‹] æ£€æµ‹PDFå¤„ç†æ¨¡å¼...")
        pdf_mode = ds.classify()
        
        # è¿›è¡Œæ¨ç†
        if pdf_mode == SupportedPdfParseMethod.OCR:
            print(f"ğŸ”¤ [MinerU-OCRæ¨¡å¼] æ£€æµ‹åˆ°éœ€è¦OCRå¤„ç†ï¼Œå¼€å§‹æ–‡å­—è¯†åˆ«...")
            print("    ğŸ“¸ æ­£åœ¨æå–å›¾ç‰‡ä¸­çš„æ–‡å­—...")
            print("    ğŸ§  è°ƒç”¨OCRå¼•æ“è¿›è¡Œæ–‡å­—è¯†åˆ«...")
            infer_result = ds.apply(doc_analyze, ocr=True)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨OCRæ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_ocr_mode(image_writer)
            print("âœ… [MinerU-OCR] OCRå¤„ç†å®Œæˆ")
        else:
            print(f"ğŸ“ [MinerU-æ–‡æœ¬æ¨¡å¼] æ£€æµ‹åˆ°å¯ç›´æ¥æå–æ–‡æœ¬ï¼Œå¼€å§‹æ–‡æœ¬å¤„ç†...")
            print("    ğŸ“„ æ­£åœ¨æå–PDFä¸­çš„æ–‡æœ¬å†…å®¹...")
            print("    ğŸ”§ åˆ†ææ–‡æ¡£ç»“æ„å’Œç‰ˆé¢...")
            infer_result = ds.apply(doc_analyze, ocr=False)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_txt_mode(image_writer)
            print("âœ… [MinerU-æ–‡æœ¬] æ–‡æœ¬æå–å®Œæˆ")
        
        print("ğŸ“‹ [MinerU-è½¬æ¢] æ­£åœ¨ç”ŸæˆMarkdownæ ¼å¼...")
        # è·å–Markdownå†…å®¹
        markdown_content = pipe_result.get_markdown("images")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        md_file_path = output_dir / f"{document_id}.md"
        print(f"ğŸ’¾ [MinerU-ä¿å­˜] ä¿å­˜Markdownæ–‡ä»¶: {md_file_path}")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # ç»Ÿè®¡ä¿¡æ¯
        lines_count = len(markdown_content.split('\n'))
        words_count = len(markdown_content.split())
        
        print("=" * 60)
        print("âœ… [MinerU-å®Œæˆ] PDFè½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"    ğŸ“Š ç”Ÿæˆå†…å®¹ç»Ÿè®¡:")
        print(f"       â€¢ Markdownæ€»é•¿åº¦: {len(markdown_content):,} å­—ç¬¦")
        print(f"       â€¢ æ€»è¡Œæ•°: {lines_count:,} è¡Œ")
        print(f"       â€¢ å•è¯æ•°: {words_count:,} ä¸ª")
        print(f"    ğŸ“ è¾“å‡ºæ–‡ä»¶: {md_file_path}")
        print("=" * 60)
        
        return markdown_content
        
    except Exception as e:
        print("=" * 60)
        print(f"âŒ [MinerU-é”™è¯¯] PDFå¤„ç†å¤±è´¥ï¼")
        print(f"    ğŸš¨ é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"    ğŸ“„ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        logger.error(f"MinerU PDF processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"PDFå¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£ï¼Œæ”¯æŒPDFã€MDå’ŒTXTæ–‡ä»¶"""
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = ['.md', '.txt', '.pdf']
    file_extension = Path(file.filename).suffix.lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .mdã€.txt å’Œ .pdf æ–‡ä»¶")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£ID
        content_hash = hashlib.md5(content).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = Path(file.filename).stem
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        
        print(f"\nğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] {file.filename}")
        print(f"ğŸ†” [æ–‡æ¡£ID] {document_id}")
        print(f"ğŸ“Š [æ–‡ä»¶å¤§å°] {len(content)} å­—èŠ‚")
        print(f"ğŸ“‹ [æ–‡ä»¶ç±»å‹] {file_extension}")
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶
        original_file_path = UPLOAD_DIR / f"{document_id}{file_extension}"
        with open(original_file_path, 'wb') as f:
            f.write(content)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
        if file_extension == '.pdf':
            # å¤„ç†PDFæ–‡ä»¶
            print(f"ğŸ”„ [PDFå¤„ç†] å¼€å§‹è½¬æ¢PDFä¸ºMarkdown...")
            markdown_content = await process_pdf_to_markdown(str(original_file_path), document_id)
            text_content = markdown_content
            
            # å°†åŸå§‹PDFæ–‡ä»¶ç¼–ç ä¸ºbase64ç”¨äºå‰ç«¯æ˜¾ç¤º
            pdf_base64 = base64.b64encode(content).decode('utf-8')
            
        else:
            # å¤„ç†æ–‡æœ¬æ–‡ä»¶
            text_content = content.decode('utf-8')
            pdf_base64 = None
        
        # å­˜å‚¨åˆ°å†…å­˜æ•°æ®åº“
        MinimalDatabaseStub.store_text(text_content)
        
        # ç«‹å³ä¸ºæ–‡æ¡£å†…å®¹æ·»åŠ æ®µè½IDï¼Œæ— éœ€ç­‰å¾…ç”Ÿæˆè®ºè¯ç»“æ„
        print("ğŸ“ [å¤„ç†æ®µè½] ä¸ºä¸Šä¼ çš„æ–‡æ¡£æ·»åŠ æ®µè½IDæ ‡è®°...")
        content_with_ids = argument_analyzer.add_paragraph_ids(text_content)
        print(f"ğŸ“ [æ®µè½å¤„ç†å®Œæˆ] å·²ä¸ºæ–‡æ¡£æ·»åŠ æ®µè½IDï¼Œå†…å®¹é•¿åº¦: {len(content_with_ids)} å­—ç¬¦")
        
        # åˆå§‹åŒ–æ–‡æ¡£çŠ¶æ€
        document_status[document_id] = {
            "status": "uploaded",
            "content": text_content,
            "filename": file.filename,
            "file_type": file_extension,
            "original_file_path": str(original_file_path),
            "pdf_base64": pdf_base64,  # ä»…PDFæ–‡ä»¶æœ‰æ­¤å­—æ®µ
            "status_demo": "not_started",
            "mermaid_code_demo": None,
            "node_mappings_demo": {},
            "error_demo": None,
            "content_with_ids": content_with_ids  # ç«‹å³è®¾ç½®å¸¦æ®µè½IDçš„å†…å®¹
        }
        
        print(f"âœ… [ä¸Šä¼ æˆåŠŸ] æ–‡æ¡£å·²ä¿å­˜å¹¶å‡†å¤‡ç”Ÿæˆæ€ç»´å¯¼å›¾")
        print("=" * 60)
        
        logger.info(f"Document uploaded: {document_id}")
        
        # è¿”å›æ–‡æ¡£ä¿¡æ¯
        response_data = {
            "success": True,
            "document_id": document_id,
            "filename": file.filename,
            "content": text_content,
            "file_type": file_extension,
            "status": "uploaded",
            "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ"
        }
        
        # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œè¿”å›base64ç¼–ç çš„åŸå§‹PDF
        if file_extension == '.pdf':
            response_data["pdf_base64"] = pdf_base64
            response_data["message"] = "PDFæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå·²è½¬æ¢ä¸ºMarkdown"
        
        return JSONResponse(response_data)
        
    except UnicodeDecodeError:
        print(f"âŒ [ç¼–ç é”™è¯¯] æ–‡ä»¶: {file.filename}")
        raise HTTPException(status_code=400, detail="æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯UTF-8ç¼–ç ")
    except Exception as e:
        print(f"âŒ [ä¸Šä¼ å¤±è´¥] æ–‡ä»¶: {file.filename}, é”™è¯¯: {str(e)}")
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/document-pdf/{document_id}")
async def get_document_pdf(document_id: str):
    """è·å–åŸå§‹PDFæ–‡ä»¶"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    if doc_info.get("file_type") != ".pdf":
        raise HTTPException(status_code=400, detail="è¯¥æ–‡æ¡£ä¸æ˜¯PDFæ–‡ä»¶")
    
    original_file_path = doc_info.get("original_file_path")
    if not original_file_path or not os.path.exists(original_file_path):
        raise HTTPException(status_code=404, detail="åŸå§‹PDFæ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=original_file_path,
        media_type='application/pdf',
        filename=doc_info["filename"]
    )

@app.post("/api/generate-argument-structure/{document_id}")
async def generate_argument_structure(document_id: str):
    """ä¸ºæŒ‡å®šæ–‡æ¡£ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    # æ£€æŸ¥çŠ¶æ€
    if doc_info.get("status_demo") == "generating":
        print(f"â³ [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­...")
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­..."
        })
    
    if doc_info.get("status_demo") == "completed" and doc_info.get("mermaid_code_demo"):
        print(f"âœ… [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„å·²åˆ†æå®Œæˆ")
        return JSONResponse({
            "success": True,
            "status": "completed",
            "mermaid_code": doc_info["mermaid_code_demo"],
            "node_mappings": doc_info.get("node_mappings_demo", {}),
            "message": "è®ºè¯ç»“æ„å·²ç”Ÿæˆ"
        })
    
    try:
        print(f"ğŸ”„ [å¼€å§‹åˆ†æ] ä¸ºæ–‡æ¡£ {document_id} å¯åŠ¨è®ºè¯ç»“æ„åˆ†æä»»åŠ¡")
        
        # æ›´æ–°çŠ¶æ€ä¸ºåˆ†æä¸­
        doc_info["status_demo"] = "generating"
        
        # å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„
        asyncio.create_task(generate_argument_structure_async(document_id, doc_info["content"]))
        
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "å¼€å§‹åˆ†æè®ºè¯ç»“æ„..."
        })
        
    except Exception as e:
        print(f"âŒ [å¯åŠ¨å¤±è´¥] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}")
        logger.error(f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        doc_info["status_demo"] = "error"
        doc_info["error_demo"] = str(e)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")

async def generate_argument_structure_async(document_id: str, content: str):
    """å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„"""
    try:
        print(f"ğŸ”„ [å¼‚æ­¥ä»»åŠ¡] å¼€å§‹ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆè®ºè¯ç»“æ„")
        argument_analyzer = ArgumentStructureAnalyzer()
        
        # ä¸ºæ–‡æœ¬æ·»åŠ æ®µè½ID
        text_with_ids = argument_analyzer.add_paragraph_ids(content)
        
        # ç”Ÿæˆè®ºè¯ç»“æ„
        result = await argument_analyzer.generate_argument_structure(text_with_ids)
        
        if result["success"]:
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document_status[document_id]["status_demo"] = "completed"
            document_status[document_id]["mermaid_code_demo"] = result["mermaid_code"]
            document_status[document_id]["node_mappings_demo"] = result["node_mappings"]
            document_status[document_id]["edges_demo"] = result["edges"]  # ä¿å­˜edgesæ•°æ®
            document_status[document_id]["content_with_ids"] = text_with_ids  # ä¿å­˜å¸¦IDçš„å†…å®¹
            
            print(f"âœ… [åˆ†æå®Œæˆ] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†ææˆåŠŸ")
            print(f"ğŸ“Š [ç”Ÿæˆç»“æœ] åŒ…å« {len(result['node_mappings'])} ä¸ªè®ºè¯èŠ‚ç‚¹å’Œ {len(result['edges'])} æ¡è¾¹")
        else:
            # åˆ†æå¤±è´¥
            document_status[document_id]["status_demo"] = "error"
            document_status[document_id]["error_demo"] = result["error"]
            print(f"âŒ [åˆ†æå¤±è´¥] æ–‡æ¡£ {document_id}: {result['error']}")
            
    except Exception as e:
        print(f"âŒ [å¼‚æ­¥åˆ†æé”™è¯¯] æ–‡æ¡£ {document_id}: {str(e)}")
        logger.error(f"å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        document_status[document_id]["status_demo"] = "error"
        document_status[document_id]["error_demo"] = str(e)

@app.get("/api/document-status/{document_id}")
async def get_document_status(document_id: str):
    """è·å–æ–‡æ¡£çŠ¶æ€å’Œè®ºè¯ç»“æ„åˆ†æè¿›åº¦"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    response_data = {
        "success": True,
        "document_id": document_id,
        "filename": doc_info.get("filename"),
        "content": doc_info.get("content"),
        "file_type": doc_info.get("file_type", ".md"),
        
        # è®ºè¯ç»“æ„åˆ†æçŠ¶æ€
        "status_demo": doc_info.get("status_demo", "not_started"),
        "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
        "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
        "edges_demo": doc_info.get("edges_demo", []),
        "error_demo": doc_info.get("error_demo"),
        "content_with_ids": doc_info.get("content_with_ids"),
    }
    
    # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œæ·»åŠ PDFç›¸å…³ä¿¡æ¯
    if doc_info.get("file_type") == ".pdf":
        response_data["pdf_base64"] = doc_info.get("pdf_base64")
        response_data["original_file_path"] = doc_info.get("original_file_path")
    
    return JSONResponse(response_data)

@app.get("/api/document/{document_id}")
async def get_document(document_id: str):
    """è·å–æ–‡æ¡£å†…å®¹å’Œè®ºè¯ç»“æ„"""
    
    try:
        # å¦‚æœæ–‡ä»¶åœ¨å†…å­˜çŠ¶æ€ä¸­å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if document_id in document_status:
            doc_info = document_status[document_id]
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": doc_info["content"],
                "filename": doc_info.get("filename", ""),
                "file_type": doc_info.get("file_type", ".md"),
                "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
                "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
                "edges_demo": doc_info.get("edges_demo", []),
                "status_demo": doc_info.get("status_demo", "not_started"),
                "error_demo": doc_info.get("error_demo"),
                "content_with_ids": doc_info.get("content_with_ids"),
                "pdf_base64": doc_info.get("pdf_base64")
            })
        else:
            # å°è¯•æŸ¥æ‰¾æ–‡ä»¶
            file_path = UPLOAD_DIR / f"{document_id}.md"
            
            if not file_path.exists():
                raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": content,
                "filename": f"{document_id}.md",
                "file_type": ".md",
                "mermaid_code_demo": None,
                "node_mappings_demo": {},
                "edges_demo": [],
                "status_demo": "not_started",
                "error_demo": None,
                "content_with_ids": None
            })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "message": "Argument Structure Analyzer API is running"}

@app.get("/")
async def root():
    return {"message": "Argument Structure Analyzer API is running"}

# æ–‡æ¡£ç»“æ„ç›¸å…³APIç«¯ç‚¹ï¼ˆä¿ç•™ç”¨äºç›®å½•ç”Ÿæˆç­‰ï¼‰

# æ–‡æ¡£ç»“æ„å’Œç›®å½•ç›¸å…³APIç«¯ç‚¹

@app.get("/api/document-structure/{document_id}")
async def get_document_structure(document_id: str):
    """è·å–æ–‡æ¡£çš„å±‚çº§ç»“æ„"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    chunks = parser.parse_to_chunks(content, document_id)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': chunks
                    }
                    
                    print(f"ğŸ“„ [è‡ªåŠ¨ç”Ÿæˆ] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº†ç»“æ„å’Œ {len(chunks)} ä¸ªchunks")
                    
                    return {
                        "success": True,
                        "structure": root.to_dict(),
                        "toc": toc,
                        "chunks": chunks,
                        "chunks_count": len(chunks)
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç»“æ„å°šæœªç”Ÿæˆï¼Œä¸”æ— æ³•è‡ªåŠ¨ç”Ÿæˆ",
                "structure": None,
                "toc": [],
                "chunks": [],
                "chunks_count": 0
            }
        
        structure_data = document_structures[document_id]
        chunks = structure_data.get('chunks', [])
        
        print(f"ğŸ“„ [API] è¿”å›æ–‡æ¡£ç»“æ„ï¼Œchunksæ•°é‡: {len(chunks)}")
        
        return {
            "success": True,
            "structure": structure_data['structure'],
            "toc": structure_data['toc'], 
            "chunks": chunks,  # è¿”å›å®é™…çš„chunksæ•°æ®
            "chunks_count": len(chunks)
        }
        
    except Exception as e:
        logger.error(f"Get document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.get("/api/document-toc/{document_id}")
async def get_document_toc(document_id: str):
    """è·å–æ–‡æ¡£ç›®å½•"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': parser.parse_to_chunks(content, document_id)
                    }
                    
                    return {
                        "success": True,
                        "toc": toc
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç›®å½•å°šæœªç”Ÿæˆ",
                "toc": []
            }
        
        structure_data = document_structures[document_id]
        return {
            "success": True,
            "toc": structure_data['toc']
        }
        
    except Exception as e:
        logger.error(f"Get document TOC error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç›®å½•å¤±è´¥: {str(e)}")

@app.post("/api/generate-document-structure/{document_id}")
async def generate_document_structure(document_id: str):
    """ç”Ÿæˆæˆ–é‡æ–°ç”Ÿæˆæ–‡æ¡£ç»“æ„"""
    try:
        if document_id not in document_status:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        content = document_status[document_id].get('content')
        if not content:
            raise HTTPException(status_code=400, detail="æ–‡æ¡£å†…å®¹ä¸ºç©º")
        
        # ä½¿ç”¨æ–‡æ¡£è§£æå™¨ç”Ÿæˆç»“æ„
        parser = DocumentParser()
        root = parser.parse_document(content, document_id)
        toc = parser.generate_toc(root)
        chunks = parser.parse_to_chunks(content, document_id)
        
        # ä¿å­˜ç»“æ„
        document_structures[document_id] = {
            'structure': root.to_dict(),
            'toc': toc,
            'chunks': chunks
        }
        
        print(f"ğŸ“„ [æ–‡æ¡£ç»“æ„] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº† {len(toc)} ä¸ªç›®å½•é¡¹ï¼Œ{len(chunks)} ä¸ªå†…å®¹å—")
        
        return {
            "success": True,
            "message": "æ–‡æ¡£ç»“æ„ç”ŸæˆæˆåŠŸ",
            "toc_items": len(toc),
            "chunks_count": len(chunks)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generate document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.post("/api/document/{document_id}/remap")
async def update_node_mappings(document_id: str, request_data: dict):
    """æ›´æ–°æ–‡æ¡£çš„èŠ‚ç‚¹æ˜ å°„å…³ç³»"""
    try:
        print(f"ğŸ“ [API] æ”¶åˆ°èŠ‚ç‚¹æ˜ å°„æ›´æ–°è¯·æ±‚ - æ–‡æ¡£ID: {document_id}")
        print(f"ğŸ“ [API] æ–°çš„èŠ‚ç‚¹æ˜ å°„: {request_data}")
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if 'node_mappings' not in request_data:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "ç¼ºå°‘ node_mappings å‚æ•°"}
            )
        
        new_node_mappings = request_data['node_mappings']
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        if document_id not in document_status:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"}
            )
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸­çš„èŠ‚ç‚¹æ˜ å°„
        document_status[document_id]['node_mappings_demo'] = new_node_mappings
        
        print(f"ğŸ“ [API] âœ… æˆåŠŸæ›´æ–°æ–‡æ¡£ {document_id} çš„èŠ‚ç‚¹æ˜ å°„")
        print(f"ğŸ“ [API] æ›´æ–°åçš„æ˜ å°„é”®æ•°é‡: {len(new_node_mappings)}")
        
        # å¯é€‰ï¼šä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆè¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“ä¿å­˜é€»è¾‘ï¼‰
        # TODO: æ·»åŠ æ•°æ®åº“æŒä¹…åŒ–é€»è¾‘
        
        return JSONResponse(content={
            "success": True,
            "message": "èŠ‚ç‚¹æ˜ å°„æ›´æ–°æˆåŠŸ",
            "document_id": document_id,
            "updated_mappings_count": len(new_node_mappings)
        })
        
    except Exception as e:
        print(f"âŒ [APIé”™è¯¯] æ›´æ–°èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ›´æ–°èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}"}
        )

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - åç«¯APIæœåŠ¡")
    print("=" * 80)
    print("ğŸ“ æœåŠ¡åœ°å€: http://localhost:8000")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ”§ æœåŠ¡æ¨¡å¼: å¼€å‘æ¨¡å¼ (æ”¯æŒçƒ­é‡è½½)")
    print("=" * 80)
    print("ğŸ“‹ æ§åˆ¶å°æ—¥å¿—è¯´æ˜:")
    print("   ğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] - æ–‡ä»¶ä¸Šä¼ ç›¸å…³ä¿¡æ¯")
    print("   ğŸ”„ [å¼€å§‹ç”Ÿæˆ] - æ€ç»´å¯¼å›¾ç”Ÿæˆä»»åŠ¡å¯åŠ¨")
    print("   ğŸš€ [å¼€å§‹ç”Ÿæˆ] - AIå¤„ç†å¼€å§‹")
    print("   ğŸ¤– [AIå¤„ç†] - è°ƒç”¨æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨")
    print("   âœ… [ç”Ÿæˆå®Œæˆ] - æ€ç»´å¯¼å›¾ç”ŸæˆæˆåŠŸ")
    print("   âŒ [ç”Ÿæˆå¤±è´¥] - ç”Ÿæˆè¿‡ç¨‹å‡ºç°é”™è¯¯")
    print("   â³ [çŠ¶æ€æŸ¥è¯¢] - å®¢æˆ·ç«¯æŸ¥è¯¢ç”ŸæˆçŠ¶æ€")
    print("=" * 80)
    print("ğŸ¯ æ–°åŠŸèƒ½: æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼")
    print("   ğŸ“Š æ ‡å‡†è¯¦ç»†æ¨¡å¼: 3-5åˆ†é’Ÿï¼Œè¯¦ç»†åˆ†æï¼Œé«˜è´¨é‡ç»“æœ")
    print("   âš¡ å¿«é€Ÿç®€åŒ–æ¨¡å¼: 1-2åˆ†é’Ÿï¼ŒåŸºç¡€ç»“æ„ï¼Œå¿«é€Ÿé¢„è§ˆ")
    print("   ğŸ“‹ APIç«¯ç‚¹: /api/generate-mindmap/{id} å’Œ /api/generate-mindmap-simple/{id}")
    print("=" * 80)
    print("ğŸš€ å¯åŠ¨æœåŠ¡ä¸­...")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
</file>

<file path="frontend/src/components/ViewerPageRefactored.js">
import React, { useState, useRef, useCallback, useEffect, useMemo } from 'react';
import { useNavigate, useLocation, useParams } from 'react-router-dom';
import { ArrowLeft, Download, Eye, EyeOff, FileText, File, Bot } from 'lucide-react';
import axios from 'axios';
import { toast, ToastContainer } from 'react-toastify';
import 'react-toastify/dist/ReactToastify.css';
import FlowDiagram from './FlowDiagram';
import ThemeToggle from './ThemeToggle';

// å¯¼å…¥è‡ªå®šä¹‰hooks
import { useDocumentViewer } from '../hooks/useDocumentViewer';
import { useMindmapGeneration } from '../hooks/useMindmapGeneration';
import { usePanelResize } from '../hooks/usePanelResize';

import { useScrollDetection } from '../hooks/useScrollDetection';

// å¯¼å…¥UIç»„ä»¶
import TableOfContents from './TableOfContents';
import PDFViewer from './PDFViewer';

import { StructuredMarkdownRenderer, DemoModeRenderer } from './DocumentRenderer';

const ViewerPageRefactored = () => {
  const navigate = useNavigate();
  const location = useLocation();
  const containerRef = useRef(null);
  const mermaidDiagramRef = useRef(null);
  
  const [showToc, setShowToc] = useState(false);

  // æ·»åŠ contentChunks ref
  const contentChunks = useRef([]);

  // ä½¿ç”¨æ–‡æ¡£æŸ¥çœ‹å™¨ hook
  const {
    documentId,
    document,
    setDocument,
    loading,
    error: documentError,
    viewMode,
    setViewMode,
    isPdfFile,
    toc,
    expandedTocItems,
    toggleTocItem,
    loadDocument,
    loadDocumentStructure
  } = useDocumentViewer();

  // ä½¿ç”¨æ€ç»´å¯¼å›¾ç”Ÿæˆ hook
  const {
    demoMindmapStatus,
    startMindmapGeneration,
    handleDownloadMarkdown,
    handleDownloadMermaid,
    handleOpenMermaidEditor,
    MindmapStatusDisplay
  } = useMindmapGeneration(documentId, document, setDocument);

  // ä½¿ç”¨é¢æ¿æ‹–æ‹½ hook
  const {
    tocPanelWidth,
    leftPanelWidth,
    isDragging,
    handleMouseDown
  } = usePanelResize();

  // ä½¿ç”¨æ»šåŠ¨æ£€æµ‹ hook
  const {
    activeChunkId,
    activeContentBlockId, // æ·»åŠ æ®µè½çº§çŠ¶æ€
    contentChunks: scrollChunks,
    handleSectionRef,
    handleContentBlockRef,
    scrollToSection,
    scrollToContentBlock,
    highlightParagraph,
    highlightMermaidNode,
    updateDynamicMapping,
    dynamicMapping,
    textToNodeMap, // æ·»åŠ é™æ€æ˜ å°„å…³ç³»
    setActiveContentBlockId // ğŸ”‘ æ·»åŠ çŠ¶æ€è®¾ç½®å‡½æ•°
  } = useScrollDetection(
    containerRef,
    documentId,
    'argument', // è®ºè¯ç»“æ„åˆ†ææ¨¡å¼
    mermaidDiagramRef
  );

  // è®¡ç®—å½“å‰éœ€è¦é«˜äº®çš„èŠ‚ç‚¹ID
  const highlightedNodeId = useMemo(() => {
    if (!activeContentBlockId) {
      return null;
    }

    // ä¼˜å…ˆä½¿ç”¨åŠ¨æ€æ˜ å°„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é™æ€æ˜ å°„
    const hasDynamicMapping = Object.keys(dynamicMapping.textToNodeMap).length > 0;
    const currentMapping = hasDynamicMapping ? dynamicMapping.textToNodeMap : textToNodeMap;
    
    const mappedNodeId = currentMapping[activeContentBlockId];
    
    console.log('ğŸ¯ [é«˜äº®è®¡ç®—] æ´»è·ƒæ®µè½:', activeContentBlockId);
    console.log('ğŸ¯ [é«˜äº®è®¡ç®—] ä½¿ç”¨æ˜ å°„ç±»å‹:', hasDynamicMapping ? 'åŠ¨æ€' : 'é™æ€');
    console.log('ğŸ¯ [é«˜äº®è®¡ç®—] æ˜ å°„ç»“æœ:', mappedNodeId);
    
    return mappedNodeId || null;
  }, [activeContentBlockId, dynamicMapping.textToNodeMap, textToNodeMap]);

  // å¤„ç†èŠ‚ç‚¹ç‚¹å‡»äº‹ä»¶
  const handleNodeClick = useCallback((nodeId) => {
    console.log('ğŸ–±ï¸ [çˆ¶ç»„ä»¶] æ¥æ”¶åˆ°èŠ‚ç‚¹ç‚¹å‡»äº‹ä»¶:', nodeId);
    
    // ğŸ”‘ æ–¹æ¡ˆ1ï¼šç‚¹å‡»åªè´Ÿè´£å¯¼èˆªï¼Œä¸è´Ÿè´£é«˜äº®
    // é«˜äº®ç”±æ»šåŠ¨æ£€æµ‹ç³»ç»Ÿç»Ÿä¸€ç®¡ç†ï¼Œç¡®ä¿çŠ¶æ€ä¸€è‡´
    console.log('ğŸ–±ï¸ [ç‚¹å‡»å¯¼èˆª] æ»šåŠ¨åˆ°å¯¹åº”æ–‡æœ¬å—ï¼Œé«˜äº®ç”±æ»šåŠ¨æ£€æµ‹è‡ªåŠ¨å¤„ç†');
    
    // æ»šåŠ¨åˆ°å¯¹åº”æ–‡æœ¬å—ï¼Œæ»šåŠ¨å®Œæˆåæ»šåŠ¨æ£€æµ‹ä¼šè‡ªåŠ¨å¤„ç†é«˜äº®
    scrollToContentBlock(nodeId);
  }, [scrollToContentBlock]);

  // ğŸ”‘ æ–°å¢ï¼šå¤„ç†èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°çš„å›è°ƒå‡½æ•°
  const handleNodeLabelUpdate = useCallback((nodeId, newLabel) => {
    console.log('ğŸ“ [èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°] åŒæ­¥æ›´æ–°documentçŠ¶æ€:', nodeId, '->', newLabel);
    
    // åŒæ­¥æ›´æ–°document.node_mappings_demoä¸­çš„å¯¹åº”èŠ‚ç‚¹æ ‡ç­¾
    setDocument(prevDoc => {
      if (!prevDoc || !prevDoc.node_mappings_demo) {
        console.warn('ğŸ“ [èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°] documentæˆ–node_mappings_demoä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°');
        return prevDoc;
      }
      
      const newNodeMappings = { ...prevDoc.node_mappings_demo };
      if (newNodeMappings[nodeId]) {
        newNodeMappings[nodeId] = { 
          ...newNodeMappings[nodeId], 
          text_snippet: newLabel 
        };
        console.log('ğŸ“ [èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°] âœ… documentçŠ¶æ€å·²åŒæ­¥æ›´æ–°');
      } else {
        console.warn('ğŸ“ [èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°] èŠ‚ç‚¹IDåœ¨node_mappingsä¸­ä¸å­˜åœ¨:', nodeId);
      }
      
      return { 
        ...prevDoc, 
        node_mappings_demo: newNodeMappings 
      };
    });
  }, [setDocument]);

  // åˆ›å»ºåŠ¨æ€æ˜ å°„çš„è¾…åŠ©å‡½æ•°
  const createDynamicMapping = useCallback((chunks, mermaidCode, nodeMapping) => {
    console.log('ğŸ”— [æ˜ å°„åˆ›å»º] å¼€å§‹åˆ›å»ºåŠ¨æ€æ˜ å°„');
    console.log('ğŸ”— [æ˜ å°„åˆ›å»º] chunksæ•°é‡:', chunks?.length);
    console.log('ğŸ”— [æ˜ å°„åˆ›å»º] mermaidCodeé•¿åº¦:', mermaidCode?.length);
    console.log('ğŸ”— [æ˜ å°„åˆ›å»º] nodeMappingç±»å‹:', typeof nodeMapping);
    
    if (!mermaidCode || !nodeMapping) {
      console.warn('ğŸ”— [æ˜ å°„åˆ›å»º] ç¼ºå°‘å¿…è¦å‚æ•°ï¼Œè·³è¿‡æ˜ å°„åˆ›å»º');
      return;
    }
    
    const newTextToNodeMap = {};
    const newNodeToTextMap = {};
    
    if (nodeMapping && typeof nodeMapping === 'object') {
      console.log('ğŸ”— [æ˜ å°„åˆ›å»º] åŸºäºAIè¯­ä¹‰å—åˆ›å»ºæ®µè½çº§æ˜ å°„');
      console.log('ğŸ”— [æ˜ å°„åˆ›å»º] nodeMappingé”®æ•°é‡:', Object.keys(nodeMapping).length);
      
      // ä¸ºæ¯ä¸ªAIè¯­ä¹‰å—åˆ›å»ºæ˜ å°„
      Object.entries(nodeMapping).forEach(([nodeId, nodeInfo]) => {
        console.log(`ğŸ”— [æ˜ å°„åˆ›å»º] å¤„ç†èŠ‚ç‚¹ ${nodeId}:`, nodeInfo);
        
        if (nodeInfo && nodeInfo.paragraph_ids && Array.isArray(nodeInfo.paragraph_ids)) {
          console.log(`ğŸ”— [æ˜ å°„åˆ›å»º] èŠ‚ç‚¹ ${nodeId} åŒ…å«æ®µè½:`, nodeInfo.paragraph_ids);
          
          // ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºåˆ°èŠ‚ç‚¹çš„æ˜ å°„
          nodeInfo.paragraph_ids.forEach(paraId => {
            if (paraId && typeof paraId === 'string') {
              // ç»Ÿä¸€æ®µè½IDæ ¼å¼
              const paragraphId = paraId.startsWith('para-') ? paraId : `para-${paraId}`;
              
              // æ®µè½åˆ°èŠ‚ç‚¹çš„æ˜ å°„ï¼ˆå¤šå¯¹ä¸€ï¼šå¤šä¸ªæ®µè½å¯èƒ½å¯¹åº”åŒä¸€ä¸ªèŠ‚ç‚¹ï¼‰
              newTextToNodeMap[paragraphId] = nodeId;
              
              console.log(`ğŸ“ [æ˜ å°„åˆ›å»º] ${paragraphId} -> èŠ‚ç‚¹ ${nodeId}`);
            } else {
              console.warn(`ğŸ“ [æ˜ å°„åˆ›å»º] æ— æ•ˆçš„æ®µè½ID:`, paraId);
            }
          });
          
          // èŠ‚ç‚¹åˆ°æ®µè½ç»„çš„æ˜ å°„ï¼ˆä¸€å¯¹å¤šï¼šä¸€ä¸ªèŠ‚ç‚¹å¯¹åº”å¤šä¸ªæ®µè½ï¼‰
          newNodeToTextMap[nodeId] = nodeInfo.paragraph_ids.map(paraId => 
            paraId.startsWith('para-') ? paraId : `para-${paraId}`
          );
          
          console.log(`ğŸ”— [æ˜ å°„åˆ›å»º] èŠ‚ç‚¹ ${nodeId} -> æ®µè½ç»„ [${newNodeToTextMap[nodeId].join(', ')}]`);
        } else {
          console.warn(`ğŸ”— [æ˜ å°„åˆ›å»º] èŠ‚ç‚¹ ${nodeId} ç¼ºå°‘æœ‰æ•ˆçš„æ®µè½IDæ•°ç»„:`, nodeInfo);
        }
      });
      
      console.log('ğŸ”— [æ˜ å°„åˆ›å»º] æ˜ å°„åˆ›å»ºå®Œæˆ');
      console.log('ğŸ”— [æ˜ å°„åˆ›å»º] æ®µè½åˆ°èŠ‚ç‚¹æ˜ å°„æ•°é‡:', Object.keys(newTextToNodeMap).length);
      console.log('ğŸ”— [æ˜ å°„åˆ›å»º] èŠ‚ç‚¹åˆ°æ®µè½æ˜ å°„æ•°é‡:', Object.keys(newNodeToTextMap).length);
      
      // è°ƒç”¨updateDynamicMappingæ¥æ›´æ–°çŠ¶æ€
      updateDynamicMapping(newTextToNodeMap, newNodeToTextMap);
    } else {
      console.warn('ğŸ”— [æ˜ å°„åˆ›å»º] nodeMappingæ— æ•ˆï¼Œè·³è¿‡æ˜ å°„åˆ›å»º');
    }
  }, [updateDynamicMapping]);

  // æ–‡æ¡£æŸ¥çœ‹åŒºåŸŸåˆ‡æ¢æŒ‰é’®
  const ViewModeToggle = () => {
    if (!isPdfFile) return null;

    return (
      <div className="flex bg-gray-100 dark:bg-gray-700 p-0.5 rounded mb-2">
        <button
          onClick={() => setViewMode('markdown')}
          className={`flex-1 flex items-center justify-center px-2 py-1 rounded text-xs font-medium transition-all ${
            viewMode === 'markdown'
              ? 'bg-white dark:bg-gray-600 text-blue-600 dark:text-blue-400 shadow-sm'
              : 'text-gray-600 dark:text-gray-400 hover:text-gray-800 dark:hover:text-gray-200'
          }`}
        >
          <FileText className="h-3 w-3 mr-1" />
          è½¬æ¢åçš„Markdown
        </button>
        <button
          onClick={() => setViewMode('pdf')}
          className={`flex-1 flex items-center justify-center px-2 py-1 rounded text-xs font-medium transition-all ${
            viewMode === 'pdf'
              ? 'bg-white dark:bg-gray-600 text-red-600 dark:text-red-400 shadow-sm'
              : 'text-gray-600 dark:text-gray-400 hover:text-gray-800 dark:hover:text-gray-200'
          }`}
        >
          <File className="h-3 w-3 mr-1" />
          åŸå§‹PDFæ–‡ä»¶
        </button>
      </div>
    );
  };

  // è·Ÿè¸ªchunksåŠ è½½çŠ¶æ€
  const [chunksLoaded, setChunksLoaded] = useState(false);

  // å½“documentIdæ”¹å˜æ—¶ï¼Œé‡ç½®chunksåŠ è½½çŠ¶æ€
  useEffect(() => {
    setChunksLoaded(false);
    contentChunks.current = []; // ä¹Ÿæ¸…ç©ºä¹‹å‰çš„chunks
  }, [documentId]);

  // åœ¨æ–‡æ¡£åŠ è½½å®Œæˆåï¼ŒåŠ è½½æ–‡æ¡£ç»“æ„å’Œchunks
  useEffect(() => {
    // åªå¯¹çœŸå®ä¸Šä¼ çš„æ–‡æ¡£ï¼ˆéç¤ºä¾‹æ¨¡å¼ï¼‰åŠ è½½ç»“æ„ï¼Œä¸”åªåŠ è½½ä¸€æ¬¡
    if (document && !documentId.startsWith('demo-') && document.content && !chunksLoaded) {
      const loadChunks = async () => {
        console.log('ğŸ“„ [æ–‡æ¡£åŠ è½½] å¼€å§‹åŠ è½½æ–‡æ¡£ç»“æ„å’Œchunks');
        const chunks = await loadDocumentStructure();
        if (chunks && chunks.length > 0) {
          contentChunks.current = chunks;
          setChunksLoaded(true); // è®¾ç½®chunksåŠ è½½å®Œæˆæ ‡å¿—
          console.log('ğŸ“„ [æ–‡æ¡£åŠ è½½] æˆåŠŸè®¾ç½®chunksåˆ°contentChunks.currentï¼Œæ•°é‡:', chunks.length);
        } else {
          console.log('ğŸ“„ [æ–‡æ¡£åŠ è½½] æ²¡æœ‰è·å–åˆ°chunksæ•°æ®');
        }
      };
      
      loadChunks();
    }
  }, [document, documentId, loadDocumentStructure, chunksLoaded]);

  // ğŸ”‘ æ–°å¢ï¼šé˜²æ­¢åŠ¨æ€æ˜ å°„é‡å¤æ‰§è¡Œçš„æ ‡å¿—
  const mappingInitialized = useRef(false);

  // åœ¨æ–‡æ¡£ã€chunkså’Œæ€ç»´å¯¼å›¾éƒ½åŠ è½½å®Œæˆåï¼Œåˆ›å»ºåŠ¨æ€æ˜ å°„
  useEffect(() => {
    // ğŸ”‘ åªæœ‰åœ¨æ‰€æœ‰æ¡ä»¶æ»¡è¶³ï¼Œå¹¶ä¸”æ˜ å°„å°šæœªåˆå§‹åŒ–æ—¶ï¼Œæ‰æ‰§è¡Œ
    if (!documentId.startsWith('demo-') && document && document.content && chunksLoaded && !mappingInitialized.current) {
      const mermaidCode = document.mermaid_code_demo;
      const nodeMapping = document.node_mappings_demo;
      
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] useEffectè§¦å‘æ¡ä»¶æ£€æŸ¥:');
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] documentIdæ˜¯å¦édemo:', !documentId.startsWith('demo-'));
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] documentå­˜åœ¨:', !!document);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] document.contentå­˜åœ¨:', !!document?.content);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] chunksLoaded:', chunksLoaded);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] mappingInitialized.current:', mappingInitialized.current);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] contentChunks.currentæ•°é‡:', contentChunks.current?.length || 0);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] mermaidCodeå­˜åœ¨:', !!mermaidCode);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] mermaidCodeé•¿åº¦:', mermaidCode?.length || 0);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] nodeMappingå­˜åœ¨:', !!nodeMapping);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] nodeMappingç±»å‹:', typeof nodeMapping);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] nodeMappingå†…å®¹:', nodeMapping);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] nodeMappingé”®æ•°é‡:', nodeMapping ? Object.keys(nodeMapping).length : 0);
      
      if (mermaidCode && contentChunks.current.length > 0) {
        console.log('ğŸ”— [ä¸»ç»„ä»¶] ğŸš€ æ­£åœ¨è¿›è¡Œé¦–æ¬¡åŠ¨æ€æ˜ å°„åˆ›å»º...');
        console.log('ğŸ”— [ä¸»ç»„ä»¶] å‚æ•°æ£€æŸ¥ - chunksæ•°é‡:', contentChunks.current.length);
        console.log('ğŸ”— [ä¸»ç»„ä»¶] å‚æ•°æ£€æŸ¥ - mermaidCodeå‰100å­—ç¬¦:', mermaidCode.substring(0, 100));
        console.log('ğŸ”— [ä¸»ç»„ä»¶] å‚æ•°æ£€æŸ¥ - nodeMappingè¯¦æƒ…:', JSON.stringify(nodeMapping, null, 2));
        
        // è°ƒç”¨æ›´æ–°åŠ¨æ€æ˜ å°„å‡½æ•°
        console.log('ğŸ”— [ä¸»ç»„ä»¶] ğŸ“ æ­£åœ¨è°ƒç”¨createDynamicMapping...');
        createDynamicMapping(contentChunks.current, mermaidCode, nodeMapping);
        console.log('ğŸ”— [ä¸»ç»„ä»¶] âœ… createDynamicMappingè°ƒç”¨å®Œæˆ');
        
        // ğŸ”‘ å…³é”®ï¼šæ ‡è®°ä¸ºå·²åˆå§‹åŒ–ï¼Œé˜²æ­¢é‡å¤æ‰§è¡Œ
        mappingInitialized.current = true;
        console.log('ğŸ”— [ä¸»ç»„ä»¶] ğŸ”’ æ˜ å°„å·²æ ‡è®°ä¸ºåˆå§‹åŒ–å®Œæˆï¼Œé˜²æ­¢é‡å¤æ‰§è¡Œ');
      } else {
        console.log('ğŸ”— [ä¸»ç»„ä»¶] âŒ åŠ¨æ€æ˜ å°„åˆ›å»ºæ¡ä»¶ä¸æ»¡è¶³:');
        if (!mermaidCode) {
          console.log('ğŸ”— [ä¸»ç»„ä»¶] - ç¼ºå°‘mermaidCodeï¼Œç­‰å¾…æ€ç»´å¯¼å›¾ç”Ÿæˆå®Œæˆ...');
        }
        if (contentChunks.current.length === 0) {
          console.log('ğŸ”— [ä¸»ç»„ä»¶] - ç¼ºå°‘contentChunksï¼Œchunksæ•°é‡:', contentChunks.current.length);
        }
      }
    } else {
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] useEffectè§¦å‘æ¡ä»¶ä¸æ»¡è¶³:');
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] - documentId:', documentId);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] - æ˜¯å¦demoæ¨¡å¼:', documentId.startsWith('demo-'));
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] - documentå­˜åœ¨:', !!document);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] - chunksLoaded:', chunksLoaded);
      console.log('ğŸ”— [ä¸»ç»„ä»¶åŠ¨æ€æ˜ å°„] - mappingInitialized.current:', mappingInitialized.current);
    }
  }, [document, chunksLoaded, createDynamicMapping, documentId]);

  // è°ƒè¯•æ–‡æ¡£çŠ¶æ€
  useEffect(() => {
    if (document) {
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] æ–‡æ¡£åŠ è½½å®Œæˆï¼ŒåŸºæœ¬ä¿¡æ¯:');
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - documentId:', documentId);
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - æ˜¯å¦demoæ¨¡å¼:', documentId.startsWith('demo-'));
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - document.contentå­˜åœ¨:', !!document.content);
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - document.contenté•¿åº¦:', document.content?.length || 0);
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - document.mermaid_code_demoå­˜åœ¨:', !!document.mermaid_code_demo);
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - document.mermaid_code_demoé•¿åº¦:', document.mermaid_code_demo?.length || 0);
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - document.node_mappings_demoå­˜åœ¨:', !!document.node_mappings_demo);
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - document.node_mappings_demoç±»å‹:', typeof document.node_mappings_demo);
      if (document.node_mappings_demo) {
        console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - node_mappings_demoé”®æ•°é‡:', Object.keys(document.node_mappings_demo).length);
        console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - node_mappings_demoæ ·æœ¬é”®:', Object.keys(document.node_mappings_demo).slice(0, 3));
      }
      console.log('ğŸ“„ [æ–‡æ¡£è°ƒè¯•] - å®Œæ•´documentå¯¹è±¡:', document);
      
      // æš´éœ²å…¨å±€è°ƒè¯•å‡½æ•°
      if (typeof window !== 'undefined') {
        window.debugDocument = () => {
          console.log('=== ğŸ“„ æ–‡æ¡£è°ƒè¯•ä¿¡æ¯ ===');
          console.log('æ–‡æ¡£ID:', documentId);
          console.log('æ–‡æ¡£å¯¹è±¡:', document);
          console.log('chunksåŠ è½½çŠ¶æ€:', chunksLoaded);
          console.log('chunksæ•°æ®:', contentChunks.current);
          console.log('æ€ç»´å¯¼å›¾ä»£ç :', document?.mermaid_code_demo?.substring(0, 200) + '...');
          console.log('èŠ‚ç‚¹æ˜ å°„:', document?.node_mappings_demo);
          console.log('=== ğŸ“„ è°ƒè¯•ä¿¡æ¯ç»“æŸ ===');
          return {
            documentId,
            document,
            chunksLoaded,
            chunks: contentChunks.current,
            mermaidCode: document?.mermaid_code_demo,
            nodeMapping: document?.node_mappings_demo
          };
        };
        console.log('ğŸ”§ [å…¨å±€è°ƒè¯•] debugDocumentå‡½æ•°å·²æŒ‚è½½ï¼Œå¯åœ¨æ§åˆ¶å°è°ƒç”¨ window.debugDocument()');
      }
    }
  }, [document, documentId, chunksLoaded]);

  // ğŸ”‘ æ–°å¢ï¼šæ·»åŠ å­èŠ‚ç‚¹çš„å›è°ƒå‡½æ•°
  const handleAddChildNode = useCallback(async (parentNodeId) => {
    try {
      console.log('ğŸ†• [çˆ¶ç»„ä»¶] æ·»åŠ å­èŠ‚ç‚¹:', parentNodeId);
      
      // ç”Ÿæˆæ–°èŠ‚ç‚¹IDå’Œè¾¹IDï¼ˆä½¿ç”¨æ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§ï¼‰
      const newNodeId = `node_${Date.now()}`;
      const newEdgeId = `edge_${parentNodeId}_${newNodeId}`;
      const newNodeLabel = 'æ–°èŠ‚ç‚¹';
      
      // æ›´æ–°documentçŠ¶æ€
      setDocument(prevDoc => {
        if (!prevDoc) {
          console.warn('ğŸ†• [çˆ¶ç»„ä»¶] documentä¸å­˜åœ¨ï¼Œæ— æ³•æ·»åŠ å­èŠ‚ç‚¹');
          return prevDoc;
        }
        
        // åˆ›å»ºæ–°çš„node_mappings
        const newNodeMappings = {
          ...prevDoc.node_mappings_demo,
          [newNodeId]: {
            text_snippet: newNodeLabel,
            paragraph_ids: []
          }
        };
        
        // åˆ›å»ºæ–°çš„edgesï¼ˆå¦‚æœå­˜åœ¨edgesæ•°ç»„ï¼‰
        const newEdges = prevDoc.edges ? [
          ...prevDoc.edges,
          {
            id: newEdgeId,
            source: parentNodeId,
            target: newNodeId,
            type: 'smoothstep'
          }
        ] : [];
        
        // æ›´æ–°mermaidä»£ç ï¼ˆæ·»åŠ æ–°çš„èŠ‚ç‚¹å’Œè¿æ¥ï¼‰
        let updatedMermaidCode = prevDoc.mermaid_code_demo || '';
        if (updatedMermaidCode) {
          updatedMermaidCode += `\n    ${parentNodeId} --> ${newNodeId}[${newNodeLabel}]`;
        }
        
        console.log('ğŸ†• [çˆ¶ç»„ä»¶] å­èŠ‚ç‚¹æ·»åŠ å®Œæˆï¼Œæ–°èŠ‚ç‚¹ID:', newNodeId);
        
        return {
          ...prevDoc,
          node_mappings_demo: newNodeMappings,
          edges: newEdges,
          mermaid_code_demo: updatedMermaidCode
        };
      });
      
      // å¦‚æœä¸æ˜¯ç¤ºä¾‹æ¨¡å¼ï¼Œè°ƒç”¨åç«¯API
      if (!documentId.startsWith('demo-')) {
        try {
          // è¿™é‡Œå¯ä»¥æ·»åŠ åç«¯APIè°ƒç”¨
          console.log('ğŸ†• [çˆ¶ç»„ä»¶] åç«¯APIè°ƒç”¨æš‚æœªå®ç°');
        } catch (apiError) {
          console.error('âŒ [çˆ¶ç»„ä»¶] æ·»åŠ å­èŠ‚ç‚¹APIè°ƒç”¨å¤±è´¥:', apiError);
        }
      }
    } catch (error) {
      console.error('âŒ [çˆ¶ç»„ä»¶] æ·»åŠ å­èŠ‚ç‚¹å¤±è´¥:', error);
    }
  }, [documentId, setDocument]);
  
  // ğŸ”‘ æ–°å¢ï¼šæ·»åŠ åŒçº§èŠ‚ç‚¹çš„å›è°ƒå‡½æ•°
  const handleAddSiblingNode = useCallback(async (siblingNodeId) => {
    try {
      console.log('ğŸ†• [çˆ¶ç»„ä»¶] æ·»åŠ åŒçº§èŠ‚ç‚¹:', siblingNodeId);
      
      // ä»å½“å‰documentçš„edgesä¸­æ‰¾åˆ°åŒçº§èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹
      const parentEdge = document?.edges?.find(edge => edge.target === siblingNodeId);
      if (!parentEdge && document?.mermaid_code_demo) {
        // å¦‚æœæ²¡æœ‰edgesæ•°ç»„ï¼Œå°è¯•ä»mermaidä»£ç ä¸­è§£æ
        const mermaidLines = document.mermaid_code_demo.split('\n');
        const parentLine = mermaidLines.find(line => line.includes(`--> ${siblingNodeId}`));
        if (parentLine) {
          const match = parentLine.match(/(\w+)\s*-->\s*\w+/);
          if (match) {
            const parentNodeId = match[1];
            await addSiblingWithParent(siblingNodeId, parentNodeId);
            return;
          }
        }
        console.warn('âŒ [çˆ¶ç»„ä»¶] æ— æ³•æ‰¾åˆ°åŒçº§èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹');
        return;
      }
      
      const parentNodeId = parentEdge?.source;
      if (!parentNodeId) {
        console.warn('âŒ [çˆ¶ç»„ä»¶] æ— æ³•ç¡®å®šçˆ¶èŠ‚ç‚¹ID');
        return;
      }
      
      await addSiblingWithParent(siblingNodeId, parentNodeId);
      
    } catch (error) {
      console.error('âŒ [çˆ¶ç»„ä»¶] æ·»åŠ åŒçº§èŠ‚ç‚¹å¤±è´¥:', error);
    }
  }, [document]);
  
  // æ·»åŠ åŒçº§èŠ‚ç‚¹çš„è¾…åŠ©å‡½æ•°
  const addSiblingWithParent = useCallback(async (siblingNodeId, parentNodeId) => {
    const newNodeId = `node_${Date.now()}`;
    const newEdgeId = `edge_${parentNodeId}_${newNodeId}`;
    const newNodeLabel = 'æ–°èŠ‚ç‚¹';
    
    // æ›´æ–°documentçŠ¶æ€
    setDocument(prevDoc => {
      if (!prevDoc) {
        console.warn('ğŸ†• [çˆ¶ç»„ä»¶] documentä¸å­˜åœ¨ï¼Œæ— æ³•æ·»åŠ åŒçº§èŠ‚ç‚¹');
        return prevDoc;
      }
      
      // åˆ›å»ºæ–°çš„node_mappings
      const newNodeMappings = {
        ...prevDoc.node_mappings_demo,
        [newNodeId]: {
          text_snippet: newNodeLabel,
          paragraph_ids: []
        }
      };
      
      // åˆ›å»ºæ–°çš„edgesï¼ˆå¦‚æœå­˜åœ¨edgesæ•°ç»„ï¼‰
      const newEdges = prevDoc.edges ? [
        ...prevDoc.edges,
        {
          id: newEdgeId,
          source: parentNodeId,
          target: newNodeId,
          type: 'smoothstep'
        }
      ] : [];
      
      // æ›´æ–°mermaidä»£ç ï¼ˆæ·»åŠ æ–°çš„èŠ‚ç‚¹å’Œè¿æ¥ï¼‰
      let updatedMermaidCode = prevDoc.mermaid_code_demo || '';
      if (updatedMermaidCode) {
        updatedMermaidCode += `\n    ${parentNodeId} --> ${newNodeId}[${newNodeLabel}]`;
      }
      
      console.log('ğŸ†• [çˆ¶ç»„ä»¶] åŒçº§èŠ‚ç‚¹æ·»åŠ å®Œæˆï¼Œæ–°èŠ‚ç‚¹ID:', newNodeId);
      
      return {
        ...prevDoc,
        node_mappings_demo: newNodeMappings,
        edges: newEdges,
        mermaid_code_demo: updatedMermaidCode
      };
    });
  }, [setDocument]);
  
  // ğŸ”‘ æ–°å¢ï¼šåˆ é™¤èŠ‚ç‚¹çš„å›è°ƒå‡½æ•°
  const handleDeleteNode = useCallback(async (nodeIdToDelete) => {
    try {
      console.log('ğŸ—‘ï¸ [çˆ¶ç»„ä»¶] åˆ é™¤èŠ‚ç‚¹:', nodeIdToDelete);
      
      // æ›´æ–°documentçŠ¶æ€
      setDocument(prevDoc => {
        if (!prevDoc) {
          console.warn('ğŸ—‘ï¸ [çˆ¶ç»„ä»¶] documentä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤èŠ‚ç‚¹');
          return prevDoc;
        }
        
        // ç§»é™¤èŠ‚ç‚¹æ˜ å°„
        const newNodeMappings = { ...prevDoc.node_mappings_demo };
        delete newNodeMappings[nodeIdToDelete];
        
        // ç§»é™¤ç›¸å…³çš„edgesï¼ˆå¦‚æœå­˜åœ¨edgesæ•°ç»„ï¼‰
        const newEdges = prevDoc.edges ? 
          prevDoc.edges.filter(edge => 
            edge.source !== nodeIdToDelete && edge.target !== nodeIdToDelete
          ) : [];
        
        // æ›´æ–°mermaidä»£ç ï¼ˆç§»é™¤ç›¸å…³çš„èŠ‚ç‚¹å’Œè¿æ¥ï¼‰
        let updatedMermaidCode = prevDoc.mermaid_code_demo || '';
        if (updatedMermaidCode) {
          const lines = updatedMermaidCode.split('\n');
          const filteredLines = lines.filter(line => 
            !line.includes(nodeIdToDelete) && 
            !line.includes(`--> ${nodeIdToDelete}`) &&
            !line.includes(`${nodeIdToDelete} -->`)
          );
          updatedMermaidCode = filteredLines.join('\n');
        }
        
        console.log('ğŸ—‘ï¸ [çˆ¶ç»„ä»¶] èŠ‚ç‚¹åˆ é™¤å®Œæˆ');
        
        return {
          ...prevDoc,
          node_mappings_demo: newNodeMappings,
          edges: newEdges,
          mermaid_code_demo: updatedMermaidCode
        };
      });
      
      // å¦‚æœä¸æ˜¯ç¤ºä¾‹æ¨¡å¼ï¼Œè°ƒç”¨åç«¯API
      if (!documentId.startsWith('demo-')) {
        try {
          // è¿™é‡Œå¯ä»¥æ·»åŠ åç«¯APIè°ƒç”¨
          console.log('ğŸ—‘ï¸ [çˆ¶ç»„ä»¶] åç«¯APIè°ƒç”¨æš‚æœªå®ç°');
        } catch (apiError) {
          console.error('âŒ [çˆ¶ç»„ä»¶] åˆ é™¤èŠ‚ç‚¹APIè°ƒç”¨å¤±è´¥:', apiError);
        }
      }
    } catch (error) {
      console.error('âŒ [çˆ¶ç»„ä»¶] åˆ é™¤èŠ‚ç‚¹å¤±è´¥:', error);
    }
  }, [documentId, setDocument]);

  // å¤„ç† node_mappings æ›´æ–°çš„å‡½æ•°
  const handleNodeMappingUpdate = useCallback(async (newNodeMappings) => {
    try {
      console.log('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] å¼€å§‹æ›´æ–° node_mappings:', newNodeMappings);
      
      // æ›´æ–°å‰ç«¯çŠ¶æ€
      setDocument(prev => ({
        ...prev,
        node_mappings_demo: newNodeMappings
      }));
      
      console.log('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] å‰ç«¯çŠ¶æ€å·²æ›´æ–°');
      
      // å¦‚æœä¸æ˜¯ç¤ºä¾‹æ¨¡å¼ï¼Œè°ƒç”¨åç«¯APIè¿›è¡ŒæŒä¹…åŒ–
      if (!documentId.startsWith('demo-')) {
        console.log('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] å¼€å§‹è°ƒç”¨åç«¯APIä¿å­˜æ˜ å°„');
        
        const response = await axios.post(`http://localhost:8000/api/document/${documentId}/remap`, {
          node_mappings: newNodeMappings
        });
        
        if (response.data.success) {
          console.log('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] âœ… åç«¯ä¿å­˜æˆåŠŸ');
          toast.success('æ‹–æ‹½æ’åºå·²ä¿å­˜');
        } else {
          console.error('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] âŒ åç«¯ä¿å­˜å¤±è´¥:', response.data.message);
          toast.error('ä¿å­˜å¤±è´¥: ' + response.data.message);
        }
      } else {
        console.log('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] ç¤ºä¾‹æ¨¡å¼ï¼Œè·³è¿‡åç«¯ä¿å­˜');
      }
      
      // æ›´æ–°åŠ¨æ€æ˜ å°„ä»¥åæ˜ æ–°çš„èŠ‚ç‚¹å…³ç³»
      if (contentChunks.current.length > 0 && document && document.mermaid_code_demo) {
        console.log('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] é‡æ–°ç”ŸæˆåŠ¨æ€æ˜ å°„');
        createDynamicMapping(contentChunks.current, document.mermaid_code_demo, newNodeMappings);
      }
      
    } catch (error) {
      console.error('ğŸ“ [èŠ‚ç‚¹æ˜ å°„æ›´æ–°] é”™è¯¯:', error);
      const errorMessage = error.response?.data?.detail || 'ä¿å­˜èŠ‚ç‚¹æ˜ å°„å¤±è´¥';
      toast.error(errorMessage);
    }
  }, [documentId, setDocument, createDynamicMapping, document]);

  // å¤„ç†æ‹–æ‹½æ’åºåçš„å›è°ƒå‡½æ•°
  const handleOrderChange = useCallback(async (newItems) => {
    try {
      console.log('ğŸ“ [æ’åºæ›´æ–°] å¼€å§‹å¤„ç†æ‹–æ‹½æ’åºç»“æœ');
      console.log('ğŸ“ [æ’åºæ›´æ–°] æ–°é¡¹ç›®é¡ºåºæ•°ç»„é•¿åº¦:', newItems?.length || 0);
      console.log('ğŸ“ [æ’åºæ›´æ–°] æ–°é¡¹ç›®é¡ºåº:', newItems);
      
      // å¥å£®æ€§æ£€æŸ¥
      if (!newItems || newItems.length === 0) {
        console.warn('ğŸ“ [æ’åºæ›´æ–°] âš ï¸ æ–°é¡¹ç›®æ•°ç»„ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†');
        return;
      }
      
      // å¥å£®æ€§æ£€æŸ¥ï¼šç¡®ä¿ document å¯¹è±¡å­˜åœ¨
      const docObj = document;
      if (!docObj) {
        console.warn('ğŸ“ [æ’åºæ›´æ–°] âš ï¸ document å¯¹è±¡ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†');
        return;
      }
      
      // é‡æ–°è®¡ç®— node_mappings - ä½¿ç”¨ SortableContentRenderer ä¸­çš„é‡æ„ç‰ˆæœ¬é€»è¾‘
      const recalculateNodeMappings = (sortedItems) => {
        console.log('ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] å¼€å§‹é‡æ–°è®¡ç®— node_mappings');
        console.log('ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] è¾“å…¥å‚æ•°:', { 
          sortedItemsLength: sortedItems?.length || 0, 
          nodeMapping: !!docObj.node_mappings_demo
        });
        
        // å¥å£®æ€§æ£€æŸ¥ï¼šå¦‚æœè¾“å…¥çš„ items æ•°ç»„ä¸ºç©ºï¼Œè¿”å›ç©ºçš„ node_mappings å¯¹è±¡
        if (!sortedItems || sortedItems.length === 0) {
          console.log('ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] âš ï¸ è¾“å…¥é¡¹ç›®ä¸ºç©ºï¼Œè¿”å›ç©ºæ˜ å°„');
          return {};
        }
        
        if (!docObj.node_mappings_demo) {
          console.log('ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] âš ï¸ ç¼ºå°‘èŠ‚ç‚¹æ˜ å°„ï¼Œè·³è¿‡é‡æ–°è®¡ç®—');
          return {};
        }
        
        const newNodeMappings = {};
        let currentNodeId = null;
        
        // è·å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹IDä½œä¸ºé»˜è®¤å€¼ï¼Œå¤„ç†æ®µè½å‡ºç°åœ¨æ‰€æœ‰åˆ†å‰²çº¿ä¹‹å‰çš„è¾¹ç•Œæƒ…å†µ
        const firstNodeId = Object.keys(docObj.node_mappings_demo)[0];
        console.log('ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] é»˜è®¤ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ID:', firstNodeId);
        
        // éå†æ’åºåçš„é¡¹ç›®åˆ—è¡¨
        sortedItems.forEach((item, index) => {
          if (item.type === 'divider') {
            // é‡åˆ°åˆ†å‰²çº¿ï¼Œè®¾ç½®å½“å‰èŠ‚ç‚¹ID
            currentNodeId = item.nodeId;
            console.log(`ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] ä½ç½® ${index}: è¿›å…¥èŠ‚ç‚¹ ${currentNodeId}`);
          } else if (item.type === 'paragraph') {
            // é‡åˆ°æ®µè½ï¼Œå°†å…¶åˆ†é…ç»™å½“å‰èŠ‚ç‚¹
            // å¦‚æœè¿˜æ²¡æœ‰é‡åˆ°åˆ†å‰²çº¿ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºé»˜è®¤å€¼
            const targetNodeId = currentNodeId || firstNodeId;
            
            if (targetNodeId) {
              // ç¡®ä¿ newNodeMappings[targetNodeId] å·²ç»å­˜åœ¨å¹¶ä¸”æ˜¯ä¸€ä¸ªåŒ…å« paragraph_ids æ•°ç»„çš„å¯¹è±¡
              if (!newNodeMappings[targetNodeId]) {
                // ä»åŸå§‹ nodeMapping ä¸­å¤åˆ¶èŠ‚ç‚¹ä¿¡æ¯
                newNodeMappings[targetNodeId] = {
                  ...docObj.node_mappings_demo[targetNodeId],
                  paragraph_ids: []
                };
                console.log(`ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] åˆå§‹åŒ–èŠ‚ç‚¹ ${targetNodeId} çš„æ˜ å°„`);
              }
              
              // å°†æ®µè½IDæ·»åŠ åˆ°å½“å‰èŠ‚ç‚¹
              newNodeMappings[targetNodeId].paragraph_ids.push(item.paragraphId);
              console.log(`ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] ä½ç½® ${index}: æ®µè½ ${item.paragraphId} åˆ†é…ç»™èŠ‚ç‚¹ ${targetNodeId}`);
            } else {
              console.warn(`ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] è­¦å‘Š: æ®µè½ ${item.paragraphId} åœ¨ä½ç½® ${index} æ²¡æœ‰å¯¹åº”çš„èŠ‚ç‚¹`);
            }
          }
        });
        
        console.log('ğŸ“ [æ’åºæ›´æ–°-é‡æ–°è®¡ç®—] æ–°çš„ node_mappings:', newNodeMappings);
        return newNodeMappings;
      };
      
      // é‡æ–°è®¡ç®—èŠ‚ç‚¹æ˜ å°„
      const newNodeMappings = recalculateNodeMappings(newItems);
      
      if (Object.keys(newNodeMappings).length === 0) {
        console.warn('ğŸ“ [æ’åºæ›´æ–°] âš ï¸ é‡æ–°è®¡ç®—ç»“æœä¸ºç©ºï¼Œè·³è¿‡åç»­å¤„ç†');
        return;
      }
      
      console.log('ğŸ“ [æ’åºæ›´æ–°] å¼€å§‹æ›´æ–°å‰ç«¯çŠ¶æ€');
      
      // æ›´æ–°å‰ç«¯çŠ¶æ€
      setDocument(prev => {
        if (!prev) {
          console.warn('ğŸ“ [æ’åºæ›´æ–°] âš ï¸ å‰ä¸€ä¸ªæ–‡æ¡£çŠ¶æ€ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°');
          return prev;
        }
        
        const updatedDocument = {
          ...prev,
          node_mappings_demo: newNodeMappings
        };
        console.log('ğŸ“ [æ’åºæ›´æ–°] å‰ç«¯çŠ¶æ€å·²æ›´æ–°');
        
        // ç«‹å³é‡æ–°ç”ŸæˆåŠ¨æ€æ˜ å°„
        if (contentChunks.current.length > 0 && prev.mermaid_code_demo) {
          console.log('ğŸ“ [æ’åºæ›´æ–°] é‡æ–°ç”ŸæˆåŠ¨æ€æ˜ å°„');
          createDynamicMapping(contentChunks.current, prev.mermaid_code_demo, newNodeMappings);
        }
        
        return updatedDocument;
      });
      
      console.log('ğŸ“ [æ’åºæ›´æ–°] å¼€å§‹è°ƒç”¨åç«¯APIä¿å­˜æ˜ å°„');
      
      // å¦‚æœä¸æ˜¯ç¤ºä¾‹æ¨¡å¼ï¼Œè°ƒç”¨åç«¯APIè¿›è¡ŒæŒä¹…åŒ–
      if (!documentId.startsWith('demo-')) {
        console.log('ğŸ“ [æ’åºæ›´æ–°] è°ƒç”¨åç«¯APIä¿å­˜èŠ‚ç‚¹æ˜ å°„');
        
        const response = await axios.post(`http://localhost:8000/api/document/${documentId}/remap`, {
          node_mappings: newNodeMappings
        });
        
        if (response.data.success) {
          console.log('ğŸ“ [æ’åºæ›´æ–°] âœ… åç«¯ä¿å­˜æˆåŠŸ');
          toast.success('æ‹–æ‹½æ’åºå·²ä¿å­˜');
        } else {
          console.error('ğŸ“ [æ’åºæ›´æ–°] âŒ åç«¯ä¿å­˜å¤±è´¥:', response.data.message);
          toast.error('ä¿å­˜å¤±è´¥: ' + response.data.message);
        }
      } else {
        console.log('ğŸ“ [æ’åºæ›´æ–°] ç¤ºä¾‹æ¨¡å¼ï¼Œè·³è¿‡åç«¯ä¿å­˜');
        toast.success('æ‹–æ‹½æ’åºå·²æ›´æ–°ï¼ˆç¤ºä¾‹æ¨¡å¼ï¼‰');
      }
      
    } catch (error) {
      console.error('ğŸ“ [æ’åºæ›´æ–°] é”™è¯¯:', error);
      const errorMessage = error.response?.data?.detail || 'å¤„ç†æ‹–æ‹½æ’åºå¤±è´¥';
      toast.error(errorMessage);
    }
  }, [documentId, document, setDocument, createDynamicMapping]);

  // åŠ è½½çŠ¶æ€
  if (loading) {
    return (
      <div className="min-h-screen flex items-center justify-center bg-gray-50 dark:bg-gray-900">
        <div className="text-center">
          <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mx-auto mb-4"></div>
          <p className="text-lg text-gray-700 dark:text-gray-300">æ­£åœ¨åŠ è½½æ–‡æ¡£...</p>
        </div>
      </div>
    );
  }

  // é”™è¯¯çŠ¶æ€
  if (documentError) {
    return (
      <div className="min-h-screen flex items-center justify-center bg-gray-50 dark:bg-gray-900">
        <div className="text-center max-w-md">
          <div className="bg-red-50 dark:bg-red-900/20 border border-red-200 dark:border-red-700 rounded-lg p-6">
            <h2 className="text-xl font-semibold text-red-800 dark:text-red-200 mb-2">åŠ è½½å¤±è´¥</h2>
            <p className="text-red-600 dark:text-red-400 mb-4">{documentError}</p>
            <div className="space-x-3">
              <button
                onClick={loadDocument}
                className="inline-flex items-center px-4 py-2 bg-red-600 dark:bg-red-500 text-white rounded-md hover:bg-red-700 dark:hover:bg-red-600 transition-colors"
              >
                é‡è¯•
              </button>
              <button
                onClick={() => navigate('/')}
                className="inline-flex items-center px-4 py-2 bg-gray-600 dark:bg-gray-500 text-white rounded-md hover:bg-gray-700 dark:hover:bg-gray-600 transition-colors"
              >
                <ArrowLeft className="w-4 h-4 mr-2" />
                è¿”å›
              </button>
            </div>
          </div>
        </div>
      </div>
    );
  }

  // æ–‡æ¡£ä¸å­˜åœ¨
  if (!document) {
    return (
      <div className="min-h-screen flex items-center justify-center bg-gray-50 dark:bg-gray-900">
        <div className="text-center">
          <p className="text-lg text-gray-700 dark:text-gray-300">æ–‡æ¡£ä¸å­˜åœ¨</p>
          <button
            onClick={() => navigate('/')}
            className="mt-4 inline-flex items-center px-4 py-2 bg-blue-600 dark:bg-blue-500 text-white rounded-md hover:bg-blue-700 dark:hover:bg-blue-600 transition-colors"
          >
            <ArrowLeft className="w-4 h-4 mr-2" />
            è¿”å›é¦–é¡µ
          </button>
        </div>
      </div>
    );
  }

  return (
    <div className="h-screen bg-gray-50 dark:bg-gray-900 overflow-hidden flex flex-col">
      {/* ä¸‰åˆ—åˆ†å‰²å®¹å™¨ */}
      <div ref={containerRef} className="flex flex-1 h-full">
        
        {/* å·¦ä¾§ç›®å½•æ  */}
        {showToc && (
          <div 
            className="bg-white dark:bg-gray-800 border-r border-gray-200 dark:border-gray-700 shadow-sm overflow-hidden flex flex-col"
            style={{ width: `${tocPanelWidth}%` }}
          >
            <div className="px-3 py-2 border-b border-gray-200 dark:border-gray-700 bg-gray-50 dark:bg-gray-700 flex-shrink-0">
              <div className="flex items-center justify-between">
                <h2 className="text-sm font-semibold text-gray-900 dark:text-white flex items-center">
                  <FileText className="w-3 h-3 mr-1" />
                  æ–‡æ¡£ç›®å½•
                </h2>
                <button
                  onClick={() => setShowToc(false)}
                  className="text-gray-400 dark:text-gray-500 hover:text-gray-600 dark:hover:text-gray-300 transition-colors"
                >
                  <EyeOff className="w-3 h-3" />
                </button>
              </div>
            </div>
            <div className="flex-1 overflow-y-auto">
              <TableOfContents 
                toc={toc}
                expandedItems={expandedTocItems}
                activeItem={activeChunkId}
                onToggle={toggleTocItem}
                onItemClick={scrollToSection}
              />
            </div>
          </div>
        )}
        
        {/* ç›®å½•åˆ†éš”çº¿ */}
        {showToc && (
          <div
            className="w-1 bg-gray-300 dark:bg-gray-600 hover:bg-blue-500 dark:hover:bg-blue-600 cursor-col-resize flex-shrink-0 transition-colors"
            onMouseDown={(e) => handleMouseDown(e, 'toc-divider')}
          >
            <div className="w-full h-full flex items-center justify-center">
              <div className="w-0.5 h-8 bg-white dark:bg-gray-700 opacity-50 rounded"></div>
            </div>
          </div>
        )}

        {/* ä¸­é—´æ–‡æ¡£é˜…è¯»å™¨ */}
        <div 
          className="bg-white dark:bg-gray-800 border-r border-gray-200 dark:border-gray-700 shadow-sm overflow-hidden flex flex-col"
          style={{ width: `${showToc ? leftPanelWidth : leftPanelWidth + tocPanelWidth}%` }}
        >
          <div className="px-3 py-2 border-b border-gray-200 dark:border-gray-700 bg-gray-50 dark:bg-gray-700 flex-shrink-0">
            <div className="flex items-center justify-between">
              <div className="flex items-center space-x-2">
                <button
                  onClick={() => navigate('/')}
                  className="inline-flex items-center px-2 py-1 text-xs text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white transition-colors"
                >
                  <ArrowLeft className="w-3 h-3 mr-1" />
                  è¿”å›
                </button>
                {!showToc && (
                  <button
                    onClick={() => setShowToc(true)}
                    className="inline-flex items-center px-2 py-1 text-xs text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white transition-colors"
                  >
                    <Eye className="w-3 h-3 mr-1" />
                    ç›®å½•
                  </button>
                )}
                <h2 className="text-sm font-semibold text-gray-900 dark:text-white">
                  æ–‡æ¡£å†…å®¹
                  {isPdfFile && (
                    <span className="ml-2 text-xs text-gray-500 dark:text-gray-400">
                      ({viewMode === 'pdf' ? 'åŸå§‹PDF' : 'è½¬æ¢åçš„Markdown'})
                    </span>
                  )}
                </h2>
              </div>
              <div className="flex items-center space-x-2">
                <ThemeToggle className="scale-75" />
                {/* è°ƒè¯•æŒ‰é’® - åªåœ¨éç¤ºä¾‹æ¨¡å¼ä¸‹æ˜¾ç¤º */}
                {!documentId.startsWith('demo-') && (
                  <button
                    onClick={() => {
                      console.log('=== è°ƒè¯•ä¿¡æ¯ ===');
                      console.log('æ–‡æ¡£ID:', documentId);
                      console.log('å½“å‰æ´»è·ƒç« èŠ‚ID:', activeChunkId);
                      console.log('å½“å‰æ´»è·ƒæ®µè½ID:', activeContentBlockId);
                      console.log('chunksæ•°é‡:', contentChunks.current?.length || 0);
                      console.log('chunksåˆ—è¡¨:', contentChunks.current?.map(c => c.chunk_id) || []);
                      console.log('åŠ¨æ€æ˜ å°„:', dynamicMapping);
                      console.log('æ€ç»´å¯¼å›¾ä»£ç é•¿åº¦:', document?.mermaid_code_demo?.length || 0);
                      console.log('èŠ‚ç‚¹æ˜ å°„:', document?.node_mappings_demo);
                      console.log('åŸå§‹å†…å®¹é•¿åº¦:', document?.content?.length || 0);
                      console.log('å¸¦æ®µè½IDå†…å®¹é•¿åº¦:', document?.content_with_ids?.length || 0);
                      console.log('å¸¦æ®µè½IDå†…å®¹å‰100å­—ç¬¦:', document?.content_with_ids?.substring(0, 100) || 'æ— ');
                      
                      // æ£€æŸ¥é¡µé¢ä¸­çš„æ®µè½å…ƒç´ 
                      const allParagraphs = document.querySelectorAll('[id^="para-"], [data-para-id]');
                      console.log('é¡µé¢ä¸­çš„æ®µè½æ•°é‡:', allParagraphs.length);
                      console.log('æ®µè½IDåˆ—è¡¨:', Array.from(allParagraphs).map(el => el.id || el.getAttribute('data-para-id')));
                      
                      // æ˜¾ç¤ºlocalStorageä¸­çš„è°ƒè¯•æ•°æ®
                      const debugData = {
                        textToNodeMap: JSON.parse(localStorage.getItem('debug_semanticTextToNodeMap') || '{}'),
                        nodeToTextMap: JSON.parse(localStorage.getItem('debug_semanticNodeToTextMap') || '{}'),
                        aiNodeMapping: JSON.parse(localStorage.getItem('debug_aiNodeMapping') || '{}')
                      };
                      console.log('localStorageè°ƒè¯•æ•°æ®:', debugData);
                      
                      alert(`è°ƒè¯•ä¿¡æ¯å·²è¾“å‡ºåˆ°æ§åˆ¶å°\nå½“å‰æ´»è·ƒç« èŠ‚: ${activeChunkId || 'æ— '}\nå½“å‰æ´»è·ƒæ®µè½: ${activeContentBlockId || 'æ— '}\næ®µè½æ•°é‡: ${allParagraphs.length}`);
                    }}
                    className="inline-flex items-center px-2 py-1 text-xs bg-purple-600 dark:bg-purple-500 text-white rounded hover:bg-purple-700 dark:hover:bg-purple-600 transition-colors"
                  >
                    ğŸ› è°ƒè¯•
                  </button>
                )}
                <button
                  onClick={handleDownloadMarkdown}
                  className="inline-flex items-center px-2 py-1 text-xs bg-green-600 dark:bg-green-500 text-white rounded hover:bg-green-700 dark:hover:bg-green-600 transition-colors"
                >
                  <Download className="w-3 h-3 mr-1" />
                  ä¸‹è½½MD
                </button>
              </div>
            </div>
            <ViewModeToggle />
          </div>
          <div className={`flex-1 ${viewMode === 'pdf' && isPdfFile ? 'overflow-hidden' : 'overflow-y-auto p-4'}`}>
            {(() => {
              // PDFæ–‡ä»¶æ¨¡å¼
              if (viewMode === 'pdf' && isPdfFile) {
                return <PDFViewer pdfBase64={document.pdf_base64} />;
              }
              
              // çº¯ç¤ºä¾‹æ¨¡å¼ï¼ˆdemo-å¼€å¤´ä¸”æ²¡æœ‰çœŸå®å†…å®¹ï¼‰
              if (documentId.startsWith('demo-') && !document.content) {
                console.log('ğŸ“„ [æ¸²æŸ“åˆ¤æ–­] çº¯ç¤ºä¾‹æ¨¡å¼');
                return (
                  <DemoModeRenderer 
                    content={null}
                    onContentBlockRef={handleContentBlockRef}
                    nodeMapping={document.node_mappings_demo}
                    onNodeMappingUpdate={handleNodeMappingUpdate}
                    onOrderChange={handleOrderChange}
                  />
                );
              }
              
              // ä¸Šä¼ æ–‡ä»¶æ¨¡å¼ - ç­‰å¾…chunksåŠ è½½
              if (!documentId.startsWith('demo-') && !chunksLoaded) {
                console.log('ğŸ“„ [æ¸²æŸ“åˆ¤æ–­] ä¸Šä¼ æ–‡ä»¶æ¨¡å¼ - ç­‰å¾…chunksåŠ è½½');
                return (
                  <div className="flex items-center justify-center h-full">
                    <div className="text-center">
                      <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
                      <p className="text-sm text-gray-500 dark:text-gray-400">æ­£åœ¨åŠ è½½æ–‡æ¡£ç»“æ„...</p>
                    </div>
                  </div>
                );
              }
              
              // ä¸Šä¼ æ–‡ä»¶æ¨¡å¼ - chunkså·²åŠ è½½ æˆ– å¸¦å†…å®¹çš„ç¤ºä¾‹æ¨¡å¼
              console.log('ğŸ“„ [æ¸²æŸ“åˆ¤æ–­] æ¸²æŸ“çœŸå®æ–‡æ¡£å†…å®¹', {
                documentId, 
                chunksLoaded, 
                chunksCount: contentChunks.current.length,
                hasContent: !!document.content,
                hasContentWithIds: !!document.content_with_ids
              });
              
              // ä¼˜å…ˆä½¿ç”¨å¸¦æ®µè½IDçš„å†…å®¹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åŸå§‹å†…å®¹
              const contentToRender = document.content_with_ids || document.content;
              console.log('ğŸ“„ [å†…å®¹é€‰æ‹©] ä½¿ç”¨å†…å®¹ç±»å‹:', document.content_with_ids ? 'å¸¦æ®µè½IDçš„å†…å®¹' : 'åŸå§‹å†…å®¹');
              
              return (
                <DemoModeRenderer 
                  content={contentToRender}
                  onContentBlockRef={handleContentBlockRef}
                  isRealDocument={!documentId.startsWith('demo-')}
                  chunks={contentChunks.current}
                  nodeMapping={document.node_mappings_demo}
                  onNodeMappingUpdate={handleNodeMappingUpdate}
                  onOrderChange={handleOrderChange}
                />
              );
            })()}
          </div>
        </div>

        {/* ä¸»åˆ†éš”çº¿ */}
        <div
          className="w-1 bg-gray-300 dark:bg-gray-600 hover:bg-blue-500 dark:hover:bg-blue-600 cursor-col-resize flex-shrink-0 transition-colors"
          onMouseDown={(e) => handleMouseDown(e, 'main-divider')}
        >
          <div className="w-full h-full flex items-center justify-center">
            <div className="w-0.5 h-8 bg-white dark:bg-gray-700 opacity-50 rounded"></div>
          </div>
        </div>

        {/* å³ä¾§è®ºè¯ç»“æ„æµç¨‹å›¾ */}
        <div 
          className="bg-white dark:bg-gray-800 overflow-hidden flex flex-col"
          style={{ width: `${100 - (showToc ? tocPanelWidth : 0) - leftPanelWidth}%` }}
        >
          {/* è®ºè¯ç»“æ„æµç¨‹å›¾åŒºåŸŸ */}
          <div className="h-full flex flex-col">
            <div className="px-4 py-3 border-b border-gray-200 dark:border-gray-700 bg-gray-50 dark:bg-gray-700 flex-shrink-0">
              <div className="flex items-center justify-between">
                <h2 className="text-base font-semibold text-gray-900 dark:text-white">è®ºè¯ç»“æ„æµç¨‹å›¾</h2>
                <div className="flex items-center space-x-2">
                  <MindmapStatusDisplay />
                  {document.mermaid_code_demo && (
                    <button
                      onClick={() => handleDownloadMermaid('demo')}
                      className="inline-flex items-center px-2 py-1 text-xs bg-blue-600 dark:bg-blue-500 text-white rounded hover:bg-blue-700 dark:hover:bg-blue-600 transition-colors"
                    >
                      <Download className="w-3 h-3 mr-1" />
                      ä¸‹è½½æµç¨‹å›¾
                    </button>
                  )}
                </div>
              </div>
              <div className="flex items-center justify-between mt-2">
                <div className="flex items-center space-x-1 text-xs text-gray-500 dark:text-gray-400">
                  <span>åˆ†ææ–‡æ¡£çš„æ ¸å¿ƒè®ºè¯ç»“æ„å’Œé€»è¾‘æµå‘</span>
                </div>
              </div>
            </div>
            <div className="flex-1 overflow-hidden">
              {/* æµç¨‹å›¾å†…å®¹åŒºåŸŸ */}
              {(demoMindmapStatus === 'completed' && document.mermaid_code_demo) ? (
                <div className="h-full overflow-hidden">
                  <FlowDiagram 
                    ref={mermaidDiagramRef}
                    apiData={{
                      mermaid_string: document.mermaid_code_demo,
                      node_mappings: document.node_mappings_demo || {},
                      document_id: documentId
                    }}
                    highlightedNodeId={highlightedNodeId}
                    onNodeClick={handleNodeClick}
                    onNodeLabelUpdate={handleNodeLabelUpdate}
                    onAddChildNode={handleAddChildNode}
                    onAddSiblingNode={handleAddSiblingNode}
                    onDeleteNode={handleDeleteNode}
                  />
                </div>
              ) : (
                <div className="flex items-center justify-center h-full">
                  <div className="text-center max-w-md px-4">
                    <div className="bg-gray-50 dark:bg-gray-700 border border-gray-200 dark:border-gray-600 rounded-lg p-4">
                      <h3 className="text-sm font-semibold text-gray-800 dark:text-gray-200 mb-3">ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾</h3>
                      
                      <button
                        onClick={() => startMindmapGeneration('demo')}
                        className="flex items-center justify-center px-4 py-3 bg-blue-600 dark:bg-blue-500 text-white rounded-lg hover:bg-blue-700 dark:hover:bg-blue-600 transition-colors w-full"
                        disabled={demoMindmapStatus === 'generating'}
                      >
                        {demoMindmapStatus === 'generating' ? (
                          <>
                            <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-white mr-2"></div>
                            <span>åˆ†æä¸­...</span>
                          </>
                        ) : (
                          <>
                            <Eye className="w-4 h-4 mr-2" />
                            <span>å¼€å§‹åˆ†æ</span>
                          </>
                        )}
                      </button>
                      
                      <p className="text-xs text-gray-500 dark:text-gray-400 mt-2">
                        å°†åˆ†ææ–‡æ¡£çš„æ ¸å¿ƒè®ºç‚¹å’Œè®ºè¯é€»è¾‘
                      </p>
                    </div>
                  </div>
                </div>
              )}
            </div>
          </div>
        </div>
      </div>
      <ToastContainer />
    </div>
  );
};

export default ViewerPageRefactored;
</file>

</files>
