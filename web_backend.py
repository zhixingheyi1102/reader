from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import asyncio
import os
import hashlib
import tempfile
import re
from datetime import datetime
from pathlib import Path
import logging
import base64
import json

# å¯¼å…¥ç°æœ‰çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨
from mindmap_generator import MindMapGenerator, MinimalDatabaseStub, get_logger, generate_mermaid_html, DocumentOptimizer

# å¯¼å…¥æ–‡æ¡£è§£æå™¨
from document_parser import DocumentParser

# å¯¼å…¥MinerUç›¸å…³æ¨¡å—
from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.config.enums import SupportedPdfParseMethod

# ======== Phase 1: å®Œæ•´çš„å†…å­˜æ ‘æ•°æ®ç»“æ„ ========

class NodeTreeNode:
    """å®Œæ•´çš„æ ‘èŠ‚ç‚¹æ•°æ®ç»“æ„"""
    def __init__(self, node_id: str):
        self.id = node_id
        self.children = []  # å­èŠ‚ç‚¹åˆ—è¡¨ï¼ŒæŒ‰æ•°å­—é¡ºåºæ’åº
        self.parent = None  # çˆ¶èŠ‚ç‚¹å¼•ç”¨
    
    def add_child(self, child_node):
        """æ·»åŠ å­èŠ‚ç‚¹å¹¶ç»´æŠ¤æ’åº"""
        child_node.parent = self
        self.children.append(child_node)
        # æŒ‰æ•°å­—é¡ºåºæ’åºå­èŠ‚ç‚¹
        self.children.sort(key=lambda x: self._get_sort_key(x.id))
    
    def _get_sort_key(self, node_id: str):
        """è·å–èŠ‚ç‚¹æ’åºé”®"""
        parts = node_id.split('.')
        try:
            return int(parts[-1])
        except ValueError:
            return 999
    
    def get_all_descendants(self):
        """è·å–æ‰€æœ‰å­å­™èŠ‚ç‚¹"""
        descendants = []
        for child in self.children:
            descendants.append(child)
            descendants.extend(child.get_all_descendants())
        return descendants
    
    def get_sibling_index(self):
        """è·å–åœ¨çˆ¶èŠ‚ç‚¹ä¸­çš„ç´¢å¼•"""
        if self.parent is None:
            return 0
        return self.parent.children.index(self)
    
    def get_siblings(self):
        """è·å–æ‰€æœ‰å…„å¼ŸèŠ‚ç‚¹ï¼ˆä¸åŒ…æ‹¬è‡ªå·±ï¼‰"""
        if self.parent is None:
            return []
        return [child for child in self.parent.children if child != self]

def build_tree_structure(node_mappings: Dict, mermaid_string: str) -> Dict[str, NodeTreeNode]:
    """æ„å»ºå®Œæ•´çš„å†…å­˜æ ‘ç»“æ„"""
    print(f"ğŸŒ³ [Phase 1] æ„å»ºæ ‘ç»“æ„ï¼ŒèŠ‚ç‚¹æ•°é‡: {len(node_mappings)}")
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹
    tree_nodes = {}
    for node_id in node_mappings.keys():
        tree_nodes[node_id] = NodeTreeNode(node_id)
        print(f"ğŸŒ³ [Phase 1] åˆ›å»ºèŠ‚ç‚¹: {node_id}")
    
    # ç¬¬äºŒæ­¥ï¼šå»ºç«‹çˆ¶å­å…³ç³»ï¼ˆåŸºäºç¼©è¿›å¼æ•°å­—IDï¼‰
    for node_id, node in tree_nodes.items():
        parent_prefix, sequence = split_id_helper(node_id)
        
        if parent_prefix is not None and parent_prefix in tree_nodes:
            parent_node = tree_nodes[parent_prefix]
            parent_node.add_child(node)
            print(f"ğŸŒ³ [Phase 1] å»ºç«‹å…³ç³»: {parent_prefix} -> {node_id}")
    
    # ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å’Œè°ƒè¯•ä¿¡æ¯
    root_nodes = [node for node in tree_nodes.values() if node.parent is None]
    print(f"ğŸŒ³ [Phase 1] æ ¹èŠ‚ç‚¹æ•°é‡: {len(root_nodes)}")
    
    for root in root_nodes:
        print(f"ğŸŒ³ [Phase 1] æ ¹èŠ‚ç‚¹: {root.id}, å­èŠ‚ç‚¹: {[child.id for child in root.children]}")
    
    return tree_nodes

def split_id_helper(node_id: str) -> tuple:
    """å°†èŠ‚ç‚¹IDåˆ†è§£ä¸ºçˆ¶IDå‰ç¼€å’Œè‡ªå·±çš„åºå·"""
    if '.' not in node_id:
        try:
            return (None, int(node_id))
        except ValueError:
            return (None, None)
    
    parts = node_id.split('.')
    try:
        sequence = int(parts[-1])
        parent_prefix = '.'.join(parts[:-1])
        return (parent_prefix, sequence)
    except ValueError:
        return (None, None)

def rename_subtree(node, new_id_prefix: str) -> Dict[str, str]:
    """è¿é”é‡å‘½åæ ¸å¿ƒå‡½æ•°"""
    rename_map = {}
    
    def recursive_rename(current_node, new_id: str):
        old_id = current_node.id
        current_node.id = new_id
        rename_map[old_id] = new_id
        
        for i, child in enumerate(current_node.children):
            child_new_id = f"{new_id}.{i + 1}"
            recursive_rename(child, child_new_id)
    
    recursive_rename(node, new_id_prefix)
    return rename_map

async def insert_divider_phase3(content: str, source_node_id: str, direction: str, new_node_id: str) -> Optional[str]:
    """Phase 3ä¸“ç”¨çš„åˆ†å‰²æ æ’å…¥å‡½æ•°"""
    try:
        print(f"ğŸ” [Phase 3æ’å…¥] æ’å…¥åˆ†å‰²æ : {new_node_id}, æ–¹å‘: {direction}")
        
        new_divider = f"--- {new_node_id} ---"
        
        if direction == 'child':
            # åœ¨æºèŠ‚ç‚¹å†…å®¹èŒƒå›´æœ«å°¾æ’å…¥
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                print(f"âŒ [Phase 3æ’å…¥] æœªæ‰¾åˆ°æºèŠ‚ç‚¹åˆ†å‰²æ : {source_node_id}")
                return None
            
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåˆ†å‰²æ æˆ–æ–‡æ¡£æœ«å°¾
            next_divider_pattern = r"\n--- [^-]+ ---"
            search_start = source_match.end()
            next_match = re.search(next_divider_pattern, content[search_start:])
            
            if next_match:
                insert_pos = search_start + next_match.start()
            else:
                insert_pos = len(content)
            
            return content[:insert_pos] + f"\n\n{new_divider}\n\n" + content[insert_pos:]
            
        elif direction == 'right-sibling':
            # åœ¨æºèŠ‚ç‚¹å­æ ‘æœ«å°¾æ’å…¥
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                return None
            
            # ç®€åŒ–å¤„ç†ï¼šåœ¨ä¸‹ä¸€ä¸ªåˆ†å‰²æ å‰æ’å…¥ï¼Œæˆ–æ–‡æ¡£æœ«å°¾
            next_divider_pattern = r"\n--- [^-]+ ---"
            search_start = source_match.end()
            next_match = re.search(next_divider_pattern, content[search_start:])
            
            if next_match:
                insert_pos = search_start + next_match.start()
            else:
                insert_pos = len(content)
            
            return content[:insert_pos] + f"\n\n{new_divider}\n\n" + content[insert_pos:]
            
        elif direction == 'left-sibling':
            # åœ¨æºèŠ‚ç‚¹åˆ†å‰²æ å‰æ’å…¥
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                return None
            
            insert_pos = source_match.start()
            return content[:insert_pos] + f"{new_divider}\n\n" + content[insert_pos:]
        
        return None
        
    except Exception as e:
        print(f"âŒ [Phase 3æ’å…¥é”™è¯¯] {str(e)}")
        return None

def update_mermaid_phase3(mermaid_string: str, new_node_id: str, new_node_label: str, direction: str, source_node_id: str) -> str:
    """Phase 3ä¸“ç”¨çš„mermaidæ›´æ–°å‡½æ•°"""
    try:
        updated_mermaid = mermaid_string or "graph TD"
        
        if not updated_mermaid.endswith('\n'):
            updated_mermaid += '\n'
        
        # æ·»åŠ æ–°èŠ‚ç‚¹å®šä¹‰
        new_node_def = f"    {new_node_id}[{new_node_label}]"
        updated_mermaid += new_node_def + '\n'
        
        # æ ¹æ®æ–¹å‘æ·»åŠ è¿æ¥
        if direction == 'child':
            connection = f"    {source_node_id} --> {new_node_id}"
        else:
            # åŒçº§èŠ‚ç‚¹ï¼šæ‰¾åˆ°æºèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹
            parent_pattern = rf'([A-Za-z0-9_.]+)\s*-->\s*{re.escape(source_node_id)}'
            parent_match = re.search(parent_pattern, updated_mermaid)
            
            if parent_match:
                parent_id = parent_match.group(1)
                connection = f"    {parent_id} --> {new_node_id}"
            else:
                # å¦‚æœæ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ï¼Œå‡è®¾æ˜¯æ ¹èŠ‚ç‚¹
                connection = f"    ROOT --> {new_node_id}"
        
        updated_mermaid += connection + '\n'
        
        print(f"ğŸ”„ [Phase 3 Mermaid] æ·»åŠ : {new_node_def}")
        print(f"ğŸ”„ [Phase 3 Mermaid] è¿æ¥: {connection}")
        
        return updated_mermaid
        
    except Exception as e:
        print(f"âŒ [Phase 3 Mermaidé”™è¯¯] {str(e)}")
        return mermaid_string or "graph TD"

# ======== End of Phase 1 & Phase 3 æ”¯æŒå‡½æ•° ========

app = FastAPI(title="Argument Structure Analyzer API", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Reactå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®æ—¥å¿—
logger = get_logger()

# åˆ›å»ºä¸Šä¼ ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# åˆ›å»ºPDFå¤„ç†ç›®å½•
PDF_OUTPUT_DIR = Path("pdf_outputs")
PDF_OUTPUT_DIR.mkdir(exist_ok=True)

# å­˜å‚¨æ–‡æ¡£çŠ¶æ€çš„å†…å­˜æ•°æ®åº“
document_status = {}

# å­˜å‚¨æ–‡æ¡£ç»“æ„çš„å†…å­˜æ•°æ®åº“
document_structures = {}

# Pydantic æ¨¡å‹å®šä¹‰
class AddNodeRequest(BaseModel):
    """æ·»åŠ èŠ‚ç‚¹çš„è¯·æ±‚æ¨¡å‹"""
    sourceNodeId: str
    direction: str  # 'child', 'left-sibling', 'right-sibling'
    parentId: Optional[str] = None
    label: Optional[str] = "æ–°èŠ‚ç‚¹"

class ArgumentStructureAnalyzer:
    """è®ºè¯ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        self.generator = MindMapGenerator()
        self.document_parser = DocumentParser()
        # æ·»åŠ DocumentOptimizerå®ä¾‹ç”¨äºAIè°ƒç”¨
        self.optimizer = DocumentOptimizer()
    
    def add_paragraph_ids(self, text: str) -> str:
        """ä¸ºæ–‡æœ¬çš„æ¯ä¸ªæ®µè½æ·»åŠ IDå·"""
        try:
            # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
            paragraphs = text.split('\n\n')
            processed_paragraphs = []
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():  # åªå¤„ç†éç©ºæ®µè½
                    # ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ IDæ ‡è®°
                    para_id = f"para-{i+1}"
                    processed_paragraph = f"[{para_id}] {paragraph.strip()}"
                    processed_paragraphs.append(processed_paragraph)
                else:
                    processed_paragraphs.append(paragraph)
            
            return '\n\n'.join(processed_paragraphs)
            
        except Exception as e:
            print(f"âŒ [æ®µè½IDæ·»åŠ é”™è¯¯] {str(e)}")
            return text
    
    def split_text_into_chunks(self, text: str, document_id: str) -> List[Dict[str, Any]]:
        """å°†æ–‡æ¡£æŒ‰Markdownæ ‡é¢˜å±‚çº§åˆ†å—å¹¶åˆ†é…å”¯ä¸€æ ‡è¯†ç¬¦"""
        try:
            # ä½¿ç”¨æ–°çš„æ–‡æ¡£è§£æå™¨
            chunks = self.document_parser.parse_to_chunks(text, document_id)
            
            # åŒæ—¶ä¿å­˜æ–‡æ¡£ç»“æ„ç”¨äºç›®å½•ç”Ÿæˆ
            root = self.document_parser.parse_document(text, document_id)
            toc = self.document_parser.generate_toc(root)
            
            document_structures[document_id] = {
                'structure': root.to_dict(),
                'toc': toc,
                'chunks': chunks
            }
            
            print(f"ğŸ“„ [æ–‡æœ¬åˆ†å—] æ–‡æ¡£ {document_id} åˆ†ä¸º {len(chunks)} ä¸ªç»“æ„åŒ–å—")
            for i, chunk in enumerate(chunks[:3]):  # æ˜¾ç¤ºå‰3ä¸ªå—çš„ä¿¡æ¯
                print(f"   å— {i}: {chunk.get('title', 'æ— æ ‡é¢˜')} (çº§åˆ« {chunk.get('level', 0)})")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ [åˆ†å—é”™è¯¯] {str(e)}")
            return []
    
    async def generate_argument_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„"""
        try:
            # æ„å»ºåŸºäºæ®µè½çš„è®ºè¯ç»“æ„åˆ†æprompt
            prompt = f"""æˆ‘å¸Œæœ›ä½ æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åˆ†æå¸ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»æˆ‘æä¾›çš„ã€å·²ç»æŒ‰æ®µè½æ ‡è®°å¥½IDçš„æ–‡æœ¬ï¼Œå¹¶åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†æ¥åˆ†æå…¶è®ºè¯ç»“æ„ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š

ç¬¬ä¸€æ­¥ï¼šæ®µè½è§’è‰²è¯†åˆ«
- åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†ï¼ˆ[para-X]æ ‡è®°ï¼‰ï¼Œåˆ†ææ¯ä¸ªæ®µè½åœ¨è®ºè¯ä¸­çš„è§’è‰²
- ä¸è¦é‡æ–°åˆ’åˆ†æ®µè½ï¼Œè€Œæ˜¯åŸºäºç°æœ‰æ®µè½æ¥ç†è§£è®ºè¯é€»è¾‘
- è¯†åˆ«æ¯ä¸ªæ®µè½æ˜¯å¼•è¨€ã€è®ºç‚¹ã€è¯æ®ã€åé©³ã€ç»“è®ºç­‰å“ªç§ç±»å‹

ç¬¬äºŒæ­¥ï¼šæ„å»ºè®ºè¯ç»“æ„æµç¨‹å›¾
- åŸºäºæ®µè½çš„è®ºè¯è§’è‰²ï¼Œæ„å»ºé€»è¾‘æµç¨‹å›¾
- å°†å…·æœ‰ç›¸åŒæˆ–ç›¸å…³è®ºè¯åŠŸèƒ½çš„æ®µè½ç»„åˆæˆé€»è¾‘èŠ‚ç‚¹
- ç”¨ç®­å¤´è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»

ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€çš„ã€å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œä¸è¦åœ¨ JSON ä»£ç å—å‰åæ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡å­—ã€‚

è¿™ä¸ª JSON å¯¹è±¡å¿…é¡»åŒ…å«ä¸‰ä¸ªé¡¶çº§é”®ï¼š"mermaid_string"ã€"node_mappings" å’Œ "edges"ã€‚

mermaid_string:
- å€¼ä¸ºç¬¦åˆ Mermaid.js è¯­æ³•çš„æµç¨‹å›¾ï¼ˆgraph TDï¼‰
- å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ç»„ç›¸å…³çš„æ®µè½ï¼ˆåŸºäºè®ºè¯åŠŸèƒ½ï¼‰
- ğŸ†• èŠ‚ç‚¹ ID å¿…é¡»ä½¿ç”¨ç¼©è¿›å¼æ•°å­—å‘½åæ–¹æ¡ˆï¼šæ ¹èŠ‚ç‚¹ä½¿ç”¨ 1, 2, 3ï¼Œå­èŠ‚ç‚¹ä½¿ç”¨ 1.1, 1.2, 1.3ï¼Œå­™èŠ‚ç‚¹ä½¿ç”¨ 1.1.1, 1.1.2 ç­‰
- ğŸ†• ä¸è¦ä½¿ç”¨å­—æ¯IDï¼ˆå¦‚A, B, Cï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ•°å­—IDï¼ˆå¦‚1, 2, 1.1, 1.2ï¼‰
- èŠ‚ç‚¹æ ‡ç­¾åº”è¯¥ç®€æ´æ¦‚æ‹¬è¯¥ç»„æ®µè½çš„æ ¸å¿ƒè®ºè¯åŠŸèƒ½ï¼ˆä¸è¶…è¿‡20å­—ï¼‰
- ä½¿ç”¨ç®­å¤´ --> è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»
- å¯ä»¥ä½¿ç”¨ä¸åŒçš„èŠ‚ç‚¹å½¢çŠ¶æ¥åŒºåˆ†ä¸åŒç±»å‹çš„è®ºè¯åŠŸèƒ½ï¼š
  - [æ–¹æ‹¬å·] ç”¨äºä¸»è¦è®ºç‚¹
  - (åœ†æ‹¬å·) ç”¨äºæ”¯æ’‘è¯æ®
  - {{èŠ±æ‹¬å·}} ç”¨äºé€»è¾‘è½¬æŠ˜æˆ–å…³é”®åˆ¤æ–­

node_mappings:
- å€¼ä¸º JSON å¯¹è±¡ï¼Œé”®ä¸º Mermaid å›¾ä¸­çš„èŠ‚ç‚¹ IDï¼ˆå¿…é¡»æ˜¯æ•°å­—æ ¼å¼ï¼Œå¦‚ "1", "2", "1.1", "1.2"ï¼‰
- æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å€¼åŒ…å«ï¼š
  - "text_snippet": è¯¥èŠ‚ç‚¹åŒ…å«æ®µè½çš„æ ¸å¿ƒå†…å®¹æ€»ç»“ï¼ˆ10-20å­—ï¼‰
  - "paragraph_ids": æ„æˆè¯¥èŠ‚ç‚¹çš„æ®µè½IDæ•°ç»„ï¼ˆå¦‚ ["para-2", "para-3"]ï¼‰
  - "semantic_role": è¯¥èŠ‚ç‚¹åœ¨è®ºè¯ä¸­çš„è§’è‰²ï¼ˆå¦‚ "å¼•è¨€"ã€"æ ¸å¿ƒè®ºç‚¹"ã€"æ”¯æ’‘è¯æ®"ã€"åé©³"ã€"ç»“è®º" ç­‰ï¼‰

edges:
- å€¼ä¸ºå¯¹è±¡æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡ä»£è¡¨ä¸€æ¡è¾¹
- æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä¸¤ä¸ªé”®ï¼š
  - "source": è¾¹çš„èµ·å§‹èŠ‚ç‚¹IDï¼ˆæ•°å­—æ ¼å¼ï¼‰
  - "target": è¾¹çš„ç›®æ ‡èŠ‚ç‚¹IDï¼ˆæ•°å­—æ ¼å¼ï¼‰
- è¿™äº›è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»ä¸€è‡´

å…³é”®è¦æ±‚ï¼š
1. ğŸ†• æ‰€æœ‰èŠ‚ç‚¹ ID å¿…é¡»ä½¿ç”¨ç¼©è¿›å¼æ•°å­—å‘½åï¼š1, 2, 3, 1.1, 1.2, 1.1.1 ç­‰ï¼Œç»å¯¹ä¸èƒ½ä½¿ç”¨å­—æ¯
2. æ‰€æœ‰èŠ‚ç‚¹ ID å¿…é¡»åœ¨ mermaid_string ä¸­å­˜åœ¨
3. paragraph_ids å¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŸæ–‡çš„æ®µè½æ ‡è®° [para-X]ï¼Œä¸å¯ä¿®æ”¹
4. åŸæ–‡çš„æ¯ä¸ªæ®µè½éƒ½åº”è¯¥è¢«åˆ†é…ç»™è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹
5. èŠ‚ç‚¹çš„åˆ’åˆ†åº”è¯¥åŸºäºæ®µè½çš„è®ºè¯åŠŸèƒ½ï¼Œç›¸å…³åŠŸèƒ½çš„æ®µè½å¯ä»¥ç»„åˆåœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸­
6. æµç¨‹å›¾åº”è¯¥æ¸…æ™°å±•ç°è®ºè¯çš„é€»è¾‘æ¨ç†è·¯å¾„
7. ä¿æŒæ®µè½çš„å®Œæ•´æ€§ï¼Œä¸è¦æ‹†åˆ†æˆ–é‡ç»„æ®µè½å†…å®¹
8. edges æ•°ç»„ä¸­çš„æ¯æ¡è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»å®Œå…¨ä¸€è‡´

ç°åœ¨ï¼Œè¯·åˆ†æä»¥ä¸‹å¸¦æœ‰æ®µè½IDçš„æ–‡æœ¬ï¼š

{text_with_ids}"""
            
            # ä½¿ç”¨DocumentOptimizerçš„generate_completionæ–¹æ³•
            # è¿›ä¸€æ­¥å¢åŠ max_tokensä»¥é¿å…å“åº”è¢«æˆªæ–­ï¼ˆOpenRouteræ—¥å¿—æ˜¾ç¤ºfinish_reasonä¸º'length'ï¼‰
            response = await self.optimizer.generate_completion(
                prompt, 
                max_tokens=16000,
                task="åˆ†æè®ºè¯ç»“æ„"
            )
            
            if not response:
                print(f"âŒ [APIè°ƒç”¨å¤±è´¥] æœªæ”¶åˆ°AIå“åº”")
                return {"success": False, "error": "APIè°ƒç”¨å¤±è´¥ï¼Œæœªæ”¶åˆ°AIå“åº”"}
            
            # ä¿å­˜APIåŸå§‹å“åº”åˆ°æ–‡ä»¶
            try:
                from datetime import datetime
                import os
                
                # åˆ›å»ºapi_responsesæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                api_responses_dir = "api_responses"
                os.makedirs(api_responses_dir, exist_ok=True)
                
                # ç”Ÿæˆæ–‡ä»¶åï¼šæ—¶é—´æˆ³_è®ºè¯ç»“æ„åˆ†æ
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                response_filename = f"{timestamp}_argument_structure_analysis.txt"
                response_filepath = os.path.join(api_responses_dir, response_filename)
                
                # ä¿å­˜åŸå§‹å“åº”å’Œç›¸å…³ä¿¡æ¯
                with open(response_filepath, 'w', encoding='utf-8') as f:
                    f.write("=== APIè°ƒç”¨ä¿¡æ¯ ===\n")
                    f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ä»»åŠ¡: è®ºè¯ç»“æ„åˆ†æ\n")
                    f.write(f"æœ€å¤§tokens: 16000\n")
                    f.write(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦\n")
                    f.write(f"æ–‡æœ¬é•¿åº¦: {len(text_with_ids)} å­—ç¬¦\n")
                    f.write("\n=== å‘é€çš„Prompt ===\n")
                    f.write(prompt)
                    f.write("\n\n=== AIåŸå§‹å“åº” ===\n")
                    f.write(response)
                    f.write(f"\n\n=== å“åº”ç»“æŸ ===\n")
                
                print(f"ğŸ’¾ [APIå“åº”ä¿å­˜] å·²ä¿å­˜åˆ°: {response_filepath}")
                
            except Exception as save_error:
                print(f"âš ï¸ [å“åº”ä¿å­˜å¤±è´¥] {str(save_error)}")
            
            # è§£æJSONå“åº”
            try:
                # è¯¦ç»†è®°å½•åŸå§‹å“åº”
                print(f"ğŸ” [åŸå§‹AIå“åº”] é•¿åº¦: {len(response)} å­—ç¬¦")
                print(f"ğŸ” [åŸå§‹å“åº”å‰200å­—ç¬¦]: {response[:200]}")
                
                # æ›´å½»åº•çš„å“åº”æ¸…ç†
                clean_response = response.strip()
                
                # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                elif clean_response.startswith('```'):
                    clean_response = clean_response[3:]
                    
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                
                clean_response = clean_response.strip()
                
                # ç§»é™¤å¯èƒ½çš„è¯´æ˜æ–‡å­—ï¼Œåªä¿ç•™JSONéƒ¨åˆ†
                json_start = clean_response.find('{')
                json_end = clean_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    clean_response = clean_response[json_start:json_end+1]
                    print(f"ğŸ”§ [æå–JSON] æå–åˆ°JSONéƒ¨åˆ†ï¼Œé•¿åº¦: {len(clean_response)}")
                else:
                    print(f"âš ï¸ [JSONæå–å¤±è´¥] æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONç»“æ„")
                
                # ç§»é™¤äº†æœ‰é—®é¢˜çš„fix_duplicate_keyså‡½æ•°ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹JSON
                
                # æ¸…ç†JSONä¸­çš„æ§åˆ¶å­—ç¬¦
                def clean_control_characters(json_str: str) -> str:
                    """æ¸…ç†JSONå­—ç¬¦ä¸²ä¸­çš„æ— æ•ˆæ§åˆ¶å­—ç¬¦"""
                    import re
                    
                    # ç§»é™¤å¼€å¤´çš„åæ–œæ ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if json_str.startswith('\\'):
                        json_str = json_str[1:]
                    
                    # æ›¿æ¢å¸¸è§çš„æ§åˆ¶å­—ç¬¦
                    # ä¿ç•™åˆæ³•çš„è½¬ä¹‰å­—ç¬¦ï¼Œæ¸…ç†æ— æ•ˆçš„æ§åˆ¶å­—ç¬¦
                    json_str = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', json_str)
                    
                    # ä¿®å¤åŒé‡è½¬ä¹‰çš„é—®é¢˜
                    json_str = json_str.replace('\\\\n', '\\n')
                    json_str = json_str.replace('\\\\r', '\\r')
                    json_str = json_str.replace('\\\\t', '\\t')
                    
                    # ç¡®ä¿å­—ç¬¦ä¸²ä¸­çš„æ¢è¡Œç¬¦è¢«æ­£ç¡®è½¬ä¹‰ï¼ˆä½†ä¸è¦é‡å¤è½¬ä¹‰ï¼‰
                    if '\\n' not in json_str:
                        json_str = json_str.replace('\n', '\\n')
                    if '\\r' not in json_str:
                        json_str = json_str.replace('\r', '\\r')
                    if '\\t' not in json_str:
                        json_str = json_str.replace('\t', '\\t')
                    
                    return json_str
                
                clean_response = clean_control_characters(clean_response)
                print(f"ğŸ” [æ¸…ç†åå“åº”å‰200å­—ç¬¦]: {clean_response[:200]}")
                
                structure_data = json.loads(clean_response)
                
                # éªŒè¯å¿…è¦çš„é”®
                if 'mermaid_string' not in structure_data or 'node_mappings' not in structure_data:
                    print(f"âŒ [æ•°æ®ç»“æ„é”™è¯¯] å“åº”é”®: {list(structure_data.keys())}")
                    return {"success": False, "error": "AIå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼šç¼ºå°‘å¿…è¦çš„é”®"}
                
                # éªŒè¯èŠ‚ç‚¹æ˜ å°„çš„ç»“æ„
                node_mappings = structure_data['node_mappings']
                valid_mappings = {}
                
                for node_id, mapping in node_mappings.items():
                    if isinstance(mapping, dict):
                        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨ï¼Œå¦‚æœç¼ºå°‘semantic_roleå°±æ·»åŠ é»˜è®¤å€¼
                        valid_mapping = {
                            "text_snippet": mapping.get("text_snippet", "è¯­ä¹‰å—å†…å®¹"),
                            "paragraph_ids": mapping.get("paragraph_ids", []),
                            "semantic_role": mapping.get("semantic_role", "è®ºè¯è¦ç´ ")
                        }
                        valid_mappings[node_id] = valid_mapping
                    else:
                        print(f"âš ï¸ [æ˜ å°„æ ¼å¼é”™è¯¯] èŠ‚ç‚¹ {node_id} çš„æ˜ å°„ä¸æ˜¯å­—å…¸æ ¼å¼")
                
                structure_data['node_mappings'] = valid_mappings
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«edgeså­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä»mermaid_stringä¸­æå–
                if 'edges' not in structure_data:
                    print("âš ï¸ [æ•°æ®ç»“æ„è­¦å‘Š] å“åº”ä¸­æ²¡æœ‰edgeså­—æ®µï¼Œå°†ä»mermaid_stringä¸­æå–")
                    # ä»mermaid_stringä¸­æå–è¾¹å…³ç³»
                    edges = []
                    mermaid_string = structure_data['mermaid_string']
                    # åŒ¹é…å½¢å¦‚ "A --> B" çš„è¾¹å®šä¹‰
                    edge_pattern = r'([A-Za-z0-9_]+)\s*-->\s*([A-Za-z0-9_]+)'
                    for match in re.finditer(edge_pattern, mermaid_string):
                        source, target = match.groups()
                        edges.append({"source": source, "target": target})
                    structure_data['edges'] = edges
                    print(f"ğŸ”§ [è‡ªåŠ¨æå–] ä»mermaid_stringä¸­æå–äº† {len(edges)} æ¡è¾¹")
                
                print(f"âœ… [è®ºè¯ç»“æ„åˆ†æ] æˆåŠŸç”ŸæˆåŒ…å« {len(structure_data['node_mappings'])} ä¸ªèŠ‚ç‚¹çš„æµç¨‹å›¾")
                
                # è¿”å›æˆåŠŸç»“æœ
                return {
                    "success": True,
                    "mermaid_code": structure_data['mermaid_string'],
                    "node_mappings": structure_data['node_mappings'],
                    "edges": structure_data['edges']
                }
                
            except json.JSONDecodeError as parse_error:
                print(f"âŒ [JSONè§£æé”™è¯¯] {str(parse_error)}")
                print(f"âŒ [å®Œæ•´åŸå§‹å“åº”]: {response}")
                print(f"âŒ [æ¸…ç†åå“åº”]: {clean_response}")
                return {"success": False, "error": f"JSONè§£æå¤±è´¥: {str(parse_error)}"}
                
        except Exception as e:
            print(f"âŒ [è®ºè¯ç»“æ„åˆ†æé”™è¯¯] {str(e)}")
            # æä¾›é™çº§ç­–ç•¥ - ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„
            try:
                fallback_structure = self.generate_fallback_structure(text_with_ids)
                print(f"ğŸ”„ [é™çº§ç­–ç•¥] ä½¿ç”¨åŸºæœ¬è®ºè¯ç»“æ„ï¼ŒåŒ…å« {len(fallback_structure['node_mappings'])} ä¸ªèŠ‚ç‚¹")
                return fallback_structure
            except Exception as fallback_error:
                print(f"âŒ [é™çº§ç­–ç•¥å¤±è´¥] {str(fallback_error)}")
                return {"success": False, "error": f"AIåˆ†æå¤±è´¥ä¸”é™çº§ç­–ç•¥ä¹Ÿå¤±è´¥: {str(e)}"}

    def generate_fallback_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„ä½œä¸ºé™çº§ç­–ç•¥"""
        import re
        
        # æå–æ‰€æœ‰æ®µè½ID
        para_ids = re.findall(r'\[para-(\d+)\]', text_with_ids)
        
        if not para_ids:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½IDï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬ç»“æ„
            return {
                "success": True,
                "mermaid_code": "graph TD\n    A[æ–‡æ¡£åˆ†æ] --> B[ä¸»è¦å†…å®¹]\n    B --> C[æ€»ç»“]",
                "node_mappings": {
                    "A": {
                        "text_snippet": "æ–‡æ¡£å¼€å§‹",
                        "paragraph_ids": ["para-1"],
                        "semantic_role": "å¼•è¨€"
                    },
                    "B": {
                        "text_snippet": "ä¸»è¦å†…å®¹",
                        "paragraph_ids": ["para-2"],
                        "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                    },
                    "C": {
                        "text_snippet": "æ–‡æ¡£ç»“è®º",
                        "paragraph_ids": ["para-3"],
                        "semantic_role": "ç»“è®º"
                    }
                }
            }
        
        # åŸºäºæ®µè½æ•°é‡ç”Ÿæˆç»“æ„
        total_paras = len(para_ids)
        
        if total_paras <= 3:
            # ç®€å•çº¿æ€§ç»“æ„
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[ä¸»ä½“]\n"
            mermaid_code += "    B --> C[ç»“è®º]"
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€éƒ¨åˆ†",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "æ–‡æ¡£ä¸»ä½“å†…å®¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:-1]] if total_paras > 2 else [f"para-{para_ids[1]}"] if total_paras > 1 else [],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"] if total_paras > 1 else [],
                    "semantic_role": "ç»“è®º"
                }
            }
        else:
            # å¤æ‚ç»“æ„ï¼šå¼•è¨€ -> å¤šä¸ªè®ºç‚¹ -> ç»“è®º
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[è®ºç‚¹1]\n"
            mermaid_code += "    A --> C[è®ºç‚¹2]\n"
            if total_paras > 5:
                mermaid_code += "    A --> D[è®ºç‚¹3]\n"
                mermaid_code += "    B --> E[ç»“è®º]\n"
                mermaid_code += "    C --> E\n"
                mermaid_code += "    D --> E"
            else:
                mermaid_code += "    B --> D[ç»“è®º]\n"
                mermaid_code += "    C --> D"
            
            # å°†æ®µè½åˆ†é…ç»™ä¸åŒèŠ‚ç‚¹
            para_per_section = max(1, total_paras // 4)
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "ç¬¬ä¸€ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:1+para_per_section]],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "ç¬¬äºŒä¸ªè®ºç‚¹", 
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+para_per_section:1+2*para_per_section]],
                    "semantic_role": "æ”¯æ’‘è¯æ®"
                }
            }
            
            if total_paras > 5:
                node_mappings["D"] = {
                    "text_snippet": "ç¬¬ä¸‰ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:-1]],
                    "semantic_role": "è¡¥å……è®ºè¯"
                }
                node_mappings["E"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"],
                    "semantic_role": "ç»“è®º"
                }
            else:
                node_mappings["D"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:]],
                    "semantic_role": "ç»“è®º"
                }
        
        return {
            "success": True,
            "mermaid_code": mermaid_code,
            "node_mappings": node_mappings
        }

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
argument_analyzer = ArgumentStructureAnalyzer()

async def process_pdf_to_markdown(pdf_file_path: str, document_id: str) -> str:
    """
    ä½¿ç”¨MinerUå¤„ç†PDFæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        pdf_file_path: PDFæ–‡ä»¶è·¯å¾„
        document_id: æ–‡æ¡£ID
        
    Returns:
        è½¬æ¢åçš„Markdownå†…å®¹
    """
    try:
        print(f"\nğŸ“„ [MinerU-PDFå¤„ç†] å¼€å§‹å¤„ç†PDFæ–‡ä»¶")
        print(f"    ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = PDF_OUTPUT_DIR / document_id
        image_dir = output_dir / "images"
        os.makedirs(image_dir, exist_ok=True)
        
        print(f"ğŸ“ [MinerU-ç›®å½•] åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ–¼ï¸  [MinerU-å›¾ç‰‡] å›¾ç‰‡ç›®å½•: {image_dir}")
        
        # åˆ›å»ºæ•°æ®è¯»å†™å™¨
        print("ğŸ”§ [MinerU-åˆå§‹åŒ–] åˆ›å»ºæ•°æ®è¯»å†™å™¨...")
        reader = FileBasedDataReader("")
        image_writer = FileBasedDataWriter(str(image_dir))
        md_writer = FileBasedDataWriter(str(output_dir))
        
        # è¯»å–PDFæ–‡ä»¶
        print("ğŸ“– [MinerU-è¯»å–] æ­£åœ¨è¯»å–PDFæ–‡ä»¶...")
        pdf_bytes = reader.read(pdf_file_path)
        print(f"ğŸ“Š [MinerU-æ•°æ®] PDFæ–‡ä»¶å¤§å°: {len(pdf_bytes)} å­—èŠ‚")
        
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        print("ğŸ—ï¸  [MinerU-æ•°æ®é›†] åˆ›å»ºPymuDocDatasetå®ä¾‹...")
        ds = PymuDocDataset(pdf_bytes)
        
        # åˆ†ç±»å¤„ç†æ¨¡å¼
        print("ğŸ” [MinerU-æ£€æµ‹] æ£€æµ‹PDFå¤„ç†æ¨¡å¼...")
        pdf_mode = ds.classify()
        
        # è¿›è¡Œæ¨ç†
        if pdf_mode == SupportedPdfParseMethod.OCR:
            print(f"ğŸ”¤ [MinerU-OCRæ¨¡å¼] æ£€æµ‹åˆ°éœ€è¦OCRå¤„ç†ï¼Œå¼€å§‹æ–‡å­—è¯†åˆ«...")
            print("    ğŸ“¸ æ­£åœ¨æå–å›¾ç‰‡ä¸­çš„æ–‡å­—...")
            print("    ğŸ§  è°ƒç”¨OCRå¼•æ“è¿›è¡Œæ–‡å­—è¯†åˆ«...")
            infer_result = ds.apply(doc_analyze, ocr=True)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨OCRæ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_ocr_mode(image_writer)
            print("âœ… [MinerU-OCR] OCRå¤„ç†å®Œæˆ")
        else:
            print(f"ğŸ“ [MinerU-æ–‡æœ¬æ¨¡å¼] æ£€æµ‹åˆ°å¯ç›´æ¥æå–æ–‡æœ¬ï¼Œå¼€å§‹æ–‡æœ¬å¤„ç†...")
            print("    ğŸ“„ æ­£åœ¨æå–PDFä¸­çš„æ–‡æœ¬å†…å®¹...")
            print("    ğŸ”§ åˆ†ææ–‡æ¡£ç»“æ„å’Œç‰ˆé¢...")
            infer_result = ds.apply(doc_analyze, ocr=False)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_txt_mode(image_writer)
            print("âœ… [MinerU-æ–‡æœ¬] æ–‡æœ¬æå–å®Œæˆ")
        
        print("ğŸ“‹ [MinerU-è½¬æ¢] æ­£åœ¨ç”ŸæˆMarkdownæ ¼å¼...")
        # è·å–Markdownå†…å®¹
        markdown_content = pipe_result.get_markdown("images")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        md_file_path = output_dir / f"{document_id}.md"
        print(f"ğŸ’¾ [MinerU-ä¿å­˜] ä¿å­˜Markdownæ–‡ä»¶: {md_file_path}")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # ç»Ÿè®¡ä¿¡æ¯
        lines_count = len(markdown_content.split('\n'))
        words_count = len(markdown_content.split())
        
        print("=" * 60)
        print("âœ… [MinerU-å®Œæˆ] PDFè½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"    ğŸ“Š ç”Ÿæˆå†…å®¹ç»Ÿè®¡:")
        print(f"       â€¢ Markdownæ€»é•¿åº¦: {len(markdown_content):,} å­—ç¬¦")
        print(f"       â€¢ æ€»è¡Œæ•°: {lines_count:,} è¡Œ")
        print(f"       â€¢ å•è¯æ•°: {words_count:,} ä¸ª")
        print(f"    ğŸ“ è¾“å‡ºæ–‡ä»¶: {md_file_path}")
        print("=" * 60)
        
        return markdown_content
        
    except Exception as e:
        print("=" * 60)
        print(f"âŒ [MinerU-é”™è¯¯] PDFå¤„ç†å¤±è´¥ï¼")
        print(f"    ğŸš¨ é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"    ğŸ“„ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        logger.error(f"MinerU PDF processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"PDFå¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£ï¼Œæ”¯æŒPDFã€MDå’ŒTXTæ–‡ä»¶"""
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = ['.md', '.txt', '.pdf']
    file_extension = Path(file.filename).suffix.lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .mdã€.txt å’Œ .pdf æ–‡ä»¶")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£ID
        content_hash = hashlib.md5(content).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = Path(file.filename).stem
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        
        print(f"\nğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] {file.filename}")
        print(f"ğŸ†” [æ–‡æ¡£ID] {document_id}")
        print(f"ğŸ“Š [æ–‡ä»¶å¤§å°] {len(content)} å­—èŠ‚")
        print(f"ğŸ“‹ [æ–‡ä»¶ç±»å‹] {file_extension}")
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶
        original_file_path = UPLOAD_DIR / f"{document_id}{file_extension}"
        with open(original_file_path, 'wb') as f:
            f.write(content)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
        if file_extension == '.pdf':
            # å¤„ç†PDFæ–‡ä»¶
            print(f"ğŸ”„ [PDFå¤„ç†] å¼€å§‹è½¬æ¢PDFä¸ºMarkdown...")
            markdown_content = await process_pdf_to_markdown(str(original_file_path), document_id)
            text_content = markdown_content
            
            # å°†åŸå§‹PDFæ–‡ä»¶ç¼–ç ä¸ºbase64ç”¨äºå‰ç«¯æ˜¾ç¤º
            pdf_base64 = base64.b64encode(content).decode('utf-8')
            
        else:
            # å¤„ç†æ–‡æœ¬æ–‡ä»¶
            text_content = content.decode('utf-8')
            pdf_base64 = None
        
        # å­˜å‚¨åˆ°å†…å­˜æ•°æ®åº“
        MinimalDatabaseStub.store_text(text_content)
        
        # ç«‹å³ä¸ºæ–‡æ¡£å†…å®¹æ·»åŠ æ®µè½IDï¼Œæ— éœ€ç­‰å¾…ç”Ÿæˆè®ºè¯ç»“æ„
        print("ğŸ“ [å¤„ç†æ®µè½] ä¸ºä¸Šä¼ çš„æ–‡æ¡£æ·»åŠ æ®µè½IDæ ‡è®°...")
        content_with_ids = argument_analyzer.add_paragraph_ids(text_content)
        print(f"ğŸ“ [æ®µè½å¤„ç†å®Œæˆ] å·²ä¸ºæ–‡æ¡£æ·»åŠ æ®µè½IDï¼Œå†…å®¹é•¿åº¦: {len(content_with_ids)} å­—ç¬¦")
        
        # åˆå§‹åŒ–æ–‡æ¡£çŠ¶æ€
        document_status[document_id] = {
            "status": "uploaded",
            "content": text_content,
            "filename": file.filename,
            "file_type": file_extension,
            "original_file_path": str(original_file_path),
            "pdf_base64": pdf_base64,  # ä»…PDFæ–‡ä»¶æœ‰æ­¤å­—æ®µ
            "status_demo": "not_started",
            "mermaid_code_demo": None,
            "node_mappings_demo": {},
            "error_demo": None,
            "content_with_ids": content_with_ids  # ç«‹å³è®¾ç½®å¸¦æ®µè½IDçš„å†…å®¹
        }
        
        print(f"âœ… [ä¸Šä¼ æˆåŠŸ] æ–‡æ¡£å·²ä¿å­˜å¹¶å‡†å¤‡ç”Ÿæˆæ€ç»´å¯¼å›¾")
        print("=" * 60)
        
        logger.info(f"Document uploaded: {document_id}")
        
        # è¿”å›æ–‡æ¡£ä¿¡æ¯
        response_data = {
            "success": True,
            "document_id": document_id,
            "filename": file.filename,
            "content": text_content,
            "file_type": file_extension,
            "status": "uploaded",
            "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ"
        }
        
        # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œè¿”å›base64ç¼–ç çš„åŸå§‹PDF
        if file_extension == '.pdf':
            response_data["pdf_base64"] = pdf_base64
            response_data["message"] = "PDFæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå·²è½¬æ¢ä¸ºMarkdown"
        
        return JSONResponse(response_data)
        
    except UnicodeDecodeError:
        print(f"âŒ [ç¼–ç é”™è¯¯] æ–‡ä»¶: {file.filename}")
        raise HTTPException(status_code=400, detail="æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯UTF-8ç¼–ç ")
    except Exception as e:
        print(f"âŒ [ä¸Šä¼ å¤±è´¥] æ–‡ä»¶: {file.filename}, é”™è¯¯: {str(e)}")
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/document-pdf/{document_id}")
async def get_document_pdf(document_id: str):
    """è·å–åŸå§‹PDFæ–‡ä»¶"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    if doc_info.get("file_type") != ".pdf":
        raise HTTPException(status_code=400, detail="è¯¥æ–‡æ¡£ä¸æ˜¯PDFæ–‡ä»¶")
    
    original_file_path = doc_info.get("original_file_path")
    if not original_file_path or not os.path.exists(original_file_path):
        raise HTTPException(status_code=404, detail="åŸå§‹PDFæ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=original_file_path,
        media_type='application/pdf',
        filename=doc_info["filename"]
    )

@app.post("/api/generate-argument-structure/{document_id}")
async def generate_argument_structure(document_id: str):
    """ä¸ºæŒ‡å®šæ–‡æ¡£ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    # æ£€æŸ¥çŠ¶æ€
    if doc_info.get("status_demo") == "generating":
        print(f"â³ [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­...")
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­..."
        })
    
    if doc_info.get("status_demo") == "completed" and doc_info.get("mermaid_code_demo"):
        print(f"âœ… [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„å·²åˆ†æå®Œæˆ")
        return JSONResponse({
            "success": True,
            "status": "completed",
            "mermaid_code": doc_info["mermaid_code_demo"],
            "node_mappings": doc_info.get("node_mappings_demo", {}),
            "message": "è®ºè¯ç»“æ„å·²ç”Ÿæˆ"
        })
    
    try:
        print(f"ğŸ”„ [å¼€å§‹åˆ†æ] ä¸ºæ–‡æ¡£ {document_id} å¯åŠ¨è®ºè¯ç»“æ„åˆ†æä»»åŠ¡")
        
        # æ›´æ–°çŠ¶æ€ä¸ºåˆ†æä¸­
        doc_info["status_demo"] = "generating"
        
        # å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„
        asyncio.create_task(generate_argument_structure_async(document_id, doc_info["content"]))
        
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "å¼€å§‹åˆ†æè®ºè¯ç»“æ„..."
        })
        
    except Exception as e:
        print(f"âŒ [å¯åŠ¨å¤±è´¥] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}")
        logger.error(f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        doc_info["status_demo"] = "error"
        doc_info["error_demo"] = str(e)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")

async def generate_argument_structure_async(document_id: str, content: str):
    """å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„"""
    try:
        print(f"ğŸ”„ [å¼‚æ­¥ä»»åŠ¡] å¼€å§‹ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆè®ºè¯ç»“æ„")
        argument_analyzer = ArgumentStructureAnalyzer()
        
        # ä¸ºæ–‡æœ¬æ·»åŠ æ®µè½ID
        text_with_ids = argument_analyzer.add_paragraph_ids(content)
        
        # ç”Ÿæˆè®ºè¯ç»“æ„
        result = await argument_analyzer.generate_argument_structure(text_with_ids)
        
        if result["success"]:
            # ğŸ†• æ£€æŸ¥å¹¶è½¬æ¢èŠ‚ç‚¹IDæ ¼å¼ï¼šå¦‚æœAIè¿”å›å­—æ¯IDï¼Œè½¬æ¢ä¸ºç¼©è¿›å¼æ•°å­—ID
            converted_result = convert_node_ids_to_numeric(result)
            
            # ğŸ†• ä½¿ç”¨è½¬æ¢åçš„node_mappingsé‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹
            rebuilt_content = rebuild_content_with_physical_dividers(text_with_ids, converted_result["node_mappings"])
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document_status[document_id]["status_demo"] = "completed"
            document_status[document_id]["mermaid_code_demo"] = converted_result["mermaid_code"]
            document_status[document_id]["node_mappings_demo"] = converted_result["node_mappings"]
            document_status[document_id]["edges_demo"] = converted_result["edges"]  # ä¿å­˜edgesæ•°æ®
            document_status[document_id]["content_with_ids"] = rebuilt_content  # ğŸ†• ä½¿ç”¨é‡å»ºçš„å†…å®¹
            
            print(f"âœ… [åˆ†æå®Œæˆ] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†ææˆåŠŸ")
            print(f"ğŸ“Š [ç”Ÿæˆç»“æœ] åŒ…å« {len(converted_result['node_mappings'])} ä¸ªè®ºè¯èŠ‚ç‚¹å’Œ {len(converted_result['edges'])} æ¡è¾¹")
            print(f"ğŸ”§ [å†…å®¹é‡å»º] å·²é‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹ï¼Œé•¿åº¦: {len(rebuilt_content)} å­—ç¬¦")
        else:
            # åˆ†æå¤±è´¥
            document_status[document_id]["status_demo"] = "error"
            document_status[document_id]["error_demo"] = result["error"]
            print(f"âŒ [åˆ†æå¤±è´¥] æ–‡æ¡£ {document_id}: {result['error']}")
            
    except Exception as e:
        print(f"âŒ [å¼‚æ­¥åˆ†æé”™è¯¯] æ–‡æ¡£ {document_id}: {str(e)}")
        logger.error(f"å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        document_status[document_id]["status_demo"] = "error"
        document_status[document_id]["error_demo"] = str(e)

def rebuild_content_with_physical_dividers(text_with_ids: str, node_mappings: Dict) -> str:
    """
    æ ¹æ®AIè¿”å›çš„node_mappingsé‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹
    
    Args:
        text_with_ids: åŒ…å«æ®µè½IDæ ‡è®°çš„åŸå§‹æ–‡æœ¬
        node_mappings: AIè¿”å›çš„èŠ‚ç‚¹æ˜ å°„ï¼ŒåŒ…å«paragraph_ids
        
    Returns:
        é‡å»ºçš„åŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹å­—ç¬¦ä¸²
    """
    try:
        print(f"ğŸ”§ [å†…å®¹é‡å»º] å¼€å§‹é‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹")
        print(f"ğŸ”§ [å†…å®¹é‡å»º] è¾“å…¥å†…å®¹é•¿åº¦: {len(text_with_ids)} å­—ç¬¦")
        print(f"ğŸ”§ [å†…å®¹é‡å»º] èŠ‚ç‚¹æ•°é‡: {len(node_mappings)}")
        
        # ç¬¬ä¸€æ­¥ï¼šè§£æåŸå§‹å†…å®¹ï¼Œæå–æ®µè½IDå’Œå¯¹åº”çš„å†…å®¹
        paragraph_content_map = {}
        
        # æŒ‰æ®µè½åˆ†å‰²å†…å®¹ï¼Œä¿ç•™æ®µè½IDæ ‡è®°
        parts = re.split(r'(\[para-\d+\])', text_with_ids)
        current_paragraph_id = None
        current_content = ''
        
        for part in parts:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ®µè½IDæ ‡è®°
            para_match = re.match(r'\[para-(\d+)\]', part.strip())
            if para_match:
                # ä¿å­˜ä¹‹å‰çš„æ®µè½å†…å®¹
                if current_paragraph_id and current_content.strip():
                    paragraph_content_map[current_paragraph_id] = current_content.strip()
                
                # è®¾ç½®æ–°çš„æ®µè½ID
                current_paragraph_id = f"para-{para_match.group(1)}"
                current_content = ''
                print(f"ğŸ”§ [å†…å®¹é‡å»º] å‘ç°æ®µè½: {current_paragraph_id}")
            else:
                # ç´¯ç§¯å†…å®¹
                if part.strip():  # åªæ·»åŠ éç©ºå†…å®¹
                    current_content += part
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_paragraph_id and current_content.strip():
            paragraph_content_map[current_paragraph_id] = current_content.strip()
        
        print(f"ğŸ”§ [å†…å®¹é‡å»º] è§£æå‡º {len(paragraph_content_map)} ä¸ªæ®µè½")
        
        # ç¬¬äºŒæ­¥ï¼šæŒ‰ç…§node_mappingsé‡æ–°ç»„ç»‡å†…å®¹
        rebuilt_content_parts = []
        
        # éå†æ‰€æœ‰èŠ‚ç‚¹ï¼ŒæŒ‰ç…§å®ƒä»¬åœ¨node_mappingsä¸­çš„é¡ºåº
        for node_id, node_data in node_mappings.items():
            # æ·»åŠ ç‰©ç†åˆ†å‰²æ 
            rebuilt_content_parts.append(f"--- {node_id} ---\n")
            print(f"ğŸ”§ [å†…å®¹é‡å»º] å¤„ç†èŠ‚ç‚¹: {node_id}")
            
            # è·å–è¯¥èŠ‚ç‚¹åŒ…å«çš„æ®µè½IDåˆ—è¡¨
            paragraph_ids = node_data.get('paragraph_ids', [])
            print(f"ğŸ”§ [å†…å®¹é‡å»º] èŠ‚ç‚¹ {node_id} åŒ…å«æ®µè½: {paragraph_ids}")
            
            # æ·»åŠ è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰æ®µè½å†…å®¹
            node_content_parts = []
            for para_id in paragraph_ids:
                if para_id in paragraph_content_map:
                    para_content = paragraph_content_map[para_id]
                    # ä¿ç•™æ®µè½IDæ ‡è®°
                    node_content_parts.append(f"[{para_id}] {para_content}")
                    print(f"ğŸ”§ [å†…å®¹é‡å»º] æ·»åŠ æ®µè½ {para_id}ï¼Œå†…å®¹é•¿åº¦: {len(para_content)}")
                else:
                    print(f"âš ï¸ [å†…å®¹é‡å»º] è­¦å‘Š: æ®µè½ {para_id} åœ¨åŸå†…å®¹ä¸­æœªæ‰¾åˆ°")
            
            # å°†èŠ‚ç‚¹çš„æ‰€æœ‰æ®µè½å†…å®¹åˆå¹¶
            if node_content_parts:
                rebuilt_content_parts.append('\n\n'.join(node_content_parts))
                rebuilt_content_parts.append('\n\n')  # èŠ‚ç‚¹é—´çš„åˆ†éš”
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶æ‰€æœ‰éƒ¨åˆ†
        rebuilt_content = ''.join(rebuilt_content_parts).strip()
        
        print(f"âœ… [å†…å®¹é‡å»º] é‡å»ºå®Œæˆ")
        print(f"âœ… [å†…å®¹é‡å»º] é‡å»ºåå†…å®¹é•¿åº¦: {len(rebuilt_content)} å­—ç¬¦")
        print(f"âœ… [å†…å®¹é‡å»º] åŒ…å« {len(node_mappings)} ä¸ªç‰©ç†åˆ†å‰²æ ")
        
        # éªŒè¯é‡å»ºç»“æœ
        divider_count = len(re.findall(r'--- [^-]+ ---', rebuilt_content))
        print(f"âœ… [å†…å®¹é‡å»º] éªŒè¯: æ‰¾åˆ° {divider_count} ä¸ªåˆ†å‰²æ ")
        
        # æ‰“å°é‡å»ºå†…å®¹çš„å‰200å­—ç¬¦ç”¨äºè°ƒè¯•
        print(f"ğŸ” [å†…å®¹é‡å»º] é‡å»ºå†…å®¹å‰200å­—ç¬¦:")
        print(f"   {rebuilt_content[:200]}...")
        
        return rebuilt_content
        
    except Exception as e:
        print(f"âŒ [å†…å®¹é‡å»ºé”™è¯¯] {str(e)}")
        import traceback
        traceback.print_exc()
        # å‡ºé”™æ—¶è¿”å›åŸå§‹å†…å®¹
        return text_with_ids

@app.get("/api/document-status/{document_id}")
async def get_document_status(document_id: str):
    """è·å–æ–‡æ¡£çŠ¶æ€å’Œè®ºè¯ç»“æ„åˆ†æè¿›åº¦"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    response_data = {
        "success": True,
        "document_id": document_id,
        "filename": doc_info.get("filename"),
        "content": doc_info.get("content"),
        "file_type": doc_info.get("file_type", ".md"),
        
        # è®ºè¯ç»“æ„åˆ†æçŠ¶æ€
        "status_demo": doc_info.get("status_demo", "not_started"),
        "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
        "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
        "edges_demo": doc_info.get("edges_demo", []),
        "error_demo": doc_info.get("error_demo"),
        "content_with_ids": doc_info.get("content_with_ids"),
    }
    
    # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œæ·»åŠ PDFç›¸å…³ä¿¡æ¯
    if doc_info.get("file_type") == ".pdf":
        response_data["pdf_base64"] = doc_info.get("pdf_base64")
        response_data["original_file_path"] = doc_info.get("original_file_path")
    
    return JSONResponse(response_data)

@app.get("/api/document/{document_id}")
async def get_document(document_id: str):
    """è·å–æ–‡æ¡£å†…å®¹å’Œè®ºè¯ç»“æ„"""
    
    try:
        # å¦‚æœæ–‡ä»¶åœ¨å†…å­˜çŠ¶æ€ä¸­å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if document_id in document_status:
            doc_info = document_status[document_id]
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": doc_info["content"],
                "filename": doc_info.get("filename", ""),
                "file_type": doc_info.get("file_type", ".md"),
                "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
                "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
                "edges_demo": doc_info.get("edges_demo", []),
                "status_demo": doc_info.get("status_demo", "not_started"),
                "error_demo": doc_info.get("error_demo"),
                "content_with_ids": doc_info.get("content_with_ids"),
                "pdf_base64": doc_info.get("pdf_base64")
            })
        else:
            # å°è¯•æŸ¥æ‰¾æ–‡ä»¶
            file_path = UPLOAD_DIR / f"{document_id}.md"
            
            if not file_path.exists():
                raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": content,
                "filename": f"{document_id}.md",
                "file_type": ".md",
                "mermaid_code_demo": None,
                "node_mappings_demo": {},
                "edges_demo": [],
                "status_demo": "not_started",
                "error_demo": None,
                "content_with_ids": None
            })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "message": "Argument Structure Analyzer API is running"}

@app.get("/")
async def root():
    return {"message": "Argument Structure Analyzer API is running"}

# æ–‡æ¡£ç»“æ„ç›¸å…³APIç«¯ç‚¹ï¼ˆä¿ç•™ç”¨äºç›®å½•ç”Ÿæˆç­‰ï¼‰

# æ–‡æ¡£ç»“æ„å’Œç›®å½•ç›¸å…³APIç«¯ç‚¹

@app.get("/api/document-structure/{document_id}")
async def get_document_structure(document_id: str):
    """è·å–æ–‡æ¡£çš„å±‚çº§ç»“æ„"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    chunks = parser.parse_to_chunks(content, document_id)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': chunks
                    }
                    
                    print(f"ğŸ“„ [è‡ªåŠ¨ç”Ÿæˆ] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº†ç»“æ„å’Œ {len(chunks)} ä¸ªchunks")
                    
                    return {
                        "success": True,
                        "structure": root.to_dict(),
                        "toc": toc,
                        "chunks": chunks,
                        "chunks_count": len(chunks)
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç»“æ„å°šæœªç”Ÿæˆï¼Œä¸”æ— æ³•è‡ªåŠ¨ç”Ÿæˆ",
                "structure": None,
                "toc": [],
                "chunks": [],
                "chunks_count": 0
            }
        
        structure_data = document_structures[document_id]
        chunks = structure_data.get('chunks', [])
        
        print(f"ğŸ“„ [API] è¿”å›æ–‡æ¡£ç»“æ„ï¼Œchunksæ•°é‡: {len(chunks)}")
        
        return {
            "success": True,
            "structure": structure_data['structure'],
            "toc": structure_data['toc'], 
            "chunks": chunks,  # è¿”å›å®é™…çš„chunksæ•°æ®
            "chunks_count": len(chunks)
        }
        
    except Exception as e:
        logger.error(f"Get document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.get("/api/document-toc/{document_id}")
async def get_document_toc(document_id: str):
    """è·å–æ–‡æ¡£ç›®å½•"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': parser.parse_to_chunks(content, document_id)
                    }
                    
                    return {
                        "success": True,
                        "toc": toc
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç›®å½•å°šæœªç”Ÿæˆ",
                "toc": []
            }
        
        structure_data = document_structures[document_id]
        return {
            "success": True,
            "toc": structure_data['toc']
        }
        
    except Exception as e:
        logger.error(f"Get document TOC error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç›®å½•å¤±è´¥: {str(e)}")

@app.post("/api/generate-document-structure/{document_id}")
async def generate_document_structure(document_id: str):
    """ç”Ÿæˆæˆ–é‡æ–°ç”Ÿæˆæ–‡æ¡£ç»“æ„"""
    try:
        if document_id not in document_status:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        content = document_status[document_id].get('content')
        if not content:
            raise HTTPException(status_code=400, detail="æ–‡æ¡£å†…å®¹ä¸ºç©º")
        
        # ä½¿ç”¨æ–‡æ¡£è§£æå™¨ç”Ÿæˆç»“æ„
        parser = DocumentParser()
        root = parser.parse_document(content, document_id)
        toc = parser.generate_toc(root)
        chunks = parser.parse_to_chunks(content, document_id)
        
        # ä¿å­˜ç»“æ„
        document_structures[document_id] = {
            'structure': root.to_dict(),
            'toc': toc,
            'chunks': chunks
        }
        
        print(f"ğŸ“„ [æ–‡æ¡£ç»“æ„] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº† {len(toc)} ä¸ªç›®å½•é¡¹ï¼Œ{len(chunks)} ä¸ªå†…å®¹å—")
        
        return {
            "success": True,
            "message": "æ–‡æ¡£ç»“æ„ç”ŸæˆæˆåŠŸ",
            "toc_items": len(toc),
            "chunks_count": len(chunks)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generate document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.post("/api/document/{document_id}/remap")
async def update_node_mappings(document_id: str, request_data: dict):
    """æ›´æ–°èŠ‚ç‚¹æ˜ å°„å…³ç³»"""
    try:
        print(f"ğŸ”„ [é‡æ˜ å°„] æ–‡æ¡£ {document_id} å¼€å§‹æ›´æ–°èŠ‚ç‚¹æ˜ å°„")
        
        if document_id not in document_status:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        # è·å–æ–°çš„èŠ‚ç‚¹æ˜ å°„
        new_node_mappings = request_data.get('node_mappings', {})
        
        if not new_node_mappings:
            raise HTTPException(status_code=400, detail="èŠ‚ç‚¹æ˜ å°„æ•°æ®ä¸ºç©º")
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        document_status[document_id]['node_mappings_demo'] = new_node_mappings
        
        print(f"ğŸ”„ [é‡æ˜ å°„] æ›´æ–°å®Œæˆï¼Œæ–°çš„èŠ‚ç‚¹æ•°é‡: {len(new_node_mappings)}")
        
        return JSONResponse({
            "success": True,
            "message": "èŠ‚ç‚¹æ˜ å°„æ›´æ–°æˆåŠŸ",
            "node_count": len(new_node_mappings)
        })
        
    except Exception as e:
        print(f"âŒ [é‡æ˜ å°„é”™è¯¯] {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}")

@app.post("/api/document/{document_id}/node/add")
async def add_node(document_id: str, request_data: AddNodeRequest):
    """æ·»åŠ æ–°èŠ‚ç‚¹åˆ°æ–‡æ¡£ç»“æ„ - ä½¿ç”¨å®Œæ•´çš„Phase 2+3é€»è¾‘"""
    try:
        print(f"ğŸš€ [Phase 2] æ”¶åˆ°æ·»åŠ èŠ‚ç‚¹è¯·æ±‚ - æ–‡æ¡£ID: {document_id}")
        print(f"ğŸš€ [Phase 2] è¯·æ±‚å‚æ•°: sourceNodeId={request_data.sourceNodeId}, direction={request_data.direction}, parentId={request_data.parentId}")
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        if document_id not in document_status:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"}
            )
        
        document_data = document_status[document_id]
        
        # è·å–å¿…è¦çš„æ–‡æ¡£æ•°æ®
        content_with_ids = document_data.get('content_with_ids', '')
        node_mappings = document_data.get('node_mappings_demo', {})
        mermaid_string = document_data.get('mermaid_code_demo', '')
        
        if not content_with_ids:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æœªåŒ…å«æ®µè½ID"}
            )
        
        # ======== Phase 1: æ„å»ºå®Œæ•´çš„å†…å­˜æ ‘ç»“æ„ ========
        print(f"ğŸŒ³ [Phase 1] å¼€å§‹æ„å»ºå†…å­˜æ ‘ç»“æ„...")
        tree_nodes = build_tree_structure(node_mappings, mermaid_string)
        
        # éªŒè¯æºèŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
        if request_data.sourceNodeId not in tree_nodes:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": f"æºèŠ‚ç‚¹ {request_data.sourceNodeId} ä¸å­˜åœ¨"}
            )
        
        source_node = tree_nodes[request_data.sourceNodeId]
        print(f"ğŸŒ³ [Phase 1] æºèŠ‚ç‚¹: {source_node.id}, å­èŠ‚ç‚¹æ•°: {len(source_node.children)}")
        
        # ======== Phase 2: å®Œæ•´çš„èŠ‚ç‚¹æ·»åŠ é€»è¾‘ ========
        print(f"ğŸš€ [Phase 2] å¼€å§‹å¤„ç† direction={request_data.direction}")
        
        rename_map = {}  # å­˜å‚¨æ‰€æœ‰éœ€è¦é‡å‘½åçš„æ˜ å°„
        new_node_id = ""
        
        if request_data.direction == 'child':
            print(f"ğŸš€ [Phase 2-child] å¤„ç†å­èŠ‚ç‚¹æ·»åŠ ")
            # a. parentNode å°±æ˜¯ sourceNode
            parent_node = source_node
            
            # b. è·å– parentNode.children åˆ—è¡¨ï¼Œæ–°èŠ‚ç‚¹çš„åºå·æ˜¯ len(children) + 1
            new_sequence = len(parent_node.children) + 1
            
            # c. ç”Ÿæˆ newNodeId (ä¾‹å¦‚ 1.2.3)
            new_node_id = f"{parent_node.id}.{new_sequence}"
            
            # d. æ­¤æ“ä½œä¸è§¦å‘é‡å‘½å
            print(f"ğŸš€ [Phase 2-child] æ–°èŠ‚ç‚¹ID: {new_node_id}, æ— éœ€é‡å‘½å")
            
        elif request_data.direction == 'right-sibling':
            print(f"ğŸš€ [Phase 2-right-sibling] å¤„ç†å³ä¾§åŒçº§æ·»åŠ ")
            
            # a. è·å– sourceNode çš„çˆ¶èŠ‚ç‚¹ parentNode
            parent_node = source_node.parent
            
            if parent_node is None:
                # æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-right-sibling] æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹")
                siblings = [node for node in tree_nodes.values() if node.parent is None]
                siblings.sort(key=lambda x: split_id_helper(x.id)[1] or 0)
                source_index = siblings.index(source_node)
                
                # c. åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å‘½å
                if source_index < len(siblings) - 1:
                    print(f"ğŸš€ [Phase 2-right-sibling] éœ€è¦é‡å‘½åï¼šæºèŠ‚ç‚¹ä¸æ˜¯æœ€åä¸€ä¸ªé¡¶çº§èŠ‚ç‚¹")
                    # d. é‡å‘½åæµç¨‹ï¼šä» source_index+1 å¼€å§‹çš„æ‰€æœ‰åç»­å…„å¼ŸèŠ‚ç‚¹
                    for i in range(source_index + 1, len(siblings)):
                        sibling = siblings[i]
                        old_id = sibling.id
                        old_sequence = split_id_helper(old_id)[1]
                        new_sequence = old_sequence + 1
                        new_sibling_id = str(new_sequence)
                        
                        # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                        subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                        rename_map.update(subtree_rename_map)
                        print(f"ğŸš€ [Phase 2-right-sibling] é‡å‘½åå…„å¼ŸèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
                
                # e. ç”Ÿæˆæ–°èŠ‚ç‚¹ID
                source_sequence = split_id_helper(source_node.id)[1]
                new_node_id = str(source_sequence + 1)
                
            else:
                # æºèŠ‚ç‚¹æœ‰çˆ¶èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-right-sibling] æºèŠ‚ç‚¹çˆ¶èŠ‚ç‚¹: {parent_node.id}")
                
                # b. åœ¨ parentNode.children æ•°ç»„ä¸­æ‰¾åˆ° sourceNode çš„ç´¢å¼• i
                source_index = parent_node.children.index(source_node)
                
                # c. åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å‘½å
                if source_index < len(parent_node.children) - 1:
                    print(f"ğŸš€ [Phase 2-right-sibling] éœ€è¦é‡å‘½åï¼šæºèŠ‚ç‚¹ä¸æ˜¯æœ€åä¸€ä¸ªå­èŠ‚ç‚¹")
                    
                    # d. é‡å‘½åæµç¨‹ï¼šä» parentNode.children[i+1] å¼€å§‹çš„æ‰€æœ‰åç»­å…„å¼ŸèŠ‚ç‚¹
                    for j in range(source_index + 1, len(parent_node.children)):
                        sibling = parent_node.children[j]
                        old_id = sibling.id
                        _, old_sequence = split_id_helper(old_id)
                        new_sequence = old_sequence + 1
                        new_sibling_id = f"{parent_node.id}.{new_sequence}"
                        
                        # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                        subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                        rename_map.update(subtree_rename_map)
                        print(f"ğŸš€ [Phase 2-right-sibling] é‡å‘½åå…„å¼ŸèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
                
                # e. ç”Ÿæˆ newNodeId: æ–°èŠ‚ç‚¹çš„åºå·æ˜¯åŸåºå· + 1
                _, source_sequence = split_id_helper(source_node.id)
                new_node_id = f"{parent_node.id}.{source_sequence + 1}"
                
        elif request_data.direction == 'left-sibling':
            print(f"ğŸš€ [Phase 2-left-sibling] å¤„ç†å·¦ä¾§åŒçº§æ·»åŠ ")
            
            # a. è·å– sourceNode çš„çˆ¶èŠ‚ç‚¹ parentNode
            parent_node = source_node.parent
            
            # c. æ–°èŠ‚ç‚¹çš„ID å°†æ˜¯ sourceNode å½“å‰çš„ID
            new_node_id = source_node.id
            
            if parent_node is None:
                # æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-left-sibling] æºèŠ‚ç‚¹æ˜¯é¡¶çº§èŠ‚ç‚¹")
                siblings = [node for node in tree_nodes.values() if node.parent is None]
                siblings.sort(key=lambda x: split_id_helper(x.id)[1] or 0)
                source_index = siblings.index(source_node)
                
                # d. å¿…é¡»é‡å‘½åï¼šä» sourceNode å¼€å§‹çš„æ‰€æœ‰èŠ‚ç‚¹
                for i in range(source_index, len(siblings)):
                    sibling = siblings[i]
                    old_id = sibling.id
                    old_sequence = split_id_helper(old_id)[1]
                    new_sequence = old_sequence + 1
                    new_sibling_id = str(new_sequence)
                    
                    # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                    subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                    rename_map.update(subtree_rename_map)
                    print(f"ğŸš€ [Phase 2-left-sibling] é‡å‘½åèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
                    
            else:
                # æºèŠ‚ç‚¹æœ‰çˆ¶èŠ‚ç‚¹
                print(f"ğŸš€ [Phase 2-left-sibling] æºèŠ‚ç‚¹çˆ¶èŠ‚ç‚¹: {parent_node.id}")
                
                # b. åœ¨ parentNode.children æ•°ç»„ä¸­æ‰¾åˆ° sourceNode çš„ç´¢å¼• i
                source_index = parent_node.children.index(source_node)
                
                # d. å¿…é¡»é‡å‘½åï¼šä» sourceNode (parentNode.children[i]) å¼€å§‹çš„æ‰€æœ‰åç»­å…„å¼ŸèŠ‚ç‚¹
                for j in range(source_index, len(parent_node.children)):
                    sibling = parent_node.children[j]
                    old_id = sibling.id
                    _, old_sequence = split_id_helper(old_id)
                    new_sequence = old_sequence + 1
                    new_sibling_id = f"{parent_node.id}.{new_sequence}"
                    
                    # ä½¿ç”¨è¿é”é‡å‘½åå‡½æ•°
                    subtree_rename_map = rename_subtree(sibling, new_sibling_id)
                    rename_map.update(subtree_rename_map)
                    print(f"ğŸš€ [Phase 2-left-sibling] é‡å‘½åèŠ‚ç‚¹ {old_id} -> {new_sibling_id}")
        
        else:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": f"ä¸æ”¯æŒçš„æ–¹å‘: {request_data.direction}"}
            )
        
        print(f"ğŸš€ [Phase 2] å®Œæˆé€»è¾‘å¤„ç†")
        print(f"ğŸš€ [Phase 2] æ–°èŠ‚ç‚¹ID: {new_node_id}")
        print(f"ğŸš€ [Phase 2] éœ€è¦é‡å‘½åçš„èŠ‚ç‚¹æ•°: {len(rename_map)}")
        print(f"ğŸš€ [Phase 2] é‡å‘½åæ˜ å°„: {rename_map}")
        
        # ======== Phase 3: åº”ç”¨å˜æ›´å¹¶è¿”å› ========
        print(f"âœ¨ [Phase 3] å¼€å§‹åº”ç”¨å˜æ›´...")
        
        new_node_label = request_data.label or "æ–°èŠ‚ç‚¹"
        
        # 1. åº”ç”¨é‡å‘½åï¼šéå†æ‰€æœ‰æ•°æ®ç»“æ„ï¼Œä½¿ç”¨ rename_map æ›¿æ¢æ—§IDä¸ºæ–°ID
        updated_content_with_ids = content_with_ids
        updated_node_mappings = node_mappings.copy()
        updated_mermaid_string = mermaid_string
        
        # åº”ç”¨é‡å‘½ååˆ° content_with_ids
        for old_id, new_id in rename_map.items():
            old_divider = f"--- {old_id} ---"
            new_divider = f"--- {new_id} ---"
            updated_content_with_ids = updated_content_with_ids.replace(old_divider, new_divider)
            print(f"âœ¨ [Phase 3] æ›´æ–°content: {old_divider} -> {new_divider}")
        
        # åº”ç”¨é‡å‘½ååˆ° node_mappings
        for old_id, new_id in rename_map.items():
            if old_id in updated_node_mappings:
                updated_node_mappings[new_id] = updated_node_mappings.pop(old_id)
                print(f"âœ¨ [Phase 3] æ›´æ–°node_mappings: {old_id} -> {new_id}")
        
        # åº”ç”¨é‡å‘½ååˆ° mermaid_string
        for old_id, new_id in rename_map.items():
            pattern = rf'\b{re.escape(old_id)}\b'
            updated_mermaid_string = re.sub(pattern, new_id, updated_mermaid_string)
            print(f"âœ¨ [Phase 3] æ›´æ–°mermaid: {old_id} -> {new_id}")
        
        # 2. æ’å…¥æ–°èŠ‚ç‚¹ï¼šå°†æ–°èŠ‚ç‚¹çš„åˆ†å‰²æ æ’å…¥åˆ°é‡å‘½ååçš„ content_with_ids ä¸­
        updated_content_with_ids = await insert_divider_phase3(
            updated_content_with_ids, 
            request_data.sourceNodeId,
            request_data.direction,
            new_node_id
        )
        
        if updated_content_with_ids is None:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ’å…¥ä½ç½®"}
            )
        
        # æ·»åŠ æ–°èŠ‚ç‚¹åˆ° node_mappings
        updated_node_mappings[new_node_id] = {
            "text_snippet": new_node_label,
            "paragraph_ids": [],
            "semantic_role": "æ–°æ·»åŠ çš„èŠ‚ç‚¹"
        }
        
        # æ›´æ–° mermaid_stringï¼Œæ·»åŠ æ–°èŠ‚ç‚¹è¿æ¥
        updated_mermaid_string = update_mermaid_phase3(
            updated_mermaid_string,
            new_node_id,
            new_node_label,
            request_data.direction,
            request_data.sourceNodeId
        )
        
        # 3. æ›´æ–°æ–‡æ¡£çŠ¶æ€
        document_status[document_id].update({
            'content_with_ids': updated_content_with_ids,
            'node_mappings_demo': updated_node_mappings,
            'mermaid_code_demo': updated_mermaid_string
        })
        
        print(f"âœ¨ [Phase 3] âœ… æˆåŠŸå®Œæˆæ‰€æœ‰å˜æ›´")
        print(f"âœ¨ [Phase 3] ğŸ“Š æœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
        print(f"   content_with_ids é•¿åº¦: {len(updated_content_with_ids)} å­—ç¬¦")
        print(f"   node_mappings æ•°é‡: {len(updated_node_mappings)}")
        print(f"   mermaid_code é•¿åº¦: {len(updated_mermaid_string)} å­—ç¬¦")
        print(f"   é‡å‘½åæ“ä½œæ•°: {len(rename_map)}")
        
        # è¿”å›æ›´æ–°åçš„å®Œæ•´æ–‡æ¡£
        return JSONResponse(content={
            "success": True,
            "message": "èŠ‚ç‚¹æ·»åŠ æˆåŠŸ",
            "document": document_status[document_id],
            "new_node_id": new_node_id,
            "rename_operations": len(rename_map)
        })
        
    except Exception as e:
        print(f"âŒ [Phase 2é”™è¯¯] æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {str(e)}"}
        )

def parse_content_structure(content: str, node_mappings: Dict) -> tuple:
    """è§£æcontent_with_idsçš„ç»“æ„ï¼Œè¿”å›åˆ†å‰²æ ä½ç½®å’ŒèŠ‚ç‚¹åŒºåŸŸ"""
    divider_positions = {}
    node_regions = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰åˆ†å‰²æ 
    divider_pattern = r'\n*---\s*([^-\n]+)\s*---\n*'
    matches = list(re.finditer(divider_pattern, content))
    
    print(f"ğŸ” [ç»“æ„è§£æ] æ‰¾åˆ° {len(matches)} ä¸ªåˆ†å‰²æ ")
    
    for i, match in enumerate(matches):
        node_id = match.group(1).strip()
        start_pos = match.start()
        end_pos = match.end()
        
        divider_positions[node_id] = {
            'start': start_pos,
            'end': end_pos,
            'match': match
        }
        
        # ç¡®å®šèŠ‚ç‚¹åŒºåŸŸï¼ˆä»åˆ†å‰²æ åˆ°ä¸‹ä¸€ä¸ªåˆ†å‰²æ æˆ–æ–‡æ¡£æœ«å°¾ï¼‰
        next_divider_start = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        
        node_regions[node_id] = {
            'start': start_pos,  # åˆ†å‰²æ å¼€å§‹ä½ç½®
            'end': next_divider_start,  # ä¸‹ä¸€ä¸ªåˆ†å‰²æ å¼€å§‹ä½ç½®æˆ–æ–‡æ¡£æœ«å°¾
            'content_start': end_pos,  # å®é™…å†…å®¹å¼€å§‹ä½ç½®ï¼ˆåˆ†å‰²æ åï¼‰
            'content_end': next_divider_start  # å®é™…å†…å®¹ç»“æŸä½ç½®
        }
        
        print(f"ğŸ” [ç»“æ„è§£æ] èŠ‚ç‚¹ {node_id}: start={start_pos}, end={next_divider_start}")
    
    return divider_positions, node_regions

def find_subtree_end(node_id: str, node_regions: Dict, content: str) -> int:
    """æ‰¾åˆ°èŠ‚ç‚¹åŠå…¶æ•´ä¸ªå­æ ‘çš„æœ«å°¾ä½ç½®ï¼ˆç”¨äºright-siblingæ’å…¥ï¼‰"""
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå‡è®¾èŠ‚ç‚¹æŒ‰å±‚çº§é¡ºåºæ’åˆ—
    # æ›´å¤æ‚çš„å®ç°éœ€è¦æ„å»ºå®é™…çš„æ ‘ç»“æ„
    if node_id in node_regions:
        return node_regions[node_id]['end']
    return len(content)

def update_mermaid_string(mermaid_string: str, new_node_id: str, new_node_label: str, direction: str, source_node_id: str, parent_id: Optional[str]) -> str:
    """æ›´æ–°mermaidå­—ç¬¦ä¸²ï¼Œæ·»åŠ æ–°èŠ‚ç‚¹å’Œè¿æ¥"""
    try:
        print(f"ğŸ”„ [Mermaidæ›´æ–°] å¼€å§‹æ›´æ–°ï¼Œæ–°èŠ‚ç‚¹: {new_node_id}, æ ‡ç­¾: {new_node_label}")
        print(f"ğŸ”„ [Mermaidæ›´æ–°] æ–¹å‘: {direction}, æºèŠ‚ç‚¹: {source_node_id}, çˆ¶èŠ‚ç‚¹: {parent_id}")
        print(f"ğŸ”„ [Mermaidæ›´æ–°] åŸå§‹Mermaidé•¿åº¦: {len(mermaid_string)}")
        
        updated_mermaid = mermaid_string or "graph TD"
        
        # ç¡®ä¿ä»¥æ¢è¡Œç¬¦ç»“å°¾
        if not updated_mermaid.endswith('\n'):
            updated_mermaid += '\n'
        
        # æ·»åŠ æ–°èŠ‚ç‚¹å®šä¹‰
        new_node_def = f"    {new_node_id}[{new_node_label}]"
        updated_mermaid += new_node_def + '\n'
        
        # æ ¹æ®æ–¹å‘å†³å®šè¿æ¥å…³ç³»
        if direction == 'child':
            # å­èŠ‚ç‚¹ï¼šæºèŠ‚ç‚¹æŒ‡å‘æ–°èŠ‚ç‚¹
            connection = f"    {source_node_id} --> {new_node_id}"
            print(f"ğŸ”„ [Mermaidæ›´æ–°] å­èŠ‚ç‚¹è¿æ¥: {source_node_id} --> {new_node_id}")
        else:
            # åŒçº§èŠ‚ç‚¹ï¼šéœ€è¦æ‰¾åˆ°å…±åŒçš„çˆ¶èŠ‚ç‚¹
            if parent_id:
                # å¦‚æœæ˜ç¡®æä¾›äº†çˆ¶èŠ‚ç‚¹IDï¼Œä½¿ç”¨å®ƒ
                target_parent = parent_id
                print(f"ğŸ”„ [Mermaidæ›´æ–°] ä½¿ç”¨æä¾›çš„çˆ¶èŠ‚ç‚¹: {target_parent}")
            else:
                # ä»ç°æœ‰çš„mermaidå­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾æºèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹
                target_parent = find_parent_node_in_mermaid(updated_mermaid, source_node_id)
                print(f"ğŸ”„ [Mermaidæ›´æ–°] ä»Mermaidä¸­æŸ¥æ‰¾åˆ°çˆ¶èŠ‚ç‚¹: {target_parent}")
            
            if target_parent:
                connection = f"    {target_parent} --> {new_node_id}"
                print(f"ğŸ”„ [Mermaidæ›´æ–°] åŒçº§èŠ‚ç‚¹è¿æ¥: {target_parent} --> {new_node_id}")
            else:
                # å¦‚æœæ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ï¼Œä½œä¸ºæ ¹èŠ‚ç‚¹å¤„ç†
                connection = f"    ROOT --> {new_node_id}"
                print(f"ğŸ”„ [Mermaidæ›´æ–°] æœªæ‰¾åˆ°çˆ¶èŠ‚ç‚¹ï¼Œä½¿ç”¨ROOTè¿æ¥")
        
        updated_mermaid += connection + '\n'
        
        print(f"âœ… [Mermaidæ›´æ–°] æ·»åŠ èŠ‚ç‚¹å®šä¹‰: {new_node_def}")
        print(f"âœ… [Mermaidæ›´æ–°] æ·»åŠ è¿æ¥: {connection}")
        print(f"âœ… [Mermaidæ›´æ–°] æ›´æ–°åé•¿åº¦: {len(updated_mermaid)}")
        
        return updated_mermaid
        
    except Exception as e:
        print(f"âŒ [Mermaidæ›´æ–°é”™è¯¯] {str(e)}")
        import traceback
        traceback.print_exc()
        return mermaid_string or "graph TD"

def find_parent_node_in_mermaid(mermaid_string: str, child_node_id: str) -> Optional[str]:
    """ä»mermaidå­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾æŒ‡å®šèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹"""
    try:
        print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] åœ¨Mermaidä¸­æŸ¥æ‰¾ {child_node_id} çš„çˆ¶èŠ‚ç‚¹")
        print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] Mermaidå†…å®¹: {mermaid_string[:200]}...")
        
        # æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼šæ›´å‡†ç¡®åœ°åŒ¹é…èŠ‚ç‚¹IDå’Œè¿æ¥å…³ç³»
        # åŒ¹é…å½¢å¦‚ "parent_node --> child_node_id" æˆ– "parent_node --> child_node_id[label]" çš„è¿æ¥
        escaped_child_id = re.escape(child_node_id)
        
        # ğŸ†• æ›´æ–°çš„åŒ¹é…æ¨¡å¼ï¼šæ”¯æŒç¼©è¿›å¼æ•°å­—IDï¼ˆåŒ…å«ç‚¹å·ï¼‰
        patterns = [
            # åŒ¹é… "parent --> child" æˆ– "parent --> child[label]" æˆ– "parent --> child "
            # æ”¯æŒç‚¹å·æ ¼å¼ï¼š1.2.3 --> 1.2.3.1
            rf'([A-Za-z0-9_.]+)\s*-->\s*{escaped_child_id}(?:\[|$|\s|-->)',
            # åŒ¹é…å¸¦ç©ºæ ¼çš„æƒ…å†µ
            rf'([A-Za-z0-9_.]+)\s*-->\s*{escaped_child_id}(?=\s|$|-->|\[)',
            # åŒ¹é…è¡Œç»“å°¾çš„æƒ…å†µ
            rf'([A-Za-z0-9_.]+)\s*-->\s*{escaped_child_id}$'
        ]
        
        for i, pattern in enumerate(patterns):
            match = re.search(pattern, mermaid_string, re.MULTILINE)
            if match:
                parent_id = match.group(1)
                print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] ä½¿ç”¨æ¨¡å¼ {i+1} æ‰¾åˆ° {child_node_id} çš„çˆ¶èŠ‚ç‚¹: {parent_id}")
                return parent_id
        
        print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] æœªæ‰¾åˆ° {child_node_id} çš„çˆ¶èŠ‚ç‚¹")
        return None
        
    except Exception as e:
        print(f"âŒ [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹é”™è¯¯] {str(e)}")
        return None

def convert_node_ids_to_numeric(analysis_result: Dict) -> Dict:
    """
    å°†AIè¿”å›çš„å­—æ¯èŠ‚ç‚¹IDè½¬æ¢ä¸ºç¼©è¿›å¼æ•°å­—ID
    
    Args:
        analysis_result: AIåˆ†æç»“æœï¼ŒåŒ…å«mermaid_codeã€node_mappingsã€edges
        
    Returns:
        è½¬æ¢åçš„åˆ†æç»“æœï¼Œæ‰€æœ‰èŠ‚ç‚¹IDéƒ½ä½¿ç”¨ç¼©è¿›å¼æ•°å­—æ ¼å¼
    """
    try:
        print(f"ğŸ”„ [IDè½¬æ¢] å¼€å§‹æ£€æŸ¥å¹¶è½¬æ¢èŠ‚ç‚¹IDæ ¼å¼")
        
        original_mappings = analysis_result.get("node_mappings", {})
        original_mermaid = analysis_result.get("mermaid_code", "")
        original_edges = analysis_result.get("edges", [])
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢ï¼ˆå¦‚æœèŠ‚ç‚¹IDéƒ½æ˜¯æ•°å­—æ ¼å¼ï¼Œå°±ä¸éœ€è¦è½¬æ¢ï¼‰
        node_ids = list(original_mappings.keys())
        needs_conversion = False
        
        for node_id in node_ids:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«éæ•°å­—éç‚¹å·å­—ç¬¦
            if not re.match(r'^[0-9.]+$', node_id):
                needs_conversion = True
                break
        
        if not needs_conversion:
            print(f"âœ… [IDè½¬æ¢] èŠ‚ç‚¹IDå·²ç»æ˜¯æ•°å­—æ ¼å¼ï¼Œæ— éœ€è½¬æ¢")
            return analysis_result
        
        print(f"ğŸ”„ [IDè½¬æ¢] æ£€æµ‹åˆ°å­—æ¯æ ¼å¼èŠ‚ç‚¹IDï¼Œå¼€å§‹è½¬æ¢ä¸ºç¼©è¿›å¼æ•°å­—æ ¼å¼")
        
        # æ„å»ºå­—æ¯åˆ°æ•°å­—çš„æ˜ å°„è¡¨
        # æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œç¡®ä¿è½¬æ¢çš„ä¸€è‡´æ€§
        sorted_node_ids = sorted(node_ids)
        id_mapping = {}
        
        # ä½¿ç”¨ç®€å•çš„é€’å¢æ•°å­—ä½œä¸ºæ ¹èŠ‚ç‚¹ID
        for i, old_id in enumerate(sorted_node_ids):
            new_id = str(i + 1)  # 1, 2, 3, ...
            id_mapping[old_id] = new_id
            print(f"ğŸ”„ [IDè½¬æ¢] {old_id} -> {new_id}")
        
        # è½¬æ¢node_mappings
        new_node_mappings = {}
        for old_id, mapping_data in original_mappings.items():
            new_id = id_mapping.get(old_id, old_id)
            new_node_mappings[new_id] = mapping_data
        
        # è½¬æ¢mermaid_stringä¸­çš„èŠ‚ç‚¹ID
        new_mermaid = original_mermaid
        for old_id, new_id in id_mapping.items():
            # ä½¿ç”¨å•è¯è¾¹ç•Œç¡®ä¿ç²¾ç¡®åŒ¹é…
            pattern = rf'\b{re.escape(old_id)}\b'
            new_mermaid = re.sub(pattern, new_id, new_mermaid)
        
        # è½¬æ¢edgesä¸­çš„èŠ‚ç‚¹ID
        new_edges = []
        for edge in original_edges:
            new_edge = {
                "source": id_mapping.get(edge["source"], edge["source"]),
                "target": id_mapping.get(edge["target"], edge["target"])
            }
            new_edges.append(new_edge)
        
        converted_result = {
            "success": True,
            "mermaid_code": new_mermaid,
            "node_mappings": new_node_mappings,
            "edges": new_edges
        }
        
        print(f"âœ… [IDè½¬æ¢] è½¬æ¢å®Œæˆï¼ŒèŠ‚ç‚¹IDå·²ç»Ÿä¸€ä¸ºç¼©è¿›å¼æ•°å­—æ ¼å¼")
        print(f"âœ… [IDè½¬æ¢] è½¬æ¢æ˜ å°„è¡¨: {id_mapping}")
        
        return converted_result
        
    except Exception as e:
        print(f"âŒ [IDè½¬æ¢é”™è¯¯] {str(e)}")
        # è½¬æ¢å¤±è´¥æ—¶è¿”å›åŸå§‹ç»“æœ
        return analysis_result

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 80)
    print("ğŸ¯ AI é˜…è¯»å™¨ - åç«¯APIæœåŠ¡ (å®Œæ•´é›†æˆç‰ˆ)")
    print("=" * 80)
    print("ğŸ“ æœåŠ¡åœ°å€: http://localhost:8000")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ”§ æœåŠ¡æ¨¡å¼: å¼€å‘æ¨¡å¼ (æ”¯æŒçƒ­é‡è½½)")
    print("=" * 80)
    print("ğŸš€ å·²é›†æˆåŠŸèƒ½: å®Œæ•´çš„èŠ‚ç‚¹æ·»åŠ ç³»ç»Ÿ (Phase 2+3)")
    print("   âœ… å®Œæ•´çš„å†…å­˜æ ‘æ•°æ®ç»“æ„ (NodeTreeNode)")
    print("   âœ… è¿é”é‡å‘½åæ”¯æŒ (rename_subtree)")
    print("   âœ… ä¸‰ç§æ·»åŠ æ–¹å‘: child, left-sibling, right-sibling")
    print("   âœ… ç¼©è¿›å¼æ•°å­—IDå‘½å (1, 2, 1.1, 1.2, 1.1.1...)")
    print("   âœ… æ™ºèƒ½æ’å…¥ä½ç½®è®¡ç®—")
    print("   âœ… è‡ªåŠ¨Mermaidå›¾æ›´æ–°")
    print("=" * 80)
    print("ğŸ“‹ æ§åˆ¶å°æ—¥å¿—è¯´æ˜:")
    print("   ğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] - æ–‡ä»¶ä¸Šä¼ ç›¸å…³ä¿¡æ¯")
    print("   ğŸ”„ [å¼€å§‹ç”Ÿæˆ] - è®ºè¯ç»“æ„åˆ†æä»»åŠ¡å¯åŠ¨")
    print("   ğŸ¤– [AIå¤„ç†] - è°ƒç”¨è®ºè¯ç»“æ„åˆ†æå™¨")
    print("   ğŸŒ³ [Phase 1] - å†…å­˜æ ‘ç»“æ„æ„å»º")
    print("   ğŸš€ [Phase 2] - èŠ‚ç‚¹æ·»åŠ é€»è¾‘å¤„ç†")
    print("   âœ¨ [Phase 3] - å˜æ›´åº”ç”¨å’Œæ•°æ®æ›´æ–°")
    print("   âœ… [åˆ†æå®Œæˆ] - è®ºè¯ç»“æ„åˆ†ææˆåŠŸ")
    print("   âŒ [åˆ†æå¤±è´¥] - åˆ†æè¿‡ç¨‹å‡ºç°é”™è¯¯")
    print("   â³ [çŠ¶æ€æŸ¥è¯¢] - å®¢æˆ·ç«¯æŸ¥è¯¢ç”ŸæˆçŠ¶æ€")
    print("=" * 80)
    print("ğŸ¯ æ”¯æŒçš„åŠŸèƒ½:")
    print("   ğŸ“Š æ™ºèƒ½è®ºè¯ç»“æ„åˆ†æ")
    print("   ğŸ—‚ï¸ æ–‡æ¡£æ®µè½è‡ªåŠ¨æ ‡è®°")
    print("   ğŸŒ åŠ¨æ€èŠ‚ç‚¹æ·»åŠ ä¸é‡æ’åº")
    print("   ğŸ“‹ å®æ—¶æ–‡æ¡£çŠ¶æ€ç®¡ç†")
    print("   ğŸ’¾ å†…å­˜æ•°æ®åº“å­˜å‚¨")
    print("=" * 80)
    print("ğŸš€ å¯åŠ¨æœåŠ¡ä¸­...")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")