from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import asyncio
import os
import hashlib
import tempfile
import re
from datetime import datetime
from pathlib import Path
import logging
import base64
from typing import List, Dict, Any
import json

# å¯¼å…¥ç°æœ‰çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨
from mindmap_generator import MindMapGenerator, MinimalDatabaseStub, get_logger, generate_mermaid_html, DocumentOptimizer

# å¯¼å…¥æ–‡æ¡£è§£æå™¨
from document_parser import DocumentParser

# å¯¼å…¥MinerUç›¸å…³æ¨¡å—
from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.config.enums import SupportedPdfParseMethod

app = FastAPI(title="Argument Structure Analyzer API", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Reactå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®æ—¥å¿—
logger = get_logger()

# åˆ›å»ºä¸Šä¼ ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# åˆ›å»ºPDFå¤„ç†ç›®å½•
PDF_OUTPUT_DIR = Path("pdf_outputs")
PDF_OUTPUT_DIR.mkdir(exist_ok=True)

# å­˜å‚¨æ–‡æ¡£çŠ¶æ€çš„å†…å­˜æ•°æ®åº“
document_status = {}



# å­˜å‚¨æ–‡æ¡£ç»“æ„çš„å†…å­˜æ•°æ®åº“
document_structures = {}

class ArgumentStructureAnalyzer:
    """è®ºè¯ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        self.generator = MindMapGenerator()
        self.document_parser = DocumentParser()
        # æ·»åŠ DocumentOptimizerå®ä¾‹ç”¨äºAIè°ƒç”¨
        self.optimizer = DocumentOptimizer()
    
    def add_paragraph_ids(self, text: str) -> str:
        """ä¸ºæ–‡æœ¬çš„æ¯ä¸ªæ®µè½æ·»åŠ IDå·"""
        try:
            # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
            paragraphs = text.split('\n\n')
            processed_paragraphs = []
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():  # åªå¤„ç†éç©ºæ®µè½
                    # ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ IDæ ‡è®°
                    para_id = f"para-{i+1}"
                    processed_paragraph = f"[{para_id}] {paragraph.strip()}"
                    processed_paragraphs.append(processed_paragraph)
                else:
                    processed_paragraphs.append(paragraph)
            
            return '\n\n'.join(processed_paragraphs)
            
        except Exception as e:
            print(f"âŒ [æ®µè½IDæ·»åŠ é”™è¯¯] {str(e)}")
            return text
    
    def split_text_into_chunks(self, text: str, document_id: str) -> List[Dict[str, Any]]:
        """å°†æ–‡æ¡£æŒ‰Markdownæ ‡é¢˜å±‚çº§åˆ†å—å¹¶åˆ†é…å”¯ä¸€æ ‡è¯†ç¬¦"""
        try:
            # ä½¿ç”¨æ–°çš„æ–‡æ¡£è§£æå™¨
            chunks = self.document_parser.parse_to_chunks(text, document_id)
            
            # åŒæ—¶ä¿å­˜æ–‡æ¡£ç»“æ„ç”¨äºç›®å½•ç”Ÿæˆ
            root = self.document_parser.parse_document(text, document_id)
            toc = self.document_parser.generate_toc(root)
            
            document_structures[document_id] = {
                'structure': root.to_dict(),
                'toc': toc,
                'chunks': chunks
            }
            
            print(f"ğŸ“„ [æ–‡æœ¬åˆ†å—] æ–‡æ¡£ {document_id} åˆ†ä¸º {len(chunks)} ä¸ªç»“æ„åŒ–å—")
            for i, chunk in enumerate(chunks[:3]):  # æ˜¾ç¤ºå‰3ä¸ªå—çš„ä¿¡æ¯
                print(f"   å— {i}: {chunk.get('title', 'æ— æ ‡é¢˜')} (çº§åˆ« {chunk.get('level', 0)})")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ [åˆ†å—é”™è¯¯] {str(e)}")
            return []
    
    async def generate_argument_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„"""
        try:
            # æ„å»ºåŸºäºæ®µè½çš„è®ºè¯ç»“æ„åˆ†æprompt
            prompt = f"""æˆ‘å¸Œæœ›ä½ æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åˆ†æå¸ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»æˆ‘æä¾›çš„ã€å·²ç»æŒ‰æ®µè½æ ‡è®°å¥½IDçš„æ–‡æœ¬ï¼Œå¹¶åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†æ¥åˆ†æå…¶è®ºè¯ç»“æ„ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š

ç¬¬ä¸€æ­¥ï¼šæ®µè½è§’è‰²è¯†åˆ«
- åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†ï¼ˆ[para-X]æ ‡è®°ï¼‰ï¼Œåˆ†ææ¯ä¸ªæ®µè½åœ¨è®ºè¯ä¸­çš„è§’è‰²
- ä¸è¦é‡æ–°åˆ’åˆ†æ®µè½ï¼Œè€Œæ˜¯åŸºäºç°æœ‰æ®µè½æ¥ç†è§£è®ºè¯é€»è¾‘
- è¯†åˆ«æ¯ä¸ªæ®µè½æ˜¯å¼•è¨€ã€è®ºç‚¹ã€è¯æ®ã€åé©³ã€ç»“è®ºç­‰å“ªç§ç±»å‹

ç¬¬äºŒæ­¥ï¼šæ„å»ºè®ºè¯ç»“æ„æµç¨‹å›¾
- åŸºäºæ®µè½çš„è®ºè¯è§’è‰²ï¼Œæ„å»ºé€»è¾‘æµç¨‹å›¾
- å°†å…·æœ‰ç›¸åŒæˆ–ç›¸å…³è®ºè¯åŠŸèƒ½çš„æ®µè½ç»„åˆæˆé€»è¾‘èŠ‚ç‚¹
- ç”¨ç®­å¤´è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»

ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€çš„ã€å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œä¸è¦åœ¨ JSON ä»£ç å—å‰åæ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡å­—ã€‚

è¿™ä¸ª JSON å¯¹è±¡å¿…é¡»åŒ…å«ä¸¤ä¸ªé¡¶çº§é”®ï¼š"mermaid_string" å’Œ "node_mappings"ã€‚

mermaid_string:
- å€¼ä¸ºç¬¦åˆ Mermaid.js è¯­æ³•çš„æµç¨‹å›¾ï¼ˆgraph TDï¼‰
- å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ç»„ç›¸å…³çš„æ®µè½ï¼ˆåŸºäºè®ºè¯åŠŸèƒ½ï¼‰
- èŠ‚ç‚¹ ID ä½¿ç”¨ç®€çŸ­çš„å­—æ¯æˆ–å­—æ¯æ•°å­—ç»„åˆï¼ˆå¦‚ï¼šA, B, C1, D2ï¼‰
- èŠ‚ç‚¹æ ‡ç­¾åº”è¯¥ç®€æ´æ¦‚æ‹¬è¯¥ç»„æ®µè½çš„æ ¸å¿ƒè®ºè¯åŠŸèƒ½ï¼ˆä¸è¶…è¿‡20å­—ï¼‰
- ä½¿ç”¨ç®­å¤´ --> è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»
- å¯ä»¥ä½¿ç”¨ä¸åŒçš„èŠ‚ç‚¹å½¢çŠ¶æ¥åŒºåˆ†ä¸åŒç±»å‹çš„è®ºè¯åŠŸèƒ½ï¼š
  - [æ–¹æ‹¬å·] ç”¨äºä¸»è¦è®ºç‚¹
  - (åœ†æ‹¬å·) ç”¨äºæ”¯æ’‘è¯æ®
  - {{èŠ±æ‹¬å·}} ç”¨äºé€»è¾‘è½¬æŠ˜æˆ–å…³é”®åˆ¤æ–­

node_mappings:
- å€¼ä¸º JSON å¯¹è±¡ï¼Œé”®ä¸º Mermaid å›¾ä¸­çš„èŠ‚ç‚¹ ID
- æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å€¼åŒ…å«ï¼š
  - "text_snippet": è¯¥èŠ‚ç‚¹åŒ…å«æ®µè½çš„æ ¸å¿ƒå†…å®¹æ€»ç»“ï¼ˆ30-80å­—ï¼‰
  - "paragraph_ids": æ„æˆè¯¥èŠ‚ç‚¹çš„æ®µè½IDæ•°ç»„ï¼ˆå¦‚ ["para-2", "para-3"]ï¼‰
  - "semantic_role": è¯¥èŠ‚ç‚¹åœ¨è®ºè¯ä¸­çš„è§’è‰²ï¼ˆå¦‚ "å¼•è¨€"ã€"æ ¸å¿ƒè®ºç‚¹"ã€"æ”¯æ’‘è¯æ®"ã€"åé©³"ã€"ç»“è®º" ç­‰ï¼‰

å…³é”®è¦æ±‚ï¼š
1. æ‰€æœ‰èŠ‚ç‚¹ ID å¿…é¡»åœ¨ mermaid_string ä¸­å­˜åœ¨
2. paragraph_ids å¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŸæ–‡çš„æ®µè½æ ‡è®° [para-X]ï¼Œä¸å¯ä¿®æ”¹
3. åŸæ–‡çš„æ¯ä¸ªæ®µè½éƒ½åº”è¯¥è¢«åˆ†é…ç»™è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹
4. èŠ‚ç‚¹çš„åˆ’åˆ†åº”è¯¥åŸºäºæ®µè½çš„è®ºè¯åŠŸèƒ½ï¼Œç›¸å…³åŠŸèƒ½çš„æ®µè½å¯ä»¥ç»„åˆåœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸­
5. æµç¨‹å›¾åº”è¯¥æ¸…æ™°å±•ç°è®ºè¯çš„é€»è¾‘æ¨ç†è·¯å¾„
6. ä¿æŒæ®µè½çš„å®Œæ•´æ€§ï¼Œä¸è¦æ‹†åˆ†æˆ–é‡ç»„æ®µè½å†…å®¹

ç°åœ¨ï¼Œè¯·åˆ†æä»¥ä¸‹å¸¦æœ‰æ®µè½IDçš„æ–‡æœ¬ï¼š

{text_with_ids}"""
            
            # ä½¿ç”¨DocumentOptimizerçš„generate_completionæ–¹æ³•
            response = await self.optimizer.generate_completion(
                prompt, 
                max_tokens=2000,
                task="åˆ†æè®ºè¯ç»“æ„"
            )
            
            if not response:
                print(f"âŒ [APIè°ƒç”¨å¤±è´¥] æœªæ”¶åˆ°AIå“åº”")
                return {"success": False, "error": "APIè°ƒç”¨å¤±è´¥ï¼Œæœªæ”¶åˆ°AIå“åº”"}
            
            # ä¿å­˜APIåŸå§‹å“åº”åˆ°æ–‡ä»¶
            try:
                from datetime import datetime
                import os
                
                # åˆ›å»ºapi_responsesæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                api_responses_dir = "api_responses"
                os.makedirs(api_responses_dir, exist_ok=True)
                
                # ç”Ÿæˆæ–‡ä»¶åï¼šæ—¶é—´æˆ³_è®ºè¯ç»“æ„åˆ†æ
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                response_filename = f"{timestamp}_argument_structure_analysis.txt"
                response_filepath = os.path.join(api_responses_dir, response_filename)
                
                # ä¿å­˜åŸå§‹å“åº”å’Œç›¸å…³ä¿¡æ¯
                with open(response_filepath, 'w', encoding='utf-8') as f:
                    f.write("=== APIè°ƒç”¨ä¿¡æ¯ ===\n")
                    f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ä»»åŠ¡: è®ºè¯ç»“æ„åˆ†æ\n")
                    f.write(f"æœ€å¤§tokens: 2000\n")
                    f.write(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦\n")
                    f.write(f"æ–‡æœ¬é•¿åº¦: {len(text_with_ids)} å­—ç¬¦\n")
                    f.write("\n=== å‘é€çš„Prompt ===\n")
                    f.write(prompt)
                    f.write("\n\n=== AIåŸå§‹å“åº” ===\n")
                    f.write(response)
                    f.write(f"\n\n=== å“åº”ç»“æŸ ===\n")
                
                print(f"ğŸ’¾ [APIå“åº”ä¿å­˜] å·²ä¿å­˜åˆ°: {response_filepath}")
                
            except Exception as save_error:
                print(f"âš ï¸ [å“åº”ä¿å­˜å¤±è´¥] {str(save_error)}")
            
            # è§£æJSONå“åº”
            try:
                # è¯¦ç»†è®°å½•åŸå§‹å“åº”
                print(f"ğŸ” [åŸå§‹AIå“åº”] é•¿åº¦: {len(response)} å­—ç¬¦")
                print(f"ğŸ” [åŸå§‹å“åº”å‰200å­—ç¬¦]: {response[:200]}")
                
                # æ›´å½»åº•çš„å“åº”æ¸…ç†
                clean_response = response.strip()
                
                # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                elif clean_response.startswith('```'):
                    clean_response = clean_response[3:]
                    
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                
                clean_response = clean_response.strip()
                
                # ç§»é™¤å¯èƒ½çš„è¯´æ˜æ–‡å­—ï¼Œåªä¿ç•™JSONéƒ¨åˆ†
                json_start = clean_response.find('{')
                json_end = clean_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    clean_response = clean_response[json_start:json_end+1]
                    print(f"ğŸ”§ [æå–JSON] æå–åˆ°JSONéƒ¨åˆ†ï¼Œé•¿åº¦: {len(clean_response)}")
                else:
                    print(f"âš ï¸ [JSONæå–å¤±è´¥] æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONç»“æ„")
                
                print(f"ğŸ” [æ¸…ç†åå“åº”å‰200å­—ç¬¦]: {clean_response[:200]}")
                
                structure_data = json.loads(clean_response)
                
                # éªŒè¯å¿…è¦çš„é”®
                if 'mermaid_string' not in structure_data or 'node_mappings' not in structure_data:
                    print(f"âŒ [æ•°æ®ç»“æ„é”™è¯¯] å“åº”é”®: {list(structure_data.keys())}")
                    return {"success": False, "error": "AIå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼šç¼ºå°‘å¿…è¦çš„é”®"}
                
                # éªŒè¯èŠ‚ç‚¹æ˜ å°„çš„ç»“æ„
                node_mappings = structure_data['node_mappings']
                valid_mappings = {}
                
                for node_id, mapping in node_mappings.items():
                    if isinstance(mapping, dict):
                        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨ï¼Œå¦‚æœç¼ºå°‘semantic_roleå°±æ·»åŠ é»˜è®¤å€¼
                        valid_mapping = {
                            "text_snippet": mapping.get("text_snippet", "è¯­ä¹‰å—å†…å®¹"),
                            "paragraph_ids": mapping.get("paragraph_ids", []),
                            "semantic_role": mapping.get("semantic_role", "è®ºè¯è¦ç´ ")
                        }
                        valid_mappings[node_id] = valid_mapping
                    else:
                        print(f"âš ï¸ [æ˜ å°„æ ¼å¼é”™è¯¯] èŠ‚ç‚¹ {node_id} çš„æ˜ å°„ä¸æ˜¯å­—å…¸æ ¼å¼")
                
                structure_data['node_mappings'] = valid_mappings
                
                print(f"âœ… [è®ºè¯ç»“æ„åˆ†æ] æˆåŠŸç”ŸæˆåŒ…å« {len(structure_data['node_mappings'])} ä¸ªèŠ‚ç‚¹çš„æµç¨‹å›¾")
                return {
                    "success": True,
                    "mermaid_code": structure_data['mermaid_string'],
                    "node_mappings": structure_data['node_mappings']
                }
                
            except json.JSONDecodeError as parse_error:
                print(f"âŒ [JSONè§£æé”™è¯¯] {str(parse_error)}")
                print(f"âŒ [å®Œæ•´åŸå§‹å“åº”]: {response}")
                print(f"âŒ [æ¸…ç†åå“åº”]: {clean_response}")
                return {"success": False, "error": f"JSONè§£æå¤±è´¥: {str(parse_error)}"}
                
        except Exception as e:
            print(f"âŒ [è®ºè¯ç»“æ„åˆ†æé”™è¯¯] {str(e)}")
            # æä¾›é™çº§ç­–ç•¥ - ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„
            try:
                fallback_structure = self.generate_fallback_structure(text_with_ids)
                print(f"ğŸ”„ [é™çº§ç­–ç•¥] ä½¿ç”¨åŸºæœ¬è®ºè¯ç»“æ„ï¼ŒåŒ…å« {len(fallback_structure['node_mappings'])} ä¸ªèŠ‚ç‚¹")
                return fallback_structure
            except Exception as fallback_error:
                print(f"âŒ [é™çº§ç­–ç•¥å¤±è´¥] {str(fallback_error)}")
                return {"success": False, "error": f"AIåˆ†æå¤±è´¥ä¸”é™çº§ç­–ç•¥ä¹Ÿå¤±è´¥: {str(e)}"}

    def generate_fallback_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„ä½œä¸ºé™çº§ç­–ç•¥"""
        import re
        
        # æå–æ‰€æœ‰æ®µè½ID
        para_ids = re.findall(r'\[para-(\d+)\]', text_with_ids)
        
        if not para_ids:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½IDï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬ç»“æ„
            return {
                "success": True,
                "mermaid_code": "graph TD\n    A[æ–‡æ¡£åˆ†æ] --> B[ä¸»è¦å†…å®¹]\n    B --> C[æ€»ç»“]",
                "node_mappings": {
                    "A": {
                        "text_snippet": "æ–‡æ¡£å¼€å§‹",
                        "paragraph_ids": ["para-1"],
                        "semantic_role": "å¼•è¨€"
                    },
                    "B": {
                        "text_snippet": "ä¸»è¦å†…å®¹",
                        "paragraph_ids": ["para-2"],
                        "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                    },
                    "C": {
                        "text_snippet": "æ–‡æ¡£ç»“è®º",
                        "paragraph_ids": ["para-3"],
                        "semantic_role": "ç»“è®º"
                    }
                }
            }
        
        # åŸºäºæ®µè½æ•°é‡ç”Ÿæˆç»“æ„
        total_paras = len(para_ids)
        
        if total_paras <= 3:
            # ç®€å•çº¿æ€§ç»“æ„
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[ä¸»ä½“]\n"
            mermaid_code += "    B --> C[ç»“è®º]"
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€éƒ¨åˆ†",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "æ–‡æ¡£ä¸»ä½“å†…å®¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:-1]] if total_paras > 2 else [f"para-{para_ids[1]}"] if total_paras > 1 else [],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"] if total_paras > 1 else [],
                    "semantic_role": "ç»“è®º"
                }
            }
        else:
            # å¤æ‚ç»“æ„ï¼šå¼•è¨€ -> å¤šä¸ªè®ºç‚¹ -> ç»“è®º
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[è®ºç‚¹1]\n"
            mermaid_code += "    A --> C[è®ºç‚¹2]\n"
            if total_paras > 5:
                mermaid_code += "    A --> D[è®ºç‚¹3]\n"
                mermaid_code += "    B --> E[ç»“è®º]\n"
                mermaid_code += "    C --> E\n"
                mermaid_code += "    D --> E"
            else:
                mermaid_code += "    B --> D[ç»“è®º]\n"
                mermaid_code += "    C --> D"
            
            # å°†æ®µè½åˆ†é…ç»™ä¸åŒèŠ‚ç‚¹
            para_per_section = max(1, total_paras // 4)
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "ç¬¬ä¸€ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:1+para_per_section]],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "ç¬¬äºŒä¸ªè®ºç‚¹", 
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+para_per_section:1+2*para_per_section]],
                    "semantic_role": "æ”¯æ’‘è¯æ®"
                }
            }
            
            if total_paras > 5:
                node_mappings["D"] = {
                    "text_snippet": "ç¬¬ä¸‰ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:-1]],
                    "semantic_role": "è¡¥å……è®ºè¯"
                }
                node_mappings["E"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"],
                    "semantic_role": "ç»“è®º"
                }
            else:
                node_mappings["D"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:]],
                    "semantic_role": "ç»“è®º"
                }
        
        return {
            "success": True,
            "mermaid_code": mermaid_code,
            "node_mappings": node_mappings
        }

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
argument_analyzer = ArgumentStructureAnalyzer()

async def process_pdf_to_markdown(pdf_file_path: str, document_id: str) -> str:
    """
    ä½¿ç”¨MinerUå¤„ç†PDFæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        pdf_file_path: PDFæ–‡ä»¶è·¯å¾„
        document_id: æ–‡æ¡£ID
        
    Returns:
        è½¬æ¢åçš„Markdownå†…å®¹
    """
    try:
        print(f"\nğŸ“„ [MinerU-PDFå¤„ç†] å¼€å§‹å¤„ç†PDFæ–‡ä»¶")
        print(f"    ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = PDF_OUTPUT_DIR / document_id
        image_dir = output_dir / "images"
        os.makedirs(image_dir, exist_ok=True)
        
        print(f"ğŸ“ [MinerU-ç›®å½•] åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ–¼ï¸  [MinerU-å›¾ç‰‡] å›¾ç‰‡ç›®å½•: {image_dir}")
        
        # åˆ›å»ºæ•°æ®è¯»å†™å™¨
        print("ğŸ”§ [MinerU-åˆå§‹åŒ–] åˆ›å»ºæ•°æ®è¯»å†™å™¨...")
        reader = FileBasedDataReader("")
        image_writer = FileBasedDataWriter(str(image_dir))
        md_writer = FileBasedDataWriter(str(output_dir))
        
        # è¯»å–PDFæ–‡ä»¶
        print("ğŸ“– [MinerU-è¯»å–] æ­£åœ¨è¯»å–PDFæ–‡ä»¶...")
        pdf_bytes = reader.read(pdf_file_path)
        print(f"ğŸ“Š [MinerU-æ•°æ®] PDFæ–‡ä»¶å¤§å°: {len(pdf_bytes)} å­—èŠ‚")
        
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        print("ğŸ—ï¸  [MinerU-æ•°æ®é›†] åˆ›å»ºPymuDocDatasetå®ä¾‹...")
        ds = PymuDocDataset(pdf_bytes)
        
        # åˆ†ç±»å¤„ç†æ¨¡å¼
        print("ğŸ” [MinerU-æ£€æµ‹] æ£€æµ‹PDFå¤„ç†æ¨¡å¼...")
        pdf_mode = ds.classify()
        
        # è¿›è¡Œæ¨ç†
        if pdf_mode == SupportedPdfParseMethod.OCR:
            print(f"ğŸ”¤ [MinerU-OCRæ¨¡å¼] æ£€æµ‹åˆ°éœ€è¦OCRå¤„ç†ï¼Œå¼€å§‹æ–‡å­—è¯†åˆ«...")
            print("    ğŸ“¸ æ­£åœ¨æå–å›¾ç‰‡ä¸­çš„æ–‡å­—...")
            print("    ğŸ§  è°ƒç”¨OCRå¼•æ“è¿›è¡Œæ–‡å­—è¯†åˆ«...")
            infer_result = ds.apply(doc_analyze, ocr=True)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨OCRæ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_ocr_mode(image_writer)
            print("âœ… [MinerU-OCR] OCRå¤„ç†å®Œæˆ")
        else:
            print(f"ğŸ“ [MinerU-æ–‡æœ¬æ¨¡å¼] æ£€æµ‹åˆ°å¯ç›´æ¥æå–æ–‡æœ¬ï¼Œå¼€å§‹æ–‡æœ¬å¤„ç†...")
            print("    ğŸ“„ æ­£åœ¨æå–PDFä¸­çš„æ–‡æœ¬å†…å®¹...")
            print("    ğŸ”§ åˆ†ææ–‡æ¡£ç»“æ„å’Œç‰ˆé¢...")
            infer_result = ds.apply(doc_analyze, ocr=False)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_txt_mode(image_writer)
            print("âœ… [MinerU-æ–‡æœ¬] æ–‡æœ¬æå–å®Œæˆ")
        
        print("ğŸ“‹ [MinerU-è½¬æ¢] æ­£åœ¨ç”ŸæˆMarkdownæ ¼å¼...")
        # è·å–Markdownå†…å®¹
        markdown_content = pipe_result.get_markdown("images")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        md_file_path = output_dir / f"{document_id}.md"
        print(f"ğŸ’¾ [MinerU-ä¿å­˜] ä¿å­˜Markdownæ–‡ä»¶: {md_file_path}")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # ç»Ÿè®¡ä¿¡æ¯
        lines_count = len(markdown_content.split('\n'))
        words_count = len(markdown_content.split())
        
        print("=" * 60)
        print("âœ… [MinerU-å®Œæˆ] PDFè½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"    ğŸ“Š ç”Ÿæˆå†…å®¹ç»Ÿè®¡:")
        print(f"       â€¢ Markdownæ€»é•¿åº¦: {len(markdown_content):,} å­—ç¬¦")
        print(f"       â€¢ æ€»è¡Œæ•°: {lines_count:,} è¡Œ")
        print(f"       â€¢ å•è¯æ•°: {words_count:,} ä¸ª")
        print(f"    ğŸ“ è¾“å‡ºæ–‡ä»¶: {md_file_path}")
        print("=" * 60)
        
        return markdown_content
        
    except Exception as e:
        print("=" * 60)
        print(f"âŒ [MinerU-é”™è¯¯] PDFå¤„ç†å¤±è´¥ï¼")
        print(f"    ğŸš¨ é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"    ğŸ“„ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        logger.error(f"MinerU PDF processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"PDFå¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£ï¼Œæ”¯æŒPDFã€MDå’ŒTXTæ–‡ä»¶"""
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = ['.md', '.txt', '.pdf']
    file_extension = Path(file.filename).suffix.lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .mdã€.txt å’Œ .pdf æ–‡ä»¶")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£ID
        content_hash = hashlib.md5(content).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = Path(file.filename).stem
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        
        print(f"\nğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] {file.filename}")
        print(f"ğŸ†” [æ–‡æ¡£ID] {document_id}")
        print(f"ğŸ“Š [æ–‡ä»¶å¤§å°] {len(content)} å­—èŠ‚")
        print(f"ğŸ“‹ [æ–‡ä»¶ç±»å‹] {file_extension}")
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶
        original_file_path = UPLOAD_DIR / f"{document_id}{file_extension}"
        with open(original_file_path, 'wb') as f:
            f.write(content)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
        if file_extension == '.pdf':
            # å¤„ç†PDFæ–‡ä»¶
            print(f"ğŸ”„ [PDFå¤„ç†] å¼€å§‹è½¬æ¢PDFä¸ºMarkdown...")
            markdown_content = await process_pdf_to_markdown(str(original_file_path), document_id)
            text_content = markdown_content
            
            # å°†åŸå§‹PDFæ–‡ä»¶ç¼–ç ä¸ºbase64ç”¨äºå‰ç«¯æ˜¾ç¤º
            pdf_base64 = base64.b64encode(content).decode('utf-8')
            
        else:
            # å¤„ç†æ–‡æœ¬æ–‡ä»¶
            text_content = content.decode('utf-8')
            pdf_base64 = None
        
        # å­˜å‚¨åˆ°å†…å­˜æ•°æ®åº“
        MinimalDatabaseStub.store_text(text_content)
        
        # ç«‹å³ä¸ºæ–‡æ¡£å†…å®¹æ·»åŠ æ®µè½IDï¼Œæ— éœ€ç­‰å¾…ç”Ÿæˆè®ºè¯ç»“æ„
        print("ğŸ“ [å¤„ç†æ®µè½] ä¸ºä¸Šä¼ çš„æ–‡æ¡£æ·»åŠ æ®µè½IDæ ‡è®°...")
        content_with_ids = argument_analyzer.add_paragraph_ids(text_content)
        print(f"ğŸ“ [æ®µè½å¤„ç†å®Œæˆ] å·²ä¸ºæ–‡æ¡£æ·»åŠ æ®µè½IDï¼Œå†…å®¹é•¿åº¦: {len(content_with_ids)} å­—ç¬¦")
        
        # åˆå§‹åŒ–æ–‡æ¡£çŠ¶æ€
        document_status[document_id] = {
            "status": "uploaded",
            "content": text_content,
            "filename": file.filename,
            "file_type": file_extension,
            "original_file_path": str(original_file_path),
            "pdf_base64": pdf_base64,  # ä»…PDFæ–‡ä»¶æœ‰æ­¤å­—æ®µ
            "status_demo": "not_started",
            "mermaid_code_demo": None,
            "node_mappings_demo": {},
            "error_demo": None,
            "content_with_ids": content_with_ids  # ç«‹å³è®¾ç½®å¸¦æ®µè½IDçš„å†…å®¹
        }
        
        print(f"âœ… [ä¸Šä¼ æˆåŠŸ] æ–‡æ¡£å·²ä¿å­˜å¹¶å‡†å¤‡ç”Ÿæˆæ€ç»´å¯¼å›¾")
        print("=" * 60)
        
        logger.info(f"Document uploaded: {document_id}")
        
        # è¿”å›æ–‡æ¡£ä¿¡æ¯
        response_data = {
            "success": True,
            "document_id": document_id,
            "filename": file.filename,
            "content": text_content,
            "file_type": file_extension,
            "status": "uploaded",
            "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ"
        }
        
        # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œè¿”å›base64ç¼–ç çš„åŸå§‹PDF
        if file_extension == '.pdf':
            response_data["pdf_base64"] = pdf_base64
            response_data["message"] = "PDFæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå·²è½¬æ¢ä¸ºMarkdown"
        
        return JSONResponse(response_data)
        
    except UnicodeDecodeError:
        print(f"âŒ [ç¼–ç é”™è¯¯] æ–‡ä»¶: {file.filename}")
        raise HTTPException(status_code=400, detail="æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯UTF-8ç¼–ç ")
    except Exception as e:
        print(f"âŒ [ä¸Šä¼ å¤±è´¥] æ–‡ä»¶: {file.filename}, é”™è¯¯: {str(e)}")
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/document-pdf/{document_id}")
async def get_document_pdf(document_id: str):
    """è·å–åŸå§‹PDFæ–‡ä»¶"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    if doc_info.get("file_type") != ".pdf":
        raise HTTPException(status_code=400, detail="è¯¥æ–‡æ¡£ä¸æ˜¯PDFæ–‡ä»¶")
    
    original_file_path = doc_info.get("original_file_path")
    if not original_file_path or not os.path.exists(original_file_path):
        raise HTTPException(status_code=404, detail="åŸå§‹PDFæ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=original_file_path,
        media_type='application/pdf',
        filename=doc_info["filename"]
    )

@app.post("/api/generate-argument-structure/{document_id}")
async def generate_argument_structure(document_id: str):
    """ä¸ºæŒ‡å®šæ–‡æ¡£ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    # æ£€æŸ¥çŠ¶æ€
    if doc_info.get("status_demo") == "generating":
        print(f"â³ [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­...")
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­..."
        })
    
    if doc_info.get("status_demo") == "completed" and doc_info.get("mermaid_code_demo"):
        print(f"âœ… [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„å·²åˆ†æå®Œæˆ")
        return JSONResponse({
            "success": True,
            "status": "completed",
            "mermaid_code": doc_info["mermaid_code_demo"],
            "node_mappings": doc_info.get("node_mappings_demo", {}),
            "message": "è®ºè¯ç»“æ„å·²ç”Ÿæˆ"
        })
    
    try:
        print(f"ğŸ”„ [å¼€å§‹åˆ†æ] ä¸ºæ–‡æ¡£ {document_id} å¯åŠ¨è®ºè¯ç»“æ„åˆ†æä»»åŠ¡")
        
        # æ›´æ–°çŠ¶æ€ä¸ºåˆ†æä¸­
        doc_info["status_demo"] = "generating"
        
        # å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„
        asyncio.create_task(generate_argument_structure_async(document_id, doc_info["content"]))
        
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "å¼€å§‹åˆ†æè®ºè¯ç»“æ„..."
        })
        
    except Exception as e:
        print(f"âŒ [å¯åŠ¨å¤±è´¥] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}")
        logger.error(f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        doc_info["status_demo"] = "error"
        doc_info["error_demo"] = str(e)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")

async def generate_argument_structure_async(document_id: str, content: str):
    """å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾"""
    try:
        print(f"\nğŸš€ [å¼€å§‹åˆ†æ] æ–‡æ¡£ID: {document_id}")
        print(f"ğŸ“„ [æ–‡æ¡£å†…å®¹] é•¿åº¦: {len(content)} å­—ç¬¦")
        print("=" * 60)
        
        # è·å–å·²ç»å¤„ç†è¿‡çš„å¸¦æ®µè½IDçš„å†…å®¹
        print("ğŸ“ [è·å–æ®µè½ID] ä½¿ç”¨å·²å¤„ç†çš„æ®µè½IDå†…å®¹...")
        text_with_ids = document_status[document_id]["content_with_ids"]
        if not text_with_ids:
            # å¦‚æœæ²¡æœ‰é¢„å¤„ç†çš„å†…å®¹ï¼Œé‡æ–°ç”Ÿæˆï¼ˆå‘åå…¼å®¹ï¼‰
            print("ğŸ“ [é‡æ–°å¤„ç†] æœªæ‰¾åˆ°é¢„å¤„ç†çš„æ®µè½IDå†…å®¹ï¼Œé‡æ–°ç”Ÿæˆ...")
            text_with_ids = argument_analyzer.add_paragraph_ids(content)
            document_status[document_id]["content_with_ids"] = text_with_ids
        
        # åˆ†æè®ºè¯ç»“æ„
        print("ğŸ§  [AIåˆ†æ] å¼€å§‹åˆ†æè®ºè¯ç»“æ„...")
        result = await argument_analyzer.generate_argument_structure(text_with_ids)
        
        if result["success"]:
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document_status[document_id]["status_demo"] = "completed"
            document_status[document_id]["mermaid_code_demo"] = result["mermaid_code"]
            document_status[document_id]["node_mappings_demo"] = result["node_mappings"]
            document_status[document_id]["content_with_ids"] = text_with_ids  # ä¿å­˜å¸¦IDçš„å†…å®¹
            
            print(f"âœ… [åˆ†æå®Œæˆ] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†ææˆåŠŸ")
            print(f"ğŸ“Š [ç”Ÿæˆç»“æœ] åŒ…å« {len(result['node_mappings'])} ä¸ªè®ºè¯èŠ‚ç‚¹")
        else:
            # åˆ†æå¤±è´¥
            document_status[document_id]["status_demo"] = "error"
            document_status[document_id]["error_demo"] = result["error"]
            print(f"âŒ [åˆ†æå¤±è´¥] æ–‡æ¡£ {document_id}: {result['error']}")
            
    except Exception as e:
        print(f"âŒ [å¼‚æ­¥åˆ†æé”™è¯¯] æ–‡æ¡£ {document_id}: {str(e)}")
        logger.error(f"å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        document_status[document_id]["status_demo"] = "error"
        document_status[document_id]["error_demo"] = str(e)

@app.get("/api/document-status/{document_id}")
async def get_document_status(document_id: str):
    """è·å–æ–‡æ¡£çŠ¶æ€å’Œè®ºè¯ç»“æ„åˆ†æè¿›åº¦"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    response_data = {
        "success": True,
        "document_id": document_id,
        "filename": doc_info.get("filename"),
        "content": doc_info.get("content"),
        "file_type": doc_info.get("file_type", ".md"),
        
        # è®ºè¯ç»“æ„åˆ†æçŠ¶æ€
        "status_demo": doc_info.get("status_demo", "not_started"),
        "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
        "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
        "error_demo": doc_info.get("error_demo"),
        "content_with_ids": doc_info.get("content_with_ids"),
    }
    
    # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œæ·»åŠ PDFç›¸å…³ä¿¡æ¯
    if doc_info.get("file_type") == ".pdf":
        response_data["pdf_base64"] = doc_info.get("pdf_base64")
        response_data["original_file_path"] = doc_info.get("original_file_path")
    
    return JSONResponse(response_data)

@app.get("/api/document/{document_id}")
async def get_document(document_id: str):
    """è·å–æ–‡æ¡£å†…å®¹å’Œè®ºè¯ç»“æ„"""
    
    try:
        # å¦‚æœæ–‡ä»¶åœ¨å†…å­˜çŠ¶æ€ä¸­å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if document_id in document_status:
            doc_info = document_status[document_id]
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": doc_info["content"],
                "filename": doc_info.get("filename", ""),
                "file_type": doc_info.get("file_type", ".md"),
                "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
                "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
                "status_demo": doc_info.get("status_demo", "not_started"),
                "error_demo": doc_info.get("error_demo"),
                "content_with_ids": doc_info.get("content_with_ids"),
                "pdf_base64": doc_info.get("pdf_base64")
            })
        else:
            # å°è¯•æŸ¥æ‰¾æ–‡ä»¶
            file_path = UPLOAD_DIR / f"{document_id}.md"
            
            if not file_path.exists():
                raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": content,
                "filename": f"{document_id}.md",
                "file_type": ".md",
                "mermaid_code_demo": None,
                "node_mappings_demo": {},
                "status_demo": "not_started",
                "error_demo": None,
                "content_with_ids": None
            })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "message": "Argument Structure Analyzer API is running"}

@app.get("/")
async def root():
    return {"message": "Argument Structure Analyzer API is running"}

# æ–‡æ¡£ç»“æ„ç›¸å…³APIç«¯ç‚¹ï¼ˆä¿ç•™ç”¨äºç›®å½•ç”Ÿæˆç­‰ï¼‰

# æ–‡æ¡£ç»“æ„å’Œç›®å½•ç›¸å…³APIç«¯ç‚¹

@app.get("/api/document-structure/{document_id}")
async def get_document_structure(document_id: str):
    """è·å–æ–‡æ¡£çš„å±‚çº§ç»“æ„"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    chunks = parser.parse_to_chunks(content, document_id)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': chunks
                    }
                    
                    print(f"ğŸ“„ [è‡ªåŠ¨ç”Ÿæˆ] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº†ç»“æ„å’Œ {len(chunks)} ä¸ªchunks")
                    
                    return {
                        "success": True,
                        "structure": root.to_dict(),
                        "toc": toc,
                        "chunks": chunks,
                        "chunks_count": len(chunks)
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç»“æ„å°šæœªç”Ÿæˆï¼Œä¸”æ— æ³•è‡ªåŠ¨ç”Ÿæˆ",
                "structure": None,
                "toc": [],
                "chunks": [],
                "chunks_count": 0
            }
        
        structure_data = document_structures[document_id]
        chunks = structure_data.get('chunks', [])
        
        print(f"ğŸ“„ [API] è¿”å›æ–‡æ¡£ç»“æ„ï¼Œchunksæ•°é‡: {len(chunks)}")
        
        return {
            "success": True,
            "structure": structure_data['structure'],
            "toc": structure_data['toc'], 
            "chunks": chunks,  # è¿”å›å®é™…çš„chunksæ•°æ®
            "chunks_count": len(chunks)
        }
        
    except Exception as e:
        logger.error(f"Get document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.get("/api/document-toc/{document_id}")
async def get_document_toc(document_id: str):
    """è·å–æ–‡æ¡£ç›®å½•"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': parser.parse_to_chunks(content, document_id)
                    }
                    
                    return {
                        "success": True,
                        "toc": toc
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç›®å½•å°šæœªç”Ÿæˆ",
                "toc": []
            }
        
        structure_data = document_structures[document_id]
        return {
            "success": True,
            "toc": structure_data['toc']
        }
        
    except Exception as e:
        logger.error(f"Get document TOC error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç›®å½•å¤±è´¥: {str(e)}")

@app.post("/api/generate-document-structure/{document_id}")
async def generate_document_structure(document_id: str):
    """ç”Ÿæˆæˆ–é‡æ–°ç”Ÿæˆæ–‡æ¡£ç»“æ„"""
    try:
        if document_id not in document_status:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        content = document_status[document_id].get('content')
        if not content:
            raise HTTPException(status_code=400, detail="æ–‡æ¡£å†…å®¹ä¸ºç©º")
        
        # ä½¿ç”¨æ–‡æ¡£è§£æå™¨ç”Ÿæˆç»“æ„
        parser = DocumentParser()
        root = parser.parse_document(content, document_id)
        toc = parser.generate_toc(root)
        chunks = parser.parse_to_chunks(content, document_id)
        
        # ä¿å­˜ç»“æ„
        document_structures[document_id] = {
            'structure': root.to_dict(),
            'toc': toc,
            'chunks': chunks
        }
        
        print(f"ğŸ“„ [æ–‡æ¡£ç»“æ„] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº† {len(toc)} ä¸ªç›®å½•é¡¹ï¼Œ{len(chunks)} ä¸ªå†…å®¹å—")
        
        return {
            "success": True,
            "message": "æ–‡æ¡£ç»“æ„ç”ŸæˆæˆåŠŸ",
            "toc_items": len(toc),
            "chunks_count": len(chunks)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generate document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.post("/api/document/{document_id}/remap")
async def update_node_mappings(document_id: str, request_data: dict):
    """æ›´æ–°æ–‡æ¡£çš„èŠ‚ç‚¹æ˜ å°„å…³ç³»"""
    try:
        print(f"ğŸ“ [API] æ”¶åˆ°èŠ‚ç‚¹æ˜ å°„æ›´æ–°è¯·æ±‚ - æ–‡æ¡£ID: {document_id}")
        print(f"ğŸ“ [API] æ–°çš„èŠ‚ç‚¹æ˜ å°„: {request_data}")
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if 'node_mappings' not in request_data:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "ç¼ºå°‘ node_mappings å‚æ•°"}
            )
        
        new_node_mappings = request_data['node_mappings']
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        if document_id not in document_status:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"}
            )
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸­çš„èŠ‚ç‚¹æ˜ å°„
        document_status[document_id]['node_mappings_demo'] = new_node_mappings
        
        print(f"ğŸ“ [API] âœ… æˆåŠŸæ›´æ–°æ–‡æ¡£ {document_id} çš„èŠ‚ç‚¹æ˜ å°„")
        print(f"ğŸ“ [API] æ›´æ–°åçš„æ˜ å°„é”®æ•°é‡: {len(new_node_mappings)}")
        
        # å¯é€‰ï¼šä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆè¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“ä¿å­˜é€»è¾‘ï¼‰
        # TODO: æ·»åŠ æ•°æ®åº“æŒä¹…åŒ–é€»è¾‘
        
        return JSONResponse(content={
            "success": True,
            "message": "èŠ‚ç‚¹æ˜ å°„æ›´æ–°æˆåŠŸ",
            "document_id": document_id,
            "updated_mappings_count": len(new_node_mappings)
        })
        
    except Exception as e:
        print(f"âŒ [APIé”™è¯¯] æ›´æ–°èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ›´æ–°èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}"}
        )

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - åç«¯APIæœåŠ¡")
    print("=" * 80)
    print("ğŸ“ æœåŠ¡åœ°å€: http://localhost:8000")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ”§ æœåŠ¡æ¨¡å¼: å¼€å‘æ¨¡å¼ (æ”¯æŒçƒ­é‡è½½)")
    print("=" * 80)
    print("ğŸ“‹ æ§åˆ¶å°æ—¥å¿—è¯´æ˜:")
    print("   ğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] - æ–‡ä»¶ä¸Šä¼ ç›¸å…³ä¿¡æ¯")
    print("   ğŸ”„ [å¼€å§‹ç”Ÿæˆ] - æ€ç»´å¯¼å›¾ç”Ÿæˆä»»åŠ¡å¯åŠ¨")
    print("   ğŸš€ [å¼€å§‹ç”Ÿæˆ] - AIå¤„ç†å¼€å§‹")
    print("   ğŸ¤– [AIå¤„ç†] - è°ƒç”¨æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨")
    print("   âœ… [ç”Ÿæˆå®Œæˆ] - æ€ç»´å¯¼å›¾ç”ŸæˆæˆåŠŸ")
    print("   âŒ [ç”Ÿæˆå¤±è´¥] - ç”Ÿæˆè¿‡ç¨‹å‡ºç°é”™è¯¯")
    print("   â³ [çŠ¶æ€æŸ¥è¯¢] - å®¢æˆ·ç«¯æŸ¥è¯¢ç”ŸæˆçŠ¶æ€")
    print("=" * 80)
    print("ğŸ¯ æ–°åŠŸèƒ½: æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼")
    print("   ğŸ“Š æ ‡å‡†è¯¦ç»†æ¨¡å¼: 3-5åˆ†é’Ÿï¼Œè¯¦ç»†åˆ†æï¼Œé«˜è´¨é‡ç»“æœ")
    print("   âš¡ å¿«é€Ÿç®€åŒ–æ¨¡å¼: 1-2åˆ†é’Ÿï¼ŒåŸºç¡€ç»“æ„ï¼Œå¿«é€Ÿé¢„è§ˆ")
    print("   ğŸ“‹ APIç«¯ç‚¹: /api/generate-mindmap/{id} å’Œ /api/generate-mindmap-simple/{id}")
    print("=" * 80)
    print("ğŸš€ å¯åŠ¨æœåŠ¡ä¸­...")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info") 