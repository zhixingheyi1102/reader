from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import asyncio
import os
import hashlib
import tempfile
import re
from datetime import datetime
from pathlib import Path
import logging
import base64
import json

# å¯¼å…¥ç°æœ‰çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨
from mindmap_generator import MindMapGenerator, MinimalDatabaseStub, get_logger, generate_mermaid_html, DocumentOptimizer

# å¯¼å…¥æ–‡æ¡£è§£æå™¨
from document_parser import DocumentParser

# å¯¼å…¥MinerUç›¸å…³æ¨¡å—
from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.config.enums import SupportedPdfParseMethod

app = FastAPI(title="Argument Structure Analyzer API", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Reactå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®æ—¥å¿—
logger = get_logger()

# åˆ›å»ºä¸Šä¼ ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# åˆ›å»ºPDFå¤„ç†ç›®å½•
PDF_OUTPUT_DIR = Path("pdf_outputs")
PDF_OUTPUT_DIR.mkdir(exist_ok=True)

# å­˜å‚¨æ–‡æ¡£çŠ¶æ€çš„å†…å­˜æ•°æ®åº“
document_status = {}

# å­˜å‚¨æ–‡æ¡£ç»“æ„çš„å†…å­˜æ•°æ®åº“
document_structures = {}

# Pydantic æ¨¡å‹å®šä¹‰
class AddNodeRequest(BaseModel):
    """æ·»åŠ èŠ‚ç‚¹çš„è¯·æ±‚æ¨¡å‹"""
    sourceNodeId: str
    direction: str  # 'child', 'left-sibling', 'right-sibling'
    parentId: Optional[str] = None
    label: Optional[str] = "æ–°èŠ‚ç‚¹"

class ArgumentStructureAnalyzer:
    """è®ºè¯ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        self.generator = MindMapGenerator()
        self.document_parser = DocumentParser()
        # æ·»åŠ DocumentOptimizerå®ä¾‹ç”¨äºAIè°ƒç”¨
        self.optimizer = DocumentOptimizer()
    
    def add_paragraph_ids(self, text: str) -> str:
        """ä¸ºæ–‡æœ¬çš„æ¯ä¸ªæ®µè½æ·»åŠ IDå·"""
        try:
            # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
            paragraphs = text.split('\n\n')
            processed_paragraphs = []
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():  # åªå¤„ç†éç©ºæ®µè½
                    # ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ IDæ ‡è®°
                    para_id = f"para-{i+1}"
                    processed_paragraph = f"[{para_id}] {paragraph.strip()}"
                    processed_paragraphs.append(processed_paragraph)
                else:
                    processed_paragraphs.append(paragraph)
            
            return '\n\n'.join(processed_paragraphs)
            
        except Exception as e:
            print(f"âŒ [æ®µè½IDæ·»åŠ é”™è¯¯] {str(e)}")
            return text
    
    def split_text_into_chunks(self, text: str, document_id: str) -> List[Dict[str, Any]]:
        """å°†æ–‡æ¡£æŒ‰Markdownæ ‡é¢˜å±‚çº§åˆ†å—å¹¶åˆ†é…å”¯ä¸€æ ‡è¯†ç¬¦"""
        try:
            # ä½¿ç”¨æ–°çš„æ–‡æ¡£è§£æå™¨
            chunks = self.document_parser.parse_to_chunks(text, document_id)
            
            # åŒæ—¶ä¿å­˜æ–‡æ¡£ç»“æ„ç”¨äºç›®å½•ç”Ÿæˆ
            root = self.document_parser.parse_document(text, document_id)
            toc = self.document_parser.generate_toc(root)
            
            document_structures[document_id] = {
                'structure': root.to_dict(),
                'toc': toc,
                'chunks': chunks
            }
            
            print(f"ğŸ“„ [æ–‡æœ¬åˆ†å—] æ–‡æ¡£ {document_id} åˆ†ä¸º {len(chunks)} ä¸ªç»“æ„åŒ–å—")
            for i, chunk in enumerate(chunks[:3]):  # æ˜¾ç¤ºå‰3ä¸ªå—çš„ä¿¡æ¯
                print(f"   å— {i}: {chunk.get('title', 'æ— æ ‡é¢˜')} (çº§åˆ« {chunk.get('level', 0)})")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ [åˆ†å—é”™è¯¯] {str(e)}")
            return []
    
    async def generate_argument_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ææ–‡æ¡£çš„è®ºè¯ç»“æ„"""
        try:
            # æ„å»ºåŸºäºæ®µè½çš„è®ºè¯ç»“æ„åˆ†æprompt
            prompt = f"""æˆ‘å¸Œæœ›ä½ æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åˆ†æå¸ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»æˆ‘æä¾›çš„ã€å·²ç»æŒ‰æ®µè½æ ‡è®°å¥½IDçš„æ–‡æœ¬ï¼Œå¹¶åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†æ¥åˆ†æå…¶è®ºè¯ç»“æ„ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š

ç¬¬ä¸€æ­¥ï¼šæ®µè½è§’è‰²è¯†åˆ«
- åŸºäºç°æœ‰çš„æ®µè½åˆ’åˆ†ï¼ˆ[para-X]æ ‡è®°ï¼‰ï¼Œåˆ†ææ¯ä¸ªæ®µè½åœ¨è®ºè¯ä¸­çš„è§’è‰²
- ä¸è¦é‡æ–°åˆ’åˆ†æ®µè½ï¼Œè€Œæ˜¯åŸºäºç°æœ‰æ®µè½æ¥ç†è§£è®ºè¯é€»è¾‘
- è¯†åˆ«æ¯ä¸ªæ®µè½æ˜¯å¼•è¨€ã€è®ºç‚¹ã€è¯æ®ã€åé©³ã€ç»“è®ºç­‰å“ªç§ç±»å‹

ç¬¬äºŒæ­¥ï¼šæ„å»ºè®ºè¯ç»“æ„æµç¨‹å›¾
- åŸºäºæ®µè½çš„è®ºè¯è§’è‰²ï¼Œæ„å»ºé€»è¾‘æµç¨‹å›¾
- å°†å…·æœ‰ç›¸åŒæˆ–ç›¸å…³è®ºè¯åŠŸèƒ½çš„æ®µè½ç»„åˆæˆé€»è¾‘èŠ‚ç‚¹
- ç”¨ç®­å¤´è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»

ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€çš„ã€å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œä¸è¦åœ¨ JSON ä»£ç å—å‰åæ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡å­—ã€‚

è¿™ä¸ª JSON å¯¹è±¡å¿…é¡»åŒ…å«ä¸‰ä¸ªé¡¶çº§é”®ï¼š"mermaid_string"ã€"node_mappings" å’Œ "edges"ã€‚

mermaid_string:
- å€¼ä¸ºç¬¦åˆ Mermaid.js è¯­æ³•çš„æµç¨‹å›¾ï¼ˆgraph TDï¼‰
- å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ç»„ç›¸å…³çš„æ®µè½ï¼ˆåŸºäºè®ºè¯åŠŸèƒ½ï¼‰
- èŠ‚ç‚¹ ID ä½¿ç”¨ç®€çŸ­çš„å­—æ¯æˆ–å­—æ¯æ•°å­—ç»„åˆï¼ˆå¦‚ï¼šA, B, C1, D2ï¼‰
- èŠ‚ç‚¹æ ‡ç­¾åº”è¯¥ç®€æ´æ¦‚æ‹¬è¯¥ç»„æ®µè½çš„æ ¸å¿ƒè®ºè¯åŠŸèƒ½ï¼ˆä¸è¶…è¿‡20å­—ï¼‰
- ä½¿ç”¨ç®­å¤´ --> è¡¨ç¤ºè®ºè¯çš„é€»è¾‘æµå‘å’Œä¾èµ–å…³ç³»
- å¯ä»¥ä½¿ç”¨ä¸åŒçš„èŠ‚ç‚¹å½¢çŠ¶æ¥åŒºåˆ†ä¸åŒç±»å‹çš„è®ºè¯åŠŸèƒ½ï¼š
  - [æ–¹æ‹¬å·] ç”¨äºä¸»è¦è®ºç‚¹
  - (åœ†æ‹¬å·) ç”¨äºæ”¯æ’‘è¯æ®
  - {{èŠ±æ‹¬å·}} ç”¨äºé€»è¾‘è½¬æŠ˜æˆ–å…³é”®åˆ¤æ–­

node_mappings:
- å€¼ä¸º JSON å¯¹è±¡ï¼Œé”®ä¸º Mermaid å›¾ä¸­çš„èŠ‚ç‚¹ ID
- æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å€¼åŒ…å«ï¼š
  - "text_snippet": è¯¥èŠ‚ç‚¹åŒ…å«æ®µè½çš„æ ¸å¿ƒå†…å®¹æ€»ç»“ï¼ˆ30-80å­—ï¼‰
  - "paragraph_ids": æ„æˆè¯¥èŠ‚ç‚¹çš„æ®µè½IDæ•°ç»„ï¼ˆå¦‚ ["para-2", "para-3"]ï¼‰
  - "semantic_role": è¯¥èŠ‚ç‚¹åœ¨è®ºè¯ä¸­çš„è§’è‰²ï¼ˆå¦‚ "å¼•è¨€"ã€"æ ¸å¿ƒè®ºç‚¹"ã€"æ”¯æ’‘è¯æ®"ã€"åé©³"ã€"ç»“è®º" ç­‰ï¼‰

edges:
- å€¼ä¸ºå¯¹è±¡æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡ä»£è¡¨ä¸€æ¡è¾¹
- æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä¸¤ä¸ªé”®ï¼š
  - "source": è¾¹çš„èµ·å§‹èŠ‚ç‚¹ID
  - "target": è¾¹çš„ç›®æ ‡èŠ‚ç‚¹ID
- è¿™äº›è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»ä¸€è‡´

å…³é”®è¦æ±‚ï¼š
1. æ‰€æœ‰èŠ‚ç‚¹ ID å¿…é¡»åœ¨ mermaid_string ä¸­å­˜åœ¨
2. paragraph_ids å¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŸæ–‡çš„æ®µè½æ ‡è®° [para-X]ï¼Œä¸å¯ä¿®æ”¹
3. åŸæ–‡çš„æ¯ä¸ªæ®µè½éƒ½åº”è¯¥è¢«åˆ†é…ç»™è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹
4. èŠ‚ç‚¹çš„åˆ’åˆ†åº”è¯¥åŸºäºæ®µè½çš„è®ºè¯åŠŸèƒ½ï¼Œç›¸å…³åŠŸèƒ½çš„æ®µè½å¯ä»¥ç»„åˆåœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸­
5. æµç¨‹å›¾åº”è¯¥æ¸…æ™°å±•ç°è®ºè¯çš„é€»è¾‘æ¨ç†è·¯å¾„
6. ä¿æŒæ®µè½çš„å®Œæ•´æ€§ï¼Œä¸è¦æ‹†åˆ†æˆ–é‡ç»„æ®µè½å†…å®¹
7. edges æ•°ç»„ä¸­çš„æ¯æ¡è¾¹å¿…é¡»ä¸ mermaid_string ä¸­çš„è¿æ¥å…³ç³»å®Œå…¨ä¸€è‡´

ç°åœ¨ï¼Œè¯·åˆ†æä»¥ä¸‹å¸¦æœ‰æ®µè½IDçš„æ–‡æœ¬ï¼š

{text_with_ids}"""
            
            # ä½¿ç”¨DocumentOptimizerçš„generate_completionæ–¹æ³•
            response = await self.optimizer.generate_completion(
                prompt, 
                max_tokens=2000,
                task="åˆ†æè®ºè¯ç»“æ„"
            )
            
            if not response:
                print(f"âŒ [APIè°ƒç”¨å¤±è´¥] æœªæ”¶åˆ°AIå“åº”")
                return {"success": False, "error": "APIè°ƒç”¨å¤±è´¥ï¼Œæœªæ”¶åˆ°AIå“åº”"}
            
            # ä¿å­˜APIåŸå§‹å“åº”åˆ°æ–‡ä»¶
            try:
                from datetime import datetime
                import os
                
                # åˆ›å»ºapi_responsesæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                api_responses_dir = "api_responses"
                os.makedirs(api_responses_dir, exist_ok=True)
                
                # ç”Ÿæˆæ–‡ä»¶åï¼šæ—¶é—´æˆ³_è®ºè¯ç»“æ„åˆ†æ
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                response_filename = f"{timestamp}_argument_structure_analysis.txt"
                response_filepath = os.path.join(api_responses_dir, response_filename)
                
                # ä¿å­˜åŸå§‹å“åº”å’Œç›¸å…³ä¿¡æ¯
                with open(response_filepath, 'w', encoding='utf-8') as f:
                    f.write("=== APIè°ƒç”¨ä¿¡æ¯ ===\n")
                    f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ä»»åŠ¡: è®ºè¯ç»“æ„åˆ†æ\n")
                    f.write(f"æœ€å¤§tokens: 2000\n")
                    f.write(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦\n")
                    f.write(f"æ–‡æœ¬é•¿åº¦: {len(text_with_ids)} å­—ç¬¦\n")
                    f.write("\n=== å‘é€çš„Prompt ===\n")
                    f.write(prompt)
                    f.write("\n\n=== AIåŸå§‹å“åº” ===\n")
                    f.write(response)
                    f.write(f"\n\n=== å“åº”ç»“æŸ ===\n")
                
                print(f"ğŸ’¾ [APIå“åº”ä¿å­˜] å·²ä¿å­˜åˆ°: {response_filepath}")
                
            except Exception as save_error:
                print(f"âš ï¸ [å“åº”ä¿å­˜å¤±è´¥] {str(save_error)}")
            
            # è§£æJSONå“åº”
            try:
                # è¯¦ç»†è®°å½•åŸå§‹å“åº”
                print(f"ğŸ” [åŸå§‹AIå“åº”] é•¿åº¦: {len(response)} å­—ç¬¦")
                print(f"ğŸ” [åŸå§‹å“åº”å‰200å­—ç¬¦]: {response[:200]}")
                
                # æ›´å½»åº•çš„å“åº”æ¸…ç†
                clean_response = response.strip()
                
                # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                elif clean_response.startswith('```'):
                    clean_response = clean_response[3:]
                    
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                
                clean_response = clean_response.strip()
                
                # ç§»é™¤å¯èƒ½çš„è¯´æ˜æ–‡å­—ï¼Œåªä¿ç•™JSONéƒ¨åˆ†
                json_start = clean_response.find('{')
                json_end = clean_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    clean_response = clean_response[json_start:json_end+1]
                    print(f"ğŸ”§ [æå–JSON] æå–åˆ°JSONéƒ¨åˆ†ï¼Œé•¿åº¦: {len(clean_response)}")
                else:
                    print(f"âš ï¸ [JSONæå–å¤±è´¥] æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONç»“æ„")
                
                print(f"ğŸ” [æ¸…ç†åå“åº”å‰200å­—ç¬¦]: {clean_response[:200]}")
                
                structure_data = json.loads(clean_response)
                
                # éªŒè¯å¿…è¦çš„é”®
                if 'mermaid_string' not in structure_data or 'node_mappings' not in structure_data:
                    print(f"âŒ [æ•°æ®ç»“æ„é”™è¯¯] å“åº”é”®: {list(structure_data.keys())}")
                    return {"success": False, "error": "AIå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼šç¼ºå°‘å¿…è¦çš„é”®"}
                
                # éªŒè¯èŠ‚ç‚¹æ˜ å°„çš„ç»“æ„
                node_mappings = structure_data['node_mappings']
                valid_mappings = {}
                
                for node_id, mapping in node_mappings.items():
                    if isinstance(mapping, dict):
                        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨ï¼Œå¦‚æœç¼ºå°‘semantic_roleå°±æ·»åŠ é»˜è®¤å€¼
                        valid_mapping = {
                            "text_snippet": mapping.get("text_snippet", "è¯­ä¹‰å—å†…å®¹"),
                            "paragraph_ids": mapping.get("paragraph_ids", []),
                            "semantic_role": mapping.get("semantic_role", "è®ºè¯è¦ç´ ")
                        }
                        valid_mappings[node_id] = valid_mapping
                    else:
                        print(f"âš ï¸ [æ˜ å°„æ ¼å¼é”™è¯¯] èŠ‚ç‚¹ {node_id} çš„æ˜ å°„ä¸æ˜¯å­—å…¸æ ¼å¼")
                
                structure_data['node_mappings'] = valid_mappings
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«edgeså­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä»mermaid_stringä¸­æå–
                if 'edges' not in structure_data:
                    print("âš ï¸ [æ•°æ®ç»“æ„è­¦å‘Š] å“åº”ä¸­æ²¡æœ‰edgeså­—æ®µï¼Œå°†ä»mermaid_stringä¸­æå–")
                    # ä»mermaid_stringä¸­æå–è¾¹å…³ç³»
                    edges = []
                    mermaid_string = structure_data['mermaid_string']
                    # åŒ¹é…å½¢å¦‚ "A --> B" çš„è¾¹å®šä¹‰
                    edge_pattern = r'([A-Za-z0-9_]+)\s*-->\s*([A-Za-z0-9_]+)'
                    for match in re.finditer(edge_pattern, mermaid_string):
                        source, target = match.groups()
                        edges.append({"source": source, "target": target})
                    structure_data['edges'] = edges
                    print(f"ğŸ”§ [è‡ªåŠ¨æå–] ä»mermaid_stringä¸­æå–äº† {len(edges)} æ¡è¾¹")
                
                print(f"âœ… [è®ºè¯ç»“æ„åˆ†æ] æˆåŠŸç”ŸæˆåŒ…å« {len(structure_data['node_mappings'])} ä¸ªèŠ‚ç‚¹çš„æµç¨‹å›¾")
                
                # è¿”å›æˆåŠŸç»“æœ
                return {
                    "success": True,
                    "mermaid_code": structure_data['mermaid_string'],
                    "node_mappings": structure_data['node_mappings'],
                    "edges": structure_data['edges']
                }
                
            except json.JSONDecodeError as parse_error:
                print(f"âŒ [JSONè§£æé”™è¯¯] {str(parse_error)}")
                print(f"âŒ [å®Œæ•´åŸå§‹å“åº”]: {response}")
                print(f"âŒ [æ¸…ç†åå“åº”]: {clean_response}")
                return {"success": False, "error": f"JSONè§£æå¤±è´¥: {str(parse_error)}"}
                
        except Exception as e:
            print(f"âŒ [è®ºè¯ç»“æ„åˆ†æé”™è¯¯] {str(e)}")
            # æä¾›é™çº§ç­–ç•¥ - ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„
            try:
                fallback_structure = self.generate_fallback_structure(text_with_ids)
                print(f"ğŸ”„ [é™çº§ç­–ç•¥] ä½¿ç”¨åŸºæœ¬è®ºè¯ç»“æ„ï¼ŒåŒ…å« {len(fallback_structure['node_mappings'])} ä¸ªèŠ‚ç‚¹")
                return fallback_structure
            except Exception as fallback_error:
                print(f"âŒ [é™çº§ç­–ç•¥å¤±è´¥] {str(fallback_error)}")
                return {"success": False, "error": f"AIåˆ†æå¤±è´¥ä¸”é™çº§ç­–ç•¥ä¹Ÿå¤±è´¥: {str(e)}"}

    def generate_fallback_structure(self, text_with_ids: str) -> Dict[str, Any]:
        """ç”ŸæˆåŸºæœ¬çš„è®ºè¯ç»“æ„ä½œä¸ºé™çº§ç­–ç•¥"""
        import re
        
        # æå–æ‰€æœ‰æ®µè½ID
        para_ids = re.findall(r'\[para-(\d+)\]', text_with_ids)
        
        if not para_ids:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½IDï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬ç»“æ„
            return {
                "success": True,
                "mermaid_code": "graph TD\n    A[æ–‡æ¡£åˆ†æ] --> B[ä¸»è¦å†…å®¹]\n    B --> C[æ€»ç»“]",
                "node_mappings": {
                    "A": {
                        "text_snippet": "æ–‡æ¡£å¼€å§‹",
                        "paragraph_ids": ["para-1"],
                        "semantic_role": "å¼•è¨€"
                    },
                    "B": {
                        "text_snippet": "ä¸»è¦å†…å®¹",
                        "paragraph_ids": ["para-2"],
                        "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                    },
                    "C": {
                        "text_snippet": "æ–‡æ¡£ç»“è®º",
                        "paragraph_ids": ["para-3"],
                        "semantic_role": "ç»“è®º"
                    }
                }
            }
        
        # åŸºäºæ®µè½æ•°é‡ç”Ÿæˆç»“æ„
        total_paras = len(para_ids)
        
        if total_paras <= 3:
            # ç®€å•çº¿æ€§ç»“æ„
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[ä¸»ä½“]\n"
            mermaid_code += "    B --> C[ç»“è®º]"
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€éƒ¨åˆ†",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "æ–‡æ¡£ä¸»ä½“å†…å®¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:-1]] if total_paras > 2 else [f"para-{para_ids[1]}"] if total_paras > 1 else [],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"] if total_paras > 1 else [],
                    "semantic_role": "ç»“è®º"
                }
            }
        else:
            # å¤æ‚ç»“æ„ï¼šå¼•è¨€ -> å¤šä¸ªè®ºç‚¹ -> ç»“è®º
            mermaid_code = "graph TD\n"
            mermaid_code += "    A[å¼•è¨€] --> B[è®ºç‚¹1]\n"
            mermaid_code += "    A --> C[è®ºç‚¹2]\n"
            if total_paras > 5:
                mermaid_code += "    A --> D[è®ºç‚¹3]\n"
                mermaid_code += "    B --> E[ç»“è®º]\n"
                mermaid_code += "    C --> E\n"
                mermaid_code += "    D --> E"
            else:
                mermaid_code += "    B --> D[ç»“è®º]\n"
                mermaid_code += "    C --> D"
            
            # å°†æ®µè½åˆ†é…ç»™ä¸åŒèŠ‚ç‚¹
            para_per_section = max(1, total_paras // 4)
            
            node_mappings = {
                "A": {
                    "text_snippet": "æ–‡æ¡£å¼•è¨€",
                    "paragraph_ids": [f"para-{para_ids[0]}"],
                    "semantic_role": "å¼•è¨€"
                },
                "B": {
                    "text_snippet": "ç¬¬ä¸€ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1:1+para_per_section]],
                    "semantic_role": "æ ¸å¿ƒè®ºç‚¹"
                },
                "C": {
                    "text_snippet": "ç¬¬äºŒä¸ªè®ºç‚¹", 
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+para_per_section:1+2*para_per_section]],
                    "semantic_role": "æ”¯æ’‘è¯æ®"
                }
            }
            
            if total_paras > 5:
                node_mappings["D"] = {
                    "text_snippet": "ç¬¬ä¸‰ä¸ªè®ºç‚¹",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:-1]],
                    "semantic_role": "è¡¥å……è®ºè¯"
                }
                node_mappings["E"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{para_ids[-1]}"],
                    "semantic_role": "ç»“è®º"
                }
            else:
                node_mappings["D"] = {
                    "text_snippet": "æ–‡æ¡£ç»“è®º",
                    "paragraph_ids": [f"para-{pid}" for pid in para_ids[1+2*para_per_section:]],
                    "semantic_role": "ç»“è®º"
                }
        
        return {
            "success": True,
            "mermaid_code": mermaid_code,
            "node_mappings": node_mappings
        }

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
argument_analyzer = ArgumentStructureAnalyzer()

async def process_pdf_to_markdown(pdf_file_path: str, document_id: str) -> str:
    """
    ä½¿ç”¨MinerUå¤„ç†PDFæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        pdf_file_path: PDFæ–‡ä»¶è·¯å¾„
        document_id: æ–‡æ¡£ID
        
    Returns:
        è½¬æ¢åçš„Markdownå†…å®¹
    """
    try:
        print(f"\nğŸ“„ [MinerU-PDFå¤„ç†] å¼€å§‹å¤„ç†PDFæ–‡ä»¶")
        print(f"    ğŸ“ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = PDF_OUTPUT_DIR / document_id
        image_dir = output_dir / "images"
        os.makedirs(image_dir, exist_ok=True)
        
        print(f"ğŸ“ [MinerU-ç›®å½•] åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ–¼ï¸  [MinerU-å›¾ç‰‡] å›¾ç‰‡ç›®å½•: {image_dir}")
        
        # åˆ›å»ºæ•°æ®è¯»å†™å™¨
        print("ğŸ”§ [MinerU-åˆå§‹åŒ–] åˆ›å»ºæ•°æ®è¯»å†™å™¨...")
        reader = FileBasedDataReader("")
        image_writer = FileBasedDataWriter(str(image_dir))
        md_writer = FileBasedDataWriter(str(output_dir))
        
        # è¯»å–PDFæ–‡ä»¶
        print("ğŸ“– [MinerU-è¯»å–] æ­£åœ¨è¯»å–PDFæ–‡ä»¶...")
        pdf_bytes = reader.read(pdf_file_path)
        print(f"ğŸ“Š [MinerU-æ•°æ®] PDFæ–‡ä»¶å¤§å°: {len(pdf_bytes)} å­—èŠ‚")
        
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        print("ğŸ—ï¸  [MinerU-æ•°æ®é›†] åˆ›å»ºPymuDocDatasetå®ä¾‹...")
        ds = PymuDocDataset(pdf_bytes)
        
        # åˆ†ç±»å¤„ç†æ¨¡å¼
        print("ğŸ” [MinerU-æ£€æµ‹] æ£€æµ‹PDFå¤„ç†æ¨¡å¼...")
        pdf_mode = ds.classify()
        
        # è¿›è¡Œæ¨ç†
        if pdf_mode == SupportedPdfParseMethod.OCR:
            print(f"ğŸ”¤ [MinerU-OCRæ¨¡å¼] æ£€æµ‹åˆ°éœ€è¦OCRå¤„ç†ï¼Œå¼€å§‹æ–‡å­—è¯†åˆ«...")
            print("    ğŸ“¸ æ­£åœ¨æå–å›¾ç‰‡ä¸­çš„æ–‡å­—...")
            print("    ğŸ§  è°ƒç”¨OCRå¼•æ“è¿›è¡Œæ–‡å­—è¯†åˆ«...")
            infer_result = ds.apply(doc_analyze, ocr=True)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨OCRæ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_ocr_mode(image_writer)
            print("âœ… [MinerU-OCR] OCRå¤„ç†å®Œæˆ")
        else:
            print(f"ğŸ“ [MinerU-æ–‡æœ¬æ¨¡å¼] æ£€æµ‹åˆ°å¯ç›´æ¥æå–æ–‡æœ¬ï¼Œå¼€å§‹æ–‡æœ¬å¤„ç†...")
            print("    ğŸ“„ æ­£åœ¨æå–PDFä¸­çš„æ–‡æœ¬å†…å®¹...")
            print("    ğŸ”§ åˆ†ææ–‡æ¡£ç»“æ„å’Œç‰ˆé¢...")
            infer_result = ds.apply(doc_analyze, ocr=False)
            
            print("âš¡ [MinerU-ç®¡é“] ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ç®¡é“å¤„ç†...")
            pipe_result = infer_result.pipe_txt_mode(image_writer)
            print("âœ… [MinerU-æ–‡æœ¬] æ–‡æœ¬æå–å®Œæˆ")
        
        print("ğŸ“‹ [MinerU-è½¬æ¢] æ­£åœ¨ç”ŸæˆMarkdownæ ¼å¼...")
        # è·å–Markdownå†…å®¹
        markdown_content = pipe_result.get_markdown("images")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        md_file_path = output_dir / f"{document_id}.md"
        print(f"ğŸ’¾ [MinerU-ä¿å­˜] ä¿å­˜Markdownæ–‡ä»¶: {md_file_path}")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # ç»Ÿè®¡ä¿¡æ¯
        lines_count = len(markdown_content.split('\n'))
        words_count = len(markdown_content.split())
        
        print("=" * 60)
        print("âœ… [MinerU-å®Œæˆ] PDFè½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"    ğŸ“Š ç”Ÿæˆå†…å®¹ç»Ÿè®¡:")
        print(f"       â€¢ Markdownæ€»é•¿åº¦: {len(markdown_content):,} å­—ç¬¦")
        print(f"       â€¢ æ€»è¡Œæ•°: {lines_count:,} è¡Œ")
        print(f"       â€¢ å•è¯æ•°: {words_count:,} ä¸ª")
        print(f"    ğŸ“ è¾“å‡ºæ–‡ä»¶: {md_file_path}")
        print("=" * 60)
        
        return markdown_content
        
    except Exception as e:
        print("=" * 60)
        print(f"âŒ [MinerU-é”™è¯¯] PDFå¤„ç†å¤±è´¥ï¼")
        print(f"    ğŸš¨ é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"    ğŸ“„ æ–‡ä»¶è·¯å¾„: {pdf_file_path}")
        print(f"    ğŸ†” æ–‡æ¡£ID: {document_id}")
        print("=" * 60)
        logger.error(f"MinerU PDF processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"PDFå¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£ï¼Œæ”¯æŒPDFã€MDå’ŒTXTæ–‡ä»¶"""
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = ['.md', '.txt', '.pdf']
    file_extension = Path(file.filename).suffix.lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .mdã€.txt å’Œ .pdf æ–‡ä»¶")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£ID
        content_hash = hashlib.md5(content).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = Path(file.filename).stem
        document_id = f"{base_filename}_{content_hash}_{timestamp}"
        
        print(f"\nğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] {file.filename}")
        print(f"ğŸ†” [æ–‡æ¡£ID] {document_id}")
        print(f"ğŸ“Š [æ–‡ä»¶å¤§å°] {len(content)} å­—èŠ‚")
        print(f"ğŸ“‹ [æ–‡ä»¶ç±»å‹] {file_extension}")
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶
        original_file_path = UPLOAD_DIR / f"{document_id}{file_extension}"
        with open(original_file_path, 'wb') as f:
            f.write(content)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
        if file_extension == '.pdf':
            # å¤„ç†PDFæ–‡ä»¶
            print(f"ğŸ”„ [PDFå¤„ç†] å¼€å§‹è½¬æ¢PDFä¸ºMarkdown...")
            markdown_content = await process_pdf_to_markdown(str(original_file_path), document_id)
            text_content = markdown_content
            
            # å°†åŸå§‹PDFæ–‡ä»¶ç¼–ç ä¸ºbase64ç”¨äºå‰ç«¯æ˜¾ç¤º
            pdf_base64 = base64.b64encode(content).decode('utf-8')
            
        else:
            # å¤„ç†æ–‡æœ¬æ–‡ä»¶
            text_content = content.decode('utf-8')
            pdf_base64 = None
        
        # å­˜å‚¨åˆ°å†…å­˜æ•°æ®åº“
        MinimalDatabaseStub.store_text(text_content)
        
        # ç«‹å³ä¸ºæ–‡æ¡£å†…å®¹æ·»åŠ æ®µè½IDï¼Œæ— éœ€ç­‰å¾…ç”Ÿæˆè®ºè¯ç»“æ„
        print("ğŸ“ [å¤„ç†æ®µè½] ä¸ºä¸Šä¼ çš„æ–‡æ¡£æ·»åŠ æ®µè½IDæ ‡è®°...")
        content_with_ids = argument_analyzer.add_paragraph_ids(text_content)
        print(f"ğŸ“ [æ®µè½å¤„ç†å®Œæˆ] å·²ä¸ºæ–‡æ¡£æ·»åŠ æ®µè½IDï¼Œå†…å®¹é•¿åº¦: {len(content_with_ids)} å­—ç¬¦")
        
        # åˆå§‹åŒ–æ–‡æ¡£çŠ¶æ€
        document_status[document_id] = {
            "status": "uploaded",
            "content": text_content,
            "filename": file.filename,
            "file_type": file_extension,
            "original_file_path": str(original_file_path),
            "pdf_base64": pdf_base64,  # ä»…PDFæ–‡ä»¶æœ‰æ­¤å­—æ®µ
            "status_demo": "not_started",
            "mermaid_code_demo": None,
            "node_mappings_demo": {},
            "error_demo": None,
            "content_with_ids": content_with_ids  # ç«‹å³è®¾ç½®å¸¦æ®µè½IDçš„å†…å®¹
        }
        
        print(f"âœ… [ä¸Šä¼ æˆåŠŸ] æ–‡æ¡£å·²ä¿å­˜å¹¶å‡†å¤‡ç”Ÿæˆæ€ç»´å¯¼å›¾")
        print("=" * 60)
        
        logger.info(f"Document uploaded: {document_id}")
        
        # è¿”å›æ–‡æ¡£ä¿¡æ¯
        response_data = {
            "success": True,
            "document_id": document_id,
            "filename": file.filename,
            "content": text_content,
            "file_type": file_extension,
            "status": "uploaded",
            "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ"
        }
        
        # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œè¿”å›base64ç¼–ç çš„åŸå§‹PDF
        if file_extension == '.pdf':
            response_data["pdf_base64"] = pdf_base64
            response_data["message"] = "PDFæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå·²è½¬æ¢ä¸ºMarkdown"
        
        return JSONResponse(response_data)
        
    except UnicodeDecodeError:
        print(f"âŒ [ç¼–ç é”™è¯¯] æ–‡ä»¶: {file.filename}")
        raise HTTPException(status_code=400, detail="æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯UTF-8ç¼–ç ")
    except Exception as e:
        print(f"âŒ [ä¸Šä¼ å¤±è´¥] æ–‡ä»¶: {file.filename}, é”™è¯¯: {str(e)}")
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/document-pdf/{document_id}")
async def get_document_pdf(document_id: str):
    """è·å–åŸå§‹PDFæ–‡ä»¶"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    if doc_info.get("file_type") != ".pdf":
        raise HTTPException(status_code=400, detail="è¯¥æ–‡æ¡£ä¸æ˜¯PDFæ–‡ä»¶")
    
    original_file_path = doc_info.get("original_file_path")
    if not original_file_path or not os.path.exists(original_file_path):
        raise HTTPException(status_code=404, detail="åŸå§‹PDFæ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=original_file_path,
        media_type='application/pdf',
        filename=doc_info["filename"]
    )

@app.post("/api/generate-argument-structure/{document_id}")
async def generate_argument_structure(document_id: str):
    """ä¸ºæŒ‡å®šæ–‡æ¡£ç”Ÿæˆè®ºè¯ç»“æ„æµç¨‹å›¾"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    # æ£€æŸ¥çŠ¶æ€
    if doc_info.get("status_demo") == "generating":
        print(f"â³ [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­...")
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "è®ºè¯ç»“æ„æ­£åœ¨åˆ†æä¸­..."
        })
    
    if doc_info.get("status_demo") == "completed" and doc_info.get("mermaid_code_demo"):
        print(f"âœ… [çŠ¶æ€æŸ¥è¯¢] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„å·²åˆ†æå®Œæˆ")
        return JSONResponse({
            "success": True,
            "status": "completed",
            "mermaid_code": doc_info["mermaid_code_demo"],
            "node_mappings": doc_info.get("node_mappings_demo", {}),
            "message": "è®ºè¯ç»“æ„å·²ç”Ÿæˆ"
        })
    
    try:
        print(f"ğŸ”„ [å¼€å§‹åˆ†æ] ä¸ºæ–‡æ¡£ {document_id} å¯åŠ¨è®ºè¯ç»“æ„åˆ†æä»»åŠ¡")
        
        # æ›´æ–°çŠ¶æ€ä¸ºåˆ†æä¸­
        doc_info["status_demo"] = "generating"
        
        # å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„
        asyncio.create_task(generate_argument_structure_async(document_id, doc_info["content"]))
        
        return JSONResponse({
            "success": True,
            "status": "generating",
            "message": "å¼€å§‹åˆ†æè®ºè¯ç»“æ„..."
        })
        
    except Exception as e:
        print(f"âŒ [å¯åŠ¨å¤±è´¥] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}")
        logger.error(f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        doc_info["status_demo"] = "error"
        doc_info["error_demo"] = str(e)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")

async def generate_argument_structure_async(document_id: str, content: str):
    """å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„"""
    try:
        print(f"ğŸ”„ [å¼‚æ­¥ä»»åŠ¡] å¼€å§‹ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆè®ºè¯ç»“æ„")
        argument_analyzer = ArgumentStructureAnalyzer()
        
        # ä¸ºæ–‡æœ¬æ·»åŠ æ®µè½ID
        text_with_ids = argument_analyzer.add_paragraph_ids(content)
        
        # ç”Ÿæˆè®ºè¯ç»“æ„
        result = await argument_analyzer.generate_argument_structure(text_with_ids)
        
        if result["success"]:
            # ğŸ†• ä½¿ç”¨AIè¿”å›çš„node_mappingsé‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹
            rebuilt_content = rebuild_content_with_physical_dividers(text_with_ids, result["node_mappings"])
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document_status[document_id]["status_demo"] = "completed"
            document_status[document_id]["mermaid_code_demo"] = result["mermaid_code"]
            document_status[document_id]["node_mappings_demo"] = result["node_mappings"]
            document_status[document_id]["edges_demo"] = result["edges"]  # ä¿å­˜edgesæ•°æ®
            document_status[document_id]["content_with_ids"] = rebuilt_content  # ğŸ†• ä½¿ç”¨é‡å»ºçš„å†…å®¹
            
            print(f"âœ… [åˆ†æå®Œæˆ] æ–‡æ¡£ {document_id} è®ºè¯ç»“æ„åˆ†ææˆåŠŸ")
            print(f"ğŸ“Š [ç”Ÿæˆç»“æœ] åŒ…å« {len(result['node_mappings'])} ä¸ªè®ºè¯èŠ‚ç‚¹å’Œ {len(result['edges'])} æ¡è¾¹")
            print(f"ğŸ”§ [å†…å®¹é‡å»º] å·²é‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹ï¼Œé•¿åº¦: {len(rebuilt_content)} å­—ç¬¦")
        else:
            # åˆ†æå¤±è´¥
            document_status[document_id]["status_demo"] = "error"
            document_status[document_id]["error_demo"] = result["error"]
            print(f"âŒ [åˆ†æå¤±è´¥] æ–‡æ¡£ {document_id}: {result['error']}")
            
    except Exception as e:
        print(f"âŒ [å¼‚æ­¥åˆ†æé”™è¯¯] æ–‡æ¡£ {document_id}: {str(e)}")
        logger.error(f"å¼‚æ­¥ç”Ÿæˆè®ºè¯ç»“æ„æ—¶å‡ºé”™: {str(e)}")
        document_status[document_id]["status_demo"] = "error"
        document_status[document_id]["error_demo"] = str(e)

def rebuild_content_with_physical_dividers(text_with_ids: str, node_mappings: Dict) -> str:
    """
    æ ¹æ®AIè¿”å›çš„node_mappingsé‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹
    
    Args:
        text_with_ids: åŒ…å«æ®µè½IDæ ‡è®°çš„åŸå§‹æ–‡æœ¬
        node_mappings: AIè¿”å›çš„èŠ‚ç‚¹æ˜ å°„ï¼ŒåŒ…å«paragraph_ids
        
    Returns:
        é‡å»ºçš„åŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹å­—ç¬¦ä¸²
    """
    try:
        print(f"ğŸ”§ [å†…å®¹é‡å»º] å¼€å§‹é‡å»ºåŒ…å«ç‰©ç†åˆ†å‰²æ çš„å†…å®¹")
        print(f"ğŸ”§ [å†…å®¹é‡å»º] è¾“å…¥å†…å®¹é•¿åº¦: {len(text_with_ids)} å­—ç¬¦")
        print(f"ğŸ”§ [å†…å®¹é‡å»º] èŠ‚ç‚¹æ•°é‡: {len(node_mappings)}")
        
        # ç¬¬ä¸€æ­¥ï¼šè§£æåŸå§‹å†…å®¹ï¼Œæå–æ®µè½IDå’Œå¯¹åº”çš„å†…å®¹
        paragraph_content_map = {}
        
        # æŒ‰æ®µè½åˆ†å‰²å†…å®¹ï¼Œä¿ç•™æ®µè½IDæ ‡è®°
        parts = re.split(r'(\[para-\d+\])', text_with_ids)
        current_paragraph_id = None
        current_content = ''
        
        for part in parts:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ®µè½IDæ ‡è®°
            para_match = re.match(r'\[para-(\d+)\]', part.strip())
            if para_match:
                # ä¿å­˜ä¹‹å‰çš„æ®µè½å†…å®¹
                if current_paragraph_id and current_content.strip():
                    paragraph_content_map[current_paragraph_id] = current_content.strip()
                
                # è®¾ç½®æ–°çš„æ®µè½ID
                current_paragraph_id = f"para-{para_match.group(1)}"
                current_content = ''
                print(f"ğŸ”§ [å†…å®¹é‡å»º] å‘ç°æ®µè½: {current_paragraph_id}")
            else:
                # ç´¯ç§¯å†…å®¹
                if part.strip():  # åªæ·»åŠ éç©ºå†…å®¹
                    current_content += part
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_paragraph_id and current_content.strip():
            paragraph_content_map[current_paragraph_id] = current_content.strip()
        
        print(f"ğŸ”§ [å†…å®¹é‡å»º] è§£æå‡º {len(paragraph_content_map)} ä¸ªæ®µè½")
        
        # ç¬¬äºŒæ­¥ï¼šæŒ‰ç…§node_mappingsé‡æ–°ç»„ç»‡å†…å®¹
        rebuilt_content_parts = []
        
        # éå†æ‰€æœ‰èŠ‚ç‚¹ï¼ŒæŒ‰ç…§å®ƒä»¬åœ¨node_mappingsä¸­çš„é¡ºåº
        for node_id, node_data in node_mappings.items():
            # æ·»åŠ ç‰©ç†åˆ†å‰²æ 
            rebuilt_content_parts.append(f"--- {node_id} ---\n")
            print(f"ğŸ”§ [å†…å®¹é‡å»º] å¤„ç†èŠ‚ç‚¹: {node_id}")
            
            # è·å–è¯¥èŠ‚ç‚¹åŒ…å«çš„æ®µè½IDåˆ—è¡¨
            paragraph_ids = node_data.get('paragraph_ids', [])
            print(f"ğŸ”§ [å†…å®¹é‡å»º] èŠ‚ç‚¹ {node_id} åŒ…å«æ®µè½: {paragraph_ids}")
            
            # æ·»åŠ è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰æ®µè½å†…å®¹
            node_content_parts = []
            for para_id in paragraph_ids:
                if para_id in paragraph_content_map:
                    para_content = paragraph_content_map[para_id]
                    # ä¿ç•™æ®µè½IDæ ‡è®°
                    node_content_parts.append(f"[{para_id}] {para_content}")
                    print(f"ğŸ”§ [å†…å®¹é‡å»º] æ·»åŠ æ®µè½ {para_id}ï¼Œå†…å®¹é•¿åº¦: {len(para_content)}")
                else:
                    print(f"âš ï¸ [å†…å®¹é‡å»º] è­¦å‘Š: æ®µè½ {para_id} åœ¨åŸå†…å®¹ä¸­æœªæ‰¾åˆ°")
            
            # å°†èŠ‚ç‚¹çš„æ‰€æœ‰æ®µè½å†…å®¹åˆå¹¶
            if node_content_parts:
                rebuilt_content_parts.append('\n\n'.join(node_content_parts))
                rebuilt_content_parts.append('\n\n')  # èŠ‚ç‚¹é—´çš„åˆ†éš”
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶æ‰€æœ‰éƒ¨åˆ†
        rebuilt_content = ''.join(rebuilt_content_parts).strip()
        
        print(f"âœ… [å†…å®¹é‡å»º] é‡å»ºå®Œæˆ")
        print(f"âœ… [å†…å®¹é‡å»º] é‡å»ºåå†…å®¹é•¿åº¦: {len(rebuilt_content)} å­—ç¬¦")
        print(f"âœ… [å†…å®¹é‡å»º] åŒ…å« {len(node_mappings)} ä¸ªç‰©ç†åˆ†å‰²æ ")
        
        # éªŒè¯é‡å»ºç»“æœ
        divider_count = len(re.findall(r'--- [^-]+ ---', rebuilt_content))
        print(f"âœ… [å†…å®¹é‡å»º] éªŒè¯: æ‰¾åˆ° {divider_count} ä¸ªåˆ†å‰²æ ")
        
        # æ‰“å°é‡å»ºå†…å®¹çš„å‰200å­—ç¬¦ç”¨äºè°ƒè¯•
        print(f"ğŸ” [å†…å®¹é‡å»º] é‡å»ºå†…å®¹å‰200å­—ç¬¦:")
        print(f"   {rebuilt_content[:200]}...")
        
        return rebuilt_content
        
    except Exception as e:
        print(f"âŒ [å†…å®¹é‡å»ºé”™è¯¯] {str(e)}")
        import traceback
        traceback.print_exc()
        # å‡ºé”™æ—¶è¿”å›åŸå§‹å†…å®¹
        return text_with_ids

@app.get("/api/document-status/{document_id}")
async def get_document_status(document_id: str):
    """è·å–æ–‡æ¡£çŠ¶æ€å’Œè®ºè¯ç»“æ„åˆ†æè¿›åº¦"""
    
    if document_id not in document_status:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    doc_info = document_status[document_id]
    
    response_data = {
        "success": True,
        "document_id": document_id,
        "filename": doc_info.get("filename"),
        "content": doc_info.get("content"),
        "file_type": doc_info.get("file_type", ".md"),
        
        # è®ºè¯ç»“æ„åˆ†æçŠ¶æ€
        "status_demo": doc_info.get("status_demo", "not_started"),
        "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
        "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
        "edges_demo": doc_info.get("edges_demo", []),
        "error_demo": doc_info.get("error_demo"),
        "content_with_ids": doc_info.get("content_with_ids"),
    }
    
    # å¦‚æœæ˜¯PDFæ–‡ä»¶ï¼Œæ·»åŠ PDFç›¸å…³ä¿¡æ¯
    if doc_info.get("file_type") == ".pdf":
        response_data["pdf_base64"] = doc_info.get("pdf_base64")
        response_data["original_file_path"] = doc_info.get("original_file_path")
    
    return JSONResponse(response_data)

@app.get("/api/document/{document_id}")
async def get_document(document_id: str):
    """è·å–æ–‡æ¡£å†…å®¹å’Œè®ºè¯ç»“æ„"""
    
    try:
        # å¦‚æœæ–‡ä»¶åœ¨å†…å­˜çŠ¶æ€ä¸­å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if document_id in document_status:
            doc_info = document_status[document_id]
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": doc_info["content"],
                "filename": doc_info.get("filename", ""),
                "file_type": doc_info.get("file_type", ".md"),
                "mermaid_code_demo": doc_info.get("mermaid_code_demo"),
                "node_mappings_demo": doc_info.get("node_mappings_demo", {}),
                "edges_demo": doc_info.get("edges_demo", []),
                "status_demo": doc_info.get("status_demo", "not_started"),
                "error_demo": doc_info.get("error_demo"),
                "content_with_ids": doc_info.get("content_with_ids"),
                "pdf_base64": doc_info.get("pdf_base64")
            })
        else:
            # å°è¯•æŸ¥æ‰¾æ–‡ä»¶
            file_path = UPLOAD_DIR / f"{document_id}.md"
            
            if not file_path.exists():
                raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return JSONResponse({
                "success": True,
                "document_id": document_id,
                "content": content,
                "filename": f"{document_id}.md",
                "file_type": ".md",
                "mermaid_code_demo": None,
                "node_mappings_demo": {},
                "edges_demo": [],
                "status_demo": "not_started",
                "error_demo": None,
                "content_with_ids": None
            })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "message": "Argument Structure Analyzer API is running"}

@app.get("/")
async def root():
    return {"message": "Argument Structure Analyzer API is running"}

# æ–‡æ¡£ç»“æ„ç›¸å…³APIç«¯ç‚¹ï¼ˆä¿ç•™ç”¨äºç›®å½•ç”Ÿæˆç­‰ï¼‰

# æ–‡æ¡£ç»“æ„å’Œç›®å½•ç›¸å…³APIç«¯ç‚¹

@app.get("/api/document-structure/{document_id}")
async def get_document_structure(document_id: str):
    """è·å–æ–‡æ¡£çš„å±‚çº§ç»“æ„"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    chunks = parser.parse_to_chunks(content, document_id)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': chunks
                    }
                    
                    print(f"ğŸ“„ [è‡ªåŠ¨ç”Ÿæˆ] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº†ç»“æ„å’Œ {len(chunks)} ä¸ªchunks")
                    
                    return {
                        "success": True,
                        "structure": root.to_dict(),
                        "toc": toc,
                        "chunks": chunks,
                        "chunks_count": len(chunks)
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç»“æ„å°šæœªç”Ÿæˆï¼Œä¸”æ— æ³•è‡ªåŠ¨ç”Ÿæˆ",
                "structure": None,
                "toc": [],
                "chunks": [],
                "chunks_count": 0
            }
        
        structure_data = document_structures[document_id]
        chunks = structure_data.get('chunks', [])
        
        print(f"ğŸ“„ [API] è¿”å›æ–‡æ¡£ç»“æ„ï¼Œchunksæ•°é‡: {len(chunks)}")
        
        return {
            "success": True,
            "structure": structure_data['structure'],
            "toc": structure_data['toc'], 
            "chunks": chunks,  # è¿”å›å®é™…çš„chunksæ•°æ®
            "chunks_count": len(chunks)
        }
        
    except Exception as e:
        logger.error(f"Get document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.get("/api/document-toc/{document_id}")
async def get_document_toc(document_id: str):
    """è·å–æ–‡æ¡£ç›®å½•"""
    try:
        if document_id not in document_structures:
            # å¦‚æœç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆ
            if document_id in document_status:
                content = document_status[document_id].get('content')
                if content:
                    parser = DocumentParser()
                    root = parser.parse_document(content, document_id)
                    toc = parser.generate_toc(root)
                    
                    # ä¿å­˜ç»“æ„
                    document_structures[document_id] = {
                        'structure': root.to_dict(),
                        'toc': toc,
                        'chunks': parser.parse_to_chunks(content, document_id)
                    }
                    
                    return {
                        "success": True,
                        "toc": toc
                    }
            
            return {
                "success": False,
                "message": "æ–‡æ¡£ç›®å½•å°šæœªç”Ÿæˆ",
                "toc": []
            }
        
        structure_data = document_structures[document_id]
        return {
            "success": True,
            "toc": structure_data['toc']
        }
        
    except Exception as e:
        logger.error(f"Get document TOC error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç›®å½•å¤±è´¥: {str(e)}")

@app.post("/api/generate-document-structure/{document_id}")
async def generate_document_structure(document_id: str):
    """ç”Ÿæˆæˆ–é‡æ–°ç”Ÿæˆæ–‡æ¡£ç»“æ„"""
    try:
        if document_id not in document_status:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        content = document_status[document_id].get('content')
        if not content:
            raise HTTPException(status_code=400, detail="æ–‡æ¡£å†…å®¹ä¸ºç©º")
        
        # ä½¿ç”¨æ–‡æ¡£è§£æå™¨ç”Ÿæˆç»“æ„
        parser = DocumentParser()
        root = parser.parse_document(content, document_id)
        toc = parser.generate_toc(root)
        chunks = parser.parse_to_chunks(content, document_id)
        
        # ä¿å­˜ç»“æ„
        document_structures[document_id] = {
            'structure': root.to_dict(),
            'toc': toc,
            'chunks': chunks
        }
        
        print(f"ğŸ“„ [æ–‡æ¡£ç»“æ„] ä¸ºæ–‡æ¡£ {document_id} ç”Ÿæˆäº† {len(toc)} ä¸ªç›®å½•é¡¹ï¼Œ{len(chunks)} ä¸ªå†…å®¹å—")
        
        return {
            "success": True,
            "message": "æ–‡æ¡£ç»“æ„ç”ŸæˆæˆåŠŸ",
            "toc_items": len(toc),
            "chunks_count": len(chunks)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generate document structure error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ–‡æ¡£ç»“æ„å¤±è´¥: {str(e)}")

@app.post("/api/document/{document_id}/remap")
async def update_node_mappings(document_id: str, request_data: dict):
    """æ›´æ–°æ–‡æ¡£çš„èŠ‚ç‚¹æ˜ å°„å…³ç³»"""
    try:
        print(f"ğŸ“ [API] æ”¶åˆ°èŠ‚ç‚¹æ˜ å°„æ›´æ–°è¯·æ±‚ - æ–‡æ¡£ID: {document_id}")
        print(f"ğŸ“ [API] æ–°çš„èŠ‚ç‚¹æ˜ å°„: {request_data}")
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if 'node_mappings' not in request_data:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "ç¼ºå°‘ node_mappings å‚æ•°"}
            )
        
        new_node_mappings = request_data['node_mappings']
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        if document_id not in document_status:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"}
            )
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸­çš„èŠ‚ç‚¹æ˜ å°„
        document_status[document_id]['node_mappings_demo'] = new_node_mappings
        
        print(f"ğŸ“ [API] âœ… æˆåŠŸæ›´æ–°æ–‡æ¡£ {document_id} çš„èŠ‚ç‚¹æ˜ å°„")
        print(f"ğŸ“ [API] æ›´æ–°åçš„æ˜ å°„é”®æ•°é‡: {len(new_node_mappings)}")
        
        # å¯é€‰ï¼šä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆè¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“ä¿å­˜é€»è¾‘ï¼‰
        # TODO: æ·»åŠ æ•°æ®åº“æŒä¹…åŒ–é€»è¾‘
        
        return JSONResponse(content={
            "success": True,
            "message": "èŠ‚ç‚¹æ˜ å°„æ›´æ–°æˆåŠŸ",
            "document_id": document_id,
            "updated_mappings_count": len(new_node_mappings)
        })
        
    except Exception as e:
        print(f"âŒ [APIé”™è¯¯] æ›´æ–°èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ›´æ–°èŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}"}
        )

@app.post("/api/document/{document_id}/node/add")
async def add_node(document_id: str, request_data: AddNodeRequest):
    """æ·»åŠ æ–°èŠ‚ç‚¹åˆ°æ–‡æ¡£ç»“æ„"""
    try:
        print(f"ğŸ†• [API] æ”¶åˆ°æ·»åŠ èŠ‚ç‚¹è¯·æ±‚ - æ–‡æ¡£ID: {document_id}")
        print(f"ğŸ†• [API] è¯·æ±‚å‚æ•°: sourceNodeId={request_data.sourceNodeId}, direction={request_data.direction}, parentId={request_data.parentId}")
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        if document_id not in document_status:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"}
            )
        
        document_data = document_status[document_id]
        
        # è·å–å¿…è¦çš„æ–‡æ¡£æ•°æ®
        content_with_ids = document_data.get('content_with_ids', '')
        node_mappings = document_data.get('node_mappings_demo', {})
        mermaid_string = document_data.get('mermaid_code_demo', '')
        
        if not content_with_ids:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æœªåŒ…å«æ®µè½ID"}
            )
        
        # ç”Ÿæˆæ–°èŠ‚ç‚¹IDå’Œæ ‡ç­¾
        new_node_id = f"node_{int(datetime.now().timestamp() * 1000)}"
        new_node_label = request_data.label or "æ–°èŠ‚ç‚¹"
        
        print(f"ğŸ†• [API] ç”Ÿæˆæ–°èŠ‚ç‚¹ID: {new_node_id}")
        
        # è§£æcontent_with_idsä»¥æ‰¾åˆ°æ’å…¥ç‚¹
        updated_content = await insert_divider_in_content(
            content_with_ids, 
            request_data.sourceNodeId,
            request_data.direction,
            new_node_id,
            node_mappings
        )
        
        if updated_content is None:
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ’å…¥ä½ç½®"}
            )
        
        # æ›´æ–°node_mappings
        updated_node_mappings = node_mappings.copy()
        updated_node_mappings[new_node_id] = {
            "text_snippet": new_node_label,
            "paragraph_ids": [],
            "semantic_role": "æ–°æ·»åŠ çš„èŠ‚ç‚¹"
        }
        
        # æ›´æ–°mermaid_string
        updated_mermaid = update_mermaid_string(
            mermaid_string,
            new_node_id,
            new_node_label,
            request_data.direction,
            request_data.sourceNodeId,
            request_data.parentId
        )
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        document_status[document_id].update({
            'content_with_ids': updated_content,
            'node_mappings_demo': updated_node_mappings,
            'mermaid_code_demo': updated_mermaid
        })
        
        print(f"ğŸ†• [API] âœ… æˆåŠŸæ·»åŠ èŠ‚ç‚¹ {new_node_id} åˆ°æ–‡æ¡£ {document_id}")
        print(f"ğŸ†• [API] ğŸ“Š æ›´æ–°åçš„æ•°æ®ç»Ÿè®¡:")
        print(f"   content_with_ids é•¿åº¦: {len(updated_content)} å­—ç¬¦")
        print(f"   node_mappings æ•°é‡: {len(updated_node_mappings)}")
        print(f"   mermaid_code é•¿åº¦: {len(updated_mermaid)} å­—ç¬¦")
        print(f"ğŸ†• [API] ğŸ“‹ æ›´æ–°åçš„ content_with_ids å‰200å­—ç¬¦:")
        print(f"   {updated_content[:200]}...")
        
        # æ„å»ºè¿”å›çš„æ–‡æ¡£æ•°æ®
        updated_document = document_status[document_id]
        
        # éªŒè¯å…³é”®æ•°æ®æ˜¯å¦å­˜åœ¨
        if not updated_document.get('content_with_ids'):
            print(f"âŒ [API] è­¦å‘Š: è¿”å›æ•°æ®ä¸­ content_with_ids ä¸ºç©º")
        if not updated_document.get('node_mappings_demo'):
            print(f"âŒ [API] è­¦å‘Š: è¿”å›æ•°æ®ä¸­ node_mappings_demo ä¸ºç©º")
        if not updated_document.get('mermaid_code_demo'):
            print(f"âŒ [API] è­¦å‘Š: è¿”å›æ•°æ®ä¸­ mermaid_code_demo ä¸ºç©º")
        
        print(f"ğŸ†• [API] ğŸ“¤ è¿”å›ç»™å‰ç«¯çš„æ•°æ®åŒ…å«ä»¥ä¸‹å­—æ®µ:")
        print(f"   {list(updated_document.keys())}")
        
        # è¿”å›æ›´æ–°åçš„å®Œæ•´æ–‡æ¡£
        return JSONResponse(content={
            "success": True,
            "message": "èŠ‚ç‚¹æ·»åŠ æˆåŠŸ",
            "document": updated_document,
            "new_node_id": new_node_id
        })
        
    except Exception as e:
        print(f"âŒ [APIé”™è¯¯] æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {str(e)}"}
        )

async def insert_divider_in_content(content: str, source_node_id: str, direction: str, new_node_id: str, node_mappings: Dict) -> Optional[str]:
    """
    åœ¨content_with_idsä¸­æ’å…¥æ–°çš„åˆ†å‰²æ æ ‡è®°
    ä½¿ç”¨ç²¾ç¡®çš„å­—ç¬¦ä¸²æ“ä½œï¼Œæ ¹æ®directionæ‰§è¡Œä¸åŒçš„æ’å…¥ç­–ç•¥
    """
    try:
        print(f"ğŸ” [ç²¾ç¡®æ’å…¥] å¼€å§‹æ’å…¥åˆ†å‰²æ ")
        print(f"ğŸ” [ç²¾ç¡®æ’å…¥] æºèŠ‚ç‚¹: {source_node_id}, æ–¹å‘: {direction}, æ–°èŠ‚ç‚¹: {new_node_id}")
        print(f"ğŸ” [ç²¾ç¡®æ’å…¥] å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # åˆ›å»ºæ–°çš„åˆ†å‰²æ æ ‡è®°
        new_divider = f"--- {new_node_id} ---"
        
        if direction == 'child':
            # å­èŠ‚ç‚¹ï¼šæ‰¾åˆ° sourceNodeId çš„æ•´ä¸ªå†…å®¹èŒƒå›´çš„æœ«å°¾ï¼Œæ’å…¥æ–°åˆ†å‰²æ 
            print(f"ğŸ” [ç²¾ç¡®æ’å…¥-child] å¤„ç†å­èŠ‚ç‚¹æ’å…¥")
            
            # æ‰¾åˆ°æºèŠ‚ç‚¹çš„åˆ†å‰²æ ä½ç½®
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                print(f"âŒ [ç²¾ç¡®æ’å…¥-child] æœªæ‰¾åˆ°æºèŠ‚ç‚¹åˆ†å‰²æ : {source_node_id}")
                return None
            
            # æ‰¾åˆ°æºèŠ‚ç‚¹å†…å®¹èŒƒå›´çš„æœ«å°¾ï¼ˆä¸‹ä¸€ä¸ªåˆ†å‰²æ çš„å¼€å§‹ä½ç½®æˆ–æ–‡æ¡£æœ«å°¾ï¼‰
            next_divider_pattern = r"\n--- [^-]+ ---"
            next_match = None
            for match in re.finditer(next_divider_pattern, content[source_match.end():]):
                next_match = match
                break
            
            if next_match:
                # åœ¨ä¸‹ä¸€ä¸ªåˆ†å‰²æ å‰æ’å…¥
                insert_pos = source_match.end() + next_match.start()
                print(f"ğŸ” [ç²¾ç¡®æ’å…¥-child] åœ¨ä½ç½® {insert_pos} æ’å…¥ï¼ˆä¸‹ä¸€ä¸ªåˆ†å‰²æ å‰ï¼‰")
            else:
                # åœ¨æ–‡æ¡£æœ«å°¾æ’å…¥
                insert_pos = len(content)
                print(f"ğŸ” [ç²¾ç¡®æ’å…¥-child] åœ¨ä½ç½® {insert_pos} æ’å…¥ï¼ˆæ–‡æ¡£æœ«å°¾ï¼‰")
            
            # æ‰§è¡Œæ’å…¥
            updated_content = content[:insert_pos] + f"\n\n{new_divider}\n\n" + content[insert_pos:]
            
        elif direction == 'left-sibling':
            # å·¦ä¾§åŒçº§ï¼šåœ¨ --- sourceNodeId --- è¿™ä¸ªå­ä¸²çš„æ­£å‰æ–¹æ’å…¥
            print(f"ğŸ” [ç²¾ç¡®æ’å…¥-left-sibling] å¤„ç†å·¦ä¾§åŒçº§æ’å…¥")
            
            source_pattern = f"--- {re.escape(source_node_id)} ---"
            source_match = re.search(source_pattern, content)
            
            if not source_match:
                print(f"âŒ [ç²¾ç¡®æ’å…¥-left-sibling] æœªæ‰¾åˆ°æºèŠ‚ç‚¹åˆ†å‰²æ : {source_node_id}")
                return None
            
            # åœ¨æºèŠ‚ç‚¹åˆ†å‰²æ æ­£å‰æ–¹æ’å…¥
            insert_pos = source_match.start()
            print(f"ğŸ” [ç²¾ç¡®æ’å…¥-left-sibling] åœ¨ä½ç½® {insert_pos} æ’å…¥ï¼ˆæºèŠ‚ç‚¹åˆ†å‰²æ å‰ï¼‰")
            
            # æ‰§è¡Œæ’å…¥
            updated_content = content[:insert_pos] + f"{new_divider}\n\n" + content[insert_pos:]
            
        elif direction == 'right-sibling':
            # å³ä¾§åŒçº§ï¼šæ„å»ºèŠ‚ç‚¹æ ‘ï¼Œæ‰¾åˆ°å­æ ‘ç»“æŸä½ç½®åæ’å…¥
            print(f"ğŸ” [ç²¾ç¡®æ’å…¥-right-sibling] å¤„ç†å³ä¾§åŒçº§æ’å…¥")
            
            # æ„å»ºèŠ‚ç‚¹æ ‘ç»“æ„
            node_tree = build_node_tree_from_content(content, node_mappings)
            if not node_tree:
                print(f"âŒ [ç²¾ç¡®æ’å…¥-right-sibling] æ— æ³•æ„å»ºèŠ‚ç‚¹æ ‘")
                return None
            
            # æ‰¾åˆ°æºèŠ‚ç‚¹åŠå…¶å­æ ‘çš„ç»“æŸä½ç½®
            subtree_end_pos = find_node_subtree_end(content, source_node_id, node_tree)
            if subtree_end_pos is None:
                print(f"âŒ [ç²¾ç¡®æ’å…¥-right-sibling] æ— æ³•æ‰¾åˆ°æºèŠ‚ç‚¹å­æ ‘ç»“æŸä½ç½®")
                return None
            
            print(f"ğŸ” [ç²¾ç¡®æ’å…¥-right-sibling] åœ¨ä½ç½® {subtree_end_pos} æ’å…¥ï¼ˆå­æ ‘æœ«å°¾åï¼‰")
            
            # æ‰§è¡Œæ’å…¥
            updated_content = content[:subtree_end_pos] + f"\n\n{new_divider}\n\n" + content[subtree_end_pos:]
            
        else:
            print(f"âŒ [ç²¾ç¡®æ’å…¥] ä¸æ”¯æŒçš„æ–¹å‘: {direction}")
            return None
        
        print(f"âœ… [ç²¾ç¡®æ’å…¥] æˆåŠŸæ’å…¥æ–°åˆ†å‰²æ ï¼ŒèŠ‚ç‚¹ID: {new_node_id}")
        print(f"âœ… [ç²¾ç¡®æ’å…¥] æ–°å†…å®¹é•¿åº¦: {len(updated_content)} å­—ç¬¦")
        
        # éªŒè¯æ’å…¥ç»“æœ
        divider_count = len(re.findall(r'--- [^-]+ ---', updated_content))
        print(f"âœ… [ç²¾ç¡®æ’å…¥] éªŒè¯ï¼šæ›´æ–°åæ‰¾åˆ° {divider_count} ä¸ªåˆ†å‰²æ ")
        
        return updated_content
        
    except Exception as e:
        print(f"âŒ [ç²¾ç¡®æ’å…¥é”™è¯¯] {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def build_node_tree_from_content(content: str, node_mappings: Dict) -> Optional[Dict]:
    """
    ä»content_with_idså’Œmermaidè¿æ¥å…³ç³»æ„å»ºèŠ‚ç‚¹æ ‘ç»“æ„
    
    Returns:
        èŠ‚ç‚¹æ ‘å­—å…¸ï¼Œæ ¼å¼: {node_id: {'children': [child_ids], 'position': (start, end)}}
    """
    try:
        print(f"ğŸŒ³ [æ„å»ºèŠ‚ç‚¹æ ‘] å¼€å§‹æ„å»ºèŠ‚ç‚¹æ ‘")
        
        # è§£ææ‰€æœ‰åˆ†å‰²æ ä½ç½®
        divider_pattern = r'--- ([^-]+) ---'
        matches = list(re.finditer(divider_pattern, content))
        
        if not matches:
            print(f"ğŸŒ³ [æ„å»ºèŠ‚ç‚¹æ ‘] æ²¡æœ‰æ‰¾åˆ°åˆ†å‰²æ ")
            return None
        
        # æ„å»ºèŠ‚ç‚¹ä½ç½®æ˜ å°„
        node_positions = {}
        for i, match in enumerate(matches):
            node_id = match.group(1).strip()
            start_pos = match.start()
            # ä¸‹ä¸€ä¸ªåˆ†å‰²æ çš„å¼€å§‹ä½ç½®ï¼Œæˆ–æ–‡æ¡£æœ«å°¾
            end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(content)
            node_positions[node_id] = (start_pos, end_pos)
            print(f"ğŸŒ³ [æ„å»ºèŠ‚ç‚¹æ ‘] èŠ‚ç‚¹ {node_id}: ä½ç½® {start_pos}-{end_pos}")
        
        # ä»node_mappingsæ„å»ºçˆ¶å­å…³ç³»ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾èŠ‚ç‚¹æŒ‰é¡ºåºæ’åˆ—ï¼‰
        # å®é™…åº”è¯¥ä»mermaid_stringè§£æè¿æ¥å…³ç³»ï¼Œä½†è¿™é‡Œä½¿ç”¨ä½ç½®é¡ºåºä½œä¸ºè¿‘ä¼¼
        node_tree = {}
        node_ids = list(node_positions.keys())
        
        for i, node_id in enumerate(node_ids):
            node_tree[node_id] = {
                'children': [],
                'position': node_positions[node_id],
                'level': 0  # ç®€åŒ–å¤„ç†ï¼Œå‡è®¾éƒ½æ˜¯åŒçº§
            }
        
        print(f"ğŸŒ³ [æ„å»ºèŠ‚ç‚¹æ ‘] æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(node_tree)} ä¸ªèŠ‚ç‚¹")
        return node_tree
        
    except Exception as e:
        print(f"âŒ [æ„å»ºèŠ‚ç‚¹æ ‘é”™è¯¯] {str(e)}")
        return None

def find_node_subtree_end(content: str, source_node_id: str, node_tree: Dict) -> Optional[int]:
    """
    æ‰¾åˆ°æºèŠ‚ç‚¹åŠå…¶æ‰€æœ‰å­å­™èŠ‚ç‚¹æ„æˆçš„å­æ ‘çš„ç»“æŸä½ç½®
    
    Args:
        content: æ–‡æ¡£å†…å®¹
        source_node_id: æºèŠ‚ç‚¹ID
        node_tree: èŠ‚ç‚¹æ ‘ç»“æ„
        
    Returns:
        å­æ ‘ç»“æŸä½ç½®ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    try:
        print(f"ğŸ” [å­æ ‘æŸ¥æ‰¾] æŸ¥æ‰¾èŠ‚ç‚¹ {source_node_id} çš„å­æ ‘ç»“æŸä½ç½®")
        
        if source_node_id not in node_tree:
            print(f"âŒ [å­æ ‘æŸ¥æ‰¾] èŠ‚ç‚¹ {source_node_id} ä¸åœ¨èŠ‚ç‚¹æ ‘ä¸­")
            return None
        
        # è·å–æºèŠ‚ç‚¹çš„ä½ç½®
        source_position = node_tree[source_node_id]['position']
        source_end = source_position[1]
        
        print(f"ğŸ” [å­æ ‘æŸ¥æ‰¾] æºèŠ‚ç‚¹ä½ç½®: {source_position}")
        
        # ç®€åŒ–å®ç°ï¼šç”±äºæ²¡æœ‰çœŸæ­£çš„çˆ¶å­å…³ç³»ï¼Œç›´æ¥è¿”å›èŠ‚ç‚¹å†…å®¹çš„ç»“æŸä½ç½®
        # åœ¨å®é™…å®ç°ä¸­ï¼Œåº”è¯¥éå†æ‰€æœ‰å­èŠ‚ç‚¹ï¼Œæ‰¾åˆ°æœ€è¿œçš„å­å­™èŠ‚ç‚¹ä½ç½®
        
        # æŸ¥æ‰¾ç´§æ¥åœ¨æºèŠ‚ç‚¹åé¢çš„å­èŠ‚ç‚¹ä»¬ï¼ˆåŸºäºç¼©è¿›æˆ–é¡ºåºåˆ¤æ–­ï¼‰
        max_end_pos = source_end
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾åŒä¸€çº§åˆ«çš„èŠ‚ç‚¹æŒ‰é¡ºåºæ’åˆ—
        # å®é™…åº”è¯¥è§£æmermaidè¿æ¥å…³ç³»æ¥ç¡®å®šçœŸæ­£çš„çˆ¶å­å…³ç³»
        for node_id, node_info in node_tree.items():
            node_start, node_end = node_info['position']
            # å¦‚æœèŠ‚ç‚¹åœ¨æºèŠ‚ç‚¹ä¹‹åä¸”æ˜¯å…¶å­èŠ‚ç‚¹ï¼ˆè¿™é‡Œç®€åŒ–åˆ¤æ–­ï¼‰
            if node_start > source_end:
                # ç®€åŒ–ï¼šåªè€ƒè™‘ç´§æ¥ç€çš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºè¾¹ç•Œ
                max_end_pos = node_start
                break
        
        print(f"ğŸ” [å­æ ‘æŸ¥æ‰¾] ç¡®å®šå­æ ‘ç»“æŸä½ç½®: {max_end_pos}")
        return max_end_pos
        
    except Exception as e:
        print(f"âŒ [å­æ ‘æŸ¥æ‰¾é”™è¯¯] {str(e)}")
        return None

def parse_content_structure(content: str, node_mappings: Dict) -> tuple:
    """è§£æcontent_with_idsçš„ç»“æ„ï¼Œè¿”å›åˆ†å‰²æ ä½ç½®å’ŒèŠ‚ç‚¹åŒºåŸŸ"""
    divider_positions = {}
    node_regions = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰åˆ†å‰²æ 
    divider_pattern = r'\n*---\s*([^-\n]+)\s*---\n*'
    matches = list(re.finditer(divider_pattern, content))
    
    print(f"ğŸ” [ç»“æ„è§£æ] æ‰¾åˆ° {len(matches)} ä¸ªåˆ†å‰²æ ")
    
    for i, match in enumerate(matches):
        node_id = match.group(1).strip()
        start_pos = match.start()
        end_pos = match.end()
        
        divider_positions[node_id] = {
            'start': start_pos,
            'end': end_pos,
            'match': match
        }
        
        # ç¡®å®šèŠ‚ç‚¹åŒºåŸŸï¼ˆä»åˆ†å‰²æ åˆ°ä¸‹ä¸€ä¸ªåˆ†å‰²æ æˆ–æ–‡æ¡£æœ«å°¾ï¼‰
        next_divider_start = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        
        node_regions[node_id] = {
            'start': start_pos,  # åˆ†å‰²æ å¼€å§‹ä½ç½®
            'end': next_divider_start,  # ä¸‹ä¸€ä¸ªåˆ†å‰²æ å¼€å§‹ä½ç½®æˆ–æ–‡æ¡£æœ«å°¾
            'content_start': end_pos,  # å®é™…å†…å®¹å¼€å§‹ä½ç½®ï¼ˆåˆ†å‰²æ åï¼‰
            'content_end': next_divider_start  # å®é™…å†…å®¹ç»“æŸä½ç½®
        }
        
        print(f"ğŸ” [ç»“æ„è§£æ] èŠ‚ç‚¹ {node_id}: start={start_pos}, end={next_divider_start}")
    
    return divider_positions, node_regions

def find_subtree_end(node_id: str, node_regions: Dict, content: str) -> int:
    """æ‰¾åˆ°èŠ‚ç‚¹åŠå…¶æ•´ä¸ªå­æ ‘çš„æœ«å°¾ä½ç½®ï¼ˆç”¨äºright-siblingæ’å…¥ï¼‰"""
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå‡è®¾èŠ‚ç‚¹æŒ‰å±‚çº§é¡ºåºæ’åˆ—
    # æ›´å¤æ‚çš„å®ç°éœ€è¦æ„å»ºå®é™…çš„æ ‘ç»“æ„
    if node_id in node_regions:
        return node_regions[node_id]['end']
    return len(content)

def update_mermaid_string(mermaid_string: str, new_node_id: str, new_node_label: str, direction: str, source_node_id: str, parent_id: Optional[str]) -> str:
    """æ›´æ–°mermaidå­—ç¬¦ä¸²ï¼Œæ·»åŠ æ–°èŠ‚ç‚¹å’Œè¿æ¥"""
    try:
        print(f"ğŸ”„ [Mermaidæ›´æ–°] å¼€å§‹æ›´æ–°ï¼Œæ–°èŠ‚ç‚¹: {new_node_id}, æ ‡ç­¾: {new_node_label}")
        print(f"ğŸ”„ [Mermaidæ›´æ–°] æ–¹å‘: {direction}, æºèŠ‚ç‚¹: {source_node_id}, çˆ¶èŠ‚ç‚¹: {parent_id}")
        print(f"ğŸ”„ [Mermaidæ›´æ–°] åŸå§‹Mermaidé•¿åº¦: {len(mermaid_string)}")
        
        updated_mermaid = mermaid_string or "graph TD"
        
        # ç¡®ä¿ä»¥æ¢è¡Œç¬¦ç»“å°¾
        if not updated_mermaid.endswith('\n'):
            updated_mermaid += '\n'
        
        # æ·»åŠ æ–°èŠ‚ç‚¹å®šä¹‰
        new_node_def = f"    {new_node_id}[{new_node_label}]"
        updated_mermaid += new_node_def + '\n'
        
        # æ ¹æ®æ–¹å‘å†³å®šè¿æ¥å…³ç³»
        if direction == 'child':
            # å­èŠ‚ç‚¹ï¼šæºèŠ‚ç‚¹æŒ‡å‘æ–°èŠ‚ç‚¹
            connection = f"    {source_node_id} --> {new_node_id}"
            print(f"ğŸ”„ [Mermaidæ›´æ–°] å­èŠ‚ç‚¹è¿æ¥: {source_node_id} --> {new_node_id}")
        else:
            # åŒçº§èŠ‚ç‚¹ï¼šéœ€è¦æ‰¾åˆ°å…±åŒçš„çˆ¶èŠ‚ç‚¹
            if parent_id:
                # å¦‚æœæ˜ç¡®æä¾›äº†çˆ¶èŠ‚ç‚¹IDï¼Œä½¿ç”¨å®ƒ
                target_parent = parent_id
                print(f"ğŸ”„ [Mermaidæ›´æ–°] ä½¿ç”¨æä¾›çš„çˆ¶èŠ‚ç‚¹: {target_parent}")
            else:
                # ä»ç°æœ‰çš„mermaidå­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾æºèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹
                target_parent = find_parent_node_in_mermaid(updated_mermaid, source_node_id)
                print(f"ğŸ”„ [Mermaidæ›´æ–°] ä»Mermaidä¸­æŸ¥æ‰¾åˆ°çˆ¶èŠ‚ç‚¹: {target_parent}")
            
            if target_parent:
                connection = f"    {target_parent} --> {new_node_id}"
                print(f"ğŸ”„ [Mermaidæ›´æ–°] åŒçº§èŠ‚ç‚¹è¿æ¥: {target_parent} --> {new_node_id}")
            else:
                # å¦‚æœæ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ï¼Œä½œä¸ºæ ¹èŠ‚ç‚¹å¤„ç†
                connection = f"    ROOT --> {new_node_id}"
                print(f"ğŸ”„ [Mermaidæ›´æ–°] æœªæ‰¾åˆ°çˆ¶èŠ‚ç‚¹ï¼Œä½¿ç”¨ROOTè¿æ¥")
        
        updated_mermaid += connection + '\n'
        
        print(f"âœ… [Mermaidæ›´æ–°] æ·»åŠ èŠ‚ç‚¹å®šä¹‰: {new_node_def}")
        print(f"âœ… [Mermaidæ›´æ–°] æ·»åŠ è¿æ¥: {connection}")
        print(f"âœ… [Mermaidæ›´æ–°] æ›´æ–°åé•¿åº¦: {len(updated_mermaid)}")
        
        return updated_mermaid
        
    except Exception as e:
        print(f"âŒ [Mermaidæ›´æ–°é”™è¯¯] {str(e)}")
        import traceback
        traceback.print_exc()
        return mermaid_string or "graph TD"

def find_parent_node_in_mermaid(mermaid_string: str, child_node_id: str) -> Optional[str]:
    """ä»mermaidå­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾æŒ‡å®šèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹"""
    try:
        print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] åœ¨Mermaidä¸­æŸ¥æ‰¾ {child_node_id} çš„çˆ¶èŠ‚ç‚¹")
        print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] Mermaidå†…å®¹: {mermaid_string[:200]}...")
        
        # æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼šæ›´å‡†ç¡®åœ°åŒ¹é…èŠ‚ç‚¹IDå’Œè¿æ¥å…³ç³»
        # åŒ¹é…å½¢å¦‚ "parent_node --> child_node_id" æˆ– "parent_node --> child_node_id[label]" çš„è¿æ¥
        escaped_child_id = re.escape(child_node_id)
        
        # å°è¯•å¤šä¸ªåŒ¹é…æ¨¡å¼
        patterns = [
            # åŒ¹é… "parent --> child" æˆ– "parent --> child[label]" æˆ– "parent --> child "
            rf'([A-Za-z0-9_]+)\s*-->\s*{escaped_child_id}(?:\[|$|\s|-->)',
            # åŒ¹é…å¸¦ç©ºæ ¼çš„æƒ…å†µ
            rf'([A-Za-z0-9_]+)\s*-->\s*{escaped_child_id}(?=\s|$|-->|\[)',
            # åŒ¹é…è¡Œç»“å°¾çš„æƒ…å†µ
            rf'([A-Za-z0-9_]+)\s*-->\s*{escaped_child_id}$'
        ]
        
        for i, pattern in enumerate(patterns):
            match = re.search(pattern, mermaid_string, re.MULTILINE)
            if match:
                parent_id = match.group(1)
                print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] ä½¿ç”¨æ¨¡å¼ {i+1} æ‰¾åˆ° {child_node_id} çš„çˆ¶èŠ‚ç‚¹: {parent_id}")
                return parent_id
        
        print(f"ğŸ” [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹] æœªæ‰¾åˆ° {child_node_id} çš„çˆ¶èŠ‚ç‚¹")
        return None
        
    except Exception as e:
        print(f"âŒ [æŸ¥æ‰¾çˆ¶èŠ‚ç‚¹é”™è¯¯] {str(e)}")
        return None

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ - åç«¯APIæœåŠ¡")
    print("=" * 80)
    print("ğŸ“ æœåŠ¡åœ°å€: http://localhost:8000")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ”§ æœåŠ¡æ¨¡å¼: å¼€å‘æ¨¡å¼ (æ”¯æŒçƒ­é‡è½½)")
    print("=" * 80)
    print("ğŸ“‹ æ§åˆ¶å°æ—¥å¿—è¯´æ˜:")
    print("   ğŸ“¤ [æ–‡ä»¶ä¸Šä¼ ] - æ–‡ä»¶ä¸Šä¼ ç›¸å…³ä¿¡æ¯")
    print("   ğŸ”„ [å¼€å§‹ç”Ÿæˆ] - æ€ç»´å¯¼å›¾ç”Ÿæˆä»»åŠ¡å¯åŠ¨")
    print("   ğŸš€ [å¼€å§‹ç”Ÿæˆ] - AIå¤„ç†å¼€å§‹")
    print("   ğŸ¤– [AIå¤„ç†] - è°ƒç”¨æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨")
    print("   âœ… [ç”Ÿæˆå®Œæˆ] - æ€ç»´å¯¼å›¾ç”ŸæˆæˆåŠŸ")
    print("   âŒ [ç”Ÿæˆå¤±è´¥] - ç”Ÿæˆè¿‡ç¨‹å‡ºç°é”™è¯¯")
    print("   â³ [çŠ¶æ€æŸ¥è¯¢] - å®¢æˆ·ç«¯æŸ¥è¯¢ç”ŸæˆçŠ¶æ€")
    print("=" * 80)
    print("ğŸ¯ æ–°åŠŸèƒ½: æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼")
    print("   ğŸ“Š æ ‡å‡†è¯¦ç»†æ¨¡å¼: 3-5åˆ†é’Ÿï¼Œè¯¦ç»†åˆ†æï¼Œé«˜è´¨é‡ç»“æœ")
    print("   âš¡ å¿«é€Ÿç®€åŒ–æ¨¡å¼: 1-2åˆ†é’Ÿï¼ŒåŸºç¡€ç»“æ„ï¼Œå¿«é€Ÿé¢„è§ˆ")
    print("   ğŸ“‹ APIç«¯ç‚¹: /api/generate-mindmap/{id} å’Œ /api/generate-mindmap-simple/{id}")
    print("=" * 80)
    print("ğŸš€ å¯åŠ¨æœåŠ¡ä¸­...")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info") 