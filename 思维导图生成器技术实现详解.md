# æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨æŠ€æœ¯å®ç°è¯¦è§£

## ğŸ—ï¸ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†ææ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆç»“æ„åŒ–çš„æ€ç»´å¯¼å›¾ã€‚ç³»ç»Ÿé‡‡ç”¨ç°ä»£å¼‚æ­¥æ¶æ„ï¼Œæ”¯æŒå¤šç§APIæä¾›å•†ï¼Œå…·æœ‰å¼ºå¤§çš„å†…å®¹åˆ†æã€å»é‡å’Œå¯è§†åŒ–èƒ½åŠ›ã€‚

## ğŸ“š ç›®å½•

1. [æ ¸å¿ƒæ¶æ„è®¾è®¡](#æ ¸å¿ƒæ¶æ„è®¾è®¡)
2. [é…ç½®ç³»ç»Ÿ](#é…ç½®ç³»ç»Ÿ)
3. [å¤šAPIæä¾›å•†æ”¯æŒ](#å¤šapiæä¾›å•†æ”¯æŒ)
4. [æ–‡æ¡£ç±»å‹æ£€æµ‹](#æ–‡æ¡£ç±»å‹æ£€æµ‹)
5. [å†…å®¹æå–å¼•æ“](#å†…å®¹æå–å¼•æ“)
6. [æ™ºèƒ½å»é‡ç®—æ³•](#æ™ºèƒ½å»é‡ç®—æ³•)
7. [æˆæœ¬æ§åˆ¶ç³»ç»Ÿ](#æˆæœ¬æ§åˆ¶ç³»ç»Ÿ)
8. [è¾“å‡ºæ ¼å¼ç”Ÿæˆ](#è¾“å‡ºæ ¼å¼ç”Ÿæˆ)
9. [é”™è¯¯å¤„ç†æœºåˆ¶](#é”™è¯¯å¤„ç†æœºåˆ¶)
10. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)

---

## ğŸ¯ æ ¸å¿ƒæ¶æ„è®¾è®¡

### ä¸»è¦ç»„ä»¶ç»“æ„

```python
# æ ¸å¿ƒç±»ç»§æ‰¿å…³ç³»å’ŒèŒè´£åˆ†å·¥
class Config:
    """ç»Ÿä¸€é…ç½®ç®¡ç†ä¸­å¿ƒ"""
    
class TokenUsageTracker:
    """æˆæœ¬å’Œä½¿ç”¨æƒ…å†µè¿½è¸ª"""
    
class DocumentOptimizer:
    """æ–‡æ¡£ä¼˜åŒ–å’ŒAPIè°ƒç”¨ç®¡ç†"""
    
class MindMapGenerator:
    """æ ¸å¿ƒæ€ç»´å¯¼å›¾ç”Ÿæˆé€»è¾‘"""
    
class MinimalDatabaseStub:
    """è½»é‡çº§æ•°æ®å­˜å‚¨æ¥å£"""
```

### æ•°æ®æµæ¶æ„

```
æ–‡æ¡£è¾“å…¥ â†’ ç±»å‹æ£€æµ‹ â†’ ä¸»é¢˜æå– â†’ å­ä¸»é¢˜æå– â†’ è¯¦ç»†ä¿¡æ¯æå– â†’ å»é‡éªŒè¯ â†’ æ ¼å¼ç”Ÿæˆ â†’ è¾“å‡º
    â†“         â†“         â†“          â†“            â†“          â†“        â†“
  ç¼“å­˜å±‚   æç¤ºå·¥ç¨‹   å¹¶å‘å¤„ç†    æ™ºèƒ½åˆ†å—     ç›¸ä¼¼åº¦æ£€æµ‹   éªŒè¯å±‚   å¤šæ ¼å¼
```

---

## âš™ï¸ é…ç½®ç³»ç»Ÿ

### ç¯å¢ƒå˜é‡ç®¡ç†

```python
from decouple import Config as DecoupleConfig, RepositoryEnv

# é…ç½®æ–‡ä»¶åŠ è½½æœºåˆ¶
config = DecoupleConfig(RepositoryEnv('.env'))

class Config:
    # APIé…ç½® - æ”¯æŒå¤šæä¾›å•†
    OPENAI_API_KEY = config.get("OPENAI_API_KEY")
    OPENAI_BASE_URL = config.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
    ANTHROPIC_API_KEY = config.get('ANTHROPIC_API_KEY')
    DEEPSEEK_API_KEY = config.get('DEEPSEEK_API_KEY')
    GEMINI_API_KEY = config.get('GEMINI_API_KEY')
    API_PROVIDER = config.get('API_PROVIDER')  # åŠ¨æ€é€‰æ‹©æä¾›å•†
    
    # æ¨¡å‹é…ç½® - é’ˆå¯¹ä¸åŒæä¾›å•†ä¼˜åŒ–
    CLAUDE_MODEL_STRING = "claude-3-5-haiku-latest"
    OPENAI_COMPLETION_MODEL = "gpt-4o-mini-2024-07-18"
    DEEPSEEK_COMPLETION_MODEL = "deepseek-chat"
    GEMINI_MODEL_STRING = "gemini-2.0-flash-lite"
    
    # æˆæœ¬æ§åˆ¶ - ç²¾ç¡®åˆ°å¾®ç¾å…ƒ
    OPENAI_INPUT_TOKEN_PRICE = 0.15/1000000
    OPENAI_OUTPUT_TOKEN_PRICE = 0.60/1000000
    # ... å…¶ä»–æä¾›å•†ä»·æ ¼é…ç½®
```

### åŠ¨æ€é…ç½®ç³»ç»Ÿ

```python
def __init__(self):
    self.config = {
        'max_summary_length': 2500,
        'max_tokens': 3000,
        'max_topics': 6,        # æœ€å¤§ä¸»é¢˜æ•°
        'max_subtopics': 4,     # æ¯ä¸ªä¸»é¢˜æœ€å¤§å­ä¸»é¢˜æ•°
        'max_details': 8,       # æ¯ä¸ªå­ä¸»é¢˜æœ€å¤§è¯¦ç»†ä¿¡æ¯æ•°
        'similarity_threshold': {
            'topic': 75,        # ä¸»é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
            'subtopic': 70,     # å­ä¸»é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
            'detail': 65        # è¯¦ç»†ä¿¡æ¯ç›¸ä¼¼åº¦é˜ˆå€¼
        },
        'reality_check': {
            'batch_size': 8,    # å¹¶è¡ŒéªŒè¯æ‰¹æ¬¡å¤§å°
            'min_verified_topics': 4,
            'min_verified_ratio': 0.6
        }
    }
```

---

## ğŸŒ å¤šAPIæä¾›å•†æ”¯æŒ

### ç»Ÿä¸€å®¢æˆ·ç«¯ç®¡ç†

```python
class DocumentOptimizer:
    def __init__(self):
        # åˆå§‹åŒ–å¤šä¸ªAPIå®¢æˆ·ç«¯
        self.openai_client = AsyncOpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL  # æ”¯æŒç¡…åŸºæµåŠ¨ç­‰å…¼å®¹æœåŠ¡
        )
        self.anthropic_client = AsyncAnthropic(api_key=Config.ANTHROPIC_API_KEY)
        self.deepseek_client = AsyncOpenAI(
            api_key=Config.DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        self.gemini_client = genai.configure(api_key=Config.GEMINI_API_KEY)
```

### æ™ºèƒ½APIè·¯ç”±

```python
async def generate_completion(self, prompt: str, max_tokens: int = 5000, 
                            request_id: str = None, task: Optional[str] = None):
    """ç»Ÿä¸€çš„APIè°ƒç”¨æ¥å£ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨è·¯ç”±åˆ°ç›¸åº”æä¾›å•†"""
    
    if Config.API_PROVIDER == "CLAUDE":
        kwargs = {
            "model": Config.CLAUDE_MODEL_STRING,
            "max_tokens": min(max_tokens, Config.CLAUDE_MAX_TOKENS),
            "messages": [{"role": "user", "content": prompt}]
        }
        response = await self.anthropic_client.messages.create(**kwargs)
        return response.content[0].text
        
    elif Config.API_PROVIDER == "DEEPSEEK":
        kwargs = {
            "model": Config.DEEPSEEK_COMPLETION_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": min(max_tokens, Config.DEEPSEEK_MAX_TOKENS)
        }
        
        # æ”¯æŒä¸åŒDeepSeekæ¨¡å‹
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            response = await self.deepseek_client.chat.completions.create(**kwargs)
        else:  # reasoneræ¨¡å‹
            response = await self.deepseek_client.chat.completions.create(**kwargs)
            
        return response.choices[0].message.content
        
    # ... å…¶ä»–æä¾›å•†å®ç°
```

---

## ğŸ” æ–‡æ¡£ç±»å‹æ£€æµ‹

### æ”¯æŒçš„æ–‡æ¡£ç±»å‹æšä¸¾

```python
class DocumentType(Enum):
    """æ”¯æŒçš„æ–‡æ¡£ç±»å‹ï¼Œæ¯ç§ç±»å‹å¯¹åº”ä¸åŒçš„åˆ†æç­–ç•¥"""
    TECHNICAL = auto()      # æŠ€æœ¯æ–‡æ¡£
    SCIENTIFIC = auto()     # ç§‘å­¦ç ”ç©¶
    NARRATIVE = auto()      # å™è¿°æ–‡æ¡£
    BUSINESS = auto()       # å•†ä¸šæ–‡æ¡£
    ACADEMIC = auto()       # å­¦æœ¯è®ºæ–‡
    LEGAL = auto()          # æ³•å¾‹æ–‡æ¡£
    MEDICAL = auto()        # åŒ»å­¦æ–‡æ¡£
    INSTRUCTIONAL = auto()  # æŒ‡å¯¼æ‰‹å†Œ
    ANALYTICAL = auto()     # åˆ†ææŠ¥å‘Š
    PROCEDURAL = auto()     # æµç¨‹æ–‡æ¡£
    GENERAL = auto()        # é€šç”¨æ–‡æ¡£
```

### ç±»å‹ç‰¹å®šçš„æç¤ºå·¥ç¨‹

```python
def _initialize_prompts(self) -> None:
    """ä¸ºæ¯ç§æ–‡æ¡£ç±»å‹åˆå§‹åŒ–ä¸“é—¨çš„æç¤ºæ¨¡æ¿"""
    self.type_specific_prompts = {
        DocumentType.TECHNICAL: {
            'topics': """åˆ†æè¿™ä¸ªæŠ€æœ¯æ–‡æ¡£ï¼Œå…³æ³¨æ ¸å¿ƒç³»ç»Ÿç»„ä»¶å’Œå…³ç³»ã€‚
            
é¦–å…ˆï¼Œè¯†åˆ«æ„æˆå®Œæ•´ç‹¬ç«‹åŠŸèƒ½å•å…ƒçš„ä¸»è¦æ¶æ„æˆ–æŠ€æœ¯ç»„ä»¶ã€‚
æ¯ä¸ªç»„ä»¶åº”è¯¥ï¼š
- æ˜¯ç‹¬ç‰¹çš„æŠ€æœ¯ç³»ç»Ÿã€æ¨¡å—æˆ–æµç¨‹
- è¶³å¤Ÿç‹¬ç«‹ï¼Œå¯ä»¥å•ç‹¬ç†è§£
- å¯¹æ•´ä½“ç³»ç»ŸåŠŸèƒ½è‡³å…³é‡è¦
- ä¸è‡³å°‘ä¸€ä¸ªå…¶ä»–ç»„ä»¶ç›¸è¿

é¿å…ä»¥ä¸‹ä¸»é¢˜ï¼š
- è¿‡äºç»†ç²’åº¦ï¼ˆå®ç°ç»†èŠ‚ï¼‰
- è¿‡äºå®½æ³›ï¼ˆæ•´ä¸ªç³»ç»Ÿç±»åˆ«ï¼‰
- æ²¡æœ‰ç³»ç»Ÿå½±å“çš„å­¤ç«‹åŠŸèƒ½
- çº¯æ–‡æ¡£å…ƒç´ 

æ€è€ƒï¼š
1. æ ¸å¿ƒæ„å»ºå—æ˜¯ä»€ä¹ˆï¼Ÿ
2. è¿™äº›éƒ¨åˆ†å¦‚ä½•ç»„åˆï¼Ÿ
3. ç»„ä»¶ä¹‹é—´å­˜åœ¨ä»€ä¹ˆä¾èµ–å…³ç³»ï¼Ÿ
4. å…³é”®çš„æŠ€æœ¯è¾¹ç•Œæ˜¯ä»€ä¹ˆï¼Ÿ

æ ¼å¼ï¼šè¿”å›è¡¨ç¤ºæœ€é«˜çº§æŠ€æœ¯æ„å»ºå—çš„ç»„ä»¶åç§°çš„JSONæ•°ç»„ã€‚""",

            'subtopics': """å¯¹äºæŠ€æœ¯ç»„ä»¶'{topic}'ï¼Œè¯†åˆ«å…¶å…³é”®å­ç»„ä»¶å’Œæ¥å£ã€‚

æ¯ä¸ªå­ä¸»é¢˜åº”è¯¥ï¼š
- ä»£è¡¨æ­¤ç»„ä»¶çš„å…³é”®æ–¹é¢
- å…·æœ‰æ˜ç¡®çš„æŠ€æœ¯èŒè´£
- ä¸ç³»ç»Ÿå…¶ä»–éƒ¨åˆ†æ¥å£
- å¯¹ç»„ä»¶çš„æ ¸å¿ƒç›®çš„æœ‰è´¡çŒ®

è€ƒè™‘ï¼š
1. æ­¤ç»„ä»¶æš´éœ²ä»€ä¹ˆæ¥å£ï¼Ÿ
2. å®ƒçš„å†…éƒ¨å­ç³»ç»Ÿæ˜¯ä»€ä¹ˆï¼Ÿ
3. å®ƒå¦‚ä½•å¤„ç†æ•°æ®æˆ–å¤„ç†è¯·æ±‚ï¼Ÿ
4. å®ƒä¸ºå…¶ä»–ç»„ä»¶æä¾›ä»€ä¹ˆæœåŠ¡ï¼Ÿ
5. å®ƒå®ç°ä»€ä¹ˆæŠ€æœ¯æ ‡å‡†æˆ–åè®®ï¼Ÿ

æ ¼å¼ï¼šè¿”å›æ„æˆæ­¤ç»„ä»¶æ¶æ„çš„æŠ€æœ¯å­ä¸»é¢˜åç§°çš„JSONæ•°ç»„ã€‚""",

            'details': """å¯¹äºæŠ€æœ¯å­ä¸»é¢˜'{subtopic}'ï¼Œè¯†åˆ«å…·ä½“çš„å®ç°æ–¹é¢å’Œè¦æ±‚ã€‚

å…³æ³¨ï¼š
1. å…³é”®ç®—æ³•æˆ–æ–¹æ³•
2. æ•°æ®ç»“æ„å’Œæ ¼å¼
3. åè®®è§„èŒƒ
4. æ€§èƒ½ç‰¹å¾
5. é”™è¯¯å¤„ç†æ–¹æ³•
6. å®‰å…¨è€ƒè™‘
7. ä¾èµ–å…³ç³»å’Œè¦æ±‚

åŒ…æ‹¬å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼š
- å®ç°ç‰¹å®šçš„
- å¯æµ‹é‡æˆ–å¯æµ‹è¯•çš„
- å¯¹ç†è§£è‡³å…³é‡è¦çš„
- ä¸é›†æˆç›¸å…³çš„

æ ¼å¼ï¼šè¿”å›æŠ€æœ¯è§„èŒƒå’Œå®ç°ç»†èŠ‚çš„JSONæ•°ç»„ã€‚"""
        },
        
        DocumentType.SCIENTIFIC: {
            'topics': """åˆ†æè¿™ä¸ªç§‘å­¦æ–‡æ¡£ï¼Œå…³æ³¨ä¸»è¦ç ”ç©¶ç»„ä»¶å’Œæ–¹æ³•æ¡†æ¶ã€‚

è¯†åˆ«ä¸»è¦ç§‘å­¦ä¸»é¢˜ï¼š
- ä»£è¡¨å®Œæ•´çš„å®éªŒæˆ–ç†è®ºå•å…ƒ
- éµå¾ªç§‘å­¦æ–¹æ³•åŸåˆ™
- æ”¯æŒç ”ç©¶ç›®æ ‡
- åŸºäºæ—¢å®šçš„ç§‘å­¦æ¦‚å¿µ

è€ƒè™‘ï¼š
1. ä¸»è¦ç ”ç©¶é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. ä½¿ç”¨äº†ä»€ä¹ˆæ–¹æ³•è®ºæ–¹æ³•ï¼Ÿ
3. åº”ç”¨äº†ä»€ä¹ˆç†è®ºæ¡†æ¶ï¼Ÿ
4. å®æ–½äº†ä»€ä¹ˆå®éªŒè®¾è®¡ï¼Ÿ
5. ä¸åŒç ”ç©¶ç»„ä»¶å¦‚ä½•äº¤äº’ï¼Ÿ

æ ¼å¼ï¼šè¿”å›ä¸»è¦ç§‘å­¦ä¸»é¢˜æˆ–ç ”ç©¶ç»„ä»¶çš„JSONæ•°ç»„ã€‚""",
            # ... ç§‘å­¦æ–‡æ¡£çš„å…¶ä»–æç¤º
        },
        # ... å…¶ä»–æ–‡æ¡£ç±»å‹çš„æç¤º
    }
```

### æ™ºèƒ½ç±»å‹æ£€æµ‹ç®—æ³•

```python
async def detect_document_type(self, content: str, request_id: str) -> DocumentType:
    """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ–‡æ¡£ç±»å‹æ£€æµ‹"""
    
    detection_prompt = """ä½ æ­£åœ¨åˆ†æä¸€ä¸ªæ–‡æ¡£ä»¥ç¡®å®šå…¶ä¸»è¦ç±»å‹å’Œç»“æ„ã€‚
    
å„æ–‡æ¡£ç±»å‹çš„å…³é”®ç‰¹å¾ï¼š
- TECHNICAL: åŒ…å«ç³»ç»Ÿè§„èŒƒã€APIæ–‡æ¡£æˆ–å®ç°ç»†èŠ‚
- SCIENTIFIC: åŒ…å«ç ”ç©¶æ–¹æ³•ã€å®éªŒæ•°æ®æˆ–ç§‘å­¦åˆ†æ
- BUSINESS: åŒ…å«å•†ä¸šç­–ç•¥ã€å¸‚åœºåˆ†ææˆ–ç»„ç»‡è§„åˆ’
- ACADEMIC: åŒ…å«å­¦æœ¯ç†è®ºã€æ–‡çŒ®ç»¼è¿°æˆ–ç ”ç©¶è®ºæ–‡
- LEGAL: åŒ…å«æ³•å¾‹æ¡æ–‡ã€åˆåŒæˆ–æ³•è§„
- MEDICAL: åŒ…å«åŒ»å­¦è¯Šæ–­ã€æ²»ç–—æ–¹æ¡ˆæˆ–å¥åº·ä¿¡æ¯
- NARRATIVE: åŒ…å«æ•…äº‹æƒ…èŠ‚ã€äººç‰©å‘å±•æˆ–å™è¿°ç»“æ„
- INSTRUCTIONAL: åŒ…å«æ“ä½œæ­¥éª¤ã€å­¦ä¹ ææ–™æˆ–æŒ‡å¯¼æ‰‹å†Œ
- ANALYTICAL: åŒ…å«æ•°æ®åˆ†æã€ç»Ÿè®¡ç ”ç©¶æˆ–è¯„ä¼°æŠ¥å‘Š
- PROCEDURAL: åŒ…å«å·¥ä½œæµç¨‹ã€æ“ä½œè§„ç¨‹æˆ–æ ‡å‡†ç¨‹åº
- GENERAL: é€šç”¨å†…å®¹ï¼Œä¸ç¬¦åˆä»¥ä¸Šç‰¹å®šç±»åˆ«

ä»…è¿”å›æœ€åŒ¹é…çš„æ–‡æ¡£ç±»å‹åç§°ï¼ˆå¦‚ï¼šTECHNICALï¼‰ã€‚"""
    
    try:
        response = await self.optimizer.generate_completion(
            detection_prompt + f"\n\næ–‡æ¡£å†…å®¹é¢„è§ˆï¼š\n{content[:2000]}...", 
            max_tokens=50, 
            request_id=request_id,
            task="detecting_document_type"
        )
        
        # è§£æå“åº”å¹¶éªŒè¯ç±»å‹
        if response:
            detected_type = response.strip().upper()
            try:
                return DocumentType.from_str(detected_type)
            except ValueError:
                logger.warning(f"Unknown document type detected: {detected_type}")
                return DocumentType.GENERAL
        else:
            return DocumentType.GENERAL
            
    except Exception as e:
        logger.error(f"Error detecting document type: {str(e)}")
        return DocumentType.GENERAL
```

---

## ğŸ§  å†…å®¹æå–å¼•æ“

### åˆ†å±‚æå–ç­–ç•¥

```python
async def generate_mindmap(self, document_content: str, request_id: str) -> str:
    """ä¸‰å±‚åˆ†çº§æå–ï¼šä¸»é¢˜ â†’ å­ä¸»é¢˜ â†’ è¯¦ç»†ä¿¡æ¯"""
    
    # ç¬¬ä¸€å±‚ï¼šä¸»é¢˜æå–
    main_topics = await self._extract_main_topics(
        document_content, type_prompts['topics'], request_id
    )
    
    # ç¬¬äºŒå±‚ï¼šå­ä¸»é¢˜æå–
    for topic in main_topics:
        subtopics = await self._extract_subtopics(
            topic, document_content, type_prompts['subtopics'], request_id
        )
        topic['subtopics'] = subtopics
        
        # ç¬¬ä¸‰å±‚ï¼šè¯¦ç»†ä¿¡æ¯æå–
        for subtopic in subtopics:
            details = await self._extract_details(
                subtopic, document_content, details_prompt_template, request_id
            )
            subtopic['details'] = details
```

### æ™ºèƒ½åˆ†å—å¤„ç†

```python
async def _extract_main_topics(self, content: str, topics_prompt: str, request_id: str):
    """ä¸»é¢˜æå–çš„åˆ†å—å¹¶è¡Œå¤„ç†"""
    
    # è®¡ç®—æœ€ä¼˜åˆ†å—å¤§å°
    chunk_size = min(len(content) // 3, 15000)  # åŠ¨æ€åˆ†å—
    chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
    
    async def process_chunk(chunk: str, chunk_idx: int) -> List[Dict[str, Any]]:
        """å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡æ¡£å—"""
        chunk_prompt = f"{topics_prompt}\n\nåˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼š\n{chunk}"
        
        try:
            response = await self._retry_generate_completion(
                chunk_prompt, 3000, request_id, f"extracting_main_topics_chunk_{chunk_idx}"
            )
            
            if response:
                # è§£æå’ŒéªŒè¯å“åº”
                parsed_topics = self._parse_llm_response(response, "array")
                validated_topics = []
                
                for topic_data in parsed_topics:
                    if self._validate_topic(topic_data):
                        validated_topics.append({
                            'name': str(topic_data.get('name', '')).strip(),
                            'importance': str(topic_data.get('importance', 'medium')).lower(),
                            'chunk_source': chunk_idx
                        })
                
                return validated_topics
            
        except Exception as e:
            logger.warning(f"Error processing chunk {chunk_idx}: {str(e)}")
            
        return []
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰å—
    chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
    chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)
    
    # åˆå¹¶ç»“æœå¹¶å»é‡
    all_topics = []
    for result in chunk_results:
        if isinstance(result, list):
            all_topics.extend(result)
    
    return await self._consolidate_topics(all_topics, request_id)
```

### é«˜çº§JSONè§£æä¸æ¢å¤

```python
def _clean_json_response(self, response: str) -> str:
    """é«˜çº§JSONå“åº”æ¸…ç†ï¼Œæ”¯æŒå¤šç§å¼‚å¸¸æƒ…å†µæ¢å¤"""
    
    if not response or not isinstance(response, str):
        return "[]"
    
    def find_json_structure(text: str) -> Optional[str]:
        """æ™ºèƒ½æŸ¥æ‰¾JSONç»“æ„"""
        # æŸ¥æ‰¾æ•°ç»„æ¨¡å¼
        array_match = re.search(r'\[[\s\S]*?\](?=\s*$|\s*[,}\]])', text)
        if array_match:
            return array_match.group(0)
        
        # æŸ¥æ‰¾å¯¹è±¡æ¨¡å¼
        object_match = re.search(r'\{[\s\S]*?\}(?=\s*$|\s*[,\]])', text)
        if object_match:
            return object_match.group(0)
        
        return None
    
    def clean_characters(text: str) -> str:
        """æ¸…ç†æ§åˆ¶å­—ç¬¦ï¼Œä¿ç•™æœ‰æ•ˆç©ºæ ¼"""
        text = self.control_chars_regex.sub('', text)
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)  # é›¶å®½å­—ç¬¦
        return text
    
    def fix_json_syntax(text: str) -> str:
        """ä¿®å¤JSONè¯­æ³•é”™è¯¯"""
        # ä¿®å¤å°¾éšé€—å·
        text = re.sub(r',(\s*})', r'\1', text)
        text = re.sub(r',(\s*\])', r'\1', text)
        
        # ä¿®å¤å¤šé‡é€—å·
        text = re.sub(r',+', ',', text)
        
        # ä¿®å¤æœªè½¬ä¹‰çš„å¼•å·
        text = self.unescaped_quotes_regex.sub(r'\\"', text)
        
        # ä¿®å¤ç™¾åˆ†æ¯”æ ¼å¼
        text = self.percentage_regex1.sub(r'\1%', text)
        text = self.percentage_regex2.sub('%', text)
        
        return text
    
    # é€æ­¥æ¸…ç†å’Œæ¢å¤
    response = clean_characters(response)
    json_content = find_json_structure(response)
    
    if json_content:
        json_content = fix_json_syntax(json_content)
        
        try:
            # éªŒè¯JSONæœ‰æ•ˆæ€§
            json.loads(json_content)
            return json_content
        except json.JSONDecodeError:
            pass
    
    # æœ€åçš„æ¢å¤å°è¯•
    return self._attempt_json_recovery(response)
```

---

## ğŸ”„ æ™ºèƒ½å»é‡ç®—æ³•

### å¤šå±‚æ¬¡ç›¸ä¼¼åº¦æ£€æµ‹

```python
async def _batch_redundancy_check(self, items, content_type='topic', 
                                context_prefix='', batch_size=10):
    """æ‰¹é‡å†—ä½™æ£€æŸ¥ï¼Œæ”¯æŒå¹¶å‘ç›¸ä¼¼åº¦æ£€æµ‹"""
    
    if len(items) <= 1:
        return items
    
    # ç»„åˆæ‰€æœ‰é¡¹ç›®å¯¹è¿›è¡Œæ¯”è¾ƒ
    pairs_to_check = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            pairs_to_check.append((i, j))
    
    # åˆ†æ‰¹å¤„ç†ä»¥é¿å…APIé™åˆ¶
    similar_pairs = set()
    
    for batch_start in range(0, len(pairs_to_check), batch_size):
        batch = pairs_to_check[batch_start:batch_start + batch_size]
        
        async def check_pair(i, j):
            """æ£€æŸ¥ä¸¤ä¸ªé¡¹ç›®çš„ç›¸ä¼¼æ€§"""
            item1_name = items[i]['name']
            item2_name = items[j]['name']
            
            # é¦–å…ˆä½¿ç”¨å¿«é€Ÿå­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            quick_similarity = fuzz.ratio(item1_name, item2_name)
            threshold = self.config['similarity_threshold'][content_type]
            
            if quick_similarity >= threshold:
                # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦ç›¸ä¼¼åº¦æ£€æŸ¥
                context1 = f"{context_prefix}: {item1_name}" if context_prefix else item1_name
                context2 = f"{context_prefix}: {item2_name}" if context_prefix else item2_name
                
                is_similar = await self.check_similarity_llm(
                    item1_name, item2_name, context1, context2
                )
                
                if is_similar:
                    return (i, j)
            
            return None
        
        # å¹¶è¡Œæ£€æŸ¥è¿™æ‰¹å¯¹
        batch_tasks = [check_pair(i, j) for i, j in batch]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        for result in batch_results:
            if result and not isinstance(result, Exception):
                similar_pairs.add(result)
    
    # æ„å»ºå»é‡åçš„é¡¹ç›®åˆ—è¡¨
    items_to_remove = set()
    for i, j in similar_pairs:
        # ä¿ç•™é‡è¦æ€§æ›´é«˜çš„é¡¹ç›®
        importance_i = self._get_importance_value(items[i]['importance'])
        importance_j = self._get_importance_value(items[j]['importance'])
        
        if importance_i >= importance_j:
            items_to_remove.add(j)
        else:
            items_to_remove.add(i)
    
    # è¿”å›å»é‡åçš„é¡¹ç›®
    deduplicated_items = [
        items[i] for i in range(len(items)) 
        if i not in items_to_remove
    ]
    
    logger.info(f"Removed {len(items_to_remove)} redundant {content_type}s "
               f"from {len(items)} total items")
    
    return deduplicated_items
```

### LLMé©±åŠ¨çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹

```python
async def check_similarity_llm(self, text1: str, text2: str, 
                             context1: str, context2: str) -> bool:
    """ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹"""
    
    similarity_prompt = f"""æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ¦‚å¿µæ˜¯å¦è¡¨è¾¾ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„æƒ³æ³•ï¼š

æ¦‚å¿µ1: "{text1}"
ä¸Šä¸‹æ–‡1: {context1}

æ¦‚å¿µ2: "{text2}"  
ä¸Šä¸‹æ–‡2: {context2}

è¯„ä¼°æ ‡å‡†ï¼š
1. æ ¸å¿ƒå«ä¹‰æ˜¯å¦ç›¸åŒï¼Ÿ
2. æ‰€æŒ‡å‘çš„å¯¹è±¡/æ¦‚å¿µæ˜¯å¦ä¸€è‡´ï¼Ÿ
3. åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­æ˜¯å¦é‡å¤ï¼Ÿ
4. æ˜¯å¦å¯ä»¥åˆå¹¶è€Œä¸æŸå¤±é‡è¦ä¿¡æ¯ï¼Ÿ

å¦‚æœä¸¤ä¸ªæ¦‚å¿µé«˜åº¦ç›¸ä¼¼æˆ–é‡å¤ï¼Œå›ç­”"æ˜¯"ã€‚
å¦‚æœå®ƒä»¬æ˜¯ä¸åŒçš„æ¦‚å¿µï¼Œå³ä½¿ç›¸å…³ï¼Œä¹Ÿå›ç­”"å¦"ã€‚

å›ç­”ï¼š"""

    try:
        response = await self.optimizer.generate_completion(
            similarity_prompt, 
            max_tokens=50,
            task="checking_content_similarity"
        )
        
        if response:
            return response.strip().lower() in ['æ˜¯', 'yes', 'true', 'ç›¸ä¼¼', 'similar']
        
    except Exception as e:
        logger.warning(f"Error in LLM similarity check: {str(e)}")
    
    return False
```

---

## ğŸ’° æˆæœ¬æ§åˆ¶ç³»ç»Ÿ

### è¯¦ç»†çš„ä½¿ç”¨è¿½è¸ª

```python
class TokenUsageTracker:
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost = 0
        self.call_counts = {}
        
        # ä»»åŠ¡åˆ†ç±»ç”¨äºæ›´å¥½çš„æŠ¥å‘Š
        self.task_categories = {
            'topics': ['extracting_main_topics', 'consolidating_topics', 'detecting_document_type'],
            'subtopics': ['extracting_subtopics', 'consolidate_subtopics'],
            'details': ['extracting_details', 'consolidate_details'],
            'similarity': ['checking_content_similarity'],
            'verification': ['verifying_against_source'],
            'emoji': ['selecting_emoji']
        }
        
        # æŒ‰ç±»åˆ«åˆå§‹åŒ–è®¡æ•°å™¨
        self.call_counts_by_category = {category: 0 for category in self.task_categories}
        self.token_counts_by_category = {
            category: {'input': 0, 'output': 0} 
            for category in self.task_categories
        }
        self.cost_by_category = {category: 0 for category in self.task_categories}
```

### åŠ¨æ€æˆæœ¬è®¡ç®—

```python
def update(self, input_tokens: int, output_tokens: int, task: str):
    """æ ¹æ®ä¸åŒAPIæä¾›å•†è®¡ç®—ç²¾ç¡®æˆæœ¬"""
    
    task_cost = 0
    if Config.API_PROVIDER == "CLAUDE":
        task_cost = (
            input_tokens * Config.ANTHROPIC_INPUT_TOKEN_PRICE +
            output_tokens * Config.ANTHROPIC_OUTPUT_TOKEN_PRICE
        )
    elif Config.API_PROVIDER == "DEEPSEEK":
        # DeepSeekä¸åŒæ¨¡å‹æœ‰ä¸åŒå®šä»·
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            task_cost = (
                input_tokens * Config.DEEPSEEK_CHAT_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_CHAT_OUTPUT_PRICE
            )
        else:  # reasoneræ¨¡å‹
            task_cost = (
                input_tokens * Config.DEEPSEEK_REASONER_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_REASONER_OUTPUT_PRICE
            )
    elif Config.API_PROVIDER == "GEMINI":
        task_cost = (
            input_tokens * Config.GEMINI_INPUT_TOKEN_PRICE +
            output_tokens * Config.GEMINI_OUTPUT_TOKEN_PRICE
        )
    else:  # OPENAI
        task_cost = (
            input_tokens * Config.OPENAI_INPUT_TOKEN_PRICE +
            output_tokens * Config.OPENAI_OUTPUT_TOKEN_PRICE
        )
    
    self.total_cost += task_cost
    # ... æ›´æ–°å…¶ä»–ç»Ÿè®¡ä¿¡æ¯
```

### æ™ºèƒ½è°ƒç”¨é™åˆ¶

```python
# ç”Ÿæˆè¿‡ç¨‹ä¸­çš„LLMè°ƒç”¨é™åˆ¶
max_llm_calls = {
    'topics': 20,      # ä¸»é¢˜æå–è°ƒç”¨é™åˆ¶
    'subtopics': 30,   # å­ä¸»é¢˜æå–è°ƒç”¨é™åˆ¶
    'details': 40      # è¯¦ç»†ä¿¡æ¯æå–è°ƒç”¨é™åˆ¶
}

# åŠ¨æ€å­—æ•°é™åˆ¶
doc_words = len(document_content.split())
word_limit = min(doc_words * 0.9, 8000)  # æœ€å¤šå¤„ç†8000å­—

# å®æ—¶ç›‘æ§å’Œæ—©åœæœºåˆ¶
if self._llm_calls['topics'] >= max_llm_calls['topics']:
    logger.info("è¾¾åˆ°ä¸»é¢˜æå–LLMè°ƒç”¨é™åˆ¶")
    break

if current_word_count > word_limit * 0.95:
    logger.info(f"æ¥è¿‘å­—æ•°é™åˆ¶ {current_word_count}/{word_limit:.0f} å­—")
    break
```

---

## ğŸ¨ è¾“å‡ºæ ¼å¼ç”Ÿæˆ

### Mermaidè¯­æ³•ç”Ÿæˆ

```python
def _generate_mermaid_mindmap(self, concepts: Dict[str, Any]) -> str:
    """ç”Ÿæˆå®Œæ•´çš„Mermaidæ€ç»´å¯¼å›¾è¯­æ³•"""
    
    mindmap_lines = ["mindmap"]
    
    # æ ¹èŠ‚ç‚¹ï¼ˆæ–‡æ¡£emojiï¼‰
    self._add_node_to_mindmap({'name': ''}, mindmap_lines, indent_level=1)
    
    # æ·»åŠ æ‰€æœ‰ä¸»é¢˜åˆ°æ ¹èŠ‚ç‚¹ä¸‹
    for topic in concepts.get('central_theme', {}).get('subtopics', []):
        self._add_node_to_mindmap(topic, mindmap_lines, indent_level=2)
    
    return "\n".join(mindmap_lines)

def _add_node_to_mindmap(self, node: Dict[str, Any], mindmap_lines: List[str], 
                        indent_level: int) -> None:
    """é€’å½’æ·»åŠ èŠ‚ç‚¹åˆ°æ€ç»´å¯¼å›¾"""
    
    # æ ¹æ®å±‚çº§ç¡®å®šèŠ‚ç‚¹å½¢çŠ¶
    if indent_level == 1:  # æ ¹èŠ‚ç‚¹
        shape = NodeShape.ROOT
        node_line = f"    {shape.apply('ğŸ“„')}"
    elif indent_level == 2:  # ä¸»é¢˜
        shape = NodeShape.TOPIC
        emoji = node.get('emoji', '')
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{emoji} {node['name']}"
        node_line = f"        {shape.apply(text)}"
    elif indent_level == 3:  # å­ä¸»é¢˜
        shape = NodeShape.SUBTOPIC
        emoji = node.get('emoji', '')
        text = f"{emoji} {node['name']}"
        node_line = f"            {shape.apply(text)}"
    else:  # è¯¦ç»†ä¿¡æ¯
        shape = NodeShape.DETAIL
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{importance} {node['name']}"
        node_line = f"                {shape.apply(text)}"
    
    mindmap_lines.append(node_line)
    
    # é€’å½’å¤„ç†å­èŠ‚ç‚¹
    for subtopic in node.get('subtopics', []):
        self._add_node_to_mindmap(subtopic, mindmap_lines, indent_level + 1)
    
    for detail in node.get('details', []):
        self._add_node_to_mindmap(detail, mindmap_lines, indent_level + 1)
```

### HTMLå¯è§†åŒ–ç”Ÿæˆ

```python
def generate_mermaid_html(mermaid_code):
    """ç”Ÿæˆäº¤äº’å¼HTMLæ€ç»´å¯¼å›¾"""
    
    # åˆ›å»ºMermaid Live Editorçš„ç¼–è¾‘é“¾æ¥
    data = {
        "code": mermaid_code,
        "mermaid": {"theme": "default"}
    }
    json_string = json.dumps(data)
    compressed_data = zlib.compress(json_string.encode('utf-8'), level=9)
    base64_string = base64.urlsafe_b64encode(compressed_data).decode('utf-8').rstrip('=')
    edit_url = f'https://mermaid.live/edit#pako:{base64_string}'
    
    # ç”Ÿæˆå®Œæ•´çš„HTMLæ¨¡æ¿
    html_template = f'''<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Mermaid Mindmap</title>
  <!-- Tailwind CSS for styling -->
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <!-- Mermaid JS for rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.0/dist/mermaid.min.js"></script>
  <style>
    body {{ margin: 0; padding: 0; }}
    #mermaid {{
      width: 100%;
      height: calc(100vh - 64px);
      overflow: auto;
    }}
  </style>
</head>
<body class="bg-gray-100">
  <div class="flex items-center justify-between p-4 bg-white shadow">
    <h1 class="text-xl font-bold">Mermaid Mindmap</h1>
    <a href="{edit_url}" target="_blank" 
       class="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600">
       Edit in Mermaid Live Editor
    </a>
  </div>
  <div id="mermaid" class="p-4">
    <pre class="mermaid">
{mermaid_code}
    </pre>
  </div>
  <script>
    mermaid.initialize({{
      startOnLoad: true,
      securityLevel: 'loose',
      theme: 'default',
      mindmap: {{ useMaxWidth: true }},
      themeConfig: {{ controlBar: true }}
    }});
  </script>
</body>
</html>'''
    return html_template
```

### Markdownå¤§çº²è½¬æ¢

```python
def _convert_mindmap_to_markdown(self, mermaid_syntax: str) -> str:
    """å°†Mermaidè¯­æ³•è½¬æ¢ä¸ºMarkdownå¤§çº²"""
    
    markdown_lines = []
    lines = mermaid_syntax.split('\n')[1:]  # è·³è¿‡'mindmap'å¤´éƒ¨
    
    for line in lines:
        if not line.strip():
            continue
        
        # è®¡ç®—ç¼©è¿›çº§åˆ«
        indent_level = len(re.match(r'^\s*', line).group()) // 4
        content = line.strip()
        
        if indent_level == 1 and '((ğŸ“„))' in content:
            continue  # è·³è¿‡æ–‡æ¡£emojièŠ‚ç‚¹
        elif indent_level == 2:  # ä¸»é¢˜ -> H1
            node_text = re.search(r'\(\((.*?)\)\)', content)
            if node_text:
                markdown_lines.append(f"# {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 3:  # å­ä¸»é¢˜ -> H2
            node_text = re.search(r'\((.*?)\)', content)
            if node_text:
                markdown_lines.append(f"## {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 4:  # è¯¦ç»†ä¿¡æ¯ -> åˆ—è¡¨é¡¹
            node_text = re.search(r'\[(.*?)\]', content)
            if node_text:
                markdown_lines.append(node_text.group(1).strip())
                markdown_lines.append("")
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    markdown_text = "\n".join(markdown_lines)
    markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
    
    return markdown_text.strip()
```

---

## âš¡ é”™è¯¯å¤„ç†æœºåˆ¶

### æŒ‡æ•°é€€é¿é‡è¯•

```python
async def _retry_with_exponential_backoff(self, func, *args, **kwargs):
    """å¸¦æŠ–åŠ¨çš„æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶"""
    
    retries = 0
    max_retries = self.retry_config['max_retries']
    base_delay = self.retry_config['base_delay']
    max_delay = self.retry_config['max_delay']
    
    while retries < max_retries:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            retries += 1
            if retries >= max_retries:
                raise
            
            # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
            delay = min(base_delay * (2 ** (retries - 1)), max_delay)
            actual_delay = random.uniform(0, delay)
            
            logger.warning(f"å°è¯• {retries}/{max_retries} å¤±è´¥: {str(e)}. "
                         f"{actual_delay:.2f}ç§’åé‡è¯•")
            
            await asyncio.sleep(actual_delay)
```

### ä¼˜é›…é™çº§ç­–ç•¥

```python
try:
    # å°è¯•å®Œæ•´å¤„ç†
    main_topics = await self._extract_main_topics(...)
except Exception as e:
    logger.warning(f"å®Œæ•´ä¸»é¢˜æå–å¤±è´¥ï¼Œå°è¯•ç®€åŒ–å¤„ç†: {str(e)}")
    
    # é™çº§åˆ°æ›´ç®€å•çš„æ–¹æ³•
    main_topics = await self._extract_simple_topics(...)
    
    if not main_topics:
        # æœ€åçš„å…œåº•æ–¹æ¡ˆ
        main_topics = self._generate_default_topics(document_content)
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

```python
def __init__(self):
    # å¤šå±‚ç¼“å­˜
    self._content_cache = {}        # å†…å®¹ç¼“å­˜
    self._emoji_cache = {}          # emojiç¼“å­˜
    self._similarity_cache = {}     # ç›¸ä¼¼åº¦è®¡ç®—ç¼“å­˜
    
def _load_emoji_cache(self):
    """ä»ç£ç›˜åŠ è½½emojiç¼“å­˜"""
    try:
        if os.path.exists(self._emoji_file):
            with open(self._emoji_file, 'r', encoding='utf-8') as f:
                loaded_cache = json.load(f)
                # å°†å­—ç¬¦ä¸²é”®è½¬æ¢å›å…ƒç»„
                self._emoji_cache = {tuple(eval(k)): v for k, v in loaded_cache.items()}
                logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(self._emoji_cache)} ä¸ªemojiæ˜ å°„")
    except Exception as e:
        logger.warning(f"åŠ è½½emojiç¼“å­˜å¤±è´¥: {str(e)}")
        self._emoji_cache = {}
```

### å¹¶å‘å¤„ç†ä¼˜åŒ–

```python
# å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£å—
chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)

# å¹¶è¡Œç›¸ä¼¼åº¦æ£€æŸ¥
batch_tasks = [check_pair(i, j) for i, j in batch]
batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

# å¹¶è¡ŒéªŒè¯
verification_tasks = [verify_node_in_chunk(node, chunk) for chunk in doc_chunks]
verification_results = await asyncio.gather(*verification_tasks, return_exceptions=True)
```

### å†…å­˜ä¼˜åŒ–

```python
# æµå¼å¤„ç†å¤§æ–‡æ¡£
async def process_large_document(self, filepath: str):
    """æµå¼å¤„ç†å¤§æ–‡æ¡£ï¼Œé¿å…å†…å­˜æº¢å‡º"""
    
    async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
        chunk_size = 8192
        buffer = ""
        
        async for line in f:
            buffer += line
            if len(buffer) >= chunk_size:
                # å¤„ç†å½“å‰å—
                await self.process_chunk(buffer)
                buffer = ""
        
        # å¤„ç†å‰©ä½™å†…å®¹
        if buffer:
            await self.process_chunk(buffer)
```

---

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
# åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
generator = MindMapGenerator()

# ç”Ÿæˆæ€ç»´å¯¼å›¾
mindmap = await generator.generate_mindmap(document_content, request_id)

# ç”ŸæˆHTMLæ–‡ä»¶
html_output = generate_mermaid_html(mindmap)

# ä¿å­˜æ–‡ä»¶
with open('mindmap.html', 'w', encoding='utf-8') as f:
    f.write(html_output)
```

### é«˜çº§é…ç½®

```python
# è‡ªå®šä¹‰é…ç½®
generator = MindMapGenerator()
generator.config.update({
    'max_topics': 8,
    'similarity_threshold': {
        'topic': 80,
        'subtopic': 75,
        'detail': 70
    }
})

# å¤„ç†ç‰¹å®šæ–‡æ¡£ç±»å‹
doc_type = DocumentType.TECHNICAL
mindmap = await generator.generate_mindmap(
    document_content, 
    request_id,
    document_type=doc_type
)
```

---

## ğŸ¯ æ€»ç»“

è¿™ä¸ªæ€ç»´å¯¼å›¾ç”Ÿæˆå™¨å±•ç°äº†ç°ä»£AIåº”ç”¨å¼€å‘çš„æœ€ä½³å®è·µï¼š

1. **æ¨¡å—åŒ–æ¶æ„**: æ¸…æ™°çš„ç»„ä»¶åˆ†ç¦»å’ŒèŒè´£åˆ’åˆ†
2. **æ™ºèƒ½å¤„ç†**: åŸºäºLLMçš„æ–‡æ¡£ç†è§£å’Œå†…å®¹æå–
3. **æ€§èƒ½ä¼˜åŒ–**: å¹¶å‘å¤„ç†ã€æ™ºèƒ½ç¼“å­˜å’Œå†…å­˜ç®¡ç†
4. **é”™è¯¯æ¢å¤**: å¤šå±‚æ¬¡çš„é”™è¯¯å¤„ç†å’Œä¼˜é›…é™çº§
5. **æˆæœ¬æ§åˆ¶**: ç²¾ç¡®çš„ä½¿ç”¨è¿½è¸ªå’Œèµ„æºç®¡ç†
6. **å¤šæ ¼å¼è¾“å‡º**: æ”¯æŒHTMLã€Markdownå’ŒMermaidæ ¼å¼
7. **å¯æ‰©å±•æ€§**: æ”¯æŒå¤šç§APIæä¾›å•†å’Œæ–‡æ¡£ç±»å‹

è¯¥é¡¹ç›®ä¸ä»…æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„æ€ç»´å¯¼å›¾ç”Ÿæˆå·¥å…·ï¼Œæ›´æ˜¯å­¦ä¹ ç°ä»£Pythonå¼‚æ­¥ç¼–ç¨‹ã€AIé›†æˆå’Œç³»ç»Ÿè®¾è®¡çš„ä¼˜ç§€æ¡ˆä¾‹ã€‚ 