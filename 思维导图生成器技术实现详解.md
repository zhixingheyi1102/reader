# 思维导图生成器技术实现详解

## 🏗️ 项目概述

本项目是一个基于大语言模型的智能思维导图生成器，能够自动分析文档内容并生成结构化的思维导图。系统采用现代异步架构，支持多种API提供商，具有强大的内容分析、去重和可视化能力。

## 📚 目录

1. [核心架构设计](#核心架构设计)
2. [配置系统](#配置系统)
3. [多API提供商支持](#多api提供商支持)
4. [文档类型检测](#文档类型检测)
5. [内容提取引擎](#内容提取引擎)
6. [智能去重算法](#智能去重算法)
7. [成本控制系统](#成本控制系统)
8. [输出格式生成](#输出格式生成)
9. [错误处理机制](#错误处理机制)
10. [性能优化策略](#性能优化策略)

---

## 🎯 核心架构设计

### 主要组件结构

```python
# 核心类继承关系和职责分工
class Config:
    """统一配置管理中心"""
    
class TokenUsageTracker:
    """成本和使用情况追踪"""
    
class DocumentOptimizer:
    """文档优化和API调用管理"""
    
class MindMapGenerator:
    """核心思维导图生成逻辑"""
    
class MinimalDatabaseStub:
    """轻量级数据存储接口"""
```

### 数据流架构

```
文档输入 → 类型检测 → 主题提取 → 子主题提取 → 详细信息提取 → 去重验证 → 格式生成 → 输出
    ↓         ↓         ↓          ↓            ↓          ↓        ↓
  缓存层   提示工程   并发处理    智能分块     相似度检测   验证层   多格式
```

---

## ⚙️ 配置系统

### 环境变量管理

```python
from decouple import Config as DecoupleConfig, RepositoryEnv

# 配置文件加载机制
config = DecoupleConfig(RepositoryEnv('.env'))

class Config:
    # API配置 - 支持多提供商
    OPENAI_API_KEY = config.get("OPENAI_API_KEY")
    OPENAI_BASE_URL = config.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
    ANTHROPIC_API_KEY = config.get('ANTHROPIC_API_KEY')
    DEEPSEEK_API_KEY = config.get('DEEPSEEK_API_KEY')
    GEMINI_API_KEY = config.get('GEMINI_API_KEY')
    API_PROVIDER = config.get('API_PROVIDER')  # 动态选择提供商
    
    # 模型配置 - 针对不同提供商优化
    CLAUDE_MODEL_STRING = "claude-3-5-haiku-latest"
    OPENAI_COMPLETION_MODEL = "gpt-4o-mini-2024-07-18"
    DEEPSEEK_COMPLETION_MODEL = "deepseek-chat"
    GEMINI_MODEL_STRING = "gemini-2.0-flash-lite"
    
    # 成本控制 - 精确到微美元
    OPENAI_INPUT_TOKEN_PRICE = 0.15/1000000
    OPENAI_OUTPUT_TOKEN_PRICE = 0.60/1000000
    # ... 其他提供商价格配置
```

### 动态配置系统

```python
def __init__(self):
    self.config = {
        'max_summary_length': 2500,
        'max_tokens': 3000,
        'max_topics': 6,        # 最大主题数
        'max_subtopics': 4,     # 每个主题最大子主题数
        'max_details': 8,       # 每个子主题最大详细信息数
        'similarity_threshold': {
            'topic': 75,        # 主题相似度阈值
            'subtopic': 70,     # 子主题相似度阈值
            'detail': 65        # 详细信息相似度阈值
        },
        'reality_check': {
            'batch_size': 8,    # 并行验证批次大小
            'min_verified_topics': 4,
            'min_verified_ratio': 0.6
        }
    }
```

---

## 🌐 多API提供商支持

### 统一客户端管理

```python
class DocumentOptimizer:
    def __init__(self):
        # 初始化多个API客户端
        self.openai_client = AsyncOpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL  # 支持硅基流动等兼容服务
        )
        self.anthropic_client = AsyncAnthropic(api_key=Config.ANTHROPIC_API_KEY)
        self.deepseek_client = AsyncOpenAI(
            api_key=Config.DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        self.gemini_client = genai.configure(api_key=Config.GEMINI_API_KEY)
```

### 智能API路由

```python
async def generate_completion(self, prompt: str, max_tokens: int = 5000, 
                            request_id: str = None, task: Optional[str] = None):
    """统一的API调用接口，根据配置自动路由到相应提供商"""
    
    if Config.API_PROVIDER == "CLAUDE":
        kwargs = {
            "model": Config.CLAUDE_MODEL_STRING,
            "max_tokens": min(max_tokens, Config.CLAUDE_MAX_TOKENS),
            "messages": [{"role": "user", "content": prompt}]
        }
        response = await self.anthropic_client.messages.create(**kwargs)
        return response.content[0].text
        
    elif Config.API_PROVIDER == "DEEPSEEK":
        kwargs = {
            "model": Config.DEEPSEEK_COMPLETION_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": min(max_tokens, Config.DEEPSEEK_MAX_TOKENS)
        }
        
        # 支持不同DeepSeek模型
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            response = await self.deepseek_client.chat.completions.create(**kwargs)
        else:  # reasoner模型
            response = await self.deepseek_client.chat.completions.create(**kwargs)
            
        return response.choices[0].message.content
        
    # ... 其他提供商实现
```

---

## 🔍 文档类型检测

### 支持的文档类型枚举

```python
class DocumentType(Enum):
    """支持的文档类型，每种类型对应不同的分析策略"""
    TECHNICAL = auto()      # 技术文档
    SCIENTIFIC = auto()     # 科学研究
    NARRATIVE = auto()      # 叙述文档
    BUSINESS = auto()       # 商业文档
    ACADEMIC = auto()       # 学术论文
    LEGAL = auto()          # 法律文档
    MEDICAL = auto()        # 医学文档
    INSTRUCTIONAL = auto()  # 指导手册
    ANALYTICAL = auto()     # 分析报告
    PROCEDURAL = auto()     # 流程文档
    GENERAL = auto()        # 通用文档
```

### 类型特定的提示工程

```python
def _initialize_prompts(self) -> None:
    """为每种文档类型初始化专门的提示模板"""
    self.type_specific_prompts = {
        DocumentType.TECHNICAL: {
            'topics': """分析这个技术文档，关注核心系统组件和关系。
            
首先，识别构成完整独立功能单元的主要架构或技术组件。
每个组件应该：
- 是独特的技术系统、模块或流程
- 足够独立，可以单独理解
- 对整体系统功能至关重要
- 与至少一个其他组件相连

避免以下主题：
- 过于细粒度（实现细节）
- 过于宽泛（整个系统类别）
- 没有系统影响的孤立功能
- 纯文档元素

思考：
1. 核心构建块是什么？
2. 这些部分如何组合？
3. 组件之间存在什么依赖关系？
4. 关键的技术边界是什么？

格式：返回表示最高级技术构建块的组件名称的JSON数组。""",

            'subtopics': """对于技术组件'{topic}'，识别其关键子组件和接口。

每个子主题应该：
- 代表此组件的关键方面
- 具有明确的技术职责
- 与系统其他部分接口
- 对组件的核心目的有贡献

考虑：
1. 此组件暴露什么接口？
2. 它的内部子系统是什么？
3. 它如何处理数据或处理请求？
4. 它为其他组件提供什么服务？
5. 它实现什么技术标准或协议？

格式：返回构成此组件架构的技术子主题名称的JSON数组。""",

            'details': """对于技术子主题'{subtopic}'，识别具体的实现方面和要求。

关注：
1. 关键算法或方法
2. 数据结构和格式
3. 协议规范
4. 性能特征
5. 错误处理方法
6. 安全考虑
7. 依赖关系和要求

包括具体的技术细节：
- 实现特定的
- 可测量或可测试的
- 对理解至关重要的
- 与集成相关的

格式：返回技术规范和实现细节的JSON数组。"""
        },
        
        DocumentType.SCIENTIFIC: {
            'topics': """分析这个科学文档，关注主要研究组件和方法框架。

识别主要科学主题：
- 代表完整的实验或理论单元
- 遵循科学方法原则
- 支持研究目标
- 基于既定的科学概念

考虑：
1. 主要研究问题是什么？
2. 使用了什么方法论方法？
3. 应用了什么理论框架？
4. 实施了什么实验设计？
5. 不同研究组件如何交互？

格式：返回主要科学主题或研究组件的JSON数组。""",
            # ... 科学文档的其他提示
        },
        # ... 其他文档类型的提示
    }
```

### 智能类型检测算法

```python
async def detect_document_type(self, content: str, request_id: str) -> DocumentType:
    """使用LLM进行智能文档类型检测"""
    
    detection_prompt = """你正在分析一个文档以确定其主要类型和结构。
    
各文档类型的关键特征：
- TECHNICAL: 包含系统规范、API文档或实现细节
- SCIENTIFIC: 包含研究方法、实验数据或科学分析
- BUSINESS: 包含商业策略、市场分析或组织规划
- ACADEMIC: 包含学术理论、文献综述或研究论文
- LEGAL: 包含法律条文、合同或法规
- MEDICAL: 包含医学诊断、治疗方案或健康信息
- NARRATIVE: 包含故事情节、人物发展或叙述结构
- INSTRUCTIONAL: 包含操作步骤、学习材料或指导手册
- ANALYTICAL: 包含数据分析、统计研究或评估报告
- PROCEDURAL: 包含工作流程、操作规程或标准程序
- GENERAL: 通用内容，不符合以上特定类别

仅返回最匹配的文档类型名称（如：TECHNICAL）。"""
    
    try:
        response = await self.optimizer.generate_completion(
            detection_prompt + f"\n\n文档内容预览：\n{content[:2000]}...", 
            max_tokens=50, 
            request_id=request_id,
            task="detecting_document_type"
        )
        
        # 解析响应并验证类型
        if response:
            detected_type = response.strip().upper()
            try:
                return DocumentType.from_str(detected_type)
            except ValueError:
                logger.warning(f"Unknown document type detected: {detected_type}")
                return DocumentType.GENERAL
        else:
            return DocumentType.GENERAL
            
    except Exception as e:
        logger.error(f"Error detecting document type: {str(e)}")
        return DocumentType.GENERAL
```

---

## 🧠 内容提取引擎

### 分层提取策略

```python
async def generate_mindmap(self, document_content: str, request_id: str) -> str:
    """三层分级提取：主题 → 子主题 → 详细信息"""
    
    # 第一层：主题提取
    main_topics = await self._extract_main_topics(
        document_content, type_prompts['topics'], request_id
    )
    
    # 第二层：子主题提取
    for topic in main_topics:
        subtopics = await self._extract_subtopics(
            topic, document_content, type_prompts['subtopics'], request_id
        )
        topic['subtopics'] = subtopics
        
        # 第三层：详细信息提取
        for subtopic in subtopics:
            details = await self._extract_details(
                subtopic, document_content, details_prompt_template, request_id
            )
            subtopic['details'] = details
```

### 智能分块处理

```python
async def _extract_main_topics(self, content: str, topics_prompt: str, request_id: str):
    """主题提取的分块并行处理"""
    
    # 计算最优分块大小
    chunk_size = min(len(content) // 3, 15000)  # 动态分块
    chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
    
    async def process_chunk(chunk: str, chunk_idx: int) -> List[Dict[str, Any]]:
        """并行处理每个文档块"""
        chunk_prompt = f"{topics_prompt}\n\n分析以下文档内容：\n{chunk}"
        
        try:
            response = await self._retry_generate_completion(
                chunk_prompt, 3000, request_id, f"extracting_main_topics_chunk_{chunk_idx}"
            )
            
            if response:
                # 解析和验证响应
                parsed_topics = self._parse_llm_response(response, "array")
                validated_topics = []
                
                for topic_data in parsed_topics:
                    if self._validate_topic(topic_data):
                        validated_topics.append({
                            'name': str(topic_data.get('name', '')).strip(),
                            'importance': str(topic_data.get('importance', 'medium')).lower(),
                            'chunk_source': chunk_idx
                        })
                
                return validated_topics
            
        except Exception as e:
            logger.warning(f"Error processing chunk {chunk_idx}: {str(e)}")
            
        return []
    
    # 并行处理所有块
    chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
    chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)
    
    # 合并结果并去重
    all_topics = []
    for result in chunk_results:
        if isinstance(result, list):
            all_topics.extend(result)
    
    return await self._consolidate_topics(all_topics, request_id)
```

### 高级JSON解析与恢复

```python
def _clean_json_response(self, response: str) -> str:
    """高级JSON响应清理，支持多种异常情况恢复"""
    
    if not response or not isinstance(response, str):
        return "[]"
    
    def find_json_structure(text: str) -> Optional[str]:
        """智能查找JSON结构"""
        # 查找数组模式
        array_match = re.search(r'\[[\s\S]*?\](?=\s*$|\s*[,}\]])', text)
        if array_match:
            return array_match.group(0)
        
        # 查找对象模式
        object_match = re.search(r'\{[\s\S]*?\}(?=\s*$|\s*[,\]])', text)
        if object_match:
            return object_match.group(0)
        
        return None
    
    def clean_characters(text: str) -> str:
        """清理控制字符，保留有效空格"""
        text = self.control_chars_regex.sub('', text)
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)  # 零宽字符
        return text
    
    def fix_json_syntax(text: str) -> str:
        """修复JSON语法错误"""
        # 修复尾随逗号
        text = re.sub(r',(\s*})', r'\1', text)
        text = re.sub(r',(\s*\])', r'\1', text)
        
        # 修复多重逗号
        text = re.sub(r',+', ',', text)
        
        # 修复未转义的引号
        text = self.unescaped_quotes_regex.sub(r'\\"', text)
        
        # 修复百分比格式
        text = self.percentage_regex1.sub(r'\1%', text)
        text = self.percentage_regex2.sub('%', text)
        
        return text
    
    # 逐步清理和恢复
    response = clean_characters(response)
    json_content = find_json_structure(response)
    
    if json_content:
        json_content = fix_json_syntax(json_content)
        
        try:
            # 验证JSON有效性
            json.loads(json_content)
            return json_content
        except json.JSONDecodeError:
            pass
    
    # 最后的恢复尝试
    return self._attempt_json_recovery(response)
```

---

## 🔄 智能去重算法

### 多层次相似度检测

```python
async def _batch_redundancy_check(self, items, content_type='topic', 
                                context_prefix='', batch_size=10):
    """批量冗余检查，支持并发相似度检测"""
    
    if len(items) <= 1:
        return items
    
    # 组合所有项目对进行比较
    pairs_to_check = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            pairs_to_check.append((i, j))
    
    # 分批处理以避免API限制
    similar_pairs = set()
    
    for batch_start in range(0, len(pairs_to_check), batch_size):
        batch = pairs_to_check[batch_start:batch_start + batch_size]
        
        async def check_pair(i, j):
            """检查两个项目的相似性"""
            item1_name = items[i]['name']
            item2_name = items[j]['name']
            
            # 首先使用快速字符串相似度
            quick_similarity = fuzz.ratio(item1_name, item2_name)
            threshold = self.config['similarity_threshold'][content_type]
            
            if quick_similarity >= threshold:
                # 使用LLM进行深度相似度检查
                context1 = f"{context_prefix}: {item1_name}" if context_prefix else item1_name
                context2 = f"{context_prefix}: {item2_name}" if context_prefix else item2_name
                
                is_similar = await self.check_similarity_llm(
                    item1_name, item2_name, context1, context2
                )
                
                if is_similar:
                    return (i, j)
            
            return None
        
        # 并行检查这批对
        batch_tasks = [check_pair(i, j) for i, j in batch]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        for result in batch_results:
            if result and not isinstance(result, Exception):
                similar_pairs.add(result)
    
    # 构建去重后的项目列表
    items_to_remove = set()
    for i, j in similar_pairs:
        # 保留重要性更高的项目
        importance_i = self._get_importance_value(items[i]['importance'])
        importance_j = self._get_importance_value(items[j]['importance'])
        
        if importance_i >= importance_j:
            items_to_remove.add(j)
        else:
            items_to_remove.add(i)
    
    # 返回去重后的项目
    deduplicated_items = [
        items[i] for i in range(len(items)) 
        if i not in items_to_remove
    ]
    
    logger.info(f"Removed {len(items_to_remove)} redundant {content_type}s "
               f"from {len(items)} total items")
    
    return deduplicated_items
```

### LLM驱动的语义相似度检测

```python
async def check_similarity_llm(self, text1: str, text2: str, 
                             context1: str, context2: str) -> bool:
    """使用LLM进行深度语义相似度检测"""
    
    similarity_prompt = f"""比较以下两个概念是否表达相同或高度相似的想法：

概念1: "{text1}"
上下文1: {context1}

概念2: "{text2}"  
上下文2: {context2}

评估标准：
1. 核心含义是否相同？
2. 所指向的对象/概念是否一致？
3. 在给定上下文中是否重复？
4. 是否可以合并而不损失重要信息？

如果两个概念高度相似或重复，回答"是"。
如果它们是不同的概念，即使相关，也回答"否"。

回答："""

    try:
        response = await self.optimizer.generate_completion(
            similarity_prompt, 
            max_tokens=50,
            task="checking_content_similarity"
        )
        
        if response:
            return response.strip().lower() in ['是', 'yes', 'true', '相似', 'similar']
        
    except Exception as e:
        logger.warning(f"Error in LLM similarity check: {str(e)}")
    
    return False
```

---

## 💰 成本控制系统

### 详细的使用追踪

```python
class TokenUsageTracker:
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost = 0
        self.call_counts = {}
        
        # 任务分类用于更好的报告
        self.task_categories = {
            'topics': ['extracting_main_topics', 'consolidating_topics', 'detecting_document_type'],
            'subtopics': ['extracting_subtopics', 'consolidate_subtopics'],
            'details': ['extracting_details', 'consolidate_details'],
            'similarity': ['checking_content_similarity'],
            'verification': ['verifying_against_source'],
            'emoji': ['selecting_emoji']
        }
        
        # 按类别初始化计数器
        self.call_counts_by_category = {category: 0 for category in self.task_categories}
        self.token_counts_by_category = {
            category: {'input': 0, 'output': 0} 
            for category in self.task_categories
        }
        self.cost_by_category = {category: 0 for category in self.task_categories}
```

### 动态成本计算

```python
def update(self, input_tokens: int, output_tokens: int, task: str):
    """根据不同API提供商计算精确成本"""
    
    task_cost = 0
    if Config.API_PROVIDER == "CLAUDE":
        task_cost = (
            input_tokens * Config.ANTHROPIC_INPUT_TOKEN_PRICE +
            output_tokens * Config.ANTHROPIC_OUTPUT_TOKEN_PRICE
        )
    elif Config.API_PROVIDER == "DEEPSEEK":
        # DeepSeek不同模型有不同定价
        if Config.DEEPSEEK_COMPLETION_MODEL == Config.DEEPSEEK_CHAT_MODEL:
            task_cost = (
                input_tokens * Config.DEEPSEEK_CHAT_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_CHAT_OUTPUT_PRICE
            )
        else:  # reasoner模型
            task_cost = (
                input_tokens * Config.DEEPSEEK_REASONER_INPUT_PRICE +
                output_tokens * Config.DEEPSEEK_REASONER_OUTPUT_PRICE
            )
    elif Config.API_PROVIDER == "GEMINI":
        task_cost = (
            input_tokens * Config.GEMINI_INPUT_TOKEN_PRICE +
            output_tokens * Config.GEMINI_OUTPUT_TOKEN_PRICE
        )
    else:  # OPENAI
        task_cost = (
            input_tokens * Config.OPENAI_INPUT_TOKEN_PRICE +
            output_tokens * Config.OPENAI_OUTPUT_TOKEN_PRICE
        )
    
    self.total_cost += task_cost
    # ... 更新其他统计信息
```

### 智能调用限制

```python
# 生成过程中的LLM调用限制
max_llm_calls = {
    'topics': 20,      # 主题提取调用限制
    'subtopics': 30,   # 子主题提取调用限制
    'details': 40      # 详细信息提取调用限制
}

# 动态字数限制
doc_words = len(document_content.split())
word_limit = min(doc_words * 0.9, 8000)  # 最多处理8000字

# 实时监控和早停机制
if self._llm_calls['topics'] >= max_llm_calls['topics']:
    logger.info("达到主题提取LLM调用限制")
    break

if current_word_count > word_limit * 0.95:
    logger.info(f"接近字数限制 {current_word_count}/{word_limit:.0f} 字")
    break
```

---

## 🎨 输出格式生成

### Mermaid语法生成

```python
def _generate_mermaid_mindmap(self, concepts: Dict[str, Any]) -> str:
    """生成完整的Mermaid思维导图语法"""
    
    mindmap_lines = ["mindmap"]
    
    # 根节点（文档emoji）
    self._add_node_to_mindmap({'name': ''}, mindmap_lines, indent_level=1)
    
    # 添加所有主题到根节点下
    for topic in concepts.get('central_theme', {}).get('subtopics', []):
        self._add_node_to_mindmap(topic, mindmap_lines, indent_level=2)
    
    return "\n".join(mindmap_lines)

def _add_node_to_mindmap(self, node: Dict[str, Any], mindmap_lines: List[str], 
                        indent_level: int) -> None:
    """递归添加节点到思维导图"""
    
    # 根据层级确定节点形状
    if indent_level == 1:  # 根节点
        shape = NodeShape.ROOT
        node_line = f"    {shape.apply('📄')}"
    elif indent_level == 2:  # 主题
        shape = NodeShape.TOPIC
        emoji = node.get('emoji', '')
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{emoji} {node['name']}"
        node_line = f"        {shape.apply(text)}"
    elif indent_level == 3:  # 子主题
        shape = NodeShape.SUBTOPIC
        emoji = node.get('emoji', '')
        text = f"{emoji} {node['name']}"
        node_line = f"            {shape.apply(text)}"
    else:  # 详细信息
        shape = NodeShape.DETAIL
        importance = self._get_importance_marker(node.get('importance', 'medium'))
        text = f"{importance} {node['name']}"
        node_line = f"                {shape.apply(text)}"
    
    mindmap_lines.append(node_line)
    
    # 递归处理子节点
    for subtopic in node.get('subtopics', []):
        self._add_node_to_mindmap(subtopic, mindmap_lines, indent_level + 1)
    
    for detail in node.get('details', []):
        self._add_node_to_mindmap(detail, mindmap_lines, indent_level + 1)
```

### HTML可视化生成

```python
def generate_mermaid_html(mermaid_code):
    """生成交互式HTML思维导图"""
    
    # 创建Mermaid Live Editor的编辑链接
    data = {
        "code": mermaid_code,
        "mermaid": {"theme": "default"}
    }
    json_string = json.dumps(data)
    compressed_data = zlib.compress(json_string.encode('utf-8'), level=9)
    base64_string = base64.urlsafe_b64encode(compressed_data).decode('utf-8').rstrip('=')
    edit_url = f'https://mermaid.live/edit#pako:{base64_string}'
    
    # 生成完整的HTML模板
    html_template = f'''<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Mermaid Mindmap</title>
  <!-- Tailwind CSS for styling -->
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <!-- Mermaid JS for rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.0/dist/mermaid.min.js"></script>
  <style>
    body {{ margin: 0; padding: 0; }}
    #mermaid {{
      width: 100%;
      height: calc(100vh - 64px);
      overflow: auto;
    }}
  </style>
</head>
<body class="bg-gray-100">
  <div class="flex items-center justify-between p-4 bg-white shadow">
    <h1 class="text-xl font-bold">Mermaid Mindmap</h1>
    <a href="{edit_url}" target="_blank" 
       class="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600">
       Edit in Mermaid Live Editor
    </a>
  </div>
  <div id="mermaid" class="p-4">
    <pre class="mermaid">
{mermaid_code}
    </pre>
  </div>
  <script>
    mermaid.initialize({{
      startOnLoad: true,
      securityLevel: 'loose',
      theme: 'default',
      mindmap: {{ useMaxWidth: true }},
      themeConfig: {{ controlBar: true }}
    }});
  </script>
</body>
</html>'''
    return html_template
```

### Markdown大纲转换

```python
def _convert_mindmap_to_markdown(self, mermaid_syntax: str) -> str:
    """将Mermaid语法转换为Markdown大纲"""
    
    markdown_lines = []
    lines = mermaid_syntax.split('\n')[1:]  # 跳过'mindmap'头部
    
    for line in lines:
        if not line.strip():
            continue
        
        # 计算缩进级别
        indent_level = len(re.match(r'^\s*', line).group()) // 4
        content = line.strip()
        
        if indent_level == 1 and '((📄))' in content:
            continue  # 跳过文档emoji节点
        elif indent_level == 2:  # 主题 -> H1
            node_text = re.search(r'\(\((.*?)\)\)', content)
            if node_text:
                markdown_lines.append(f"# {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 3:  # 子主题 -> H2
            node_text = re.search(r'\((.*?)\)', content)
            if node_text:
                markdown_lines.append(f"## {node_text.group(1).strip()}")
                markdown_lines.append("")
        elif indent_level == 4:  # 详细信息 -> 列表项
            node_text = re.search(r'\[(.*?)\]', content)
            if node_text:
                markdown_lines.append(node_text.group(1).strip())
                markdown_lines.append("")
    
    # 清理多余的空行
    markdown_text = "\n".join(markdown_lines)
    markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
    
    return markdown_text.strip()
```

---

## ⚡ 错误处理机制

### 指数退避重试

```python
async def _retry_with_exponential_backoff(self, func, *args, **kwargs):
    """带抖动的指数退避重试机制"""
    
    retries = 0
    max_retries = self.retry_config['max_retries']
    base_delay = self.retry_config['base_delay']
    max_delay = self.retry_config['max_delay']
    
    while retries < max_retries:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            retries += 1
            if retries >= max_retries:
                raise
            
            # 指数退避 + 随机抖动
            delay = min(base_delay * (2 ** (retries - 1)), max_delay)
            actual_delay = random.uniform(0, delay)
            
            logger.warning(f"尝试 {retries}/{max_retries} 失败: {str(e)}. "
                         f"{actual_delay:.2f}秒后重试")
            
            await asyncio.sleep(actual_delay)
```

### 优雅降级策略

```python
try:
    # 尝试完整处理
    main_topics = await self._extract_main_topics(...)
except Exception as e:
    logger.warning(f"完整主题提取失败，尝试简化处理: {str(e)}")
    
    # 降级到更简单的方法
    main_topics = await self._extract_simple_topics(...)
    
    if not main_topics:
        # 最后的兜底方案
        main_topics = self._generate_default_topics(document_content)
```

---

## 🚀 性能优化策略

### 智能缓存系统

```python
def __init__(self):
    # 多层缓存
    self._content_cache = {}        # 内容缓存
    self._emoji_cache = {}          # emoji缓存
    self._similarity_cache = {}     # 相似度计算缓存
    
def _load_emoji_cache(self):
    """从磁盘加载emoji缓存"""
    try:
        if os.path.exists(self._emoji_file):
            with open(self._emoji_file, 'r', encoding='utf-8') as f:
                loaded_cache = json.load(f)
                # 将字符串键转换回元组
                self._emoji_cache = {tuple(eval(k)): v for k, v in loaded_cache.items()}
                logger.info(f"从缓存加载了 {len(self._emoji_cache)} 个emoji映射")
    except Exception as e:
        logger.warning(f"加载emoji缓存失败: {str(e)}")
        self._emoji_cache = {}
```

### 并发处理优化

```python
# 并行处理多个文档块
chunk_tasks = [process_chunk(chunk, i) for i, chunk in enumerate(chunks)]
chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)

# 并行相似度检查
batch_tasks = [check_pair(i, j) for i, j in batch]
batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

# 并行验证
verification_tasks = [verify_node_in_chunk(node, chunk) for chunk in doc_chunks]
verification_results = await asyncio.gather(*verification_tasks, return_exceptions=True)
```

### 内存优化

```python
# 流式处理大文档
async def process_large_document(self, filepath: str):
    """流式处理大文档，避免内存溢出"""
    
    async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
        chunk_size = 8192
        buffer = ""
        
        async for line in f:
            buffer += line
            if len(buffer) >= chunk_size:
                # 处理当前块
                await self.process_chunk(buffer)
                buffer = ""
        
        # 处理剩余内容
        if buffer:
            await self.process_chunk(buffer)
```

---

## 📊 使用示例

### 基础使用

```python
# 创建生成器实例
generator = MindMapGenerator()

# 生成思维导图
mindmap = await generator.generate_mindmap(document_content, request_id)

# 生成HTML文件
html_output = generate_mermaid_html(mindmap)

# 保存文件
with open('mindmap.html', 'w', encoding='utf-8') as f:
    f.write(html_output)
```

### 高级配置

```python
# 自定义配置
generator = MindMapGenerator()
generator.config.update({
    'max_topics': 8,
    'similarity_threshold': {
        'topic': 80,
        'subtopic': 75,
        'detail': 70
    }
})

# 处理特定文档类型
doc_type = DocumentType.TECHNICAL
mindmap = await generator.generate_mindmap(
    document_content, 
    request_id,
    document_type=doc_type
)
```

---

## 🎯 总结

这个思维导图生成器展现了现代AI应用开发的最佳实践：

1. **模块化架构**: 清晰的组件分离和职责划分
2. **智能处理**: 基于LLM的文档理解和内容提取
3. **性能优化**: 并发处理、智能缓存和内存管理
4. **错误恢复**: 多层次的错误处理和优雅降级
5. **成本控制**: 精确的使用追踪和资源管理
6. **多格式输出**: 支持HTML、Markdown和Mermaid格式
7. **可扩展性**: 支持多种API提供商和文档类型

该项目不仅是一个功能完整的思维导图生成工具，更是学习现代Python异步编程、AI集成和系统设计的优秀案例。 